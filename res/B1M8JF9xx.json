{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488574294430, "tcdate": 1478295373462, "number": 468, "id": "B1M8JF9xx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1M8JF9xx", "signatures": ["~Yuhuai_Wu1"], "readers": ["everyone"], "content": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 18, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396601058, "tcdate": 1486396601058, "number": 1, "id": "ryWpnG8ul", "invitation": "ICLR.cc/2017/conference/-/paper468/acceptance", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper describes a method to estimate likelihood scores for a range of models defined by a decoder.\n \n This work has some issues. The paper mainly applies existing ideas. As discussed on openreview, the isotropic Gaussian noise model used to create a model with a likelihood is questionable, and it's unclear how useful likelihoods are when models are obviously wrong. However, the results, lead to some interesting conclusions, and on balance this is a good paper.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396601611, "id": "ICLR.cc/2017/conference/-/paper468/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396601611}}}, {"tddate": null, "tmdate": 1486336198373, "tcdate": 1486336198373, "number": 12, "id": "HJATeEr_g", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "Sy-o31H_l", "signatures": ["~Roger_Baker_Grosse1"], "readers": ["everyone"], "writers": ["~Roger_Baker_Grosse1"], "content": {"title": "HMC and noise hyperparameters", "comment": "Thank you for your careful and insightful comment. \n\nYour suggestion about learning to model both the mean and covariance is interesting. We actually considered such an approach, but we decided it was unlikely to give reasonable noise estimates for purposes of log-likelihood estimation. Suppose, first of all, that you\u2019re fitting the variance parameter for a spherical Gaussian. If the parameter is nonzero, the discriminator will easily be able to distinguish real and generated samples by checking whether there\u2019s any noise on the boundaries of the image. There would therefore be overwhelming pressure on the generator to choose a parameter indistinguishable from zero, which defeats the purpose of modeling noise.\n\nWe think a similar effect would happen even for a diagonal noise model. I.e., if there is significant i.i.d. noise, the discriminator will be able to pick up on this (e.g. by noticing high-frequency content). Therefore, the generator would prefer very small amounts of noise, which probably would not be very good from a log-likelihood standpoint. So even if we used a procedure like this, we\u2019d still need to go through an additional step of estimating one or more noise parameters using a validation set.\n\nIn short, because we\u2019re comparing models in terms of log-likelihoods, the only reasonable way to choose variance parameters would be using some sort of likelihood-based criterion. But combining a likelihood criterion with the standard GAN objective would essentially require a novel algorithm, so we opted to simply estimate a scalar noise parameter on the held-out set. \n\nAs far as HMC adaptation, tuning HMC hyperparameters is a hard problem and some form of adaptation is certainly required. In principle, one could apply a diminishing adaptation scheme at each AIS step, but this would be prohibitively expensive. We opted to do adaptation online, similar to prior work, which (as you point out) raises issues of potential bias in the MCMC stationary distribution. If the temperature is adapted slowly enough, the distribution will be approximately stationary on the timescale of HMC exploring the space, and of hyperparameter adaptation; therefore, the bias would be no more or less a problem in our setting compared with more typical applications of HMC. For smaller numbers of intermediate distributions, there might be a lag in the hyperparameter adaptation, leading to worse log-likelihood estimates than if we were given the optimal hyperparameters at each step. But this would manifest in a large gap between the bounds, and the user would then need to increase the number of temperatures.\n\nIt\u2019s an interesting question how to adapt HMC hyperparameters in the context of AIS. I suppose one principled solution would be to have a first pass where you run HMC at various temperatures to tune the hyperparameters, and then a second pass where you run AIS without adaptation, and linearly interpolate the hyperparameters chosen in the first pass. This would yield strong mathematical guarantees at the expense of additional computation.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1486318745282, "tcdate": 1486318745282, "number": 11, "id": "Sy-o31H_l", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["~Matt_Graham1"], "readers": ["everyone"], "writers": ["~Matt_Graham1"], "content": {"title": "Adaptive HMC implementation and Gaussian decoder model", "comment": "Thanks for the interesting paper.\n\nI have a couple of queries / comments:\n\nIn the code you provide at https://github.com/tonywu95/eval_gen you use a HMC implementation which adapts the step-size to achieve a target acceptance rate. As you reference in the paper this is taken from the implementation in the Theano tutorial at http://deeplearning.net/tutorial/hmc.html which in turn I think is based on Marc\u2019Aurelio Ranzato's implementation in the code available at http://www.cs.toronto.edu/~ranzato/publications/factored3wayRBM/code/factored3wayBM_04May2010.zip.\n\nAs far as I can tell the implementation does not use an explicitly vanishing / diminishing adaptation scheme? The step size appears to be multiplicatively adapted based on the difference between an exponential average of the acceptance rate for each chain and a target accept rate. This adaptation seems to be applied for every transition step in the AIS forward / reverse passes. Due to the induced dependence of the transitions at each inverse temperature on past states via the adapting step size, it is not clear to me that this will define a valid set of Markov transitions as required (I think?) for validity of the AIS scheme. I very possibly may be missing something here which means this isn't the case but if so it might be worth clarifying why the adaptation scheme is valid here.\n\nPart of my worry is that ensuring a convergent adaptive scheme seems particularly difficult here as while for a stationary target distribution we might expect for the adaptive step-size scheme employed to (roughly) converge if the local geometry of the distribution does not vary too much across the typical set, as the target distribution for each transition differs here, it seems plausible the step size will continue varying significantly across many transitions.\n\nWith regards to the Gaussian observation model used: instead of using a fixed isotropic covariance Gaussian observation model for the GAN and GMMN decoders with variance parameter fit on a validation set, a possible alternative could be to explicitly include an additive Gaussian noise term in the output of the decoder for the GAN / GMMN and use this 'noisy' decoder in the usual training procedure for these models. Though inclusion of an output noise term is atypical in these models there is no reason why it is not possible do so as the overall decoder can still be expressed in the form of a deterministic function of a fixed length vector drawn from a fixed prior density e.g. Gaussian. \n\nMore concretely: if `m` and `s` are two parametric functions (with possibly shared parameters) which specify the mean and standard deviation of a conditional Gaussian density as in a typical VAE decoder,`z_1` is a vector of standard Gaussian variates representing the typical latent representation used in a GAN / GMMN, and `z_2` is a second vector of 'noise' standard Gaussian variates of length equal to the output dimension of `m`, then a decoder defined by\n\n    x = m(z_1) + s(z_1) * z_2\n\nwill naturally define a density with full support on the output space (assuming a strictly positive output to `s`) , have a known joint density on `(x, z_1)` that can be used for marginal likelihood estimation within your framework and still has the form `x = G(z)` (with `z = [z_1; z_2]`) and so the parameters of `m` and `s` can still be trained in a standard GAN / GMMN framework.\n\nAlthough as noted this choice of a decoder model is atypical for a GAN and GMMN, it has the advantage over using a fixed isotropic Gaussian noise model of meaning the generative model trained is the actual one used for marginal likelihood estimation and in allowing models with a non-isotropic covariance in the conditional density on the output given latent representation to be used.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1483242482343, "tcdate": 1483242482343, "number": 10, "id": "Sk5enxUHl", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["~Yuhuai_Wu1"], "readers": ["everyone"], "writers": ["~Yuhuai_Wu1"], "content": {"title": "A new version of the paper is updated", "comment": "A new version of the paper is updated. We revised the section 2.2 on the introduction to AIS, and several other places according to reviewers' comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1483241976806, "tcdate": 1483241976806, "number": 9, "id": "rkbZ5g8Hl", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "ByuUr2Amx", "signatures": ["~Yuhuai_Wu1"], "readers": ["everyone"], "writers": ["~Yuhuai_Wu1"], "content": {"title": "Rebuttal to AnonReviewer4", "comment": "Thank the reviewer for providing helpful comments.\n\nRelative contribution small:\nIt\u2019s encouraging that our writing is clear enough that the ideas seem to flow naturally. But the approach is obvious only in retrospect. There has been a large amount of interest in the questions surrounding likelihoods of decoder-based models, including whether the models are missing modes or memorizing training examples. Our submission is the first to quantitatively analyze these issues using accurate log-likelihood estimates. Anecdotally, in our conversations with other researchers who cared about these questions and were familiar with the basic techniques, nobody suggested using a similar approach.\n\nHere are some of our novel contributions: (1) formulating KDE as simple importance sampling, which shows that it gives a log-likelihood stochastic lower bound for a particular generative model (with Gaussian observations); this let us formulate the problem of estimating the log-likelihoods more accurately under this generative model; (2) using BDMC to validate the log-likelihood estimates in decoder-based models (a very different setting from where it was previously applied); (3) using the q network as the initial distribution, which makes the AIS estimator very efficient for VAEs, (4) using posterior samples as a way to diagnose missing modes. \n\nMuch progress in science comes about because someone designs and validates a new genetic technique and then uses it to measure phenomena that were previously inaccessible. This is exactly what we wanted to achieve in this work: we engineered and validated a much improved log-likelihood estimator (orders of magnitude more accurate than the previous approach!), and then used it to answer important questions about generative models which we previously didn\u2019t have a way to answer.\n\n\nSection 2.3: Thanks reviewer for pointing out the typo with missing \u201clog\u201d. We have corrected it. Note that the expectation is taken over the stochasticity of producing the estimate log p\u2019(x). Mathematically, the inequality holds for every x (not just on average w.r.t. x).\n\nq(z|x): The approximate posterior, i.e., the recognition network is used as the initialization distribution for AIS chains. The x refers to the given data to be evaluated. \n\noverfitting:\nWhat we mean by \u201coverfitting\u201d follows the standard usage in the field. A model overfits if it models idiosyncrasies in the training set, thereby achieving a training likelihood much higher than its generalization (test) likelihood. A model underfits if it fails to model the structure in the training set. Despite the names, the two are not mutually exclusive, i.e. a given model can both overfit and underfit the data. (For instance, if a network somehow memorized 50% of the training examples and ignored the other 50%, it would both overfit and underfit, as reflected in poor likelihood on the training set coupled with a large gap between training and test.)\n\nYou are correct that it\u2019s not generally meaningful to compare the \u201camount of overfitting\u201d of two different models -- overfitting isn\u2019t a quantity we can measure. However, in our experiments, the size of the effect was large: the VAEs overfit substantially, whereas we saw no evidence of overfitting in the case of the GANs or GMMNs.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1483241896604, "tcdate": 1483241493920, "number": 7, "id": "S1CGdl8Bg", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "rksbBAGEg", "signatures": ["~Yuhuai_Wu1"], "readers": ["everyone"], "writers": ["~Yuhuai_Wu1"], "content": {"title": "Rebuttal to AnonReviewer3", "comment": "Thank the reviewer for providing helpful comments.\n\nDifferent number of examples:\nWe ran our experiments on academic scale infrastructure, so we didn\u2019t have the resources to run the maximum number of examples in all conditions. We will release an updated version where all experiments use the maximum number of test examples. However, we reported confidence intervals for the results, and even with only 1000 examples for the training sets, the differences between models were still highly statistically significant.\n\nAIS+encoder:\nAIS works better when the initial distribution is a better match to the target (posterior) distribution. In those cases where an encoder net is available, it ought to provide a better initial distribution than the prior (which is what we used in the rest of the experiments). Therefore, AIS+encoder can reach comparable accuracy in a much smaller number of steps.\n\n# of AIS chains:\nWe have tested different number of independent chains. In our experiments we find that when the number of intermediate distribution goes beyond 1000, the evaluation does not vary much for different choices of number of chains. 16 chains are enough for evaluation.\n\nLarge gaps for GAN-50:\nWe speculate that the posterior distribution might be more complicated in the case of GAN-50. However, the difference between GAN-50 and the other conditions is not necessarily surprising or interesting; there\u2019s no reason that different models ought to have posterior distributions which are equally difficult to sample from. We suspect the fact that the other five models have very similar BDMC gaps is just a coincidence.\n\n\nMinor comments:\nWe have edited table 1 and the reference in the paper\nThey are average value.\nAdding another point for AIS will either make the plot too large in y-axis or x-axis.\nYes the BDMC gap refer to the values in Table 2.\nThanks for pointing out this typo. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1483241782463, "tcdate": 1483241782463, "number": 8, "id": "Sk0EFlUSx", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "B1aIVjbVg", "signatures": ["~Yuhuai_Wu1"], "readers": ["everyone"], "writers": ["~Yuhuai_Wu1"], "content": {"title": "Rebuttal to AnonReviewer2", "comment": "We thank the reviewer for recognizing its educational value.\n\u201cHow AIS works?\u201d -- We have edited the paper to include a short discussion on this.\n\u201cFigure 2\u201d-- Thanks for pointing out the typo. We changed the caption as the reviewer suggests.\n\u201cUsing reconstruction\u201d -- We agree with the reviewer. Visualizing reconstruction is a way to show one particular property of the model: whether it misses any important modes of the data distribution. And we also agree that this is not a sufficient condition to be a good generative model.  \n\u201cDensity networks\u201d -- We have added the citation.\n\u201cAIS/posterior\u201d-- We edited the paper to show the relevance.\n\u201cMeaningful\u201d-- We changed the phrasing."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1483055422527, "tcdate": 1483055422527, "number": 6, "id": "HkUrWQ7Hx", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "SkdYDMQrl", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Comments", "comment": "Thanks for the response, it's a bit clarifying.\n\nAs you mention, the Gaussian observation model has been used extensively in the past, that is true. That doesn't mean that it should still be under use. KDE was extensively used in the past as well and now we can see extremely good reasons for not using it altogether. While the choice of each observation model is important, a topic that as you mention discussed at a certain length, both in the paper and in the comments, the fact that you are using an observation model at all has to be justified more than just by saying it's a way to have likelihood well defined. The GANs we use in practice are quite different from the ones you use in your paper by adding that noise. The learned std judging by figure 2, (a) is about 0.1, which is a lot for images normalized to [0, 1]. If you were to compare the Wasserstein distance to the true distribution in a simple experiment you would see a drastic decrease in performance when adding the noise. If you have a distribution P and you create P' by adding noise in the end, what properties does the likelihood of P' tell you about P? This is a question that I think has been swept under the carpet by phrasing P and P' as if they were almost the same thing, while in reality they are very different.\n\nAs a last comment, I didn't mean to ask a full background tutorial or introduction on AIS, but I do think it's important to motivate the method properly since it takes a central role in your paper. Obviously showing the proofs is too extreme, and it is likely my aim to to implement it from scratch from your paper was overly ambitious. However, for those not too familiar with AIS, we end up with little intuition about how or why it works."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1483052928395, "tcdate": 1483052928395, "number": 5, "id": "SkdYDMQrl", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "HkYFmYfrx", "signatures": ["~Roger_Baker_Grosse1"], "readers": ["everyone"], "writers": ["~Roger_Baker_Grosse1"], "content": {"title": "Gaussian observations and reproducibility", "comment": "Thank you for your careful feedback.\n\nobservation model:\nYou\u2019re right that it\u2019s awkward to define a good observation model for GANs and GMMNs which makes the likelihood well-defined. We talk about this issue in Sections 3 and 5.2, and at greater length in Appendix B. We used the Gaussian observation model in order to be consistent with previous work which reported log-likelihood values for GANs and GMMNs. As discussed in Appendix B, Gaussianity is a problematic assumption, and one can improve the likelihood values substantially using a heavier-tailed noise model. However, this effect is much smaller than the log-likelihood differences between GANs and VAEs, so it can\u2019t explain the gap. One could probably do a similar analysis which fits non-isotropic covariance structure, but we couldn\u2019t find a straightforward way to do this.\n\nWe were careful to compare all three models on an equal playing field. I.e., even though it\u2019s straightforward to train VAEs with a more general (e.g. diagonal or full) covariance model, we trained them with the same isotropic Gaussian model as the GANs and GMMNs. After training, their variance parameter was fit using the validation set, just like for the other models.\n\nRegardless of the appropriateness of the Gaussian noise model, our experiments uncovered real phenomena. E.g., analyzing the likelihoods led us to discover the GANs\u2019 and GMMNs\u2019 mode dropping behavior, which is not an artifact of the observation model.\n\nreproducibility:\nThe reader who wants to use our method can run our code: https://github.com/tonywu95/eval_gen\n\nThe reader who wants to reproduce the method from scratch will need to do a bit more homework. AIS and HMC are well-known algorithms in our field; while we gave a mathematically precise presentation of our method, unfortunately there isn\u2019t space in the background section to give a tutorial introduction to AIS. For proofs of the basic properties of AIS, such as the convergence guarantees or the unbiasedness of the estimates, I\u2019d refer you to Neal\u2019s original AIS paper (which fortunately is very readable).\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1483015040962, "tcdate": 1483015040962, "number": 4, "id": "HkYFmYfrx", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["~Martin_Arjovsky1"], "readers": ["everyone"], "writers": ["~Martin_Arjovsky1"], "content": {"title": "Comments", "comment": "I have mixed opinions about this work.\n\nOn the one hand I think it does a really nice job at providing a simple method to evaluate mode dropping and overfitting in generative models, which is one of the most important problems in machine learning now. I can see myself and many more people using these ideas to evaluate aspects of generative models\n\nOn the other hand, there is a constant use of the term \"log-likelihood for GANs\", which is extremely misleading. The log-likelihood of GANs is obviously ill-defined, and the assumption of \"Gaussian observation\" is incredibly false, since the true after-sampling variance that GANs are trained on is 0. You should relate how the model with noise added relates to properties on the actual distribution induced by the generator with no gaussian noise, instead of assuming the generator has Gaussian noise in the end, which is obviously false for a GAN and true for a VAE (which is trained this way since the reconstruction error assumes this kind of noise). I'm not asking for a precise mathematical statement of this relation, but I do think the writing sweeps this issue under the carpet, and the addition of isotropic noise on a distribution with an incredibly complex covariance matrix is more than questionable.\n\nAs a final note, I think section 2.2 which describes the central algorithm is very poorly written. After reading the section several times I have no idea how to implement it or how the sampling process is defined in the end. In my opinion, you should try to describe better:\n\n- Which is the sampling distribution of the estimate of p(x) at time t. I believe this is p(x) = E_{z ~ p_t(z)} [ p(x | z) p(z) / p_t(z) ] but I'm far from sure.\n- Explain why the successive estimator's are unbiased (if it's in the above form this is trivial)\n- Explain why the estimators have less variance as t -> T.\n- Explain what the theoretical guarantees of AIS are.\n\nMinor:\n- Typo in line 5 of page 4, \"produces\" -> \"produce\"\n- I appreciate the wall clock times in figure 2.\n\nOverall I think this is a cool paper, with some great consequences, but I think some issues (especially writing) should be polished."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1481987374504, "tcdate": 1481987331299, "number": 3, "id": "rksbBAGEg", "invitation": "ICLR.cc/2017/conference/-/paper468/official/review", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["ICLR.cc/2017/conference/paper468/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper468/AnonReviewer3"], "content": {"title": "Interesting work, their evaluation framework is available online, good contribution to the community ", "rating": "7: Good paper, accept", "review": "# Review\nThis paper proposes a quantitative evaluation for decoder-based generative models that use Annealed Importance Sampling (AIS) to estimate log-likelihoods. Quantitative evaluations are indeed much needed since for some models, like Generative Adversarial Networks (GANs) and Generative Moment Matching Networks (GMMNs), qualitative evaluation of samples is still frequently used to assess their generative capability. Even though, there exist quantitative evaluations like Kernel Density Estimation (KDE), the authors show how AIS is more accurate than KDE and how it can be used to perform fine-grained comparison between generative models (GAN, GMMs and Variational Autoencoders (VAE)).\n\nThe authors report empirical results comparing two different decoder architectures that were both trained, on the continuous MNIST dataset, using the VAE, GAN and GMMN objectives. They also trained an Importance Weighted Autoencoder (IWAE) on binarized MNIST and show that, in this case, the IWAE bound underestimates the true log-likelihoods by at least 1 nat (which is significant for this dataset) according to the AIS evaluation of the same model.\n\n\n# Pros\nTheir evaluation framework is public and is definitely a nice contribution to the community.\n\nThis paper gives some insights about how GAN behaves from log-likelihood perspective. The authors disconfirm the commonly proposed hypothesis that GAN are memorizing training data. The authors also observed that GANs miss important modes of the data distribution.\n\n\n# Cons/Questions\nIt is not clear for me why sometimes the experiments were done using different number of examples (100, 1000, 10000) coming from different sources (trainset, validset, testset or simulation/generated by the model). For instance, in Table 2 why results were not reported using all 10,000 examples of the testing set?\n\nIt is not clear why in Figure 2c, AIS is slower than AIS+encoder? Is the number of intermediate distributions the same in both?\n\n16 independent chains for AIS seems a bit low from what I saw in the literature (e.g. in [Salakhutdinov & Murray, 2008] or [Desjardins etal., 2011], they used 100 chains). Could it be that increasing the number of chains helps tighten the confidence interval reported in Table 2?\n\nI would have like the authors to give their intuitions as to why GAN50 has a BDMC gap of 10 nats, i.e. 1 order of magnitude compared to the others?\n\n\n# Minor comments\nTable 1 is not referenced in the text and lacks description of what the different columns represent.\nFigure 2(a), are the reported values represents the average log-likelihood of 100 (each or total?) training and validation examples of MNIST (as described in Section 5.3.2).\nFigure 2(c), I'm guessing it is on binarized MNIST? Also, why are there fewer points for AIS compared to IWAE and AIS+encoder?\nAre the BDMC gaps mentioned in Section 5.3.1 the same as the ones reported in Table2 ?\nTypo in caption of Figure 3: \"(c) GMMN-10\" but actually showing GMMN-50 according to the graph title and subcaption.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512576005, "id": "ICLR.cc/2017/conference/-/paper468/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper468/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper468/AnonReviewer4", "ICLR.cc/2017/conference/paper468/AnonReviewer2", "ICLR.cc/2017/conference/paper468/AnonReviewer3"], "reply": {"forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512576005}}}, {"tddate": null, "tmdate": 1481909333190, "tcdate": 1481909333190, "number": 2, "id": "B1aIVjbVg", "invitation": "ICLR.cc/2017/conference/-/paper468/official/review", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["ICLR.cc/2017/conference/paper468/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper468/AnonReviewer2"], "content": {"title": "Great empirical work", "rating": "7: Good paper, accept", "review": "Summary:\nThis paper describes how to estimate log-likelihoods of currently popular decoder-based generative models using annealed importance sampling (AIS) and HMC. It validates the method using bidirectional Monte Carlo on the example of MNIST, and compares the performance of GANs and VAEs.\n\n\nReview:\nAlthough this seems like a fairly straight-forward application of AIS to me (correct me if I missed an important trick to make this work), I very much appreciate the educational value and empirical contributions of this paper. It should lead to clarity in debates around the density estimation performance of GANs, and should enable more people to use AIS.\n\nSpace permitting, it might be a good idea to try to expand the description of AIS. All the components of AIS are mentioned and a basic description of the algorithm is given, but the paper doesn\u2019t explain well \u201cwhy\u201d the algorithm does what it does/why it works.\n\nI was initially confused by the widely different numbers in Figure 2. On first glance my expectation was that this Figure is comparing GAN, GMMN and IWAE (because of the labeling at the bottom and because of the leading words in the caption\u2019s descriptions). Perhaps mention in the caption that (a) and (b) use continuous MNIST and (c) uses discrete MNIST. \u201cGMMN-50\u201d should probably be \u201cGMMN-10\u201d.\n\n\nUsing reconstructions for evaluation of models may be a necessary but is not sufficient condition for a good model. Depending on the likelihood, a posterior sample might have very low density under the prior, for example. It would be great if the authors could point out and discuss the limitations of this test a bit more.\n\n\nMinor:\n\nPerhaps add a reference to MacKay\u2019s density networks (MacKay, 1995) for decoder-based generative models.\n\nIn Section 2.2, the authors write \u201cthe prior over z can be drastically different than the true posterior p(z|x), especially in high dimension\u201d. I think the flow of the paper could be improved here, especially for people less familiar with importance sampling/AIS. I don\u2019t think the relevance of the posterior for importance sampling is clear at this point in the paper.\n\nIn Section 2.3 the authors claim that is often more \u201cmeaningful\u201d to estimate p(x) in log-space because of underflow problems. \u201cMeaningful\u201d seems like the wrong word here. Perhaps revise to say that it\u2019s more practical to estimate log p(x) because of underflow problems, or to say that it\u2019s more meaningful to estimate log p(x) because of its connection to compression/surprise/entropy.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512576005, "id": "ICLR.cc/2017/conference/-/paper468/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper468/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper468/AnonReviewer4", "ICLR.cc/2017/conference/paper468/AnonReviewer2", "ICLR.cc/2017/conference/paper468/AnonReviewer3"], "reply": {"forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512576005}}}, {"tddate": null, "tmdate": 1481717071640, "tcdate": 1481717071634, "number": 1, "id": "ByuUr2Amx", "invitation": "ICLR.cc/2017/conference/-/paper468/official/review", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["ICLR.cc/2017/conference/paper468/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper468/AnonReviewer4"], "content": {"title": "Application of BDMCMC to generative models. Relative contribution small. ", "rating": "6: Marginally above acceptance threshold", "review": "The paper describes a method to evaluate generative models such as VAE, GAN and GMMN. This is very much needed in our community where we still eyeball generated images to judge the quality of a model. However, the technical increment over the NIPS 16 paper: \u201cMeasuring the reliability of MCMC inference with bidirectional Monte Carlo\u201d is very small, or nonexistent (but please correct me if I am wrong!).  (Grosse et al). The relative contribution of this paper is the application of this method to generative models. \nIn section 2.3 the authors seem to make a mistake. They write E[p\u2019(x)] <= p(x) but I think they mean: E[log p\u2019(x)] <= log E[p\u2019(x)] = log p(x). Also,  for what value of x? If p(x) is normalized it can\u2019t be true for all values of x. Anyways, I think there are typos here and there and the equations could be more precise.\nOn page 5 top of the page it is said that the AIS procedure can be initialized with q(z|x) instead of p(z). However, it is unclear what value of x is then picked? Is it perhaps Ep(x)[q(z|x)] ?\nI am confused with the use of the term overfitting (p8 bottom). Does a model A overfit relative to a another model B if the test accuracy of A is higher than that of B even though the gap between train and test accuracy is also higher for B than for A. I think not. Perhaps the last sentence on page 8 should say that VAE-50 underfits less than GMMN-50?\nThe experimental results are interesting in that it exposes the fact that GANs and GMMNs seem to have much lover test accuracy than VAE despite the fact that their samples look great. \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512576005, "id": "ICLR.cc/2017/conference/-/paper468/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper468/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper468/AnonReviewer4", "ICLR.cc/2017/conference/paper468/AnonReviewer2", "ICLR.cc/2017/conference/paper468/AnonReviewer3"], "reply": {"forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512576005}}}, {"tddate": null, "tmdate": 1480912381385, "tcdate": 1480912381379, "number": 3, "id": "HkSZCwf7g", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "log-likelihood", "comment": "Very interesting paper to read. But I am a little confused that marginal log-likelihood (log p_\\theta(x)) shown in this paper is usually more than 200, while the log p(x) shown in variational auto-encoder on mnist  is usually around -100(Both on 50 dimensions of VAE).  Does your paper show the same log p_\\theta(x) as that in VAE?\n\nAppreciate if you could solve my puzzle. Thanks.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1480806445588, "tcdate": 1480806139565, "number": 1, "id": "Hy4bkRl7x", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "ByCFNOy7x", "signatures": ["~Yuhuai_Wu1"], "readers": ["everyone"], "writers": ["~Yuhuai_Wu1"], "content": {"title": "reply to AnonReviewer3", "comment": "1) Yes, f_t(z) is the unnormalized intermediate distributions. The initial distribution has f_1(z) = p_1(z) = p(z), with normalizing constant 1.\n2) Have not tried evaluating the inception score. Note that the inception score is obtained by input the images into \"Inception\" model, which is usually trained on color-images (imagenet). I will try training a MNIST classifier to obtain a \"MNIST\" score if that's needed.\n3) Yes. We report the best numbers for IWAE and KDE evaluation using GPUs and CPUs.\n4) LLD stands for log-likelihood,  see the label on y-axis. Abbreviated because of space limit. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1480806256592, "tcdate": 1480806243715, "number": 2, "id": "B1nvkCx7g", "invitation": "ICLR.cc/2017/conference/-/paper468/public/comment", "forum": "B1M8JF9xx", "replyto": "SJoeWMyQl", "signatures": ["~Yuhuai_Wu1"], "readers": ["everyone"], "writers": ["~Yuhuai_Wu1"], "content": {"title": "reply to AnonReviewer2", "comment": "It refers to deterministic reconstruction, which can also be interpreted as the mean of the distribution $p_\\sigma(x | z)$."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287564451, "id": "ICLR.cc/2017/conference/-/paper468/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1M8JF9xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper468/reviewers", "ICLR.cc/2017/conference/paper468/areachairs"], "cdate": 1485287564451}}}, {"tddate": null, "tmdate": 1480717445861, "tcdate": 1480717445857, "number": 2, "id": "ByCFNOy7x", "invitation": "ICLR.cc/2017/conference/-/paper468/pre-review/question", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["ICLR.cc/2017/conference/paper468/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper468/AnonReviewer3"], "content": {"title": "Geometric average; GPU implementation of WAE and KDE", "question": "Regarding equations leading to Equation 5, does f_1(z) represents the unnormalized part of p_1(z)=p(z) (which is the prior p(z) in this case according to the text at the top of page 4)? I think f_1(z) = p(z) (or maybe Z_1=1 ?) is needed to obtain Equation 5.\n\nFor the Figure 1, have tried evaluating the images according to the image-quality measure (Salimans et al.)? If so, are the values similar between the three models?\n\nWhen you report computation time, are the implementations of IWAE and KDE use GPU?\n\nAlso, in Figures 1, 2 and 3, what is LLD stands for?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959265297, "id": "ICLR.cc/2017/conference/-/paper468/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper468/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper468/AnonReviewer2", "ICLR.cc/2017/conference/paper468/AnonReviewer3"], "reply": {"forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959265297}}}, {"tddate": null, "tmdate": 1480691955535, "tcdate": 1480691955531, "number": 1, "id": "SJoeWMyQl", "invitation": "ICLR.cc/2017/conference/-/paper468/pre-review/question", "forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "signatures": ["ICLR.cc/2017/conference/paper468/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper468/AnonReviewer2"], "content": {"title": "\"Running the decoder\"", "question": "Does \"running the decoder on z\" (page 9) refer to sampling from $p_\\sigma(x | z)$  or deterministic reconstruction?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Quantitative Analysis of Decoder-Based Generative Models", "abstract": "The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of some popular models such as generative adversarial networks and generative moment matching networks, is a decoder network, a parametric deep neural net that defines a generative distribution. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.", "pdf": "/pdf/e22d0adc0c056dd2641fdd179725b94402def68a.pdf", "TL;DR": "We propose to use Annealed Importance Sampling to evaluate decoder-based generative network, and investigate various properties of these models.", "paperhash": "wu|on_the_quantitative_analysis_of_decoderbased_generative_models", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["cs.toronto.edu", "cs.cmu.edu", "umontreal.ca", "ttic.edu", "openai.com"], "authors": ["Yuhuai Wu", "Yuri Burda", "Ruslan Salakhutdinov", "Roger Grosse"], "authorids": ["ywu@cs.toronto.edu", "yburda@openai.com", "rsalakhu@cs.cmu.edu", "rgrosse@cs.toronto.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959265297, "id": "ICLR.cc/2017/conference/-/paper468/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper468/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper468/AnonReviewer2", "ICLR.cc/2017/conference/paper468/AnonReviewer3"], "reply": {"forum": "B1M8JF9xx", "replyto": "B1M8JF9xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper468/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959265297}}}], "count": 19}