{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730166345, "tcdate": 1509134133489, "number": 752, "cdate": 1518730166333, "id": "r16Vyf-0-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "r16Vyf-0-", "original": "Hy2VkfZ0Z", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260085015, "tcdate": 1517249883305, "number": 582, "cdate": 1517249883290, "id": "BJmDBJpBf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "r16Vyf-0-", "replyto": "r16Vyf-0-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper had some quality and clarity issues and the lack of motivation for the approach was pointed out by multiple reviewers.  Just too far away from the acceptance threshold."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515787459657, "tcdate": 1511725141863, "number": 1, "cdate": 1511725141863, "id": "SyA8u9OlG", "invitation": "ICLR.cc/2018/Conference/-/Paper752/Official_Review", "forum": "r16Vyf-0-", "replyto": "r16Vyf-0-", "signatures": ["ICLR.cc/2018/Conference/Paper752/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting idea, weak evaluation", "rating": "6: Marginally above acceptance threshold", "review": "Summary\n\nThis paper extends self-attention layers (Vaswani et al., 2017) from sequences to images and proposes to use the layers as part of PixelCNNs (van den Oord et al., 2016). The proposed model is evaluated in terms of visual appearance of samples and log-likelihoods. The authors find a small improvement in terms of log-likelihood over PixelCNNs and that super-resolved CelebA images are able to fool human observers significantly more often than PixelRNN based super-resolution (Dahl et al., 2017).\n\nReview\n\nAutoregressive models are of large interest to the ICLR community and exploring new architectures is a valuable contribution. Using self-attention in autoregressive models is an intriguing idea. It is a little bit disappointing that the added model complexity only yields a small improvement compared to the more straight-forward modifications of the PixelCNN++. I think the paper would benefit from a little bit more work, but I am open to adjusting my score based on feedback.\n\nI find it somewhat surprising that the proposed model is only slightly better in terms of log-likelihood than a PixelRNN, but much better in terms of human evaluation \u2013\u00a0given that both models were optimized for log-likelihood. Was the setup used with Mechanical Turk exactly the same as the one used by Dahl et al.? These types of human evaluations can be extremely sensitive to changes in the setup, even the phrasing of the task can influence results. E.g., presenting images scaled differently can mask certain artifacts. In addition, the variance between subjects can be very high. Ideally, each method included in the comparison would be re-evaluated using the same set of observers. Please include error bars.\n\nThe CelebA super-resolution task furthermore seems fairly limited. Given the extreme downsampling of the input, the task becomes similar to simply generating any realistic image. A useful baseline would be the following method: Store the entire training set. For a given query image, look for the nearest neighbor in the downsampled space, then return the corresponding high-resolution image. This trivial method might not only perform well, it also highlights a flaw in the evaluation: Any method which returns stored high-resolution images \u2013\u00a0even if they don\u2019t match the input \u2013\u00a0would perform at 50%. To fix this, the human observers should also receive the low-resolution image and be asked to identify the correct corresponding high-resolution image.\n\nUsing multiplicative operations to model images seems important. How does the self-attention mechanism relate to \u201cgated\u201d convolutions used in PixelCNNs? Could gated convolutions not also be considered a form of self-attention?\n\nThe presentation/text could use some work. Much of the text assumes that the reader is familiar with Vaswani et al. (2017) but could easily be made more self-contained by directly including the definitions used. E.g., the encoding of positions using sines and cosines or the multi-head attention model. I also felt too much of the architecture is described in prose and could be more efficiently and precisely conveyed in equations.\n\nOn page 7 the authors write \u201cwe believe our cherry-picked images for various classes to be of higher perceptual quality\u201d. This is a meaningless result, not only because the images were cherry-picked. Generating realistic images is trivial - you just need to store the training images. Analyzing samples generated by a generative model (outside the context of an application) should therefore only be used for diagnostic purposes or to build intuitions but not to judge the quality of a model.\n\nPlease consider rephrasing the last sentence of the abstract. Generating images which \u201clook pretty cool\u201d should not be the goal of a serious machine learning paper or a respected machine learning conference.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642503144, "id": "ICLR.cc/2018/Conference/-/Paper752/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper752/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper752/AnonReviewer1", "ICLR.cc/2018/Conference/Paper752/AnonReviewer3", "ICLR.cc/2018/Conference/Paper752/AnonReviewer2"], "reply": {"forum": "r16Vyf-0-", "replyto": "r16Vyf-0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642503144}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642503195, "tcdate": 1511936609027, "number": 2, "cdate": 1511936609027, "id": "BkYvzAixG", "invitation": "ICLR.cc/2018/Conference/-/Paper752/Official_Review", "forum": "r16Vyf-0-", "replyto": "r16Vyf-0-", "signatures": ["ICLR.cc/2018/Conference/Paper752/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Requires improvement", "rating": "3: Clear rejection", "review": "This paper extends the PixelCNN/RNN based (conditional) image generation approaches with self-attention mechanism. \n\nPros:\n- qualitatively the proposed method has good results in several tasks\n\nCons:\n- writing needs to be improved\n- lack of motivation\n- not easy to follow technique details\n\n\nThe motivation part is missing. It seems to me that the paper simply try to combine the Transformer with PixelCNN/RNN based image generation without a clear explanation why this is needed. Why self-attention is so important for image generation? Why not just a deeper network with more parameters? Throughout the paper I cannot find a clear answer for this. Based on this I couldn't see a clear contribution. \n\nThe paper is difficult to keep the track given the current flow. Each subsection of section 3 starts with technique details without explaining why we do this. Some sentences like \"look pretty cool\" is not academic. \n\nThe experiments lack comparisons except the human evaluation, while the log-likelihood improvement is marginal. I am wondering how the human evaluation is conducted. Does it compare all the competing algorithms against the same sub-samples of the GT data? How many pairs have been compared for each algorithm?  Apart from this metric, I would like to see qualitative comparison between competing algorithms in the paper as well. Also other approaches e.g. SRGAN could be compared. \n\nI am also interested about the author's claim that the implementation error that influences the log-likelihood. Has this been fixed after the deadline?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642503144, "id": "ICLR.cc/2018/Conference/-/Paper752/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper752/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper752/AnonReviewer1", "ICLR.cc/2018/Conference/Paper752/AnonReviewer3", "ICLR.cc/2018/Conference/Paper752/AnonReviewer2"], "reply": {"forum": "r16Vyf-0-", "replyto": "r16Vyf-0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642503144}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642503159, "tcdate": 1512524158342, "number": 3, "cdate": 1512524158342, "id": "SJIFtpEbM", "invitation": "ICLR.cc/2018/Conference/-/Paper752/Official_Review", "forum": "r16Vyf-0-", "replyto": "r16Vyf-0-", "signatures": ["ICLR.cc/2018/Conference/Paper752/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Missing motivation and mathematical details", "rating": "5: Marginally below acceptance threshold", "review": "In this paper the authors propose an autoregressive image generation model that incorporates a self-attention mechanism. The latter is inspired by the work of [Vaswani et al., 2016], which was proposed for sequences and is extended to 2D images in this work. The authors apply their model to super-resolution of face images, as well as image completion (aka inpainting) and generation, both unconditioned or conditioned on one of a small number of image classes from the CIFAR10 and ImageNet datasets. The authors evaluate their method in terms of visual quality of their generated images via an Amazon Mechanical Turk survey and quantitatively by reporting slightly improved log-likelihoods. \n\nWhile the paper is well written, the motivation for combining self-attention and autoregressive models remains unclear unfortunately, even more though as the reported quantitative improvement in terms of log-likelihood are only marginal. The technical exposition is at times difficult to follow with some design decisions of the network layout being quite ad hoc and not well motivated. Expressing the involved operations in mathematical terms would help comprehend some of the technical details and add to the reproducibility of the proposed model. \n\nAnother concern is the experimental evaluation. While the reported log-likelihoods are only marginally better, the authors report a significant boost in how often humans are fooled by the generated images. While the image generation is conditioned on the low-resolution input, the workers in the Amazon Mechanical Turk study get to see the high-resolution images only. Of course, a human observer would pick the one image out of the two shown images which is more realistic although it might have nothing to do with the input image, which seems wrong. Instead, the workers should see the low-res input image and then have to decide which high-res image seems a better match or more likely.\n\nOverall, the presented work looks quite promising and an interesting line of research. However, in its present form the manuscript doesn't seem quite ready for publication yet. Though, I would strongly encourage the authors to make the exposition more self-contained and accessible, in particular through rigorous mathematical terms, which would help comprehend the involved operations and help understand the proposed mechanism.\n\nAdditional comments:\n- Abstract: \"we also believe to look pretty cool\". Please re-consider the wording here. Generating \"pretty cool\" images  should not be the goal of a scientific work.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642503144, "id": "ICLR.cc/2018/Conference/-/Paper752/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper752/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper752/AnonReviewer1", "ICLR.cc/2018/Conference/Paper752/AnonReviewer3", "ICLR.cc/2018/Conference/Paper752/AnonReviewer2"], "reply": {"forum": "r16Vyf-0-", "replyto": "r16Vyf-0-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642503144}}}, {"tddate": null, "ddate": null, "tmdate": 1514057333328, "tcdate": 1514057333328, "number": 3, "cdate": 1514057333328, "id": "r1pOCmnMM", "invitation": "ICLR.cc/2018/Conference/-/Paper752/Official_Comment", "forum": "r16Vyf-0-", "replyto": "SyA8u9OlG", "signatures": ["ICLR.cc/2018/Conference/Paper752/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper752/Authors"], "content": {"title": "Thank you for your review. Please find our responses below.", "comment": "We thank the reviewer for the thorough and insightful review.\nReviewer:\n...surprising that the proposed model is only slightly better in terms of log-likelihood ... Was the setup used ... exactly the same as the one used by Dahl et al.?\n\nOur response:\nOur generation models had not finished training on ImageNet. We now have significantly better perplexities (3.78 bits/dim, 3.77 with checkpoint averaging) than the row PixelRNN (3.86 bits/dim) and the Gated PixelCNN (3.83 bits/dim, previous SOTA) models. Gated PixelCNN improved over the previous SOTA by only 0.03, our improvement is twice as large. \n\nReviewer:\n\u2026 human evaluations can be extremely sensitive to changes in the setup \u2026 Please include error bars.\n\n\nOur response:\nWe included error bars which show that the variance is small, and the subjects (50 per image) are fairly clear on their preferences of images. We ensured that we use the exact same evaluation setup, down to the interface presented to the subjects.\n\nReviewer:\nThe CelebA super-resolution task \u2026 to fix this, the human observers should also receive the low-resolution image\n\nOur response:\nWe agree, yet followed Dahl, et al.\u2019s procedure exactly for comparability. While the shortcoming of the evaluation does present a potential loophole, allowing the model to generate images that do not downsample back to the input image, our model does not exploit this. It does generate images that, when downsampled, yield images very close to the low resolution input.\n\nTo demonstrate this, we compared the pixel/channel-level L2^2-distance between the low-resolution input image and the downsampled output image. Across 300 images from the CelebA test set, the average per-pixel, per-channel distance in normalized intensities between the input and the downsampled output images is 0.0106. The average distance between each of the low-resolution input images and 100 other downsampled images from the CelebA test set each is 0.1188. Given these are all cropped images of faces, we believe the difference to be significant. To underline this, we chose those two input images for which the downsampled version of the output image generated by our model is most different from the input image according to this metric and made them available here. Even here the downsampled output of our model is very similar to the original input.\n\nDue to shortage of space, we kindly request the reviewer to refer to the links in our response to Anonymous Reviewer 2.\n\nThe respective distances are 0.0344 and 0.03119 (between the input and model output images, each downsampled). We hope this shows that our model generates plausible images adhering to the intended constraint: that when downsampled they are very similar to the original low-resolution input image.\n\nThe input images constitute rich conditioning information such as hair and skin color, object position and pose, background color, etc. We believe there is real demand for models improving perceptual detail in images without a specific, expected output.\n\n\nReviewer:\n...Could gated convolutions not also be considered a form of self-attention?\n\n\n\nOur response:\nThere is some similarity to the multiplicative effects in self attention and gated CNNs, but there are also clear differences. Both use gating to scale the activations in multiplicative terms, which can prevent gradients from \u2018blowing up\u2019.\n\nIn self-attention, we have two sources of multiplicative interactions: 1) softmax-gated query key inner products which give us multiplicative effects between query and key representations, and 2) multiplicative interaction of the softmax weights with all values at each position, once per head. In self-attention, we first filter (softmax gating), and then aggregate (linear combination of values) per head, while gated convolutions first aggregate (applying the kernel) and then filter (gating). Because of the large receptive field of local self-attention, we achieve multiplicative interactions between positions that are far apart, which can be computationally expensive for convolutions. Both gating mechanisms are complementary and can be used together, e.g. the gating from gated PixelCNN could replace our position-wise FFNN layers.\n\nReviewer:\nThe presentation/text could use some work. \u2026.\n\nOur response:\nWe hoped to fit the submission within 9 pages, focusing on novel content at the expense of being self-contained. However, Equation 1 describes the computation applied to each position in every layer completely - the only exception being multiple heads. If accepted, we will repeat more unchanged details of the model.\n\nReviewer:\nThe authors write \u201cwe believe our cherry-picked images for various classes to be of higher perceptual quality\u201d.\n\nOur response:\nWe have revised our language and now write \u201cwe believe our curated images for various classes to be of reasonable perceptual quality\u201d, without comparing the perceptual quality to other work.\n\nWe removed the statement on \u201cpretty cool\u201d images from the abstract.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728339, "id": "ICLR.cc/2018/Conference/-/Paper752/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r16Vyf-0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper752/Authors|ICLR.cc/2018/Conference/Paper752/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper752/Authors|ICLR.cc/2018/Conference/Paper752/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper752/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper752/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper752/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper752/Reviewers", "ICLR.cc/2018/Conference/Paper752/Authors", "ICLR.cc/2018/Conference/Paper752/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728339}}}, {"tddate": null, "ddate": null, "tmdate": 1514057276313, "tcdate": 1514057276313, "number": 2, "cdate": 1514057276313, "id": "rkNH073Mf", "invitation": "ICLR.cc/2018/Conference/-/Paper752/Official_Comment", "forum": "r16Vyf-0-", "replyto": "BkYvzAixG", "signatures": ["ICLR.cc/2018/Conference/Paper752/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper752/Authors"], "content": {"title": "Thank you for your review. Please find our responses below", "comment": "We thank the reviewer for your insightful review.\n\nAt the time of submission, our conditional and unconditional generation generation models had not finished training on ImageNet, the harder task. We now have even better perplexities (3.78 bits/dim, 3.77 with checkpoint averaging) than the row PixelRNN (3.86 bits/dim) and the Gated PixelCNN (3.83 bits/dim, previous state of the art) models.\nGated PixelCNN improved over the previous state of the art by only 0.03, while our improvement is twice as large. Over the entire image of 3072 dimensions, the improvement in bits is quite significant.\n\nReviewer:\nThe motivation part is missing. ... Why not just a deeper network with more parameters? Throughout the paper I cannot find a clear answer for this. Based on this I couldn't see a clear contribution. \n\nOur response:\nWe agree with the reviewer that our motivation might not have been described well in the original submission. We added a more thorough motivation to the introduction of the paper, which we repeat here in summary for your convenience. \n\nOne disadvantage of CNNs compared to RNNs is their typically fairly limited receptive field. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion, especially with a small number of layers. Growing the receptive field, like deepening the network, however, comes at a significant cost in number of parameters and hence computation and can make training such models more challenging.\n\nWith the Image Transformer, we aim to find a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more\nparallelizable PixelCNN and its various extensions.\n\nWe propose eschewing recurrent and convolutional networks in favor of a model based entirely on a locally restricted form of multi-head self-attention that could also be interpreted as a sparsely parameterized form of convolution,  allowing  for  significantly  larger  receptive  fields  than  CNNs  at  the  same  number  of parameters.\n\nWe furthermore added additional experiments indicating that indeed, increasing the size of the receptive field significantly improves the performance of our model, allowing it to (now significantly, see below) outperform the state of the art. These show that increasing the perceptive field from 16 to 256 positions, for instance, improves perplexity on CIFAR-10 Test from 3.47 to 2.99.\n\n\nReviewer:\nThe paper is difficult to keep the track given the current flow. Each subsection of section 3 starts with technique details without explaining why we do this. Some sentences like \"look pretty cool\" is not academic. \n\nOur response:\nWe removed that sentence from the abstract, added additional material on the motivation, as summarized above, and tried to improve the overall flow in a flew places.\n\n\nReviewer:\nThe experiments lack comparisons except the human evaluation,  \u2026 Does it compare all the competing algorithms against the same sub-samples of the GT data? ...\n\nOur response:\nWe follow the same evaluation procedure as Dahl, et al.\u2019s paper but do not use the same exact sub-samples as we could not recover them. For each model, we use 50 randomly selected dev images where each image is rated by 50 workers. We use a different set of workers for each model. Also, our latest results on imagenet unconditional generation perplexities show a significant improvement in log likelihoods over the previous state-of-the-art. \n\nReviewer:\nApart from this metric, I would like to see qualitative comparison between competing algorithms in the paper as well. Also other approaches e.g. SRGAN could be compared.\n\nOur Response:\nIt would be very difficult to conduct a proper qualitative evaluation because we are missing representative samples from the various algorithms. We hope that our human evaluation numbers capture some qualitative differences between our model and pixel cnn.\n\nReviewer:\nI am also interested about the author's claim that the implementation error that influences the log-likelihood. Has this been fixed after the deadline?\n\nOur response:\nWe have indeed fixed the bug. The resulting images are now free of artifacts and the log-likelihood did improve. That said, we are still in the middle of conducting a final, apples-to-apples comparison between 1D and 2D self attention on various super-resolution tasks and will include the results of this in the final paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728339, "id": "ICLR.cc/2018/Conference/-/Paper752/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r16Vyf-0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper752/Authors|ICLR.cc/2018/Conference/Paper752/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper752/Authors|ICLR.cc/2018/Conference/Paper752/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper752/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper752/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper752/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper752/Reviewers", "ICLR.cc/2018/Conference/Paper752/Authors", "ICLR.cc/2018/Conference/Paper752/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728339}}}, {"tddate": null, "ddate": null, "tmdate": 1514057198941, "tcdate": 1514057198941, "number": 1, "cdate": 1514057198941, "id": "H1vxCX3GM", "invitation": "ICLR.cc/2018/Conference/-/Paper752/Official_Comment", "forum": "r16Vyf-0-", "replyto": "SJIFtpEbM", "signatures": ["ICLR.cc/2018/Conference/Paper752/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper752/Authors"], "content": {"title": "Thank you for your review. Please find our responses below.", "comment": "We thank the reviewer for your review.\nAt submission time, our generation models had not finished training on ImageNet, the harder task. We now have significantly better perplexities (3.78 bits/dim, 3.77 with checkpoint averaging) than the row PixelRNN (3.86 bits/dim) and the Gated PixelCNN (3.83 bits/dim, previous SOTA) models.\nGated PixelCNN improved over the previous SOTA by only 0.03, while our improvement is twice as large. The improvement in bits over an entire image with 3072 positions is significant.\n\nReviewer:\nWhile the paper is well written, \u2026. some design decisions of the network layout being quite ad hoc and not well motivated.\n\nResponse:\nWe agree that our motivation was not sufficiently described in our original submission. We added a much more detailed motivation in the introduction, summarized here for convenience.\n\nA disadvantage of CNNs compared to RNNs is their typically limited receptive field. This can adversely affect their ability to model long-range phenomena common in images, such as symmetry and occlusion in a small number of layers. Growing the receptive field, or deepening the network, comes at great cost in number of parameters and computation and can make training such models harder.\n\nIn this work we aim to find a better balance in the trade-off between the virtually unlimited receptive field of the necessarily sequential PixelRNN and the limited receptive field of the much more parallelizable PixelCNN.\n\nThe locally restricted form of multi-head self-attention we propose could also be interpreted as a sparsely parameterized form of convolution, with significantly larger receptive fields than CNNs at the same number of parameters.\n\nWe added experimental results that indicating that indeed, increasing the size of the receptive field improves the performance of our model significantly.\n\n\nReviewer:\nExpressing the involved operations in mathematical terms would help comprehend ... \n\nOur response:\nWe agree, though Equation 1 does describe the computation performed per position in the each of the layers completely, with the only exception being multiple heads. If the paper is accepted, we will elaborate more on the details of the model to make the content more self-contained, repeating the equations for multi-head attention and the positional encodings, etc.\n\nReviewer:\nAnother concern is the experimental evaluation....\n\nOur response:\nWe agree, but decided to exactly follow Dahl, et al.\u2019s procedure for comparability. While this shortcoming does present a potential loophole by allowing the model to generate images that do not downsample back to the input image (which we consider to be rich conditioning), our model does not exploit this, but instead generates images that, when downsampled again, yield images very close to the low resolution input.\n\nTo demonstrate this, we conducted an analysis comparing the pixel/channel-level L2^2-distance between the low-resolution input image and the downsampled output image.\nAcross a set of 300 images from the CelebA test set, the average per-pixel, per-channel distance in normalized intensities between the input and the downsampled output images is 0.0106. The average distance between the each of the low-resolution input images and 100 other downsampled images from the CelebA test set each is 0.1188. Given that these are all cropped images of faces, we believe the difference to be significant. To underline this, we chose those two input images for which the downsampled version of the output image generated by our model is most different from the input image according to this metric and made them available. The downsampled output of our model is still very similar to the original, downsampled input.\n\nExample 1\noriginal input image:  http://tiny.cc/kq8mpy\ndownsampled input image:  http://tiny.cc/xq8mpy\nsuper-resolved generated image (generated by the model):  http://tiny.cc/7r8mpy\ndownsampled super-resolved generated image:  http://tiny.cc/gs8mpy\n\nExample 2\noriginal input image:  http://tiny.cc/kt8mpy\ndownsampled input image: http://tiny.cc/tt8mpy\nsuper-resolved generated image (generated by the model):  http://tiny.cc/5t8mpy\ndownsampled super-resolved generated image:  http://tiny.cc/eu8mpy\n\nThe respective distances are 0.0344 and 0.03119 (between the downsampled original and the downsampled model output). We hope this persuades the reviewer that our model generates plausible images that adhere to the intended constraint: that they, when downsampled, are very similar to the original low-resolution input image.\n\nReviewer:\n...strongly encourage the authors to make the exposition more self-contained and accessible, \u2026.\n\nOur Response:\nWe hope the additional motivation helped improve the accessibility. If the paper is accepted, we will be happy to elaborate more on the details of the model from Vaswani et al. (2017) to make the content more self-contained.\n\nWe removed the statement about generating \u201cpretty cool\u201d images from the abstract.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Image Transformer", "abstract": "Image generation has been successfully cast as an autoregressive sequence generation\nor transformation problem. Recent work has shown that self-attention is\nan effective way of modeling textual sequences. In this work, we generalize a\nrecently proposed model architecture based on self-attention, the Transformer, to\na sequence modeling formulation of image generation with a tractable likelihood.\nBy restricting the self-attention mechanism to attend to local neighborhoods we\nsignificantly increase the size of images the model can process in practice, despite\nmaintaining significantly larger receptive fields per layer than typical convolutional\nneural networks. We propose another extension of self-attention allowing it\nto efficiently take advantage of the two-dimensional nature of images.\nWhile conceptually simple, our generative models trained on two image data sets\nare competitive with or significantly outperform the current state of the art in autoregressive\nimage generation on two different data sets, CIFAR-10 and ImageNet.\nWe also present results on image super-resolution with a large magnification ratio,\napplying an encoder-decoder configuration of our architecture. In a human\nevaluation study, we show that our super-resolution models improve significantly\nover previously published autoregressive super-resolution models. Images they\ngenerate fool human observers three times more often than the previous state of\nthe art.", "pdf": "/pdf/889a293123991c5e20325bdcdc61df41c3404252.pdf", "paperhash": "vaswani|image_transformer", "_bibtex": "@misc{\nvaswani2018image,\ntitle={Image Transformer},\nauthor={Ashish Vaswani and Niki Parmar and Jakob Uszkoreit and Noam Shazeer and Lukasz Kaiser},\nyear={2018},\nurl={https://openreview.net/forum?id=r16Vyf-0-},\n}", "keywords": ["image generation", "super-resolution", "self-attention", "transformer"], "authors": ["Ashish Vaswani", "Niki Parmar", "Jakob Uszkoreit", "Noam Shazeer", "Lukasz Kaiser"], "authorids": ["avaswani@google.com", "nikip@google.com", "uszkoreit@google.com", "noam@google.com", "lukaszkaiser@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825728339, "id": "ICLR.cc/2018/Conference/-/Paper752/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "r16Vyf-0-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper752/Authors|ICLR.cc/2018/Conference/Paper752/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper752/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper752/Authors|ICLR.cc/2018/Conference/Paper752/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper752/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper752/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper752/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper752/Reviewers", "ICLR.cc/2018/Conference/Paper752/Authors", "ICLR.cc/2018/Conference/Paper752/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825728339}}}], "count": 8}