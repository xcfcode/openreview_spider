{"notes": [{"id": "BJeY6sR9KX", "original": "rke6jAwtF7", "number": 824, "cdate": 1538087873290, "ddate": null, "tcdate": 1538087873290, "tmdate": 1545355438485, "tddate": null, "forum": "BJeY6sR9KX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJg7LJeIxE", "original": null, "number": 1, "cdate": 1545105226714, "ddate": null, "tcdate": 1545105226714, "tmdate": 1545354478694, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Meta_Review", "content": {"metareview": "This work provides two contributions: 1) Brain-Score, that quantifies how a given network's responses compare to responses from natural systems; 2) CORnet-S, an architecture trained to optimize Brain-Score, that performs well on Imagenet.\nAs noted by all reviewers, this work is interesting and shows a promising approach to quantifying how brain-like an architecture is, with the limitations inherent to the fact that there is a lot about natural visual processing that we don't fully understand. However, the work here starts from the premise that being more similar to current metrics of brain processes is by itself a good thing -- without a better understanding of what features of brain processing are responsible for good performance and which are mere by-products, this premise is not one that would appeal to most of ICLR audience. In fact, the best performing architectures on imagenet are not the best scoring for Brain-Score. Overall, this work is quite intriguing and well presented, but as pointed out by some reviewers, requires a \"leap of faith\" in matching signatures of brain processes that most of the ICLR audience is unlikely to be willing to take.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting take on quantifying similarity of networks to brain visual processing, unclear significance of that result for ICLR audiences"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper824/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353072230, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353072230}}}, {"id": "BkxCrz7fk4", "original": null, "number": 13, "cdate": 1543807558044, "ddate": null, "tcdate": 1543807558044, "tmdate": 1543807558044, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "B1g8YyWMkE", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "concerns addressed", "comment": "Thank you for answering my comments, and for running the additional tests."}, "signatures": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "BklwxmF537", "original": null, "number": 1, "cdate": 1541210863401, "ddate": null, "tcdate": 1541210863401, "tmdate": 1543807436421, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Review", "content": {"title": "promising work, further tests needed", "review": "This is an interesting paper in which the authors propose a shallow neural network, the architecture of which was optimized to maximize brain score, and show that it outperforms other shallow networks at imagenet classification. The goal of this paper \u2014 allowing brain data to improve our neural nets \u2014 is great. The paper is well written (except for a comment on clarity right below), and presents an interesting take on solving this problem. \n\nIt is a little unclear how the authors made CORnet optimize brain score: \u201cHowever, note that CORnet-S was developed using Brain-Score as a guiding benchmark and although it was never directly used in model search or optimization, testing CORnet-S on Brain-Score is not a completely independent test.\u201d Making these steps clearer is crucial for evaluating better what the model means. In the discussion \u201cWe have tested hundreds of architectures before finding CORnet-S circuitry and thus it is possible that the proposed circuits could have a strong relation to biological implementations.\u201d implies that the authors trained models with different architectures until the brain score was maximized after training. A hundred(s) times of training on 2760 + 2400 datapoints are probably plenty to overfit the brainscore datasets. The brain score is probably compromised after this, and it would be hard to make claims about the results on the brain modeling side. The author acknowledge this limitation, but perhaps a better thing is to add an additional dataset, perhaps one from different animal recordings, or from human fMRI? \n\nArguably, the goal of the paper is to obtain a model that overpowers other simpler models, and not necessarily to make claims about the brain. The interesting part of the paper is that the shallow model does work better than other shallow models. The authors mention that brain score helps CORnet be better at generalizing to other datasets. Including these results would definitely strengthen the claim since both brain score and imagenet have been trained on hundreds of times so far. \n\nAnother way to show that brain score helps is to show it generalizes above or differently from optimizing other models. What would have happened if the authors stuck to a simple, shallow model and instead of optimizing brain score optimized performance (hundreds of times) on some selected image dataset (this selection dataset is separate from imagenet, but the actual training is done on imagenet) and then tested performance on imagenet? Is the effect due to the brain or to the independent testing on another dataset?\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Review", "cdate": 1542234368533, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335810779, "tmdate": 1552335810779, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1g8YyWMkE", "original": null, "number": 12, "cdate": 1543798654337, "ddate": null, "tcdate": 1543798654337, "tmdate": 1543798654337, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BklwxmF537", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "additional questions?", "comment": "Hi,\n\nwe were wondering if our new analyses and comments were sufficient or whether you had additional remarks that we could address before the deadline?"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "HJlTN1bzJE", "original": null, "number": 11, "cdate": 1543798580972, "ddate": null, "tcdate": 1543798580972, "tmdate": 1543798580972, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BkgUZyJi27", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "additional questions?", "comment": "Hi,\n\nwe were wondering if our new analyses and comments were sufficient or whether you had additional remarks that we could address before the deadline?"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "S1lN6e1RRX", "original": null, "number": 10, "cdate": 1543528636390, "ddate": null, "tcdate": 1543528636390, "tmdate": 1543528648579, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "HJlPsv75C7", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "Concerns addressed", "comment": "Thank you, my concerns were satisfactorily addressed."}, "signatures": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "Bkgywd7cRm", "original": null, "number": 8, "cdate": 1543284823498, "ddate": null, "tcdate": 1543284823498, "tmdate": 1543333349021, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BklwxmF537", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "clarifications & generalization analyses", "comment": "Thank you for your review.\n\n1. Regarding CORnet optimization:\n  a. First, we\u2019d like to clarify that the search for a good CORnet architecture was done by hand. We trained a few models with different architectures, evaluated their ImageNet and Brain-Score performance, and tried to improve architectural choices in the next iteration. While we tried to limit our knowledge of Brain-Scores during model building, you are rightfully pointing out that CORnet ought to be evaluated on an independent set.\n\n  b. We therefore collected a new behavioral dataset that used new objects that we\u2019ve not used before in any of our benchmarks. The correlation between the old and the new behavioral rankings was strong (.83).\n\n  c. We also compared rankings on an independent neural dataset collected on the same images as before, and found a very strong correlation too (.93)\n\n  d. To push models further, we also compared the rankings of neural predictivity on a very dissimilar dataset (a subset of MS COCO). The correlation was still robust (.76)\n\n  e. The details of these analyses can be found in Section 4.2 and also in Appendix C. Overall, CORnet-S is among the top models for all these analyses.\n\n2. Regarding quantifying model generalization on other Machine Learning datasets: This is a good point, thus we strived to provide some measure of generalization in the revised manuscript (Fig. 2; Section 4.2). Following Kornblith et al. (2018), we compared model rankings when their classifiers were retrained for CIFAR-100. Here, we observed that Brain-Score is a good predictor of how well models will transfer to CIFAR-100 as fixed feature encoders (r=.69).\n\n3. Regarding the idea to optimize model search not using Brain-Score. This is an interesting idea, however, one that would take many more that just a few weeks to test:\n  a. As mentioned before, the search is not automated, so it might take a very long time (in the case of CORnet it took over a year)\n\n  b. Training these models on ImageNet takes a few days and several GPUs at least, so given resource constraints, it would be challenging to perform this search.\n\n  c. Given how correlated Brain-Score and ImageNet performance are, it is not unlikely that other datasets would lead to a similar circuitry. The key difference between optimizing for Brain-Score and optimizing for another dataset is that Brain-Score is not a proxy to what we want. Rather, it is the exact measure for how brain-like a given model is. Adding new recordings to Brain-Score that break existing models will thus further constrain these mechanistic hypotheses of the brain. By optimizing for Brain-Score we are making sure that we are optimizing for our ultimate goal \u2014 a model of the human visual system."}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "BygKIUQ9R7", "original": null, "number": 4, "cdate": 1543284304848, "ddate": null, "tcdate": 1543284304848, "tmdate": 1543285026553, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BkgUZyJi27", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "more analyses", "comment": "Thank you again for your review!\n\n1. Comments on using temporal responses:\n  This is a good suggestion that we were also interested in. To get a sense whether temporal information adds more discriminatory power on top of the existing benchmarks, we compared model predictivity at two response bins: 90-110 ms (early response) and 190-210 ms (late response), since, following the June 2018 arXiv paper, the main differences in model scores appeared to emerge at later time steps (>170 ms). The resulting model scores can be found in Figure 8. From our analysis, early responses seem to be well-predicted across most models. Differences emerge in the late responses, however they do not appear to be different from the model scores on the mean 70-170 ms data. Similarly to the arXiv paper, we found that a feed-forward model already predicts temporal responses very well, and the recurrent models only out-perform the feed-forward model when the image is explicitly taken off (cf. arXiv paper, Fig. 5C). We hypothesize that the images shown for the current set of recordings might simply not warrant the functional use of recurrent connections. However, we are hopeful that future recordings focusing in on this temporal aspect (such as https://www.biorxiv.org/content/early/2018/06/26/354753) might help to further distinguish between models. So far, we have not yet observed this and thus chose not to include these scores. To contribute to community efforts in this domain, we are releasing temporal (20 ms bins) neural recordings from the data presented in this paper in the Brain-Score platform.\n\n2. Comments on performing more analyses:\n\n  a. This is a great suggestion and we quantified the necessity of each component in the CORnet-S circuity (see new Fig. 5 in the updated paper). We identified that having enough recurrent steps was the single most predictive factor. Having a large enough bottleneck and a skip connection was other important factors, especially for ImageNet. Other factors that contributed mostly to ImageNet performance were the number of convolutions within an area, the total number of areas in the model, and the use of Batch Normalization. Again, an even larger bottleneck that originally considered helped slightly for ImageNet performance, but not for Brain-Score. However, having one less layer on the block circuitry or one more area in the model did not seem to affect Brain-Score significantly.\n\n  b. We also varied the length of the \u201clocal\u201d recurrent connection. Note that this recurrence is not exactly local as it goes two convolutional layers back. Reducing the length of this recurrent connection from two layers to one layer reduces model performance, and re-running features through the entire circuitry seems to be most beneficial. Considering that recurrence virtually increases the depth of the network, longer recurrent connections seem intuitive since they create the network with the largest virtual depth.\n\n  c. It may also be interesting to add longer range feedback connections that would go across model areas, e.g., from V4 to V1. This is only meaningful when unrolling in time is done in a biological manner so that feedback can be properly integrated with feed-forward inputs. We tested several CORnet variants with biological time unrolling and feedback from V4 to V1 (but simpler block structure to fit the model into memory) but observed no real change in adding a feedback connection. Our interpretation is that feedback connections do not automatically help or hurt model\u2019s performance but a wider search might pinpoint to specific types of connections that may help. However, this would be a topic for another study."}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "BkxQpPQc0Q", "original": null, "number": 7, "cdate": 1543284667442, "ddate": null, "tcdate": 1543284667442, "tmdate": 1543284667442, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "Hyx7wwpchX", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "generalization analyses", "comment": "Thank you for your review!\n\n1. Contribution to ML:\n\n  a. Following your comment, we tested how similar model ranking on Brain-Score is to the ranking on four independent and very different datasets (see Section 4.2 and Figure 3). We found a robust generalization of scores to other brain datasets (neural and behavioral) as well as to CIFAR-100. Using the models from this study as fixed feature encoders, Brain-Score is quite predictive of how well a model will perform on this ML dataset.\n\n  b. Taking a step back, we emphasized in the revised paper that the main purpose of Brain-Score is to help build brain-like models. Those may or may not be also good machine learning models. For instance, human visual system seems to be more robust to various image perturbations, so machine learning models could benefit from that. On the other hand, we explicitly demand models to make mistakes if humans make mistakes. This is typically orthogonal to typical ML goals of reaching maximal performance.\n\n2. Contribution to neuroscience:\n  a. In the revised manuscript, we attempted to clarify (Sections 4.2 and 5) that constraining models with more and more brain data will necessarily converge them to the inner workings of the brain. Already today with three limited datasets that currently underlie Brain-Score we observe a very high predictive power of neural responses in the intermediate layers of primate visual system. These models have not been trained in any way to match these internal representations. We think this is a strong indication that the models are already good mechanistic hypotheses of the inner workings of the primate visual system. Adding data to Brain-Score that breaks existing models will help us to further refine these models in making them more brain-like.\n\n  b. Perhaps this comment was meant to question whether, under Brain-Score\u2019s guidance, we would ever find the circuitries that match those in the brain. Having no anatomical constraints in Brain-Score at the moment, it would be hard to expect those circuitries to emerge. As detailed in our CORnet-S circuitry analysis, many different circuitries appear to work sufficiently well at the moment. Perhaps this reflects the lack of constraints in Brain-Score at the moment. On the other hand, it remains unclear to what extent we want to model visual system to begin with. Do we need spikes? Biochemical details? Or should we focus on functional properties only? With Brain-Score, we follow the latter, at least for now, and, given that models can predict neural and behavioral responses very accurately, we think they are already a good functional match which now needs to be further refined as described above.\n\n(continued in separate comment due to max characters)"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "HJlPsv75C7", "original": null, "number": 6, "cdate": 1543284639278, "ddate": null, "tcdate": 1543284639278, "tmdate": 1543284639278, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "Hyx7wwpchX", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "clarifications", "comment": "\n3. The importance of recurrence. We now performed a detailed analysis of what makes CORnet-S work and the amount of recurrence indeed turned out to be the main predictive factor (see Figure 5) of both Brain-Score and ImageNet top-1 performance. Among other factors that played a role, we identified the presence of a skip connection, the size of the bottleneck, and the number of convolutions within each model area (for ImageNet only) as the key contributing factors to CORnet-S\u2019 performance.\n\n4. We found that there was a correlation (.65 for V4 and .87 or IT) which is strong enough to connect neurons to behavior but not sufficient for behavior alone to explain the entire neural population, warranting a composite set of benchmarks.\n\n5. Can the number of neurons explain differences in neural predictivity?\n  a. In Figure 6 we show that there does not appear to be a correlation between the number of features and Brain-Score. \n\n  b. Also note that normally neural predictivity analyses are done by first PCA\u2019ing features down to 1000 components and then building a regression map between neural responses and model responses. This is perhaps another good control showing that at least the number of features that go into regression is always matched across models.\n\n6. Fig. 1 gray dots: We now included an explanation in the caption that these values come from a class of simplified five-layer models that had various hyperparameters manipulated (values captured at different points during model training). The purpose of this test was to see the correlation between ImageNet and Brain-Score in lower performance regimes.\n\n7. Consider for instance two models with IT scores of .580 and .581. Our current approach would simply use these values for the final Brain-Score without re-weighting. We also considered an alternative approach to compute the Brain-Score where models would be ranked on each benchmark separately and then receive a mean \u201cBrain-Rank\u201d. However in that case, the above example with two very close values would lead to the two models receiving different ranks with the same distance as if the values were .580 and .620. This is just to say that we chose to preserve the distance in scores as opposed to a ranking approach. We clarified this in the paper.\n\n8. Comparison against June 2018 arXiv paper. We now included a new section comparing CORnet-S to other recurrent models (see Section 3.2), including the June 2018 one. Briefly, CORnet-S is an attempt to build a high performing model (both on ImageNet and Brain-Score) while keeping it as simple as possible. The June model focuses more on the anatomical necessity of recurrent and feedback connections without constraining the model size. As a result, CORnet-S is substantially simpler (shallower, simpler connectivity, takes much less GPU memory, and is mapped to brain areas) than the June 2018 one."}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "SJgSTjDd6X", "original": null, "number": 3, "cdate": 1542122428915, "ddate": null, "tcdate": 1542122428915, "tmdate": 1542122428915, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "B1ewCw7HTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "Thanks!", "comment": "Great, thank you for the quick clarification! All of these are great suggestions, and we are currently running analyses to quantify what parts are critical to the model."}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "B1ewCw7HTQ", "original": null, "number": 2, "cdate": 1541908430521, "ddate": null, "tcdate": 1541908430521, "tmdate": 1541908430521, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "r1goI8dgTQ", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "clarifying \"type of weight\"", "comment": "The question is generally asking about what specifically about the CORNET-S architecture makes it perform well \non Brain-Score (and which for ImageNet) compared to other similar networks.    The types of weights refers to feedback vs recurrent vs convolutional filters vs pooling (and connections between which layers are most critical).\nI was just asking for some form of ablation study or analysis similar to Figure 4 in https://arxiv.org/pdf/1807.00053v2.pdf  to show what parts of your architecture you feel are critical for a good \nmodel of visual processing in the brain."}, "signatures": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "r1goI8dgTQ", "original": null, "number": 1, "cdate": 1541600851392, "ddate": null, "tcdate": 1541600851392, "tmdate": 1541600851392, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BkgUZyJi27", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "content": {"title": "can you please clarify \"type of weight\"?", "comment": "Thank you for your review. Could you please clarify what you mean by the \"type of weight in the network\"?"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607060, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJeY6sR9KX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper824/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper824/Authors|ICLR.cc/2019/Conference/Paper824/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers", "ICLR.cc/2019/Conference/Paper824/Authors", "ICLR.cc/2019/Conference/Paper824/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607060}}}, {"id": "Hyx7wwpchX", "original": null, "number": 2, "cdate": 1541228379410, "ddate": null, "tcdate": 1541228379410, "tmdate": 1541533660538, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Review", "content": {"title": "Interesting attempt to bridge the gap between ML and neuroscience", "review": "In this interesting study, the authors propose a score (BrainScore) to (1) compare neural representations of an ANN trained on imagenet with primate neural activity in V4 and IT, and (2) test whether ANN and primate make the same mistakes on image classification.  They also create a shallow recurrent neural network (Cornet) that performs well according to their score and also reasonably well on imagenet classification task given its shallow architecture.\n\nThe analyses are rigorous and the idea of such a score as a tool for guiding neuroscientists building models of the visual system is novel and interesting.\n\nMajor drawbacks:\n\n1. Uncertain contribution to ML: it remains unclear whether architectures guided by the brain score will indeed generalize better to other tasks, as the authors suggest.\n\n2. Uncertain contribution to neuroscience: it remains unclear whether finding the ANN resembling the real visual system most among a collection of models will inform us about the inner working of the brain.\n\n\nThe article would also benefit from the following clarifications:\n\n3. Are the recurrent connections helping performance of Cornet on imagenet and/or on BrainScore?\n\n4. Did you find a correlation between the neural predictivity score and behavioral predictivity score across networks tested? If yes, it would be interesting to mention.\n\n5. When comparing neural predictivity score across models, is a model with more neurons artificially advantaged by the simple fact that there is more likely a linear combination of neurons that map to primate neural activations? Is cross-validation enough to control for this potential bias?\n\n6. Fig1: what are the gray dots?\n\n7. \u201cbut it also does not make any assumptions about significant differences in the scores, which would be present in ranking. \u201c\nWhat does this mean?\n\n8. How does Cornet compare to this other recent work: https://arxiv.org/abs/1807.00053 (June 20 2018) ?\n\nConclusion:\nThis study presents an interesting attempt at bridging the gap between machine learning and neuroscience. Although the impact that this score will have in both ML and Neuroscience fields remains uncertain, the work is sufficiently novel and interesting to be published at ICLR. I am fairly confident in my evaluation as I work at the intersection of deep learning and neuroscience.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Review", "cdate": 1542234368533, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335810779, "tmdate": 1552335810779, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgUZyJi27", "original": null, "number": 3, "cdate": 1541234430131, "ddate": null, "tcdate": 1541234430131, "tmdate": 1541533660228, "tddate": null, "forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "invitation": "ICLR.cc/2019/Conference/-/Paper824/Official_Review", "content": {"title": "Not enough analysis", "review": "Please consider this rubric when writing your review:\n1. Briefly establish your personal expertise in the field of the paper.\n2. Concisely summarize the contributions of the paper.\n3. Evaluate the quality and composition of the work.\n4. Place the work in context of prior work, and evaluate this work's novelty.\n5. Provide critique of each theorem or experiment that is relevant to your judgment of the paper's novelty and quality.\n6. Provide a summary judgment if the work is significant and of interest to the community.\n\n1. I am a researcher working at the intersection of machine learning and\nbiological vision.  I have experience with neural network models and\nvisual neurophysiology.\n\n2. This paper makes two contributions: 1) It develops Brain-Score - a\ndataset and error metric for animal visual single-cell recordings.  2)\nIt develops (and brain-scores) a new shallow(ish) recurrent network\nthat performs well on ImageNet and scores highly on brain-score. \n\n3. The development of Brain-Score is a useful invention for the field.  A nice\naspect of Brain-Score is that responses in both V4 and IT as well as behavioral\nresponses are provided.   I think it could be more useful if the temporal dynamics (instead of the\nmean number of spikes) was included.    This would allow to compare temporal\nresponses in order to compare \"brain-like\" matches.\n\n4. This general idea is somewhat similar to a June 2018 Arxiv paper\n(Task-Driven Convolutional Recurrent Models of the Visual System)\nhttps://arxiv.org/abs/1807.00053\nbut this is a novel contribution as it is uses the Brain-Score dataset.\n\nOne limitation of this approach relative to the June 2018 ArXiv paper\nis that the Brain-Score method is just representing the mean neural\nresponse to each image - The Arxiv paper shows that different models\ncan have different temporal responses that can also be used to decide\nwhich is a closer match to the brain.\n\n5. More analysis of why CORNET-S is best among compact models would greatly\nstrengthen this paper.  What do the receptive fields look like?  How do they compare\nto the other models.  What about the other high performing networks (e.g. DenseNet-169)?\nHow sensitive are the results to each type of weight in the network?   What about feedback connections\n(instead of local recurrent connections)? \n\n6. This paper makes a significant contribution, in part due to the\ndevelopment and open-sourcing of Brain-Score.  The significance of the\ncontribution of the CORnet-S architecture is limited by the\nlack of analysis into what aspects make it better than other models.\n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper824/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures", "abstract": "Deep artificial neural networks with spatially repeated processing (a.k.a., deep convolutional ANNs) have been established as the best class of candidate models of visual processing in the primate ventral visual processing stream. Over the past five years, these ANNs have evolved from a simple feedforward eight-layer architecture in AlexNet to extremely deep and branching NASNet architectures, demonstrating increasingly better object categorization performance. Here we ask, as ANNs have continued to evolve in performance, are they also strong candidate models for the brain? To answer this question, we developed Brain-Score, a composite of neural and behavioral benchmarks for determining how brain-like a model is, together with an online platform where models can receive a Brain-Score and compare against other models. \nDespite high scores, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. To further map onto anatomy and validate our approach, we built CORnet-S: an ANN guided by Brain-Score with the anatomical constraints of compactness and recurrence. Although a shallow model with four anatomically mapped areas and recurrent connectivity, CORnet-S is a top model on Brain-Score and outperforms similarly compact models on ImageNet. Analyzing CORnet-S circuitry variants revealed recurrence as the main predictive factor of both Brain-Score and ImageNet top-1 performance.\n", "keywords": ["Computational Neuroscience", "Brain-Inspired", "Neural Networks", "Simplified Models", "Recurrent Neural Networks", "Computer Vision"], "authorids": ["qbilius@mit.edu", "msch@mit.edu", "dicarlo@mit.edu"], "authors": ["Jonas Kubilius", "Martin Schrimpf", "Ha Hong", "Najib J. Majaj", "Rishi Rajalingham", "Elias B. Issa", "Kohitij Kar", "Pouya Bashivan", "Jonathan Prescott-Roy", "Kailyn Schmidt", "Aran Nayebi", "Daniel Bear", "Daniel L. K. Yamins", "James J. DiCarlo"], "pdf": "/pdf/6b2e025882ea72ea75bd39874f17c68eb1daf76b.pdf", "paperhash": "kubilius|aligning_artificial_neural_networks_to_the_brain_yields_shallow_recurrent_architectures", "_bibtex": "@misc{\nkubilius2019aligning,\ntitle={Aligning Artificial Neural Networks to the Brain yields Shallow Recurrent Architectures},\nauthor={Jonas Kubilius and Martin Schrimpf and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Kailyn Schmidt and Aran Nayebi and Daniel Bear and Daniel L. K. Yamins and James J. DiCarlo},\nyear={2019},\nurl={https://openreview.net/forum?id=BJeY6sR9KX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper824/Official_Review", "cdate": 1542234368533, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJeY6sR9KX", "replyto": "BJeY6sR9KX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper824/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335810779, "tmdate": 1552335810779, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper824/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 16}