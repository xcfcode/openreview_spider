{"notes": [{"id": "B1ethsR9Ym", "original": "B1eKGST9KX", "number": 733, "cdate": 1538087857486, "ddate": null, "tcdate": 1538087857486, "tmdate": 1545355403584, "tddate": null, "forum": "B1ethsR9Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Look Ma, No GANs! Image Transformation with ModifAE", "abstract": "Existing methods of image to image translation require multiple steps in the training or modification process, and suffer from either an inability to generalize, or long training times. These methods also focus on binary trait modification, ignoring continuous traits. To address these problems, we propose ModifAE: a novel standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make continuous trait image modifications. As a standalone image modification network, ModifAE requires fewer parameters and less time to train than existing models. We empirically show that ModifAE produces significantly more convincing and more consistent continuous face trait modifications than the previous state-of-the-art model.", "keywords": ["Computer Vision", "Deep Learning", "Autoencoder", "GAN", "Image Modification", "Social Traits", "Social Psychology"], "authorids": ["chada@ucsd.edu", "b4tam@ucsd.edu"], "authors": ["Chad Atalla", "Bartholomew Tam", "Amanda Song", "Gary Cottrell"], "TL;DR": "ModifAE is a standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make image modifications (without GANs).", "pdf": "/pdf/0e0aa9731c95e85abb8abc36e85bc1bc938ee415.pdf", "paperhash": "atalla|look_ma_no_gans_image_transformation_with_modifae", "_bibtex": "@misc{\natalla2019look,\ntitle={Look Ma, No {GAN}s! Image Transformation with Modif{AE}},\nauthor={Chad Atalla and Bartholomew Tam and Amanda Song and Gary Cottrell},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ethsR9Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1x94rMxeE", "original": null, "number": 1, "cdate": 1544721714060, "ddate": null, "tcdate": 1544721714060, "tmdate": 1545354509414, "tddate": null, "forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper733/Meta_Review", "content": {"metareview": "1. Describe the strengths of the paper.  As pointed out by the reviewers and based on your expert opinion.\n\n- The paper tackles an interesting and relevant problem for ICLR: guided image modification of images (in this case of facial attributes).\n- The proposed method is in general well-explained (although some details are lacking)\n \n2. Describe the weaknesses of the paper. As pointed out by the reviewers and based on your expert opinion. Be sure to indicate which weaknesses are seen as salient for the decision (i.e., potential critical flaws), as opposed to weaknesses that the authors can likely fix in a revision.\n\n- The training set of faces and associated attributes were annotated using a pre-trained model which introduced a bias into the annotations used for training the method.\n- The experimental results weren't convincing. The qualitative results showed no clear advantage of the proposed method and the quantitative comparison to StarGAN only considered two attribute manipulations and only found a statistically significant different in performance for one of those.\nThe second weakness was the key determining factor in the AC's final recommendation. \n\n3. Discuss any major points of contention. As raised by the authors or reviewers in the discussion, and how these might have influenced the decision. If the authors provide a rebuttal to a potential reviewer concern, it\u2019s a good idea to acknowledge this and note whether it influenced the final decision or not. This makes sure that author responses are addressed adequately.\n\nThere were no major points of contention and no author feedback.\n \n4. If consensus was reached, say so. Otherwise, explain what the source of reviewer disagreement was and why the decision on the paper aligns with one set of reviewers or another.\n\nThe reviewers reached a consensus that the paper should be rejected.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "interesting problem, but experimental results aren't promising"}, "signatures": ["ICLR.cc/2019/Conference/Paper733/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper733/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Look Ma, No GANs! Image Transformation with ModifAE", "abstract": "Existing methods of image to image translation require multiple steps in the training or modification process, and suffer from either an inability to generalize, or long training times. These methods also focus on binary trait modification, ignoring continuous traits. To address these problems, we propose ModifAE: a novel standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make continuous trait image modifications. As a standalone image modification network, ModifAE requires fewer parameters and less time to train than existing models. We empirically show that ModifAE produces significantly more convincing and more consistent continuous face trait modifications than the previous state-of-the-art model.", "keywords": ["Computer Vision", "Deep Learning", "Autoencoder", "GAN", "Image Modification", "Social Traits", "Social Psychology"], "authorids": ["chada@ucsd.edu", "b4tam@ucsd.edu"], "authors": ["Chad Atalla", "Bartholomew Tam", "Amanda Song", "Gary Cottrell"], "TL;DR": "ModifAE is a standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make image modifications (without GANs).", "pdf": "/pdf/0e0aa9731c95e85abb8abc36e85bc1bc938ee415.pdf", "paperhash": "atalla|look_ma_no_gans_image_transformation_with_modifae", "_bibtex": "@misc{\natalla2019look,\ntitle={Look Ma, No {GAN}s! Image Transformation with Modif{AE}},\nauthor={Chad Atalla and Bartholomew Tam and Amanda Song and Gary Cottrell},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ethsR9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper733/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353104446, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper733/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper733/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper733/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353104446}}}, {"id": "r1xLJAL4aX", "original": null, "number": 3, "cdate": 1541856733686, "ddate": null, "tcdate": 1541856733686, "tmdate": 1541856733686, "tddate": null, "forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper733/Official_Review", "content": {"title": "Authors propose a trait modification network that is trained as a standalone auto-encoder and is able to model continous trait interpolations in the latent space. The network requires less parameters than competitors and the training process less painful than other approaches based on GANs. ", "review": "* The construction of the training dataset is clearly flawed by the use of an automatic algorithm that would certainly introduce a strong bias and noisy labels. Even though the dataset is supposed to encode continuous traits, the validation with human subjects is performed in a binary fashion.\n\n* I miss more formality in the presentation of the methodology. Figure 3. does not seem very self-explanatory, nor does the caption. Which is the dimensionality of the input trait vector?. I assume the input would be the trait ratings predicted by the human subjects. However in the experiments training seems to be done with a maximum of two traits. This makes me wonder how the dense part of the network can handle the dimensionality blow-up to match the latent space dimensionality without suffering from overfitting. I would appreciate some disussion regarding this.\n\n* While I appreciate a section reasoning why the method is supposed to work, those claims should be backed with an ablation study in the experimental section.  \n\n* The qualitative results show a few examples which I find very hard to evaluate due to the low-resolution of the predictions. In both traits there seems to be the same facial features modified and I can't find much difference between trustworthy and aggresssive (the labels could be swapped and I would have the same opinion on the results). I miss additional trait examples that would make clearer if the network is learning something besides generating serious and happy faces.\n\n* The qualitative comparison with StarGAN seems unfair, as if one checks the original paper their results are certainly more impressive than what Figure 5 shows.\n\n* The authors show only two traits in the experiments which makes me a bit suspicious about the performance of the network with the rest of traits. The training datset considers up to 40 traits.\n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper733/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Look Ma, No GANs! Image Transformation with ModifAE", "abstract": "Existing methods of image to image translation require multiple steps in the training or modification process, and suffer from either an inability to generalize, or long training times. These methods also focus on binary trait modification, ignoring continuous traits. To address these problems, we propose ModifAE: a novel standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make continuous trait image modifications. As a standalone image modification network, ModifAE requires fewer parameters and less time to train than existing models. We empirically show that ModifAE produces significantly more convincing and more consistent continuous face trait modifications than the previous state-of-the-art model.", "keywords": ["Computer Vision", "Deep Learning", "Autoencoder", "GAN", "Image Modification", "Social Traits", "Social Psychology"], "authorids": ["chada@ucsd.edu", "b4tam@ucsd.edu"], "authors": ["Chad Atalla", "Bartholomew Tam", "Amanda Song", "Gary Cottrell"], "TL;DR": "ModifAE is a standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make image modifications (without GANs).", "pdf": "/pdf/0e0aa9731c95e85abb8abc36e85bc1bc938ee415.pdf", "paperhash": "atalla|look_ma_no_gans_image_transformation_with_modifae", "_bibtex": "@misc{\natalla2019look,\ntitle={Look Ma, No {GAN}s! Image Transformation with Modif{AE}},\nauthor={Chad Atalla and Bartholomew Tam and Amanda Song and Gary Cottrell},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ethsR9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper733/Official_Review", "cdate": 1542234388531, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper733/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335791168, "tmdate": 1552335791168, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper733/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJl7SNponX", "original": null, "number": 2, "cdate": 1541293114535, "ddate": null, "tcdate": 1541293114535, "tmdate": 1541533732779, "tddate": null, "forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper733/Official_Review", "content": {"title": "Modify social attributes on face images results here in low-quality images", "review": "The paper is about changing the attributes of a face image to let it look more aggressive, trustworthy etc. by means of a standalone autoencoder (named ModifAE). The approach is weak starting from the construction of the training set. Since continue social attributes on face images does not exist yet, CelebA dataset is judged by Song et al. (2017) with continuous face ratings and use the predicted ratings to train ModifAE. This obviously introduces a bias driven by the source regression model. The hourglass model is clearly explained. The experiments are not communicating: the to qualitative examples are not the best showcase for the attributes into play (attractive, emotional), and requires to severely magnify the pdf to spot something. This obviously show the Achille\u2019s heel of these works, i.e., working with miniature images. Figure 5, personally, is about who among modifAE and stargan does less bad, since the resulting images are of low quality (the last row speaks loud about that)\nQuantitative results are really speaking user tests, so I will cal it as they are, user tests. They work only on two attributes, and show a reasonable advantage over stargan only for one attribute. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper733/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Look Ma, No GANs! Image Transformation with ModifAE", "abstract": "Existing methods of image to image translation require multiple steps in the training or modification process, and suffer from either an inability to generalize, or long training times. These methods also focus on binary trait modification, ignoring continuous traits. To address these problems, we propose ModifAE: a novel standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make continuous trait image modifications. As a standalone image modification network, ModifAE requires fewer parameters and less time to train than existing models. We empirically show that ModifAE produces significantly more convincing and more consistent continuous face trait modifications than the previous state-of-the-art model.", "keywords": ["Computer Vision", "Deep Learning", "Autoencoder", "GAN", "Image Modification", "Social Traits", "Social Psychology"], "authorids": ["chada@ucsd.edu", "b4tam@ucsd.edu"], "authors": ["Chad Atalla", "Bartholomew Tam", "Amanda Song", "Gary Cottrell"], "TL;DR": "ModifAE is a standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make image modifications (without GANs).", "pdf": "/pdf/0e0aa9731c95e85abb8abc36e85bc1bc938ee415.pdf", "paperhash": "atalla|look_ma_no_gans_image_transformation_with_modifae", "_bibtex": "@misc{\natalla2019look,\ntitle={Look Ma, No {GAN}s! Image Transformation with Modif{AE}},\nauthor={Chad Atalla and Bartholomew Tam and Amanda Song and Gary Cottrell},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ethsR9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper733/Official_Review", "cdate": 1542234388531, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper733/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335791168, "tmdate": 1552335791168, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper733/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkghQKgonX", "original": null, "number": 1, "cdate": 1541241124050, "ddate": null, "tcdate": 1541241124050, "tmdate": 1541533732574, "tddate": null, "forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper733/Official_Review", "content": {"title": "an autocoding model as an alternative to GANs for continuous trait image modifications", "review": "Overview and contributions: The authors propose the ModifAE model that is based on an autoencoder neural network for continuous trait image modifications. ModifAE requires fewer parameters and less time to train than existing generative models. The authors also present experiments to show that ModifAE produces more convincing and more consistent continuous face trait modifications than the current baselines.\n\nStrengths:\n1. Nice presentation of the model.\n2. Good experiments to justify improved running time and fewer number of parameters.\n\nWeaknesses:\n1. I am not completely convinced by the results in Figure 4. It doesn't seem like the model is able to pick up on subtle facial expressions and generate them in a flexible manner. In fact, the images look very very similar regardless of the value of the traits. Furthermore, the authors claim that \"In general, as she becomes more emotional, her smile increases, and as she is made more attractive, her smile increases as well, as smiling subjects are judged as more attractive\". I believe attractiveness and emotions are much more diverse and idiosyncratic than just the size of her smile...\n2. From Figure 5 it seems like ModifAE generates images that are lower in quality as compared to StarGAN. Can the authors comment on this point? How can ModifAE be improved to generate higher-quality images?\n\nQuestions to authors:\n1. Weakness points 1 and 2.\n2. This did not affect my rating, but I am slightly concerned by the labelings as seen in Figure 1. Is it reasonable to infer traits like \"trustworthy\", \"attractive\", \"aggressive\", \"responsible\" from images? Are these traits really what we should be classifying people's faces as, and are there any possible undesirable/sensitive biases from the dataset that our models could learn? I would like to hear the author's opinions on the ethical implications of these datasets and models. \n\nPresentation improvements, typos, edits, style, missing references:\nNone", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper733/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Look Ma, No GANs! Image Transformation with ModifAE", "abstract": "Existing methods of image to image translation require multiple steps in the training or modification process, and suffer from either an inability to generalize, or long training times. These methods also focus on binary trait modification, ignoring continuous traits. To address these problems, we propose ModifAE: a novel standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make continuous trait image modifications. As a standalone image modification network, ModifAE requires fewer parameters and less time to train than existing models. We empirically show that ModifAE produces significantly more convincing and more consistent continuous face trait modifications than the previous state-of-the-art model.", "keywords": ["Computer Vision", "Deep Learning", "Autoencoder", "GAN", "Image Modification", "Social Traits", "Social Psychology"], "authorids": ["chada@ucsd.edu", "b4tam@ucsd.edu"], "authors": ["Chad Atalla", "Bartholomew Tam", "Amanda Song", "Gary Cottrell"], "TL;DR": "ModifAE is a standalone neural network, trained exclusively on an autoencoding task, that implicitly learns to make image modifications (without GANs).", "pdf": "/pdf/0e0aa9731c95e85abb8abc36e85bc1bc938ee415.pdf", "paperhash": "atalla|look_ma_no_gans_image_transformation_with_modifae", "_bibtex": "@misc{\natalla2019look,\ntitle={Look Ma, No {GAN}s! Image Transformation with Modif{AE}},\nauthor={Chad Atalla and Bartholomew Tam and Amanda Song and Gary Cottrell},\nyear={2019},\nurl={https://openreview.net/forum?id=B1ethsR9Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper733/Official_Review", "cdate": 1542234388531, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1ethsR9Ym", "replyto": "B1ethsR9Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper733/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335791168, "tmdate": 1552335791168, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper733/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}