{"notes": [{"id": "_QdvdkxOii6", "original": "KEcoRarKl96", "number": 616, "cdate": 1601308073970, "ddate": null, "tcdate": 1601308073970, "tmdate": 1614985656861, "tddate": null, "forum": "_QdvdkxOii6", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "94TwFmCEbt", "original": null, "number": 1, "cdate": 1610040505553, "ddate": null, "tcdate": 1610040505553, "tmdate": 1610474112766, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Although all reviewers agree that this is an interesting analysis of sample efficiency in Deep RL over the past few years, there is also a consensus that it is not enough material for an ICLR paper. I also share this sentiment, which motivates the \"Reject\" decision. This work could have been made stronger by reproducing previous results (even partially) and sharing the corresponding code, so as to provide a fair and controlled comparison of algorithms, and setting the stage for future progress in this area. In its form, it is better suited for a presentation at a workshop than for the main conference."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040505540, "tmdate": 1610474112751, "id": "ICLR.cc/2021/Conference/Paper616/-/Decision"}}}, {"id": "KluXSnbjrUK", "original": null, "number": 1, "cdate": 1603339986484, "ddate": null, "tcdate": 1603339986484, "tmdate": 1606250705383, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Review", "content": {"title": "A meta-analysis of sample efficiency in deep RL", "review": "##################################\n\nSummary: \nThis paper conducts a meta-analysis of the trend in sample efficiency in deep RL. The authors argue that this is an informative measure of the progress in the field, in addition to the usual metrics of reward for given tasks, as it is an important consideration when applying deep RL to real world problems. They measure sample efficiency by the number of samples from the environment that is needed to achieve some reward. The amount of time to double sample efficiency is computed for Atari and continuous control environments with state or pixel input.\n\n##################################\n\nPros: \n1. In deep RL, usually evaluation is done by reporting the reward achieved for some set of environments S after some number of steps N during training. Since S and N vary between papers, it is difficult to measure progress in this field. By using sample efficiency as a metric, this paper is able to give a retrospective of the progress for environments with pixel input, which appears to be missing from the literature.\n2. The paper is able to estimate the fraction of progress due to 1) increased sample sizes 2) algorithmic improvements. I found this to be the most interesting conclusion from the paper.\n3. Based on the history of the algorithms, the paper provides discussion of the main breakthroughs, which is very helpful.\n\nCons:\n1. On the technical side, the paper does not seem to have much novelty. The analysis approach is the same as that of Hernandez & Brown (2020), with minor changes to adapt to the deep RL setting.\n2. There are quite a few limitations to the analysis, which the authors discuss in Section 5 and Appendix D. My main concern is about the hyper-parameters. Deep RL is notoriously sensitive, and the way that researchers select hyper-parameters can have great effect on the final results. Therefore, I am concerned that differences in the amount of effort used to tune hyper-parameters may either mask improvements in sample efficiency or lead to spurious conclusions that sample efficiency has improved. Could the authors discuss how this issue could affect the conclusions in the paper?\n\n##################################\n\nOverall: \nI am not sure that this work in its current form is the right fit for ICLR. It does not have much technical novelty, and its reliance on reported training curves, in my opinion, decreases the credibility of the results. In addition, since it is a meta-analysis, I would suggest that a venue like the \"ML Retrospectives, Surveys & meta-Analyses (ML- RSA)\" workshop at NeurIPS would be a better fit.\n\n##################################\n\nFurther comments and questions:\n1. I think a more in-depth discussion of the factors leading to progress in sample efficiency, currently in Appendix C, would be very helpful to researchers thinking about new ideas. Therefore, I suggest that it be moved to the main paper. \n2. In Section 4.2, how large is the variation in the amount of frames used in the unrestricted benchmark, and do most of those algorithms use >>200M frames? I think that a lot of variation in the amount of frames would imply a less accurate fit of the trend.\n\n##################################\n\nUpdate after reading other reviews and author responses:\n\nThanks to the authors for answering my questions. However, my main concerns were not addressed, so I will keep my score. ", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper616/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139082, "tmdate": 1606915801635, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper616/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Review"}}}, {"id": "aDBthiOtQ52", "original": null, "number": 3, "cdate": 1604296218311, "ddate": null, "tcdate": 1604296218311, "tmdate": 1606243482390, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Review", "content": {"title": "Not enough material for a paper", "review": "This paper aims to measure sample efficiency of available RL  methods instead of algorithmic improvement.  It draws a conclusion about continuing improved RL sample efficiency in the past few years.  This conclusion is rather thin to fill a complete paper. \nAnd the evaluation procedure is rather limited: it directly uses data reported from the original papers without re-producing them at the same environment. These different papers came across a span of at least four years;  although Mujoco is a standard benchmark,  it also has evolved multiple times, basically, there are so many uncontrolled variables in the plot reported by authors. I consider these results not eligible for interpretation. And the algorithms considered here for sample efficiency are not on the same ground, some are off-policy, some are on-policy, off-policy algorithms are naturally far more sample efficient than on-policy, they can't be compared in this way.  Also, authors only report results for score 400 and 2000, which is also inadequate.    In the pixel experiments, it mentions SLAC and dreaming, they are not purely pixel-based RL, rather a latent space is trained at first; so it's so a valid comparison between them and methods like CuRL. In the state experiments, SLAC is not purely state-based, it trains VAE first for a low-dim latent space. \n\nIn all, the material presented in this paper does not fill a complete paper, and the evaluation protocol and conclusions are not valid.  ", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper616/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139082, "tmdate": 1606915801635, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper616/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Review"}}}, {"id": "KtGQFHMp3m", "original": null, "number": 11, "cdate": 1606243153306, "ddate": null, "tcdate": 1606243153306, "tmdate": 1606243153306, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "wfcOvIazy9D", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for the updates! Unfortunately I'll keep the original score."}, "signatures": ["ICLR.cc/2021/Conference/Paper616/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_QdvdkxOii6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper616/Authors|ICLR.cc/2021/Conference/Paper616/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment"}}}, {"id": "6YP9S9KkIZQ", "original": null, "number": 10, "cdate": 1605434841739, "ddate": null, "tcdate": 1605434841739, "tmdate": 1605434841739, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "KluXSnbjrUK", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your time!\n\nWe moved Appendix C into the main text. This also includes a paragraph on hyperparameters. We also added another comment on how hyperparameter tuning might correspond to unsustainable short term progress near the end of the discussion.\n\nThe first five results in the unrestricted benchmark use 200M Frames, the next 250M and the last three use 22.8B, 55.3B and 20B frames. As we mention in the paper, the results are in 4.2. are especially uncertain because of the low amount of data points compared to the other analyses. "}, "signatures": ["ICLR.cc/2021/Conference/Paper616/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_QdvdkxOii6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper616/Authors|ICLR.cc/2021/Conference/Paper616/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment"}}}, {"id": "wfcOvIazy9D", "original": null, "number": 9, "cdate": 1605434815382, "ddate": null, "tcdate": 1605434815382, "tmdate": 1605434815382, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "bQkvpLiFj0u", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your time!\n\nWe agree that agents like MuZero and R2D2 cannot be used for conclusions about data efficiency and did not aim to do so. If you told us, where you got the impression that we did, we are happy to clarify this in the paper. \n\nWe have moved appendix C to the main paper and will try to further improve on the section. We will also consider, how we can expand on our analysis for continuous control. \n\nWe corrected the date for FRODO and double checked all other dates in Figure 1 (a)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper616/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_QdvdkxOii6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper616/Authors|ICLR.cc/2021/Conference/Paper616/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment"}}}, {"id": "E4suj2UxrZc", "original": null, "number": 8, "cdate": 1605434785262, "ddate": null, "tcdate": 1605434785262, "tmdate": 1605434785262, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "aDBthiOtQ52", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your time!\n\nWe only used MuJoCo results from gym-v1 up to 2019 and see no indication that there is a discontinuity at the point in time, where multiple versions are taken into consideration for the first time. \n\nWe agree that progress in sample efficiency was largely driven by off-policy algorithms. However, we are of the opinion that the availability of competitive off-policy algorithms for continuous control represents progress in sample efficiency compared to a time, where such algorithms were not available, such that comparing on-policy algorithms to off-policy algorithms (and later model-based algorithms) does not present a problem. Similarly, we are of the opinion that for a broad evaluation of progress in DRL sample efficiency, algorithms training a latent space first can be compared to algorithms that don\u2019t do this, as long as the frames used for training the latent space are counted towards the amount of training samples. We are under the impression that this was done for Dreaming and SLAC and added more detailed information to appendix B. \n\nLastly, we did in fact report results for three different scores on four different DeepMind control suite tasks and two different scores for five different MuJoCo/gym tasks in the appendix. We added a reference to the appendix in a footnote in the relevant section to make this more clear. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper616/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_QdvdkxOii6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper616/Authors|ICLR.cc/2021/Conference/Paper616/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment"}}}, {"id": "n6LTubH7AV0", "original": null, "number": 7, "cdate": 1605434715983, "ddate": null, "tcdate": 1605434715983, "tmdate": 1605434715983, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "QYo0CdbpmnF", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your time!\n\nWe moved appendix C, which summarizes important contributors to improved sample efficiency to the main paper. We will try to further improve on the section. We also added a sentence about the role of hardware in the clock time speedups.\n\nRegarding (i), We agree that the number of used gradient steps could be an issue, in fact is a special case of the general issue with hyperparameters that is discussed in the appendix and section 5.4 (which was moved from the appendix). However, even if SLAC was using multiple gradient steps, we believe that excluding algorithms for tuning their hyperparameters for sample efficiency would distort results even more, as algorithms designed for strong sample efficiency are also most likely to tune their hyperparameters for sample efficiency, especially in a domain where an increasing amount of papers specifically target sample efficiency. \n\nAlso, all versions of SLAC seem to report using a single gradient step per environment step for the DM Control Suite. Arxiv versions 1 to 3 also report a single step for gym, while version 4 reports using three steps on gym. As the reported results change for the Control Suite but not gym between v3 and v4, this might be a typo. We neither considered version 3 or 4, as both were published after our study\u2019s cutoff date. If you are aware of whether the CURL paper refers to arxiv version 2 or a later version with \u201cSLACv2\u201c, we would be grateful for that information. As far as we are aware, there are no training curves/fine grained results available for the version of CURL that uses 3 gradient steps, but would be more than happy to include the results in our analysis if that data was to be published. However, note that the results with 3 and 1 gradient steps seem quite close for both 100k and 500k frames, in most cases, which is also evidence that the effect of additional gradient steps might be quite limited. \n\nRegarding (ii), we changed the results for CURL to the results from Figure 8 in the original publication. Thank your for pointing out the issue with the numbers from DRQ. We in fact report results for three different scores on four different DeepMind control suite tasks and two different scores for five different MuJoCo/gym tasks in the appendix and also refer to them in our analysis. We added a reference to the appendix in a footnote in the relevant section to make this more clear. \n\nRegarding (iii): This is tricky: Arxiv versions 1 and 2 of SLAC don\u2019t report using these exploratory steps and the pseudocode algorithm also does not include them. Versions 3 and 4 report 10k agent steps but claims they are included in the plots. We added a more detailed description of this problem to the appendix and also performed a sensitivity analysis that showed that the effect of adding 10000 environment steps to all SLAC results changes doubling times by at most 0.3 months (and a lot less for most environment/task combinations)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper616/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_QdvdkxOii6", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper616/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper616/Authors|ICLR.cc/2021/Conference/Paper616/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923869019, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Comment"}}}, {"id": "bQkvpLiFj0u", "original": null, "number": 2, "cdate": 1603911445285, "ddate": null, "tcdate": 1603911445285, "tmdate": 1605024646051, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Review", "content": {"title": "Review", "review": "The paper is trying to make extensive and systematic investigation in deep RL papers to measure the recent progress in the broad literatures. The authors look into published state-of-the-art results in Atari, state-based continuous control and pixel-based continuous control. Inferring from papers' published training curves, the authors estimated and found sample efficiency doubling time is 10-18 months for Atari, 5-24 months for continuous control and 4-9 months for pixel-based control tasks.\n\nPros:\n+ The reviewer really likes the extensive efforts put on the paper, which includes deep survey on published DRL literatures, analysis on progress of agents, open discussions on limitations of the analysis in Appendix part D, etc;\n+ The authors state super clearly about their methodology in the study in Section 3, where they explain how they measure the sample efficiency, how they select publications and SOTA result for analysis and what's the source of numbers used for analysis;\n\nCons: \n- The biggest concern the reviewer has, which is also discussed and stated in Appendix D, is that many of the published papers, especially those on Atari, were tuned and focused on the performance *score* at 200M frames (or other frame limits), instead of optimising for data efficiency. In fact, as also be mentioned in the paper, van Hasselt et al. 2019 and Kielak 2020 noted that if you tune  the hyperparms for data efficiency, you may draw a very different conclusion. At the same time, the reviewer acknowledged that reproducing all RL results and tuning them separately for data efficiency would require dramatic amount of time and compute resources;\n- Related to the above point, when taking the training curve as reference to plot the data efficiency progress figures like Figure 1, It may be ok for the 200M frames evaluation agents, but when it comes to high frame regimes which R2D2 or MuZero uses, they were aiming for absolute performance score instead of aiming for data efficiency. IMO, it's hard to draw conclusions on data efficiency with those agents which consume billions of frames and they didn't consider data efficiency;\n-  The continuous control tasks might be a better place to conduct study on data efficiency where more data efficiency related publications go into and those literatures and applications care much more about data efficiency than the ones on Atari. The reviewer found that the survey put more effort and content on the Atari results instead of the control tasks could be improved;\n- The reviewer would love to see more discussions like the Appendix part C on how different agents improved data efficiency. It might be worth expanding these sections and put those into the main paper instead of the Appendix. Answering how DRL has progressed on data efficiency is an essential addition to measuring how much it has progressed;\n \n\n\nMinor:\n- For Figure 1 (a) some agent was labelled at a wrong published year, e.g. FRODO was published in 2020 instead of 2018.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper616/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139082, "tmdate": 1606915801635, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper616/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Review"}}}, {"id": "QYo0CdbpmnF", "original": null, "number": 4, "cdate": 1604369547881, "ddate": null, "tcdate": 1604369547881, "tmdate": 1605024645990, "tddate": null, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "invitation": "ICLR.cc/2021/Conference/Paper616/-/Official_Review", "content": {"title": "Review", "review": "Summary:\n\nThe paper proposes to retrospectively benchmark the sample-efficiency on widely used simulated deep RL benchmarks such as Atari and DMControl across the years. The paper shows some interesting trends with respect to how both the algorithmic improvements as well as the use of increased number of frames have driven the progress in the SoTA scores reported on the Atari HNS benchmark. The paper also shows that there has been exponential progress in the sample-efficiency on both Atari and DMControl with nice log-linear plots. This is an interesting analysis, on the lines of OpenAI's papers on Scaling Laws, but done for Deep RL.\n\nPros:\n\nUseful review article for the community.\nCommunicates progress in very visible and scientifically precise manner.\nDisentangles progress made in algorithmic efficiency and using more frames (distributed training), and wall clock time reduction.\nCons:\n\nThe DMControl results could be presented better.. Specific points below:\nDeepMind Control results: (i) In DMControl plots, using SLACv2 and other methods on the same plot is incorrect because SLACv2 performs more gradient updates per batch while other methods do not and it has been known that it is a contributing factor to sample efficiency. The author is encouraged to check an ablation in CURL Appendix E.2 where CURL is shown to be superior to SLAC-v2 on 3/4 envs when using similar update frequencies for 500k, and 2/4 for 100k steps respectively. (ii) Performance numbers used for CURL have been taken from DrQ which do not reflect the accurate numbers. Based on the numbers reported in RAD, for both CURL and RAD, the performance for Walker from CURL seems to be at 403 +/- 24, while DrQ reports 344 +/- 132. Would recommend taking numbers directly from updated versions of CURL/RAD on arXiv which are consistent with each other. Would also recommend adding multiple environments to do the analysis on DMControl as opposed to just one arbitrary hand-picked environment without a clear justification. (iii) As pointed out in DrQ, SLAC uses 100k exploration steps which are not counted in the reported sample-efficiency values. Check Table 1 in DrQ. I believe this is an issue with SLACv2 as well.\n\nThe paper should talk a little more detail about how the hardware has changed the game in terms of wall clock time reduction, what kind of hardware, etc. A plot wrt computation flops would be useful to have.\n\nThe amount of actionable insight gathered from the paper - in terms of - after reading this paper, what can I do to make my DRL systems more efficient? - is not clear to me. I would be curious if the authors can explain this to me given there's a chance I missed it.\n\nRating: Weak Reject - I believe the paper has a lot of room for improvement and has to provide some strong good insights out of the analysis, but I am open to updating my views based on author response.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper616/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper616/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Measuring Progress in Deep Reinforcement Learning Sample Efficiency ", "authorids": ["~Florian_E._Dorner1"], "authors": ["Florian E. Dorner"], "keywords": ["Deep Reinforcement Learning", "Sample Efficiency"], "abstract": "Sampled environment transitions are a critical input to deep reinforcement learning (DRL) algorithms. Current DRL benchmarks often allow for the cheap and easy generation of large amounts of samples such that perceived progress in DRL does not necessarily correspond to improved sample efficiency. As simulating real world processes is often prohibitively hard and collecting real world experience is costly, sample efficiency is an important indicator for economically relevant applications of DRL. We investigate progress in sample efficiency on Atari games and continuous control tasks by comparing the amount of samples that a variety of algorithms need to reach a given performance level according to training curves in the corresponding publications. We find exponential progress in sample efficiency with estimated doubling times of around 10 to 18 months on Atari, 5 to 24 months on state-based continuous control and of around 4 to 9 months on pixel-based continuous control depending on the specific task and performance level.", "one-sentence_summary": "We measure progress in deep reinforcement learning sample efficiency using training curves from published papers. ", "pdf": "/pdf/d2b5b622c8c995a07f56c05166b348b135314fc3.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "dorner|measuring_progress_in_deep_reinforcement_learning_sample_efficiency", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=g1SWcOzDUu", "_bibtex": "@misc{\ndorner2021measuring,\ntitle={Measuring Progress in Deep Reinforcement Learning Sample Efficiency },\nauthor={Florian E. Dorner},\nyear={2021},\nurl={https://openreview.net/forum?id=_QdvdkxOii6}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_QdvdkxOii6", "replyto": "_QdvdkxOii6", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper616/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538139082, "tmdate": 1606915801635, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper616/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper616/-/Official_Review"}}}], "count": 11}