{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124447713, "tcdate": 1517725200075, "number": 18, "cdate": 1517725200075, "id": "S1dGIXVUz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "S1dGIXVUz", "signatures": ["~Amirsina_Torfi1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Attention-Based Guided Structured Sparsity of Deep Neural Networks", "abstract": "Network pruning is aimed at imposing sparsity in a neural network architecture by increasing the portion of zero-valued weights for reducing its size energy-efficiency consideration and increasing evaluation speed. In most of the conducted research efforts, the sparsity is enforced for network pruning without any attention to the internal network characteristics such as unbalanced outputs of the neurons or more specifically the distribution of the weights and outputs of the neurons. That may cause severe accuracy drop due to uncontrolled sparsity. In this work, we propose an attention mechanism that simultaneously controls the sparsity intensity and supervised network pruning by keeping important information bottlenecks of the network to be active. On CIFAR-10, the proposed method outperforms the best baseline method by 6% and reduced the accuracy drop by 2.6x at the same level of sparsity.", "paperhash": "torfi|attentionbased_guided_structured_sparsity_of_deep_neural_networks", "_bibtex": "@misc{\n  torfi2018attention-based,\n  title={Attention-Based Guided Structured Sparsity of Deep Neural Networks},\n  author={Amirsina Torfi and Rouzbeh Asghari Shirvani},\n  year={2018},\n  url={https://openreview.net/forum?id=S1dGIXVUz}\n}", "authorids": ["atorfi@vt.edu", "rouzbeh.asgharishir@bison.howard.edu"], "authors": ["Amirsina Torfi", "Rouzbeh Asghari Shirvani"], "keywords": ["Deep Netwroks", "Sparsity", "Attention", "Convolutional Networks"], "pdf": "/pdf/30680ab0a5f188d9271b380fb3ace48087502131.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582865893, "tcdate": 1520556225523, "number": 1, "cdate": 1520556225523, "id": "Syt6_IytG", "invitation": "ICLR.cc/2018/Workshop/-/Paper18/Official_Review", "forum": "S1dGIXVUz", "replyto": "S1dGIXVUz", "signatures": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer1"], "content": {"title": "Nice contribution", "rating": "7: Good paper, accept", "review": "This paper proposes an approach to enforce sparseness in deep neural networks in order to learn smaller models that maintain the accuracy of the full (over-parameterized) model. The proposed objective function is an extension of the structured sparsity loss proposed by Wen et al 2016. The extension is a group variance loss, which can be interpreted as applying an attention mechanism over the output channels\u2019 weights. Under a 90% sparsity rate they obtain lower errors rates than Wen et al and other sparsity induction methods on MNIST and CIFAR-10. \nThis is a nice contribution as a workshop paper. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attention-Based Guided Structured Sparsity of Deep Neural Networks", "abstract": "Network pruning is aimed at imposing sparsity in a neural network architecture by increasing the portion of zero-valued weights for reducing its size energy-efficiency consideration and increasing evaluation speed. In most of the conducted research efforts, the sparsity is enforced for network pruning without any attention to the internal network characteristics such as unbalanced outputs of the neurons or more specifically the distribution of the weights and outputs of the neurons. That may cause severe accuracy drop due to uncontrolled sparsity. In this work, we propose an attention mechanism that simultaneously controls the sparsity intensity and supervised network pruning by keeping important information bottlenecks of the network to be active. On CIFAR-10, the proposed method outperforms the best baseline method by 6% and reduced the accuracy drop by 2.6x at the same level of sparsity.", "paperhash": "torfi|attentionbased_guided_structured_sparsity_of_deep_neural_networks", "_bibtex": "@misc{\n  torfi2018attention-based,\n  title={Attention-Based Guided Structured Sparsity of Deep Neural Networks},\n  author={Amirsina Torfi and Rouzbeh Asghari Shirvani},\n  year={2018},\n  url={https://openreview.net/forum?id=S1dGIXVUz}\n}", "authorids": ["atorfi@vt.edu", "rouzbeh.asgharishir@bison.howard.edu"], "authors": ["Amirsina Torfi", "Rouzbeh Asghari Shirvani"], "keywords": ["Deep Netwroks", "Sparsity", "Attention", "Convolutional Networks"], "pdf": "/pdf/30680ab0a5f188d9271b380fb3ace48087502131.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582865701, "id": "ICLR.cc/2018/Workshop/-/Paper18/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper18/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper18/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper18/AnonReviewer3"], "reply": {"forum": "S1dGIXVUz", "replyto": "S1dGIXVUz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper18/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper18/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582865701}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582701043, "tcdate": 1520700110609, "number": 2, "cdate": 1520700110609, "id": "SkD0cK-YM", "invitation": "ICLR.cc/2018/Workshop/-/Paper18/Official_Review", "forum": "S1dGIXVUz", "replyto": "S1dGIXVUz", "signatures": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer2"], "content": {"title": "Over complex regularization term, insufficient evaluation.", "rating": "4: Ok but not good enough - rejection", "review": "This paper propose to prune the network weights by using a group sparsity and a variance encouraging term to regularize the weights.\nThe text of paper is not very easy to read, for example, the name \"attention mechanism\" is not very well explained. I understand the idea mainly through the equation. \n\nPro:\nThis paper has proposed to add one more regularization term, namely the variance encourage term to the SSL method. It is experimentally shown that this regularization term is effective in pruning the network without losing much accuracy.\n\nCon:\nIn my opinion, the regularizer itself is too complex to be an elegant method. It contains nested squares, square roots, and reciprocals, making the computation graph over complex.\nThe final loss function has three regularization terms each with a hyperparameter lambda to tune.\nThese are the two points I dislike about the idea of this paper.\nRegarding experiments, the chosen architecture are too old to be convincing. One could simply argue that newer structures with better inductive bias could out perform the pruned network with the same number of parameter. As the authors also point out that manually reducing the network size could lead to similar results.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attention-Based Guided Structured Sparsity of Deep Neural Networks", "abstract": "Network pruning is aimed at imposing sparsity in a neural network architecture by increasing the portion of zero-valued weights for reducing its size energy-efficiency consideration and increasing evaluation speed. In most of the conducted research efforts, the sparsity is enforced for network pruning without any attention to the internal network characteristics such as unbalanced outputs of the neurons or more specifically the distribution of the weights and outputs of the neurons. That may cause severe accuracy drop due to uncontrolled sparsity. In this work, we propose an attention mechanism that simultaneously controls the sparsity intensity and supervised network pruning by keeping important information bottlenecks of the network to be active. On CIFAR-10, the proposed method outperforms the best baseline method by 6% and reduced the accuracy drop by 2.6x at the same level of sparsity.", "paperhash": "torfi|attentionbased_guided_structured_sparsity_of_deep_neural_networks", "_bibtex": "@misc{\n  torfi2018attention-based,\n  title={Attention-Based Guided Structured Sparsity of Deep Neural Networks},\n  author={Amirsina Torfi and Rouzbeh Asghari Shirvani},\n  year={2018},\n  url={https://openreview.net/forum?id=S1dGIXVUz}\n}", "authorids": ["atorfi@vt.edu", "rouzbeh.asgharishir@bison.howard.edu"], "authors": ["Amirsina Torfi", "Rouzbeh Asghari Shirvani"], "keywords": ["Deep Netwroks", "Sparsity", "Attention", "Convolutional Networks"], "pdf": "/pdf/30680ab0a5f188d9271b380fb3ace48087502131.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582865701, "id": "ICLR.cc/2018/Workshop/-/Paper18/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper18/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper18/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper18/AnonReviewer3"], "reply": {"forum": "S1dGIXVUz", "replyto": "S1dGIXVUz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper18/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper18/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582865701}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582677300, "tcdate": 1520734610117, "number": 3, "cdate": 1520734610117, "id": "rk55-zGKM", "invitation": "ICLR.cc/2018/Workshop/-/Paper18/Official_Review", "forum": "S1dGIXVUz", "replyto": "S1dGIXVUz", "signatures": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer3"], "content": {"title": "A controller mechanism for network pruning is proposed based on previous works by further extending the group sparsity loss in SSL with group variance loss. Experiments demonstrate the robustness of proposed model.", "rating": "5: Marginally below acceptance threshold", "review": "\nContributions:\n1. The novel sparsity regularization is capable of being incorporated for any layer type (Conv & FC as given in paper)\n2. Seen as an extension for SSL, only minor modifications are needed to implement this idea\n\nDrawbacks:\n1. Explanation on group variance loss in the Model Part is required with more details, and so is the last visualization in Fig.1 \n2. Experiments would be more convincing if large data-sets(ImageNet) and networks with complex architecture(ResNet) were included\n3. When mentioning the \u201cInformation Bottleneck\u201d in the paper, authors failed in giving related evidences\n\nI\u2019m open to change my opinion if more persuasive experiments are attached.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attention-Based Guided Structured Sparsity of Deep Neural Networks", "abstract": "Network pruning is aimed at imposing sparsity in a neural network architecture by increasing the portion of zero-valued weights for reducing its size energy-efficiency consideration and increasing evaluation speed. In most of the conducted research efforts, the sparsity is enforced for network pruning without any attention to the internal network characteristics such as unbalanced outputs of the neurons or more specifically the distribution of the weights and outputs of the neurons. That may cause severe accuracy drop due to uncontrolled sparsity. In this work, we propose an attention mechanism that simultaneously controls the sparsity intensity and supervised network pruning by keeping important information bottlenecks of the network to be active. On CIFAR-10, the proposed method outperforms the best baseline method by 6% and reduced the accuracy drop by 2.6x at the same level of sparsity.", "paperhash": "torfi|attentionbased_guided_structured_sparsity_of_deep_neural_networks", "_bibtex": "@misc{\n  torfi2018attention-based,\n  title={Attention-Based Guided Structured Sparsity of Deep Neural Networks},\n  author={Amirsina Torfi and Rouzbeh Asghari Shirvani},\n  year={2018},\n  url={https://openreview.net/forum?id=S1dGIXVUz}\n}", "authorids": ["atorfi@vt.edu", "rouzbeh.asgharishir@bison.howard.edu"], "authors": ["Amirsina Torfi", "Rouzbeh Asghari Shirvani"], "keywords": ["Deep Netwroks", "Sparsity", "Attention", "Convolutional Networks"], "pdf": "/pdf/30680ab0a5f188d9271b380fb3ace48087502131.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582865701, "id": "ICLR.cc/2018/Workshop/-/Paper18/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper18/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper18/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper18/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper18/AnonReviewer3"], "reply": {"forum": "S1dGIXVUz", "replyto": "S1dGIXVUz", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper18/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper18/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582865701}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573581760, "tcdate": 1521573581760, "number": 165, "cdate": 1521573581421, "id": "HJIAA0AYf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "S1dGIXVUz", "replyto": "S1dGIXVUz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Attention-Based Guided Structured Sparsity of Deep Neural Networks", "abstract": "Network pruning is aimed at imposing sparsity in a neural network architecture by increasing the portion of zero-valued weights for reducing its size energy-efficiency consideration and increasing evaluation speed. In most of the conducted research efforts, the sparsity is enforced for network pruning without any attention to the internal network characteristics such as unbalanced outputs of the neurons or more specifically the distribution of the weights and outputs of the neurons. That may cause severe accuracy drop due to uncontrolled sparsity. In this work, we propose an attention mechanism that simultaneously controls the sparsity intensity and supervised network pruning by keeping important information bottlenecks of the network to be active. On CIFAR-10, the proposed method outperforms the best baseline method by 6% and reduced the accuracy drop by 2.6x at the same level of sparsity.", "paperhash": "torfi|attentionbased_guided_structured_sparsity_of_deep_neural_networks", "_bibtex": "@misc{\n  torfi2018attention-based,\n  title={Attention-Based Guided Structured Sparsity of Deep Neural Networks},\n  author={Amirsina Torfi and Rouzbeh Asghari Shirvani},\n  year={2018},\n  url={https://openreview.net/forum?id=S1dGIXVUz}\n}", "authorids": ["atorfi@vt.edu", "rouzbeh.asgharishir@bison.howard.edu"], "authors": ["Amirsina Torfi", "Rouzbeh Asghari Shirvani"], "keywords": ["Deep Netwroks", "Sparsity", "Attention", "Convolutional Networks"], "pdf": "/pdf/30680ab0a5f188d9271b380fb3ace48087502131.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}