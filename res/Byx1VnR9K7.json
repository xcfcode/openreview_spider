{"notes": [{"id": "Byx1VnR9K7", "original": "rJe5tfC5KQ", "number": 1412, "cdate": 1538087974850, "ddate": null, "tcdate": 1538087974850, "tmdate": 1545355400557, "tddate": null, "forum": "Byx1VnR9K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1eg0LElg4", "original": null, "number": 1, "cdate": 1544730312190, "ddate": null, "tcdate": 1544730312190, "tmdate": 1545354511965, "tddate": null, "forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1412/Meta_Review", "content": {"metareview": "The paper considers the problem of imitating multi-modal expert demonstrations using a variational auto-encoder to embed demonstrated trajectories into a structured latent space. The problem is important, and the paper is well written. The model is shown to work well on toy examples. However, as pointed out by the reviewers, given that multi-modal has been studied before, the approach should have been compared both in theory and in practice to existing methods and baselines (e.g., InfoGAIL). Furthermore, the technical contribution is somewhat limited as it using an existing model on a new application domain.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Incremental solution, missing baselines"}, "signatures": ["ICLR.cc/2019/Conference/Paper1412/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1412/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1412/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352848424, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1412/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1412/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1412/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352848424}}}, {"id": "H1l9q4gqAm", "original": null, "number": 3, "cdate": 1543271569622, "ddate": null, "tcdate": 1543271569622, "tmdate": 1543271578224, "tddate": null, "forum": "Byx1VnR9K7", "replyto": "Sye9uPhBn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1412/Official_Comment", "content": {"title": "Thank you for your constructive feedback.", "comment": "We propose a new trajectory-level VAE which is different compared with previous work. The model is an alternative fully probabilistic model to capture state sequence dependencies, which is easy to train simply by gradient descent and has a promising performance on a range of problems. We agree that further experiments are needed as we mentioned in the comments for R1 and R2.  All the results we show for the rolling window case generate actions on-the-fly. The initial state is observed and a subsequent L actions are generated, after which the new observed state is fed into the model and so on. We will do more ablation studies to compare this with feeding the observed states directly into  the policy decoder during test, and clarify the experiment set up in section 3.3.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1412/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1412/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1412/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626251, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byx1VnR9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference/Paper1412/Reviewers", "ICLR.cc/2019/Conference/Paper1412/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1412/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1412/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1412/Authors|ICLR.cc/2019/Conference/Paper1412/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1412/Reviewers", "ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference/Paper1412/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626251}}}, {"id": "rylVt4lq0X", "original": null, "number": 2, "cdate": 1543271547784, "ddate": null, "tcdate": 1543271547784, "tmdate": 1543271547784, "tddate": null, "forum": "Byx1VnR9K7", "replyto": "SyxpLO9_2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1412/Official_Comment", "content": {"title": "Thank you for your constructive feedback.", "comment": "The main contribution of the paper is to introduce a consistent trajectory-level VAE which does not need simulation during training and serves as an alternative for capturing state sequence structure. We will strengthen our paper by experiments on the real MineCraft environment, quantitative comparison with SoTA algorithms and ablation studies, as well as the extension to bootstrapping reinforcement learning.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1412/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1412/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1412/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626251, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byx1VnR9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference/Paper1412/Reviewers", "ICLR.cc/2019/Conference/Paper1412/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1412/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1412/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1412/Authors|ICLR.cc/2019/Conference/Paper1412/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1412/Reviewers", "ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference/Paper1412/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626251}}}, {"id": "Syenv4xqR7", "original": null, "number": 1, "cdate": 1543271524078, "ddate": null, "tcdate": 1543271524078, "tmdate": 1543271524078, "tddate": null, "forum": "Byx1VnR9K7", "replyto": "SJeeDkc227", "invitation": "ICLR.cc/2019/Conference/-/Paper1412/Official_Comment", "content": {"title": "Thank you for your constructive feedback.", "comment": "Our model differs from Wang et al in the sense that our VAE is on the trajectory level, which enables it to better identify the latent variable that differentiates different behaviors from the whole trajectory.  Co-Reyes et al  also uses a trajectory-level VAE, but our work differs from theirs in that our model is fully probabilistic and consistent. Therefore no extra penalty term is needed as in Co-Reyes et al to force the state decoder to be consistent with the action decoder. \n\nThank you for the suggestions for the ablation studies. We will conduct a comprehensive analysis on the impact of the state decoder/policy decoder. \n\nWe will fix the typos, add the references and clarify experiment setup in a revision.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1412/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1412/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1412/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621626251, "tddate": null, "super": null, "final": null, "reply": {"forum": "Byx1VnR9K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference/Paper1412/Reviewers", "ICLR.cc/2019/Conference/Paper1412/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1412/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1412/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1412/Authors|ICLR.cc/2019/Conference/Paper1412/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1412/Reviewers", "ICLR.cc/2019/Conference/Paper1412/Authors", "ICLR.cc/2019/Conference/Paper1412/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621626251}}}, {"id": "SJeeDkc227", "original": null, "number": 3, "cdate": 1541345111737, "ddate": null, "tcdate": 1541345111737, "tmdate": 1541533153522, "tddate": null, "forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1412/Official_Review", "content": {"title": "Review", "review": "This paper presents an approach to multi-modal imitation learning by using a variational auto-encoder to embed demonstrated trajectories into a structured latent space that captures the multi-modal structure. This is done through a stochastic neural network with a bi-directional LSTM and mean pooling architecture that predicts the mean and log-variance of the latent state. This is followed by a state and action/policy decoder (both LSTMs) that recursively generate trajectories from latent space samples. The entire model is trained by optimising the ELBO on a set of pre-specified expert demonstrations. At test time, samples are generated from the latent space and recursively decoded to generate state and action trajectories. The method is tested on three low-dimensional continuous control tasks and is able to learn structured latent spaces capturing the modes in the training data as well as generating good trajectory reconstructions.\n\nLearning from multi-modal demonstration data is an important sub-area in imitation learning. As the paper pointed out, there has been a lot of recent work in this area. A lot of the ideas in this paper are similar to those proposed in prior work -- the network for embedding the trajectory is similar to the ones from Wang et al & Co-Reyes et al with the major difference being in the structure of the action decoder (and what inputs to encoder). Also, prior work has dealt with problems that are high-dimensional (Wang et al) and has shown results when operating directly on visual data (InfoGAIL). Comparatively, the results in this paper are on toy problems. \n\nAs there is no direct comparison to prior work provided in the paper, it is hard to quantify how much better the proposed approach is in comparison to prior work. For example, the \"2D Circle Example\" was taken from the InfoGAIL paper. It would have been good to use that as a baseline example to compare those two methods and highlight the advantages of the proposed approach -- did it require less data? fewer environment interactions? etc. \n\nThe results on the Zombie Attack Scenario seem poor. Specifically, in the avoid scenario, the approach seems to fail almost half the time. It would be good if the authors spend more time on this -- again, a comparison to prior work would establish some baselines and give us a good idea of the expected performance on this scenario. The videos show a single representative example for the \"Attack\" and \"Avoid\" scenarios. More examples including failures need to be included so that the distribution of results can be captured. \n\nThere is little in terms of generalisation or ablation studies in the paper. For example, in the Zombie Attack Scenario one could generate data with different zombie behaviours and measure performance on held out behaviours. Similarly, as an ablation, the authors could look at directly predicting actions instead of states & actions (states could be generated through a pre-trained dynamics model).\n\nFigure 6. is hard to parse and could be explained better. No details are provided on the network architecture (number/size of the LSTM/fully connected layers), number of demonstrations used, training algorithm, hyper-parameters etc. \n\nFew typos in the paper: \n  Page 6 - between the animation links 'avoiding' 'region'\n  Fig 7 caption - the zombie but are not in attacking range -> but the zombies are not in the attacking range,\n\nRelevant citations that can be added:\n1) Hausman, K., Chebotar, Y., Schaal, S., Sukhatme, G., & Lim, J. J. (2017). Multi-modal imitation learning from unstructured demonstrations using generative adversarial nets. In Advances in Neural Information Processing Systems (pp. 1235-1245).\n2) Tamar, A., Rohanimanesh, K., Chow, Y., Vigorito, C., Goodrich, B., Kahane, M., & Pridmore, D. (2018). Imitation Learning from Visual Data with Multiple Intentions.\n\nOverall, I find the paper to be incremental and lacking good experimental results and comparisons. The strengths of the paper are not clear and need to be explained and evaluated well. Substantial work is needed to significantly improve the paper before it can be accepted.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1412/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1412/Official_Review", "cdate": 1542234235136, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1412/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335941549, "tmdate": 1552335941549, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1412/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SyxpLO9_2m", "original": null, "number": 2, "cdate": 1541085269413, "ddate": null, "tcdate": 1541085269413, "tmdate": 1541533153285, "tddate": null, "forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1412/Official_Review", "content": {"title": "Good but generic model, contribution limited ", "review": "\nThis paper proposes a VAE for modelling state-action sequences using a single latent variable rather than one per timestep. The authors empirically demonstrate that this model works on toy 2D examples and a simplified 2D Minecraft-like environment. Although I am unaware of other works that use a VAE in this setting, the model is still quite generic, thus requires further application to justify its significance. This paper is clear and well written. \n\nThe current contribution of this paper is limited, however it could be improved in a number of ways. The main component lacking from this paper is a meaningful comparison to other related works. Its unclear what the advantage of this model is over other models and so a thorough comparison to other sequence models would really help this paper. As mentioned in the conclusion, another direction for this work would be to bootstrap reinforcement learning. If this bootrapping was demonstrated then it would make this paper\u2019s contribution stronger. Finally, another important direction for improvement for this paper would be to demonstrate its usefulness on more complex environments, instead of only 2D examples. \n\nPros:\n- clear and well written\n- model works on toy examples\nCons:\n- lack of baseline comparisons\n- lack of contributions\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1412/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1412/Official_Review", "cdate": 1542234235136, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1412/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335941549, "tmdate": 1552335941549, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1412/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Sye9uPhBn7", "original": null, "number": 1, "cdate": 1540896626326, "ddate": null, "tcdate": 1540896626326, "tmdate": 1541533153084, "tddate": null, "forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1412/Official_Review", "content": {"title": "Review", "review": "The paper proposes an imitation learning model able to generate trajectories based on some expert trajectories. The assumption is that observed trajectories contain multi-modal (i.e. style) information that is not naturally captured by existing methods. The authors proposed a VAE based architecture that uses a prior distribution P(z) to simultaneously generate (state-action) pairs based on a LSTM decoder (actually, one LSTM for the states and one interleaved LSTM for the actions). This decoder is learned using a classical VAE auto-encoding loss, observed trajectories being encoder through a bi-LSTM. Experiments are made on three toy examples: a simple 2d Navigation case exhibiting 3 different 'styles', a 2D circle example with also 3 different styles, and a zombie attack scenario with two different styles. The results show that the model is able to capture different clusters of trajectories. \n\nFirst of all, the paper does not propose a new model, but an instantiation of an existing model to a particular case. The main difference with SoTA is that the authors propose to both decode states and actions without using a simulator. The contribution of the paper is thus quite light. Moreover, it is unclear how the model can be used to get a policy corresponding to a particular mode. Can we use the learned decoders to generate actions on-the-fly in a real/simulated environment? Right now (section 3.3), actions are generated on generated states, but not on observed ones.  The paper has to clarify this point since just generating trajectories seems to be a little bit useless. In general Section 3.3 lacks of details (e.g the rolling window is also unclear). Also, the model could be described a little bit more in term of architecture, particularly on the critical point about how the two decoding LSTMs are interacting. \n\nFrom the experimental point of view, the paper attacks very simple cases, without any comparison with state-of-the-art, and without almost any quantitative results. If Section 4.1 and 4.2 are useful to explore the ability of the model on simple cases, I would recommend the authors to merge these two sections in one smaller one, and then to focus on more realistic experiments. For example, it seems to me that the experimental setting proposed for example in [Li et al.] on driving styles could be interesting, and would allow a comparison with existing methods. Also the model proposed in [Co-Reyes et al.] could be an interesting comparison (at least, keeping the principle of this paper, without the hierarchical structure), particularly because this model is based on the use of a simulator while the proposed one is not. If a performance close to this baseline can be obtained with your model, it would be interesting for the community.\n\nRight now, the experimental part and the too small contribution of the paper are not enough for acceptance. I would suggest the authors to:\n* better describe their contribution i.e model architecture and how the model can be used to obtain a real policy\n* use 'stronger' use cases for the experiments, and particularly existing use cases\n* provide a deep quantitative and qualitative comparison with SoTA\n\nPro:\n* simple method, no need of a simulator\n\nCons:\n* not clear how to move from trajectory generation to a real policy\n* small contribution\n* too light experimental study without comparison with baselines and state of the art\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1412/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Trajectory VAE for multi-modal imitation", "abstract": "We address the problem of imitating multi-modal expert demonstrations in sequential decision making problems. In many practical applications, for example video games, behavioural demonstrations are readily available that contain multi-modal structure not captured by typical existing imitation learning approaches. For example, differences in the observed players' behaviours may be representative of different underlying playstyles.\n\n In this paper, we use a generative model to capture different emergent playstyles in an unsupervised manner, enabling the imitation of a diverse range of distinct behaviours. We utilise a variational autoencoder to learn an embedding of the different types of expert demonstrations on the trajectory level, and jointly learn a latent representation with a policy. In experiments on a range of 2D continuous control problems representative of Minecraft environments, we empirically demonstrate that our model can capture a multi-modal structured latent space from the demonstrated behavioural trajectories. ", "keywords": ["imitation learning", "latent variable model", "variational autoencoder", "diverse behaviour"], "authorids": ["xiaoyu.lu@stats.ox.ac.uk", "t-jastuh@microsoft.com", "katja.hofmann@microsoft.com"], "authors": ["Xiaoyu Lu", "Jan Stuehmer", "Katja Hofmann"], "TL;DR": "A trajectory-VAE method for imitating multi-modal expert demonstrations in sequential decision making problems.", "pdf": "/pdf/52c9267181e391cc67d809146013480f00272fa9.pdf", "paperhash": "lu|trajectory_vae_for_multimodal_imitation", "_bibtex": "@misc{\nlu2019trajectory,\ntitle={Trajectory {VAE} for multi-modal imitation},\nauthor={Xiaoyu Lu and Jan Stuehmer and Katja Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=Byx1VnR9K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1412/Official_Review", "cdate": 1542234235136, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "Byx1VnR9K7", "replyto": "Byx1VnR9K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1412/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335941549, "tmdate": 1552335941549, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1412/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}