{"notes": [{"id": "H1e6ij0cKQ", "original": "H1g3pscctm", "number": 662, "cdate": 1538087844749, "ddate": null, "tcdate": 1538087844749, "tmdate": 1545355393580, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkgBH-9gxE", "original": null, "number": 1, "cdate": 1544753469212, "ddate": null, "tcdate": 1544753469212, "tmdate": 1545354517695, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Meta_Review", "content": {"metareview": "this is an interesting approach to use reinforcement learning to replace CRF for sequence tagging, which would potentially be beneficial when the tag set is gigantic. unfortunately the conducted experiments do not really show this, which makes it difficult to see whether the proposed approach is indeed a viable alternative to CRF for sequence tagging with a large tag set. this sentiment was shared by all the reviewers, and R1 especially pointed out major and minor issues with the submission and was not convinced by the authors' response.", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "mismatched goals, evaluation and comparison"}, "signatures": ["ICLR.cc/2019/Conference/Paper662/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper662/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353134439, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper662/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper662/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper662/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353134439}}}, {"id": "ByeZJ0I_C7", "original": null, "number": 4, "cdate": 1543167449105, "ddate": null, "tcdate": 1543167449105, "tmdate": 1543167449105, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "SygQFc2_pX", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "content": {"title": "Clarity", "comment": "\"- Our main goal is to replace CRF in tagging, especially for tasks with large number of labels. `\"\n\nBut then you should look at tagging tasks which is what (neural) CRF approaches are good for; transliteration, as pointed out in the introduction of the paper is not such a task. \n\n- \"Combining MLE and RL requires two forward computation in decoder RNN, one conditioning on gold standard, another on model-generated tokens. \"\n\nAnd why is this a problem? Assuming it is the extra computation, then there should be a careful comparison on processing time needed between the methods. \n\n\"- Reward reshaping can be used to convert non-decomposable loss functions such as BLEU into step-size rewards. Reward(t) = BLEU(t) - BLEU(t-1)\"\n\nBut this is an approximation, so whether it will work depends on many factors. In any case, this sounds more like an argument on BLEU's flexibility not of the proposed method.\n\n-  \" There is no preprocess alignment done for CRF in transliteration. \"\n\nThe paper states: \"To do so, we pad both sequences with extra end symbols up to a\nfixed maximum length, and let CRF decode until the end of the padded source sequence.\"\nAs CRF is a tagging method, somehow there is an alignment of input to output characters, which is why you need the padding."}, "signatures": ["ICLR.cc/2019/Conference/Paper662/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper662/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624216, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1e6ij0cKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper662/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper662/Authors|ICLR.cc/2019/Conference/Paper662/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624216}}}, {"id": "SygQFc2_pX", "original": null, "number": 3, "cdate": 1542142586899, "ddate": null, "tcdate": 1542142586899, "tmdate": 1542142586899, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "HJxbDOnEnm", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "content": {"title": "Clarity", "comment": "Thank so much for your insightful comments. \n-  \n- Our main goal is to replace CRF in tagging, especially for tasks with large number of labels. `\n\n- MIXER uses REINFORCE, table 1illustrates that REINFORCE family fails compared to actor-critic. \n We both have a regressor as critic, but MIXER doesn't bootstrap its estimates in the computed returns.\n\n- Combining MLE and RL requires two forward computation in decoder RNN, one conditioning on gold standard, another on model-generated tokens. \n\n- The prediction is correct, but the advantage given by the critic is wrong, so we skip the update on actor. \n\n- Average over 20 runs, on epoch 13, adjusted actor-critic are better than actor-critic and reinforce models.\n\n- Reward reshaping can be used to convert non-decomposable loss functions such as BLEU into step-size rewards. \nReward(t) = BLEU(t) - BLEU(t-1)\n\n- End-to-End training time and the required GPU memory for one training epoch on CCG supertagging. \n\n- We tried the open source code for AC of Bahdanau on transliteration, but it completely failed.  We only obtained dev and test split from Leblonde et al. (2018) on spelling correction dataset.\n\n- We will review the mentioned paper. \n\n- Scheduled Sampling is inspired by Dagger.\n\n- Character embeddings are fine-tuned during training.\n\n- There is no preprocess alignment done for CRF in transliteration. "}, "signatures": ["ICLR.cc/2019/Conference/Paper662/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624216, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1e6ij0cKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper662/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper662/Authors|ICLR.cc/2019/Conference/Paper662/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624216}}}, {"id": "BkgkqFBq3Q", "original": null, "number": 3, "cdate": 1541196166660, "ddate": null, "tcdate": 1541196166660, "tmdate": 1541533796322, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Official_Review", "content": {"title": "Actor critic for sequence labeling; not very novel but good results on transliteration; inadequate comparison", "review": "The authors propose actor-critic method for sequence labeling and show that it performs better (is more stable than) other RL approaches and also outperforms other techniques for countering exposure bias like scheduled sampling.\n\nThe results show very small improvement in tagging tasks like NER and CCG supertagging compared to other approaches ; but they show good improvement in the transliteration task which is more of a transduction task than a tagging task. \n\nThis authors also discus the adjusted training procedure which accounts for bad performance of the critic model in the initial stages of training. The approach is not very novel because actor-critic for more general sequence-to-sequence models (arguably more complex than tagging) has already been explored in the literature (Bahdanau et al., cited by the authors). Major difference in the proposed approach is the use of stepwise hamming-loss based reward and it is unclear whether this is a major contribution which  sets it apart from the previous work on AC for sequence modeling. For example, a good comparison would be to do tagging in seq2seq style and use the approach proposed in the existing AC work to show the value of the approach proposed here.\n\nAlso, minor claims about thoroughness of comparison with CRF are ill-founded as previous published work on tagging has indeed compared CRFs, independent, LSTM/RNN based models.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper662/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Official_Review", "cdate": 1542234408516, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper662/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335774960, "tmdate": 1552335774960, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper662/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1e59mLw27", "original": null, "number": 2, "cdate": 1541002130135, "ddate": null, "tcdate": 1541002130135, "tmdate": 1541533796121, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Official_Review", "content": {"title": "The paper pretenses reinforcement learning algorithms for dealing with the \"exposure bias\" problem of RNNs in sequence labeling tasks. The paper suffers from clarity issues - for example it was hard to understand the exposure bias problem. I also miss an important comparison in the experimental section - to the LSTM-CRF model.", "review": "The paper pretenses reinforcement learning algorithms for dealing with the \"exposure bias\" problem of RNNs in sequence labeling problems.  While I admire the thoroughness of  both the algorithmic work and experimental setup, I am afraid the paper suffers from two major problems:\n\n1. The paper suffers from serious clarity issues. Particularly, the main problem the paper deals with - exposure bias- is not well explained. I admit that while I am working with RNNs on a regular basis, I was not familiar with this problem. Unfortunately, I was also not able to understand it from the paper.  This may be a very basic concept, but a paper must be self-contained. Unfortunately, after reading the paper, front to cover, I cannot tell what is the problem the authors are trying to solve (except, of course, from providing a better training algorithm for RNNs).\n\n2. As the authors say already in the abstract, one of the best performing models on structured NLP tasks is LSTM-CRF, which combines the power of both the neural and the structured prediction frameworks. However, the authors do not compare their solution to LSTM-CRF, but only to LSTM and to CRF. This is a very important baseline, and without a proper comparison it is hard to evaluation the contribution of this paper.\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper662/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Official_Review", "cdate": 1542234408516, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper662/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335774960, "tmdate": 1552335774960, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper662/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJxbDOnEnm", "original": null, "number": 1, "cdate": 1540831321508, "ddate": null, "tcdate": 1540831321508, "tmdate": 1541533795919, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Official_Review", "content": {"title": "Unclear focus of the paper: tagging or sequence generation? Comparisons not informative", "review": "I found the paper difficult to follow. The method proposed is not well motivated, and  the literature review explains well the novelty. Here are some questions/points for discussion:\n\n- the token-level MLE training is not what causes the exposure bias: one can train with MLE and still avoid it by generating appropriate sequences using the RNN, as in scheduled sampling. The problem with MLE (or cross entropy) is that the labels to be predicted might not be the correct ones. See the paper by Ranzato et al. (ICLR2016) for a good discussion of the issue: https://arxiv.org/pdf/1511.06732.pdf\n\n- The criticism against previous works for not comparing agains CRFs seems odd: CRFs are given the number of labels, words, etc. to predict, typically the same as the number of words to be tagged. If one  has this, as well as binary rewards for each decision, then there is little benefit for RL/IL based approaches to be used. The point for them is the use of non-decomposable loss functions such as BLEU, which are not common in tagging, but in tasks like MT, where CRFs can't be used. In fact, for the transliteration experiments in the paper, the CRF approach is padded to perform the task, which highlights that it is not the right comparison. \n\n- the approach proposed seems very similar to MIXER, which also learns a regressor to predict the reward for each action. A direct comparison both in terms of how the approaches operate and empirically is needed.\n\n- why is it a problem that previous works by Ranzato, Bahdanau and Paulus combine MLE and RL? You are using the same supervision, ie. the labeled corpus.\n\n- the adjusted training seems to essentially not reward correct predictions (top branch in the equation). Why is this a good idea?\n\n- In figure 1 it is not clear at all that the proposed approach works; depending on the epoch the ranking among the three variants differs\n\n\n- what does it mean for one method to surpass the other in flexibilty? If anything the requirement for immediate rewards after every action restricts flexibility, as one can't use non-decomposable loss functions such as BLEU which are prety common in NLP.\n\n- How is the training efficiency measured in the paper?\n\n- Why not compare against MIXER, as well as more recent work by Leblonde et al. (2018): https://arxiv.org/abs/1706.04499 ? I don't see why the Rennie et al. 2017 method is picked for comparison.\n\n- It is not true that in IL one needs a gold standard policy, one can learn with sub-optimal policies, see Sun et al. (2018): https://arxiv.org/pdf/1703.01030.pdf\n\n- It is odd to say that an approach proposed earlier (Dagger) reduces to a variant of a later proposed one (Scheduled sampling), the reduction should be the other way around\n\n- are the randomly initialized character embeddings for transliteration tuned during training?\n\n- How were the alignments for training the CRF obtained?\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper662/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Official_Review", "cdate": 1542234408516, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1e6ij0cKQ", "replyto": "H1e6ij0cKQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper662/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335774960, "tmdate": 1552335774960, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper662/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxxaVYCh7", "original": null, "number": 2, "cdate": 1541473463698, "ddate": null, "tcdate": 1541473463698, "tmdate": 1541473463698, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "BkgkqFBq3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "content": {"title": "Clarity", "comment": "Our main goal is to replace CRF in tagging, especially for tasks with large number of labels. `\nWe would appreciate if we can be referred to an existing paper comparing Seq2Seq with encoder RNN + CRF decoding."}, "signatures": ["ICLR.cc/2019/Conference/Paper662/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624216, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1e6ij0cKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper662/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper662/Authors|ICLR.cc/2019/Conference/Paper662/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624216}}}, {"id": "rygn_-YR37", "original": null, "number": 1, "cdate": 1541472627766, "ddate": null, "tcdate": 1541472627766, "tmdate": 1541472627766, "tddate": null, "forum": "H1e6ij0cKQ", "replyto": "H1e59mLw27", "invitation": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "content": {"title": "Clarity", "comment": "1- The concern is well understood, though related works have already defined it clearly. The bias is originated from the method of training, not the RNN itself.\n\n2-Throughout the paper:\n\nCRF: LSTM encoder + CRF decoding with MLE training \nRNN: LSTM encoder + LSTM decoder with MLE training\nAC-RNN: LSTM encoder + LSTM decoder with MLE & Actor-Critic training"}, "signatures": ["ICLR.cc/2019/Conference/Paper662/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EFFICIENT SEQUENCE LABELING WITH ACTOR-CRITIC TRAINING", "abstract": "Neural approaches to sequence labeling often use a Conditional Random Field (CRF) to model their output dependencies, while Recurrent Neural Networks (RNN) are used for the same purpose in other tasks. We set out to establish RNNs as an attractive alternative to CRFs for sequence labeling. To do so, we address one of the RNN\u2019s most prominent shortcomings, the fact that it is not exposed to its own errors with the maximum-likelihood training. We frame the prediction of the output sequence as a sequential decision-making process, where we train the network with an adjusted actor-critic algorithm (AC-RNN). We comprehensively compare this strategy with maximum-likelihood training for both RNNs and CRFs on three structured-output tasks. The proposed AC-RNN efficiently matches the performance of the CRF on NER and CCG tagging, and outperforms it on Machine Transliteration. We also show that our training strategy is significantly better than other techniques for addressing RNN\u2019s exposure bias, such as Scheduled Sampling, and Self-Critical policy training.\n", "keywords": ["Structured Prediction", "Reinforcement Learning", "NLP"], "authorids": ["snajafi@ualberta.ca", "colin.a.cherry@gmail.com", "gkondrak@ualberta.ca"], "authors": ["Saeed Najafi", "Colin Cherry", "Greg Kondrak"], "pdf": "/pdf/fdb32ddb4a126c40b5a21a39393cac0d57cd1a88.pdf", "paperhash": "najafi|efficient_sequence_labeling_with_actorcritic_training", "_bibtex": "@misc{\nnajafi2019efficient,\ntitle={{EFFICIENT} {SEQUENCE} {LABELING} {WITH} {ACTOR}-{CRITIC} {TRAINING}},\nauthor={Saeed Najafi and Colin Cherry and Greg Kondrak},\nyear={2019},\nurl={https://openreview.net/forum?id=H1e6ij0cKQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper662/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621624216, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1e6ij0cKQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper662/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper662/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper662/Authors|ICLR.cc/2019/Conference/Paper662/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper662/Reviewers", "ICLR.cc/2019/Conference/Paper662/Authors", "ICLR.cc/2019/Conference/Paper662/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621624216}}}], "count": 9}