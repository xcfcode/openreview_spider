{"notes": [{"id": "BJGjOi09t7", "original": "SylKn7ccKQ", "number": 385, "cdate": 1538087795082, "ddate": null, "tcdate": 1538087795082, "tmdate": 1545355400332, "tddate": null, "forum": "BJGjOi09t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ByxQTPVxeV", "original": null, "number": 1, "cdate": 1544730554977, "ddate": null, "tcdate": 1544730554977, "tmdate": 1545354512178, "tddate": null, "forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Meta_Review", "content": {"metareview": "The paper introduces a variant of the variational autoencoder (VAE) for probabilistic non-negative matrix factorization. The main idea is to use a Weibull distribution in the latent space. There is agreement among the reviewers that the paper is technically sound and well written, but that it lacks in motivation and demonstration of utility of the proposed method.\nAll the reviewers think the approach is not particularly novel and somewhat incremental. The main issue is that the empirical evaluation of the algorithm is also quite limited. Specifically, it should have been compared with Bayesian NMF. Many papers have addressed Bayesian NMF with variational inference (Cemgil; Fevotte & Dikmen; Hoffman, Blei & Cook) like in VAE. Experimentally, Bayesian NMF and the proposed PAE-NMF could easily be quantitatively compared on matrix completion tasks. Overall, there was consensus among the reviewers that the paper is not ready for publication.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Somewhat incremental and Missing NMF baselines"}, "signatures": ["ICLR.cc/2019/Conference/Paper385/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper385/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353234763, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper385/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper385/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper385/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353234763}}}, {"id": "ryxLyMtQyN", "original": null, "number": 6, "cdate": 1543897565916, "ddate": null, "tcdate": 1543897565916, "tmdate": 1543897565916, "tddate": null, "forum": "BJGjOi09t7", "replyto": "S1xSbZZl0X", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Official_Comment", "content": {"title": "Thank you", "comment": "Thank you for the clarification"}, "signatures": ["ICLR.cc/2019/Conference/Paper385/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper385/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper385/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610707, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJGjOi09t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper385/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper385/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper385/Authors|ICLR.cc/2019/Conference/Paper385/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610707}}}, {"id": "rkx-tS5FCm", "original": null, "number": 5, "cdate": 1543247225164, "ddate": null, "tcdate": 1543247225164, "tmdate": 1543247225164, "tddate": null, "forum": "BJGjOi09t7", "replyto": "SJl1lttvAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Official_Comment", "content": {"title": "Further rebuttal from authors", "comment": "Thank you for the reply to our response.\n\nComment 1. For the update of W, it would be better to perform optimization in a constrained non-negative sparse than to force the negative values to zero.\n\nResponse: In previous work on using autoencoders for NMF (not published yet but we intend to publish sometime soon) we investigated the use of multiplicative updates for the W matrix, which guarantees to keep the W matrix non-negative. We did not see any major differences in results when performing that and when doing a gradient descent followed by setting negative terms back to zero, which is also faster than the multiplicative update method. \n\nComment 2. Since the L2 loss corresponds to a Gaussian model, the probability for v to be negative is not zero, which is again a little bit unpleasant if the data are inherently non-negative.\n\nResponse: By construction both h and W are non-negative so it is not possible for the reconstruction to be negative. Within the minimum description length approach, the probability distribution of the error just specifies the message length for sending the errors. The more accurate the distribution the shorter the message length. We could spend more time understanding the distributions of errors, but this would in our judgment give us little gain and take focus away from the main message of the paper (In a full MDL approach we would have to transmit the parameters of the distribution for errors. If we use complicated distributions this can lead to an increase in the description length. It is an interesting, but non-trivial task to study methods for communicating errors, but given the page restrictions it does not seem appropriate in this submission). We would like to point out that nothing \"unpleasant\" happens. The algorithm is perfectly stable. If we used, for example, a truncated Gaussian this would again lead to an L2 loss with an additional constant factor that would not change anything.\n\n\nComment 3. The advantages of the proposed method need to be better demonstrated in the updated version of this paper. \nResponse: we have added a paragraph to the motivation into our new manuscript which we will upload. There are two obvious comparisons to be made for PAE-NMF: 1) with NMF and 2) with probabilistic forms of NMF:\n1st comparison with NMF: it is clear where the advantages lie: we can now sample from the distribution, get a probabilistic link between inputs and latent variables, and because of the KL-divergence term have an automatic way of regularising the method. \n2nd comparison with probabilistic NMF: here the advantages of our method are more subtle but they are effectively equivalent to the advantages of a VAE over other probabilistic methods, which are explained in \u201cAuto-Encoding Variation Bayes\u201d by Kingma and Welling. \n\nAs ever with NMF it is difficult to conclusively prove that one solution is better than another empirically as there is no ground truth for most real world data sets. However, there are important choices in running NMF that gives significantly different results. This paper proposes a method that introduces a principled self-regularisation scheme that encodes the uncertainty in the model through a probability distribution on h. The main argument for doing this is a logical argument, although as much as possible we have tried to support this empirically. We believe our method is novel and deserves to be known. Superficially it may seem that what we are proposing provides nothing different from what a Bayesian approach gives. However, as we argue in the updated paper, Bayesian approaches suffer from requiring a good prior, but in most applications it is very unclear how one would obtain such a prior. This would force the use of model selection techniques, but these would seem to be extremely expensive as it would require finding the full posterior and then require the optimisation of hyper-parameters. We believe our approach provides substantial advantages over any other method that we are aware of."}, "signatures": ["ICLR.cc/2019/Conference/Paper385/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper385/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621610707, "tddate": null, "super": null, "final": null, "reply": {"forum": "BJGjOi09t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper385/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper385/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper385/Authors|ICLR.cc/2019/Conference/Paper385/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621610707}}}, {"id": "S1xSbZZl0X", "original": null, "number": 3, "cdate": 1542619388896, "ddate": null, "tcdate": 1542619388896, "tmdate": 1542619388896, "tddate": null, "forum": "BJGjOi09t7", "replyto": "H1xMbCCbsX", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Public_Comment", "content": {"comment": "We thank this reviewer for their generous review.\n\nComment: The presentation of the VAE objective is a bit oblique. The statement \"they require a different objectiv function\" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)\nOur response: the first point here is very valid \u2013 that is an imprecise sentence and we will improve it. The second point is also correct,  we should not have had an equality there as it is an approximation, we will also fix that in the manuscript.\n\nComment: \"with \\hat v the reconstructed vector\" Not clear. I assume \\hat v is reconstructed from a sample from q given v ?\nOur response: yes that is correct, we will add a sentence about how vhat is formed there.\n\nComment: There is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\\lambda_2)^(k_2)\nOur response: Thank you very much for spotting that typo, we will correct it.", "title": "Author response to reviewer"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311852541, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJGjOi09t7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311852541}}}, {"id": "HklILgbxCX", "original": null, "number": 1, "cdate": 1542619214316, "ddate": null, "tcdate": 1542619214316, "tmdate": 1542619355723, "tddate": null, "forum": "BJGjOi09t7", "replyto": "BklPXKoVh7", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Public_Comment", "content": {"comment": "Thank you very much for the review. We will answer each comment in turn.\n\nComment 1: the motivation of the proposed methodology is not clear to me. What is the interest of the proposed auto-encoding strategy w.r.t NMF ? There is no experimental comparison either. Besides the probabilistic embedding (which exists in NMF as well), is there something PAE-NMF can do better than NMF ? There is probably something, but the paper does not bring a convincing answer.\nOur response: Although probabilistic methods for NMF have been developed, even a full Bayesian framework still faces the problem that, for the vast majority of problems where NMF is used, we have little idea about what is the appropriate prior. We would therefore be forced to do model selection or introduce hyperparameters and perform an inference (maximum likelihood or Bayesian) based on the evidence. However, as the posterior is such cases is unlikely to be analytic this is likely to involve highly time consuming Monte Carlo. In doing so we would expect to get results close to those we obtain. However, for machine learning algorithms to be of value they must be practical. Our approach, following a minimum description length methodology, provides a principled method for achieving automatic regularisation. Because it fits within the framework of deep learning it is relatively straightforward and quick to implement (using software such as Keras or pytorch with built-in automatic differentiation, fast gradient descent algorithms, and GPU support). In addition our approach provides a considerable degree of flexibility (e.g. in continuous updating, allowing exogenous data, etc.), which we believe might be much more complicated to achieve in a fully probabilistic approach. Obviously we failed to properly articulate these advantages in our original submission and will correct this when we submit our corrections.\n\nIn terms of experimental comparison \u2013 that is always challenging in unsupervised learning, especially with limited space to go deeply into the comparison. We could add equivalent NMF results to some of the figures we provide, however, we do not think it would add significantly to the purpose of the paper which is to introduce the method and idea behind PAE-NMF.\n\nComment 2: the paper missed important references to nonnegative auto-encoders, in particular:\nhttps://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf (https://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf)\nOur response: This is a very fair point, we are happy to add in this reference to this paper.\n\nComment3 : the review of probabilistic NMF works is limited, see e.g.,\nhttps://paris.cs.illinois.edu/pubs/smaragdis-spm2014.pdf\nOur response: We are happy to extend our review of probabilistic NMF in this paper but should note that we are at the limit of 8 pages at the moment and there will always be a limitation on what can be added.\n\nComment:4  more details are needed about inference in Section 2.4\nOur response: we certainly could write considerably more but as our method is based upon the work of VAEs we did not want to go over work that has already been done and discussed. Also, we are at the edge of the page limit. \n\nMinor comments:\nMinor Comment: the notations z and h are sometimes confusing, what about using h every where ?\nOur response: The use of z was due to its familiarity to researchers with a background in VAEs. If this was confusing we are happy to make this change whilst altering our paper.\n\nMinor Comment: it\u2019s not clear to me how the first term in (1) is equal to the second term in (2\nOur response: this should have been an approximation sign rather than an equals sign. We are assuming (not an unreasonable assumption) that the errors, the gap between v-hat and v, will be approximately Gaussian. This is a common assumption made in nearly all VAEs.", "title": "Author reply to reviewer"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311852541, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJGjOi09t7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311852541}}}, {"id": "S1l_jl-xRX", "original": null, "number": 2, "cdate": 1542619295598, "ddate": null, "tcdate": 1542619295598, "tmdate": 1542619341880, "tddate": null, "forum": "BJGjOi09t7", "replyto": "rylhntPTs7", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Public_Comment", "content": {"comment": "Thank you very much for the review. We will answer each comment in turn.\n\nComment 1. What is the updating rule for W_f? Is it multiplicative? In Sec 2.4, The value of W_f is kept to be nonnegative by \"setting negative terms to zero\". Does it mean once one entry is set to zero, it would never be positive in the sequential gradient steps?\nOur response: we use a gradient descent method, not a multiplicative update rule which means that values of Wf which have gone to zero can become non-zero again. We did not discuss this in our paper as it is standard in neural networks to use gradient descent (or variants of) methods, while in NMF methods such as Lee and Seung\u2019s multiplicative update is commonly used. We will add a brief mention of the update method in our resubmitted paper.\n\nComment 2. Although the proposed model is claimed to be probabilistic, the L2 loss function in equation (2) implies that the data generated from the model could be negative. How would the proposed approach handle other loss function of NMF such as KL (e.g., under Poisson assumption)?\nOur response: we are a little unclear about the first part of this comment. The data, v-hat, generated by the model cannot be negative because the output v-hat=Wf*h where h is draw from the Weibull distribution, which is non-negative, and Wf is forced to remain non-negative by the algorithm. It is certainly true that the difference between v-hat and v can be positive or negative. We have not investigated the use of the KL divergence for the error term but we see no obvious reason why this would not work effectively.\n\nComment 3. The nonnegative variant sounds interesting, but the experimental results are quite limited. It is unclear how the proposed approach would compare to other probabilistic NMF models and algorithms, or the standard VAE as a generative model. It seems the proposed method can do as good as NMF or VAE in some aspects. This begs the question of when would the proposed approach be superior to others and in what aspect?\nOur response: here we refer to our reply to Comment 2 from the first reviewer and copy the same comment below. The experiments are there to back up the idea which is the main part of the paper. It is challenging in many unsupervised learning problems to conclusively show the technique is effective. In this case we were trying to demonstrate some key features of our method.\n\nAlthough probabilistic methods for NMF have been developed, even a full Bayesian framework still faces the problem that, for the vast majority of problems where NMF is used, we have little idea about what is the appropriate prior. We would therefore be forced to do model selection or introduce hyperparameters and perform an inference (maximum likelihood or Bayesian) based on the evidence. However, as the posterior is such cases is unlikely to be analytic this is likely to involve highly time consuming Monte Carlo. In doing so we would expect to get results close to those we obtain. However, for machine learning algorithms to be of value they must be practical. Our approach, following a minimum description length methodology, provides a principled method for achieving automatic regularisation. Because it fits within the framework of deep learning it is relatively straightforward and quick to implement (using software such as Keras or pytorch with built-in automatic differentiation, fast gradient descent algorithms, and GPU support). In addition our approach provides a considerable degree of flexibility (e.g. in continuous updating, allowing exogenous data, etc.), which we believe might be much more complicated to achieve in a fully probabilistic approach. Obviously we failed to properly articulate these advantages in our original submission and will correct this when we submit our corrections.\n\nMinor Comments:\nIn some places where the parameter are constrained to be nonnegative, it would be more clear to use notations such as R_+ instead of R.\nOur response: a fair point which could add to clarity \u2013 we will make this change in our manuscript.", "title": "Author response to reviewer"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311852541, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "BJGjOi09t7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper385/Authors", "ICLR.cc/2019/Conference/Paper385/Reviewers", "ICLR.cc/2019/Conference/Paper385/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311852541}}}, {"id": "BklPXKoVh7", "original": null, "number": 3, "cdate": 1540827422789, "ddate": null, "tcdate": 1540827422789, "tmdate": 1541534039996, "tddate": null, "forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Official_Review", "content": {"title": "lacking motivation and comparisons", "review": "The paper is generally well-written (lacking details in some sections though). My main criticism is about the lack of motivation for nonnegative VAE and lack of comparison with NMF.\n\nComments:\n- the motivation of the proposed methodology is not clear to me. What is the interest of the proposed auto-encoding strategy w.r.t NMF ? There is no experimental comparison either. Besides the probabilistic embedding (which exists in NMF as well), is there something PAE-NMF can do better than NMF ? There is probably something, but the paper does not bring a convincing answer.\n- the paper missed important references to nonnegative auto-encoders, in particular:\nhttps://paris.cs.illinois.edu/pubs/paris-icassp2017.pdf\n- the review of probabilistic NMF works is limited, see e.g.,\nhttps://paris.cs.illinois.edu/pubs/smaragdis-spm2014.pdf\n- more details are needed about inference in Section 2.4\n\nMinor comments:\n- the notations z and h are sometimes confusing, what about using h every where ?\n- it\u2019s not clear to me how the first term in (1) is equal to the second term in (2", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper385/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Official_Review", "cdate": 1542234473537, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper385/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335712164, "tmdate": 1552335712164, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper385/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylhntPTs7", "original": null, "number": 2, "cdate": 1540352436387, "ddate": null, "tcdate": 1540352436387, "tmdate": 1541534039795, "tddate": null, "forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Official_Review", "content": {"title": "Experimental evaluations are mostly qualitative", "review": "This paper replaces the Gaussian latent variables in standard VAE with the Weibull distribution and therefore presents a VAE solution to nonnegative matrix factorization, under squared Euclidean distance loss function. The literature review summarizes four related work very well. The adopted Weibull distribution provides a tractable inverse CDF function and analytical form of the KL divergence, facilitating VAE inference. In particular, the effects of the entropy term are discussed in detail.  Experiments illustrate the generated data from the model,  the learned part-based dictionaries, and the distributions of latent variables from similar data points.  \n\nQuestions: \n\n1. What is the updating rule for W_f? Is it multiplicative?  In Sec 2.4, The value of W_f is kept to be nonnegative by \"setting negative terms to zero\". Does it mean once one entry is set to zero, it would never be positive in the sequential gradient steps? \n\n2. Although the proposed model is claimed to be probabilistic, the L2 loss function in equation (2) implies that the data generated from the model could be negative. How would the proposed approach handle other loss function of NMF such as KL (e.g., under Poisson assumption)?  \n\n3. The nonnegative variant sounds interesting, but the experimental results are quite limited. It is unclear how the proposed approach would compare to other probabilistic NMF models and algorithms, or the standard VAE as a generative model. It seems the proposed method can do as good as NMF or VAE in some aspects. This begs the question of when would the proposed approach be superior to others and in what aspect? \n\nMinor: \nIn some places where the parameter are constrained to be nonnegative, it would be more clear to use notations such as R_+ instead of R.  ", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper385/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Official_Review", "cdate": 1542234473537, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper385/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335712164, "tmdate": 1552335712164, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper385/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xMbCCbsX", "original": null, "number": 1, "cdate": 1539595769749, "ddate": null, "tcdate": 1539595769749, "tmdate": 1541534039568, "tddate": null, "forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "invitation": "ICLR.cc/2019/Conference/-/Paper385/Official_Review", "content": {"title": "Well written, interesting new idea, modest technical contribution, limited demonstration.", "review": "TITLE\nA VARIATIONAL AUTOENCODER FOR PROBABILISTIC NON-NEGATIVE MATRIX FACTORISATION\n\nREVIEW SUMMARY\n\nWell written, interesting new idea, modest technical contribution, limited demonstration.\n\nPAPER SUMMARY\n\nThe paper presents an approach to NMF within a variational autoencoder framework. It uses a Weibull distribution in the latent space. \n\nQUALITY\n\nThe work appears technically sound except for minor typos. \n\nCLARITY\n\nOverall the paper is a pleasure to read. Only the presentation of the standard vae could be more clear.\n\nORIGINALITY\n\nThe method is (to my knowledge) novel. \n\nSIGNIFICANCE\n\nI think this paper is a significant contribution. I feel I have learned something from reading it, and am motivated to try out this approach. I believe there should be a wide general interest. The technical contribution is perhaps somewhat modest, as the paper fairly straightforwardly includes non-negativity in a vae setting, but I think this is a good idea. The demonstration of the algorithm is also quite limited - I would have enjoyed seeing this applied to some more reaslistic, practical problems, where perhaps the quantification of uncertaincy (which is one of the main benefits of a vae-based nmf) would come more directly into play. \n\nFURTHER COMMENTS\n\npage 3\n\nThe presentation of the VAE objective is a bit oblique. The statement \"they require a different objectiv function\" is not wrong, but also not very precise. The equality in eq. (2) is incorrect (I assume this is meant to be a stochastic approximation, i.e. the expectation over q approximated by sampling?)\n\n\"with \\hat v  the reconstructed vector\" Not clear. I assume \\hat v is reconstructed from a sample from q given v ?\n\nThere is a typo in eq. 3. The first factor in the second to last term should be (lambda_1/\\lambda_2)^(k_2)\n\n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper385/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation", "abstract": "We introduce and demonstrate the variational autoencoder (VAE) for probabilistic non-negative matrix factorisation (PAE-NMF). We design a network which can perform non-negative matrix factorisation (NMF) and add in aspects of a VAE to make the coefficients of the latent space probabilistic. By restricting the weights in the final layer of the network to be non-negative and using the non-negative Weibull distribution we produce a probabilistic form of NMF which allows us to generate new data and find a probability distribution that effectively links the latent and input variables. We demonstrate the effectiveness of PAE-NMF on three heterogeneous datasets: images, financial time series and genomic.", "keywords": ["Non-negative matrix factorisation", "Variational autoencoder", "Probabilistic"], "authorids": ["ses2g14@ecs.soton.ac.uk", "apb@ecs.soton.ac.uk", "mn@ecs.soton.ac.uk"], "authors": ["Steven Squires", "Adam Prugel-Bennett", "Mahesan Niranjan"], "pdf": "/pdf/fbb739d61b4b03aa36c9892bd691cecc79f5e3f2.pdf", "paperhash": "squires|a_variational_autoencoder_for_probabilistic_nonnegative_matrix_factorisation", "_bibtex": "@misc{\nsquires2019a,\ntitle={A Variational Autoencoder for Probabilistic Non-Negative Matrix Factorisation},\nauthor={Steven Squires and Adam Prugel-Bennett and Mahesan Niranjan},\nyear={2019},\nurl={https://openreview.net/forum?id=BJGjOi09t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper385/Official_Review", "cdate": 1542234473537, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BJGjOi09t7", "replyto": "BJGjOi09t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper385/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335712164, "tmdate": 1552335712164, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper385/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}