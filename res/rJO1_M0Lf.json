{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573614774, "tcdate": 1521573614774, "number": 300, "cdate": 1521573614420, "id": "HJvx1J15z", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "rJO1_M0Lf", "replyto": "rJO1_M0Lf", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "abstract": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of a small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light on the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.", "pdf": "/pdf/cc576ba713d7b99bed43c421ae25843fe3aae6c9.pdf", "TL;DR": "The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.", "paperhash": "sagun|empirical_analysis_of_the_hessian_of_overparametrized_neural_networks", "_bibtex": "@misc{\nsagun2018empirical,\ntitle={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},\nauthor={Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, Leon Bottou},\nyear={2018},\nurl={https://openreview.net/forum?id=rJrTwxbCb},\n}", "keywords": ["Deep Learning", "Over-parametrization", "Hessian", "Eigenvalues", "Flat minima", "Large batch Small batch"], "authors": ["Levent Sagun", "Utku Evci", "V. Ugur Guney", "Yann Dauphin", "Leon Bottou"], "authorids": ["leventsagun@gmail.com", "ue225@nyu.edu", "vug@fb.com", "yann@dauphin.io", "leonb@fb.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730171665, "tcdate": 1518376928154, "number": 72, "cdate": 1518376928154, "id": "rJO1_M0Lf", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "rJO1_M0Lf", "original": "rJrTwxbCb", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "abstract": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of a small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light on the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.", "pdf": "/pdf/cc576ba713d7b99bed43c421ae25843fe3aae6c9.pdf", "TL;DR": "The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.", "paperhash": "sagun|empirical_analysis_of_the_hessian_of_overparametrized_neural_networks", "_bibtex": "@misc{\nsagun2018empirical,\ntitle={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},\nauthor={Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, Leon Bottou},\nyear={2018},\nurl={https://openreview.net/forum?id=rJrTwxbCb},\n}", "keywords": ["Deep Learning", "Over-parametrization", "Hessian", "Eigenvalues", "Flat minima", "Large batch Small batch"], "authors": ["Levent Sagun", "Utku Evci", "V. Ugur Guney", "Yann Dauphin", "Leon Bottou"], "authorids": ["leventsagun@gmail.com", "ue225@nyu.edu", "vug@fb.com", "yann@dauphin.io", "leonb@fb.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730171665, "tcdate": 1509128124694, "number": 602, "cdate": 1518730171655, "id": "rJrTwxbCb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rJrTwxbCb", "original": "HkN6PeW0-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Empirical Analysis of the Hessian of Over-Parametrized Neural Networks", "abstract": "We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et. al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the *flatness* of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create *large* connected components at the bottom of the landscape. Second, the dependence of a small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light on the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.", "pdf": "/pdf/2da95be1c41c0fdbbe2c99efe625f845d9993d3d.pdf", "TL;DR": "The loss surface is *very* degenerate, and there are no barriers between large batch and small batch solutions.", "paperhash": "sagun|empirical_analysis_of_the_hessian_of_overparametrized_neural_networks", "_bibtex": "@misc{\nsagun2018empirical,\ntitle={Empirical Analysis of the Hessian of Over-Parametrized Neural Networks},\nauthor={Levent Sagun and Utku Evci and V. Ugur Guney and Yann Dauphin and Leon Bottou},\nyear={2018},\nurl={https://openreview.net/forum?id=rJrTwxbCb},\n}", "keywords": ["Deep Learning", "Over-parametrization", "Hessian", "Eigenvalues", "Flat minima", "Large batch Small batch"], "authors": ["Levent Sagun", "Utku Evci", "V. Ugur Guney", "Yann Dauphin", "Leon Bottou"], "authorids": ["leventsagun@gmail.com", "ue225@nyu.edu", "vug@fb.com", "yann@dauphin.io", "leonb@fb.com"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}