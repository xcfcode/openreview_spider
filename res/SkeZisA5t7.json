{"notes": [{"id": "SkeZisA5t7", "original": "ByxKtPnqtm", "number": 595, "cdate": 1538087832609, "ddate": null, "tcdate": 1538087832609, "tmdate": 1550793099705, "tddate": null, "forum": "SkeZisA5t7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Byeze-HJeN", "original": null, "number": 1, "cdate": 1544667369696, "ddate": null, "tcdate": 1544667369696, "tmdate": 1545354504998, "tddate": null, "forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Meta_Review", "content": {"metareview": "This paper suggests that noise-regularized estimators of mutual information in deep neural networks should be adaptive, in the sense that the variance of the regularization noise should be proportional to the range of the hidden activity. Two adaptive estimators are proposed: (1) an entropy-based adaptive binning (EBAB) estimator that chooses the bin boundaries such that each bin contains the same number of unique observed activation levels, and (2) an adaptive kernel density estimator (aKDE) that adds isotropic Gaussian noise, where the variance of the noise is proportional to the maximum activity value in a given layer. These estimators are then used to show that (1) ReLU networks can compress, but that compression may or may not occur depending on the specific weight initialization; (2) different nonsaturating noninearities exhibit different information plane behaviors over the course of training; and (3) L2 regularization in ReLU networks encourages compression. The paper also finds that only compression in the last (softmax) layer correlates with generalization performance. The reviewers liked the range of experiments and found the observations in the paper interesting, but had reservations about the lack of rigor in the paper (no theoretical analysis of the convergence of the proposed estimator), were worried that post-hoc addition of noise distorts the function of the network, and felt that there wasn't much insight provided on the cause of compression in deep neural networks. The AC shares these concerns, and considers them to be more significant than the reviewers do, but doesn't wish to override the reviewers' recommendation that the paper be accepted.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Accept (Poster)", "title": "Interesting visualizations, but more rigor and analysis would help"}, "signatures": ["ICLR.cc/2019/Conference/Paper595/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper595/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353160905, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper595/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353160905}}}, {"id": "HkeSOAARCX", "original": null, "number": 10, "cdate": 1543593581065, "ddate": null, "tcdate": 1543593581065, "tmdate": 1543593581065, "tddate": null, "forum": "SkeZisA5t7", "replyto": "Hkxrz7YV07", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "content": {"title": "Reply to Rebuttal", "comment": "Thank you for the rebuttal. The modifications proposed do address my concerns. Also, I do agree that the scale of experiments should not be the only factor for evaluating the quality of an article. I am moving my score to 7 hoping to see more grounded results and extensions of the presented work."}, "signatures": ["ICLR.cc/2019/Conference/Paper595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper595/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615418, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeZisA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper595/Authors|ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615418}}}, {"id": "ByldLTe9nm", "original": null, "number": 3, "cdate": 1541176655713, "ddate": null, "tcdate": 1541176655713, "tmdate": 1543593214138, "tddate": null, "forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Review", "content": {"title": "Interesting observations!", "review": "The authors of this paper studied the popular belief that deep neural networks do information compression for supervised tasks. They studied this compression behavior with tanh and ReLU (and it's variants) activation functions which are saturating and non saturating in nature respectively. \n\nThe compression score is computed using Mutual Information Estimation which when computed are usually infinite. For finite mutual information values, noise can be added to hidden activations. For this purpose, two approaches namely Entropy Based Binning(EBAB) and adaptive Kernel Density Estimation(aKDE) were explored. EBAB adds noise to the hidden activations by binning and aKDE by Gaussian noise. Their results show that both EBAB and aKDE exhibit compression in case of ReLU, although this behavior is the strongest in tanh. \n\nFinally, When compression score was plotted against accuracy, higher rates of compression did not show significant correlation with generalization. Hence showing evidence that generalization(or good performance) can be achieved even without information bottleneck(information compression).\n\nQualms:\n1. Figure 7's description that ELU, Swish and centered softplus functions doing compression is not very apparent. \n2. Figure 9b: Regression line between compression score and accuracy shows a positive correlation between them. This seems contradictory to the inference.\n3. The experiments were done on a 5-layer network with 10-7-5-4-3 nodes respectively on a toy data of 12-bit binary vectors. The study could have included bigger networks with popular datasets which would give substantial support to the trend observed on toy data.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper595/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Review", "cdate": 1542234423867, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335759766, "tmdate": 1552335759766, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkxxUIYERQ", "original": null, "number": 7, "cdate": 1542915656409, "ddate": null, "tcdate": 1542915656409, "tmdate": 1542915656409, "tddate": null, "forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "content": {"title": "Revision", "comment": "We are grateful to the reviewers for their detailed feedback on our submission.\n\nWe have uploaded a revised script. It includes three new graphs in Figure 8, a reworked Figure 9, more details about the experimental setup in the \"1.1 Methods\" section. Captions to figures that demonstrate information planes have been expanded to include more explanations. "}, "signatures": ["ICLR.cc/2019/Conference/Paper595/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615418, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeZisA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper595/Authors|ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615418}}}, {"id": "Hkxrz7YV07", "original": null, "number": 6, "cdate": 1542914828786, "ddate": null, "tcdate": 1542914828786, "tmdate": 1542914828786, "tddate": null, "forum": "SkeZisA5t7", "replyto": "ByldLTe9nm", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "content": {"title": "Review response", "comment": "We are thankful to our referees for their careful and perceptive comments on our paper and are pleased by their positive overall opinion.\n \nIn response to your comments: \n\n-\"1. Figure 7's description that ELU, Swish and centered softplus functions doing compression is not very apparent.\"\n\nAccording to Shwartz-Ziv and Tishby (2017) information compression is marked by a decrease in I(X,T) values with a simultaneous increase in I(T,Y) during training. We have added a clarification of this in the introduction section: \u201cCompression for a given layer is signified by a decrease in I(T,X) value, while I(T,Y) is increasing.  Fitting behaviour refers to both values increasing.\u201d. \nIn Figure 7 we present information plots of networks using different activation functions in their hidden layers. However all used softmax in the output layer, thus the leftward movement of the leftmost layer line should be disregarded. Out of the various activation functions used in the hidden layers, it was ELU, centered softplus and Swish that showed the strongest visible decrease in I(X,T) values, which is also reflected through our compression metric and visualised on Figure 9 (b). We have added extra clarification to the caption of Figure 7: \u201cInformation planes of networks, using different non-saturating activation functions in the hidden layers. However all networks used softmax function in the output layer. Therefore, the shapes of the leftmost lines on all the information planes show less variation. For every activation function 50 random initializations were trained and averaged mutual information values were used for the information planes presented above.\u201d.\n \n-\"2. Figure 9b: Regression line between compression score and accuracy shows a positive correlation between them. This seems contradictory to the inference.\"\n\nThe p-value for the regression line on Figure 9 (b) was 0.35, too large to claim that there is significant correlation. However, we agree that \u00abgeneralization accuracy is \u2026 not affected by compression of hidden layers\u00bb in the discussion section could be interpreted as misleading, given the provided evidence. We apologise for this inexactness and we have changed the wording to \u00abnot significantly affected by the compression of the hidden layers\u00bb.\n \n-\"3. The experiments were done on a 5-layer network with 10-7-5-4-3 nodes respectively on a toy data of 12-bit binary vectors. The study could have included bigger networks with popular datasets which would give substantial support to the trend observed on toy data.\"\n\nThe main constraint to using larger networks (CNN with MNIST, for example) is the computational costs of mutual information calculations, which exceed the computation required for the network training, as well as memory costs of storing hidden activity for hundreds of epochs. Computational requirements for mutual information estimation for even a modest CNN are a few orders of magnitude greater than those for the configuration that we have been experimenting with. Given that we have observed strong sensitivity of information plane behaviour to the randomness of initialization, the resources required to reproduce these findings (using 50 initializations for every activation function) with larger networks were intractable for this project, while presenting an information plot for a single initialization could be misleading due to high variability of behaviour.\nPresented with the trade-off between studying CNNs using only one random initialization or smaller networks with a multitude of intializations, which can show average behaviour, we have made the decision to do the latter. As outlined in the methods section, the chosen configuration has also provided the ease of comparison with previous studies that used it.\nHowever, we are hopeful that hardware limitations will be less prevalent in further studies. "}, "signatures": ["ICLR.cc/2019/Conference/Paper595/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615418, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeZisA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper595/Authors|ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615418}}}, {"id": "S1etTbY4CX", "original": null, "number": 5, "cdate": 1542914497293, "ddate": null, "tcdate": 1542914497293, "tmdate": 1542914497293, "tddate": null, "forum": "SkeZisA5t7", "replyto": "ryeconnthX", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "content": {"title": "Review response", "comment": "We are thankful to our referees for their careful and perceptive comments on our paper and are pleased by their positive overall opinion.\n \nTo address your comments individually: \n \n-\"It is not clear in the paper that if the binning is applied just for visualizing the information plane or for computing the activities of hidden units in upper layers.\"\n\nThe calculation of mutual information was done after all training has been done and all hidden activity was saved for every epoch. Binning was used solely for calculating MI after all training has been completed.  We have modified the text (see \u20181.1 Methods\u2019 section) to give a better description of the experimental procedures:\n\u201cDuring the training process, the hidden activity for different epochs was saved. The calculation of mutual information was done using the saved hidden activity, after training has been completed; the two processes did not interfere with one another.\u201d.  \n \n-\"It would be difficult to draw conclusions based on the experimental results, even they come from the average of 50 individual networks.\"\n\nTo address our choice of the number of random initializations: when we compared information plane plots using averages over five and ten some variation was visible. However 10 and 25 averaged intializations lead to very similar results. Despite good convergence even at 25 initializations, we used 50 for ease of comparison with previous studies. We have modified the \u20181.1 Methods\u2019 section to include the following: \u201cBased on the observed data, even ten initializations provide a clear picture of the average network behaviour, but for the purpose of consistency with previous studies, we used 50 initializations.\u201d. \n \n-\"Also, the experiences are performed using a particular task, it is not sure if similar behavior is observed in other tasks.\"\n\nIn this contribution we set out to study compression in itself, using robust mutual information estimators. We also wanted to see the effect that various activation functions have on this behaviour. That is why we have chosen to use the same dataset as previous study, as discussed in the \u20181.1 Methods\u2019 section. \nWe have experimented with other binary tasks and compression was present when MI was measured with adaptive estimators. The results that we have seen were not markedly different from those that we have observed with the main dataset.  Even though the relationship between task structure and strength of compression is a very interesting topic, it was beyond the scope of this investigation. \n \n-\"It is, however, more important to understand what makes the compression.\"\n\nWe agree with the reviewer that it could be very interesting to deduce the key precursor to compression and investigate its significance. However, our main effort is focused on outlining mutual information estimates that are robust and applicable to deterministic neural networks regardless of the activation function employed. We were also mainly interested in determining whether compression can happen for non-saturating activation functions in the first place. We believe our paper presents enough evidence to support this effort. "}, "signatures": ["ICLR.cc/2019/Conference/Paper595/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615418, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeZisA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper595/Authors|ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615418}}}, {"id": "Bkxk6xFN0Q", "original": null, "number": 4, "cdate": 1542914231134, "ddate": null, "tcdate": 1542914231134, "tmdate": 1542914231134, "tddate": null, "forum": "SkeZisA5t7", "replyto": "Hklpw8gLnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "content": {"title": "Review response 2/2", "comment": "- \"The first Schmidhuber citation of the paper seems a bit out of place. I think he himself would say that deep learning has been going on for much longer than since 2015. (in fact I think you could just remove the entire first paragraph, it is just unnecessary boilerplate) \"\n\nWe have put the first paragraph there to provide better context for the paper and to make it slightly more comprehensive for people outside the field of deep learning. We agree that a reference to one paper from 2015 might seem confusing . Although, the paper that we reference is an excellent compilation of research related to the field, so we have changed the citation to say: \u201c \u2026(reviewed by Schmidhuber (2015))\u201d. \n\n- \"Why should there be a direct correlation between compression and generalization? For example, it is known that training DNNs with soft targets improves test accuracy in classification, or even forcing softness in both targets and representations [2] also improves test accuracy.\"\n\nThe parallel between compression and generalization has been drawn originally by Shwartz-Ziv & Tishby (2017), as the paper claimed that the implementation of the information bottleneck is the reason why neural networks are able to perform well. We have not found evidence strong enough to support this claim in general, but we saw a significant correlation between performance and compression of the softmax layer.\n\n- \"I'm still personally not sold on binning as a strategy to evaluate MI. Did you perform experiments that show that the observed difference is consistent if more computation is done to approximate MI, and not just an artefact of max-entropy binning?\"\n\nWe have tried many techniques to estimate MI, including bin-free non-parametric methods like aKDE. The purpose of out paper was to point out potential artefacts with previous binning implementations, then propose a much more flexible binning method (EBAB), which turned out to give similar estimates as aKDE. Also, since hidden activity data was not sparse, binning is as a reliable estimator, when not affected by the magnitude differences of the hidden activity. \nOur main conclusion remains that whichever noise-bearing estimation method is used to measure mutual information in the context of deterministic DNNs, unless the noise is proportional to the magnitude of the hidden activity, MI estimates will be inconsistent, and present a misleading picture on the information plane.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper595/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615418, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeZisA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper595/Authors|ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615418}}}, {"id": "BkeLvlKNRX", "original": null, "number": 3, "cdate": 1542914142174, "ddate": null, "tcdate": 1542914142174, "tmdate": 1542914150672, "tddate": null, "forum": "SkeZisA5t7", "replyto": "Hklpw8gLnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "content": {"title": "Review response 1/2", "comment": "We are thankful to our referees for their careful and perceptive comments on our paper and are pleased by their positive overall opinion. We have added content to clarify the figures in more detail and addressed minor errors that have been kindly pointed out by the reviewer. \n \nTo address your comments individually: \n \n-\"Quantitatively, the proposed compression score is interesting, but as the authors say, simplistic. It seems to me that we care more about the converged models than the whole training trajectory; how does this score evolve with time?\"\n\nThe score compares the most uncompressed state of a layer (largest I(X,T) value observed) and its last observed value. Depending on the amount of fitting this score can stay at zero for a significant portion of training when I(X,T) keeps increasing. If there is little fitting and compression is happening, the score will increase throughout training. Considering the large variety of behaviours that we have observed on the information plane, there is no unique pattern that this score has during training, but it tends to be closer to zero in the initial stages of training. \n \n-\"I think an important part of discussion that lacks in this paper is a more in-depth take as to how these findings relate to the Zhang et al [1] memorization vs generalization paper and its follow ups. There seem to be many links to be drawn.\"\n\nWe were careful not to make serious claims about the relationship between information compression and memorization without a thorough investigation. However, we are aware that there could be interesting results from examining this link, and we have mentioned it in the discussion as a potential research opportunity.\n \n-\"The writing of the paper is good, but the writing of the captions could be improved. (the hard page limit of ICLR is 10 pages and your paper has a lot of captions, so I think investing into a bit more text would be good)\"\n\nWe have added more text to the majority of the captions, especially for figures depicting the information plane. In the captions we explicitly comment on the dynamics of networks\u2019 layers, for the ease of interpretability to readers who see these plots for the first time. \n\n- \"It might be worth to re-explain what the information plane plots are in a figure caption, not just in the text (the text also doesn't really explain that each point is a moment in training, and each thread a different layer, this paper should be readable by someone who has never seen these plots before). \"\n \nWe have changed the introduction section to give a better better explanation of how to interpret compression on the information plane, as well as a more explicit explanations in the captions of information plots.\n\n-\"It's not clear what is going on in figure 5, I can guess but, again, this paper should be readable by anyone in the field. -You mention different initializations, but which exactly?\"\n\nThe stochasticity of an initialization has two sources: weight initialization and data shuffling. We have modified the methods section to include information about our initialization procedure: \u201cWeight initialization was done using random truncated Gaussian initialization from Glorot & Bengio (2010), with 50 instances of this initialization procedure for every network configuration used. ... 80% of the dataset was used for training, using batches of 512 samples.\u201d.\n \n-\"What makes you say that 5c has no compression but that 5a does compression first? It should be explained explicitly.\"\n \nFigure 5 shows the information plots of a ReLU network, with different random initialization instances. The spectrum of behaviour presented here aims to demonstrate that fitting-compression phases are not innate to all neural networks, as claimed by previous studies, and there is a significant amount of heterogeneity involved. We have included more explicit explanations about this in the caption to Figure 5.\n\n-\"I believe what you say about Figure 8, but the plots are so similar that it is hard to compare them visually. Maybe a different kind of superposition into a single plot would better illustrate the compression effect of L2?\"\n \nWe agree that it might be time consuming to make out the difference in compression between layers on Figure 8. We have included three extra plots to Figure 8, each one showing only one layer line, with different L2 penalties superposed. The compression is easily distinguished on these plots, while the original plots better demonstrate that these layer lines are pulled to a single point. We have also made sure that these plots are readable in greyscale. \n \n- \"Typo in the x axis caption of figures 9\".\n \nThank you, this has been fixed. \n \n- \"Figure 9a is not readable in greyscale (or by a colorblind person), consider using a different symbol for the softmax scatter (and adding this symbol to the legend)\". \n \nWe thank the reviewer for this suggestion. We have reworked this figure to make it more accessible. \n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper595/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615418, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkeZisA5t7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper595/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper595/Authors|ICLR.cc/2019/Conference/Paper595/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers", "ICLR.cc/2019/Conference/Paper595/Authors", "ICLR.cc/2019/Conference/Paper595/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615418}}}, {"id": "ryeconnthX", "original": null, "number": 2, "cdate": 1541160097902, "ddate": null, "tcdate": 1541160097902, "tmdate": 1541533857851, "tddate": null, "forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Review", "content": {"title": "This paper uses information to show compression of DNN with unbounded activation function, but it gives no questions to what form the compression.", "review": "This paper proposes a method for the estimation of mutual information for networks with unbounded activation functions and the use of L2 regularization to induce more compression.  The use of information planes to study the training behavior of networks is not new.  This paper addresses the issue of unbounded  hidden state activities.  As the differential mutual information in DNN is ill-defined, the authors proposed to add noise to the hidden activity by using the binning process.   It is not clear in the paper that if the binning is applied just for visualizing the information plane or for computing the activities of hidden units in upper layers.   If it is the latter one, it creates unnecessary distortions to the DNN.  As the authors pointed out, different initializations can lead to different behaviour on the information plane.  It would be difficult to draw conclusions based on the experimental results, even they come from the average of 50 individual networks.  Also, the experiences are performed using a particular task, it is not sure if similar behavior is observed in other tasks.   It is, however, more important to understand what makes the compression.   For the L2 regularization, the compression is expected as the regularization tends to limit the values of the  weights. ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper595/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Review", "cdate": 1542234423867, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335759766, "tmdate": 1552335759766, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hklpw8gLnQ", "original": null, "number": 1, "cdate": 1540912741160, "ddate": null, "tcdate": 1540912741160, "tmdate": 1541533857612, "tddate": null, "forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "invitation": "ICLR.cc/2019/Conference/-/Paper595/Official_Review", "content": {"title": "Review", "review": "This paper has 3 principal contributions: it proposes a different way of measuring mutual information in a neural network, proposes a compression score to compare different models, then empirically analyses different activation functions and L2 weights.\n\nThis work seems like a welcome addition to the IB thread. To me the most interesting result is simply that activation functions aren't simply about gradient flow, and that they may each have properties that are more or less desirable depending on the domain they might be used on. The authors are careful in the wording of their conclusions, I think with reason; while these results are useful in that there seem to be consistently different behaviors coming from different hyperparameters, information planes show a relatively qualitative part of the picture.\n\nQuantitatively, the proposed compression score is interesting, but as the authors say, simplistic. It seems to me that we care more about the converged models than the whole training trajectory; how does this score evolve with time?\n\nI think an important part of discussion that lacks in this paper is a more in-depth take as to how these findings relate to the Zhang et al [1] memorization vs generalization paper and its follow ups. There seem to be many links to be drawn.\n\nThis work is overall a good contribution, but I'll have to agree with the authors' conclusion that more principled analysis methods are required to have a solid grasp of the training dynamics of DNNs. The writing of the paper is good, but the writing of the captions could be improved. (the hard page limit of ICLR is 10 pages and your paper has a lot of captions, so I think investing into a bit more text would be good)\n\n\nComments:\n- It might be worth to re-explain what the information plane plots are in a figure caption, not just in the text (the text also doesn't really explain that each point is a moment in training, and each thread a different layer, this paper should be readable by someone who has never seen these plots before). \n- It's not clear what is going on in figure 5, I can guess but, again, this paper should be readable by anyone in the field. You mention different initializations, but which exactly? What makes you say that 5c has no compression but that 5a does compression first? It should be explained explicitly.\n- I believe what you say about Figure 8, but the plots are so similar that it is hard to compare them visually. Maybe a different kind of superposition into a single plot would better illustrate the compression effect of L2?\n- Typo in the x axis caption of figures 9.\n- Figure 9a is not readable in greyscale (or by a colorblind person), consider using a different symbol for the softmax scatter (and adding this symbol to the legend).\n- The first Schmidhuber citation of the paper seems a bit out of place. I think he himself would say that deep learning has been going on for much longer than since 2015. (in fact I think you could just remove the entire first paragraph, it is just unnecessary boilerplate)\n- Why should there be a direct correlation between compression and generalization? For example, it is known that training DNNs with soft targets improves test accuracy in classification, or even forcing softness in both targets and representations [2] also improves test accuracy.\n- I'm still personally not sold on binning as a strategy to evaluate MI. Did you perform experiments that show that the observed difference is consistent if more computation is done to approximate MI, and not just an artefact of max-entropy binning?\n\n[1] Zhang et al (2016) https://arxiv.org/abs/1611.03530\n[2] Verma et al (2018) https://arxiv.org/abs/1806.05236\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper595/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adaptive Estimators Show Information Compression in Deep Neural Networks", "abstract": "To improve how neural networks function it is crucial to understand their learning process. The information bottleneck theory of deep learning proposes that neural networks achieve good generalization by compressing their representations to disregard information that is not relevant to the task. However, empirical evidence for this theory is conflicting, as compression was only observed when networks used saturating activation functions. In contrast, networks with non-saturating activation functions achieved comparable levels of task performance but did not show compression. In this paper we developed more robust mutual information estimation techniques, that adapt to hidden activity of neural networks and produce more sensitive measurements of activations from all functions, especially unbounded functions. Using these adaptive estimation techniques, we explored compression in networks with a range of different activation functions. With two improved methods of estimation, firstly, we show that saturation of the activation function is not required for compression, and the amount of compression varies between different activation functions. We also find that there is a large amount of variation in compression between different network initializations. Secondary, we see that L2 regularization leads to significantly increased compression, while preventing overfitting. Finally, we show that only compression of the last layer is positively correlated with generalization.", "keywords": ["deep neural networks", "mutual information", "information bottleneck", "noise", "L2 regularization"], "authorids": ["ic14436@bristol.ac.uk", "conor.houghton@bristol.ac.uk", "cian.odonnell@bristol.ac.uk"], "authors": ["Ivan Chelombiev", "Conor Houghton", "Cian O'Donnell"], "TL;DR": "We developed robust mutual information estimates for DNNs and used them to observe compression in networks with non-saturating activation functions", "pdf": "/pdf/7a6068db7ab7385c7320232c410d12379d1589c4.pdf", "paperhash": "chelombiev|adaptive_estimators_show_information_compression_in_deep_neural_networks", "_bibtex": "@inproceedings{\nchelombiev2018adaptive,\ntitle={Adaptive Estimators Show Information Compression in Deep Neural Networks},\nauthor={Ivan Chelombiev and Conor Houghton and Cian O'Donnell},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=SkeZisA5t7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper595/Official_Review", "cdate": 1542234423867, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkeZisA5t7", "replyto": "SkeZisA5t7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper595/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335759766, "tmdate": 1552335759766, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper595/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}