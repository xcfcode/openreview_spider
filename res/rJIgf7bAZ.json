{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730156982, "tcdate": 1509138929632, "number": 1136, "cdate": 1518730156974, "id": "rJIgf7bAZ", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rJIgf7bAZ", "original": "BJVCb7bAW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "nonreaders": [], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260075597, "tcdate": 1517250241220, "number": 886, "cdate": 1517250241203, "id": "rJY6LypSf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The reviewers are unanimous that this is an interesting paper, but that ultimately the empirical results are not sufficiently promising to warrant the added complexity."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642388552, "tcdate": 1511826733700, "number": 1, "cdate": 1511826733700, "id": "B1LVBmqgf", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Review", "forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "signatures": ["ICLR.cc/2018/Conference/Paper1136/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "This paper proposes a new algorithm for discovering options, but the benefits of the algorithm are not clear empirically.", "rating": "3: Clear rejection", "review": "This paper treats option discovery as being analogous to discovering useful latent variables.  The proposed formulation assumes there is a policy over options, which invokes an option\u2019s policy to select actions at each timestep until the option\u2019s termination function is activated.  A contribution of this paper is to learn all possible options that might have caused an observed trajectory, and to update parameters for all these pertinent option-policies with backprop.  The proposed method, IOPG, is compared to A3C and the option-critic (OC) on four continuous control tasks in Mujoco, and IOPG has the best performance on one of the four domains.\n\nThe primary weakness of this paper is the absence of performance or conceptual improvements in exchange for the additional complexity of using options.  The only domain where IOPG outperforms both A3C and OC is the Walker2D-v1 domain, and the reported performance on that domain (~800) is far below the performance of other methods (shown on OpenAI\u2019s Gym site or in the PPO paper). Also, there is not much analysis on what kind of options are learned with this approach, beyond noting that the options seem clustered on tSNE plots.  Given the close match between the A3C agent and the IOPG agent on the other three domains, I expect that the system is mostly relying on the base A3C components with limited contributions from the extensions introduced in the network for options.  \n\nThe clarity of the paper\u2019s contributions could be improved. The contribution of options might be made more clearly in smaller domains or in more detailed experiments. How is the termination beta provided from the network?  How frequently did the policy over options switch between them?  How was the number of options selected, and what happens when the number of possible options is varied from 1 to 4 or beyond 4?  To what extent was there overlap in the learned policies to realize the proposed algorithmic benefit of learning multiple option-policies from the same transitions?  The results in this paper do not provide strong support for using the proposed method.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642388452, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1136/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1136/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1136/AnonReviewer3"], "reply": {"forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642388452}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642388511, "tcdate": 1511977136742, "number": 2, "cdate": 1511977136742, "id": "B1F3lO2lG", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Review", "forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "signatures": ["ICLR.cc/2018/Conference/Paper1136/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "The paper is well written and presents a good extension of infernece based option discovery. However, the results are not convincing and there is a crucial issue in the assumptions of the algorithm. ", "rating": "3: Clear rejection", "review": "The paper presents a new policy gradient technique for learning options. The option index is treated as latent variable and, in order to compute the policy gradient, the option distribution for the current sample is computed by using a forward pass. Hence, a single sample can be used to update all options and not just the option that has been used for this sample.\n \nThe idea of the paper is good but the novelty is limited. As noted by the authors, the idea of using inference for option discovery has already been presented in Daniel2016. Note that the option discovery process is Daniel2016 is not limited to linear sub-policies, only the policy update strategy is. So the main contribution is to use a new policy update strategy, i.e., policy gradients, for inference based option discovery. Thats fine but should be stated more clearly in the paper. The paper is also written very well and the topic is relevant for the ICLR conference. \n\nHowever, the paper has two main problems:\n- The results are not convincing. In most domains, the performance is similar to the A3C algorithm (which does not use inference based option discovery), so the impact of this paper seems limited. \n\n- One of the main assumptions of the algorithm is wrong. The assumption is that rewards from the past are not correlated with actions in the future conditioned on the state s_t (otherwise we would always have a correlation) ,which  is needed to use the policy gradient theorem. The assumption is only true for MDPs. However, using the option index as latent variable yields a PoMDP. There, this assumption does not hold any more. Example: Reward at time step t-1 depends on the action, which again depends on the option o_t-1. Action at time step t depends on o_t. Hence, there is a strong correlation between reward r_t-1 and action a_t+1 as o_t and o_t+1 are strongly correlated. o_t is not a conditional variable of the policy as it is not part of the state, thats why this assumption does not work any more.\n\nSummary: The paper is well written and presents a good extension of inference based option discovery. However, the results are not convincing and there is a crucial issue in the assumptions of the algorithm. \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642388452, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1136/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1136/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1136/AnonReviewer3"], "reply": {"forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642388452}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642388469, "tcdate": 1512456443977, "number": 3, "cdate": 1512456443977, "id": "ByV--6Xbz", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Review", "forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "signatures": ["ICLR.cc/2018/Conference/Paper1136/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Interesting, but not impactful", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes what is essentially an off-policy method for learning options in complex continuous problems.  The idea is to use policy gradient style algorithms to update a suite of options using relatively \n\nOn the positive side, I like the core idea of this paper.  The idea of updating multiple options at once is a good one.  I think the authors should definitely continue to investigate this line of work.  I also appreciated that the authors took the time to try and visualize what was learned.  The paper is generally well-written and easy to read.\n\nOn the negative side: ultimately, the algorithm doesn't seem to work all that well.  Empirically, the method doesn't seem to perform substantially better than other algorithms, although there seems to be some slight advantage.  A clearly missing comparison would be something like TRPO or DDPG.\n\nFigure 1 was helpful in understanding marginalization and the forward algorithm.  Thanks.\n\nWas there really only 4 options that were learned?  How would this scale to more?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642388452, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper1136/AnonReviewer2", "ICLR.cc/2018/Conference/Paper1136/AnonReviewer1", "ICLR.cc/2018/Conference/Paper1136/AnonReviewer3"], "reply": {"forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642388452}}}, {"tddate": null, "ddate": null, "tmdate": 1515189631111, "tcdate": 1515189631111, "number": 5, "cdate": 1515189631111, "id": "SyDFSOp7z", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "signatures": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "content": {"title": "Updated Version", "comment": "We have uploaded an updated version of the paper, which addresses some of the concerns the reviewers had, as well as providing additional information on the nature of the options learned."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723859, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJIgf7bAZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1136/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers", "ICLR.cc/2018/Conference/Paper1136/Authors", "ICLR.cc/2018/Conference/Paper1136/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723859}}}, {"tddate": null, "ddate": null, "tmdate": 1513215072066, "tcdate": 1513215072066, "number": 4, "cdate": 1513215072066, "id": "H1uwNUJMz", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "forum": "rJIgf7bAZ", "replyto": "ByV--6Xbz", "signatures": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "content": {"title": "Response", "comment": "We thank the reviewer for their time and insight. Individual points are addressed below.\n\n\n> Empirically, the method doesn't seem to perform substantially better than other algorithms, although there seems to be some slight advantage.  A clearly missing comparison would be something like TRPO or DDPG.\n\nWe do not expect IOPG to outperform A3C significantly in a single task, but expect benefits in interpretability and transferability. Please see our official comment for our more detailed response to this. While TRPO has been shown to outperform A3C in certain situations, we feel that the policy update strategy is largely independent of the option learning method presented here. That is, it should not be too difficult to write an algorithm that uses trust region updates with option learning. We compare to A3C so that the value of our contribution in isolation is more clear. We could also add a comparison to an inferred-option extension of a more powerful policy search algorithm such as PPO, TRPO, or DDPG.\n\n\n> Was there really only 4 options that were learned?  How would this scale to more?\n\nThe number of options learned is prespecified as a hyperparameter, as is the case in several option learning methods. The computational complexity is quadratic in the number of options, with linear memory complexity. We will add an experiment comparing the number of options in the next version of the paper.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723859, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJIgf7bAZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1136/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers", "ICLR.cc/2018/Conference/Paper1136/Authors", "ICLR.cc/2018/Conference/Paper1136/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723859}}}, {"tddate": null, "ddate": null, "tmdate": 1513215018542, "tcdate": 1513215018542, "number": 3, "cdate": 1513215018542, "id": "BkGEN81fz", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "forum": "rJIgf7bAZ", "replyto": "B1LVBmqgf", "signatures": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "content": {"title": "Response", "comment": "Thank you to the reviewer for their insightful comments. Individual points are addressed below.\n\n\n> The only domain where IOPG outperforms both A3C and OC is the Walker2D-v1 domain, and the reported performance on that domain (~800) is far below the performance of other methods (shown on OpenAI\u2019s Gym site or in the PPO paper).\n\nWe do not expect IOPG to outperform A3C significantly in a single task, but expect benefits in interpretability and transferability. Please see our comment for our more detailed response to this. We would like to add here that we view the option learning strategy contributed here to be largely independent to the method used for policy optimization. This is to say that it should be easy to write a PPO-IOPG algorithm with the benefits of both. We compare to A3C so that the value of our contribution in isolation is more clear. We could also add a comparison to an inferred-option extension to PPO.\n\n\n> How is the termination beta provided from the network? \n\nWe apologize for forgetting to include this. Termination is sampled for the currently active option from a linear sigmoid layer on top of the policy network, as an additional head. We will clarify this in the updated version of the paper.\n\n\n> How frequently did the policy over options switch between them? \n\nWe will add this information to the appendix in the next version of the paper.\n\n\n> How was the number of options selected, and what happens when the number of possible options is varied from 1 to 4 or beyond 4?\n\nMost existing option learning methods require specification of the number of options as a hyperparameter. In general this is optimized according to the task at hand. Here, however, we did no optimization over this parameter, but we'll be happy to add an experiment to the next version of the paper.\n\n\n> To what extent was there overlap in the learned policies to realize the proposed algorithmic benefit of learning multiple option-policies from the same transitions?\n\nAt the start of learning, the policies tend to overlap highly due to random initialization. Because of this, early training benefits from the simultaneous update, as all options are implicated in every action. As training progresses, the t-SNE experiments demonstrate that is little overlap between final policies. Each policy appears to be active in a different region of state space. This is likely due to the fact that the most likely option is most updated, rather than a single option being updated improperly in the event of an unlikely action.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723859, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJIgf7bAZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1136/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers", "ICLR.cc/2018/Conference/Paper1136/Authors", "ICLR.cc/2018/Conference/Paper1136/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723859}}}, {"tddate": null, "ddate": null, "tmdate": 1513214923778, "tcdate": 1513214923778, "number": 2, "cdate": 1513214923778, "id": "BkV07I1fM", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "forum": "rJIgf7bAZ", "replyto": "B1F3lO2lG", "signatures": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "content": {"title": "Response", "comment": "We thank the reviewer for taking the time to evaluate our paper. Individual points are addressed below.\n\n\n> The assumption [that rewards are independent of future actions, conditioned on the current state] is only true for MDPs. However, using the option index as latent variable yields a PoMDP. There, this assumption does not hold any more.\n\nUnder the standard set of assumptions this would be correct. As shown in the line before Eqn. 3, the conditional assumption that we make is slightly different. It is true that a_k and r_j are not independent in general. However, they are conditionally independent given s_k and s_j, and a_j. We are conditioning on all of the observed states and observed actions since the start of the trajectory. Since the reward only depends on these observed variables, no information is passed to future actions.\n\n\n> As noted by the authors, the idea of using inference for option discovery has already been presented in Daniel2016. Note that the option discovery process is Daniel2016 is not limited to linear sub-policies, only the policy update strategy is. So the main contribution is to use a new policy update strategy, i.e., policy gradients, for inference based option discovery\n\nWe agree that the graphical model employed here is the same as that used in Daniel2016. However, the option inference step is not the same, since they employ the use of backward information, while we only require forwards information. This means that our algorithm can be employed online, while the one presented in Daniel2016 can only be applied in the episodic case, where updates are made only after the episode is terminated.\n\n\n> The results are not convincing. In most domains, the performance is similar to the A3C algorithm (which does not use inference based option discovery), so the impact of this paper seems limited.\n\nWe do not expect IOPG to outperform A3C significantly in a single task, but expect benefits in interpretability and transferability. Please see our official comment for our more detailed response to this. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723859, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJIgf7bAZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1136/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers", "ICLR.cc/2018/Conference/Paper1136/Authors", "ICLR.cc/2018/Conference/Paper1136/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723859}}}, {"tddate": null, "ddate": null, "tmdate": 1513213601741, "tcdate": 1513213601741, "number": 1, "cdate": 1513213601741, "id": "H1ci0S1zG", "invitation": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "forum": "rJIgf7bAZ", "replyto": "rJIgf7bAZ", "signatures": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper1136/Authors"], "content": {"title": "Performance of IOPG", "comment": "We would like to thank the reviewers for their insightful comments. Here, we focus on the issue that all three reviewers raised: that A3C does as well as IOPG in most environments.\n\nIt is our opinion that A3C ought to perform roughly as well as IOPG. The optimization performed is nearly identical between the two algorithms, where IOPG is parameterized in a particular manner such that options can be learned. We developed IOPG as a data-efficient method to optimize several options simultaneously. We present it in a general form, without any sort of regularization on the structure of the options. Even without such regularization, the options learned by IOPG express some worthwhile characteristics, which several existing option learning algorithms cannot produce: namely temporal extension, and spatial separation. Without additional problem-specific regularization on the structure of those options, there is no reason to expect performance improvements in the single-task setting.\n\nThis said, we feel that the extra structure learned by IOPG yields several benefits. Options can be useful for the interpretation of agent behaviours, as our t-SNE experiments (Fig. 3) show. Further there is strong evidence to suggest that learned options can be useful for transfer learning (OptionGAN: Henderson et al. 2018, Option-Critic: Bacon et al. 2017, Subgoal Discovery: McGovern and Barto 2001). We feel that these benefits make IOPG a worthwhile algorithm, especially since it comes at no cost to data efficiency, variance, or asymptotic learning compared to A3C. We are currently working on experiments that better quantify such upsides."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An inference-based policy gradient method for learning options", "abstract": "In the pursuit of increasingly intelligent learning systems, abstraction plays a vital role in enabling sophisticated decisions to be made in complex environments. The options framework provides formalism for such abstraction over sequences of decisions.  However most models require that options be given a priori, presumably specified by hand, which is neither efficient, nor scalable. Indeed, it is preferable to learn options directly from interaction with the environment. Despite several efforts, this remains a difficult problem: many approaches require access to a model of the environmental dynamics, and inferred options are often not interpretable, which limits our ability to explain the system behavior for verification or debugging purposes.  In this work we develop a novel policy gradient method for the automatic learning of policies with options.  This algorithm uses inference methods to simultaneously improve all of the options available to an agent, and thus can be employed in an off-policy manner, without observing option labels. Experimental results show that the options learned can be interpreted. Further, we find that the method presented here is more sample efficient than existing methods, leading to faster and more stable learning of policies with options.", "pdf": "/pdf/760c3354a1ccdad4ba240b1e7e3be36fdc545981.pdf", "TL;DR": "We develop a novel policy gradient method for the automatic learning of policies with options using a differentiable inference step.", "paperhash": "smith|an_inferencebased_policy_gradient_method_for_learning_options", "_bibtex": "@misc{\nj.2018an,\ntitle={An inference-based policy gradient method for learning options},\nauthor={Matthew J. A. Smith and Herke van Hoof and Joelle Pineau},\nyear={2018},\nurl={https://openreview.net/forum?id=rJIgf7bAZ},\n}", "keywords": ["reinforcement learning", "hierarchy", "options", "inference"], "authors": ["Matthew J. A. Smith", "Herke van Hoof", "Joelle Pineau"], "authorids": ["matthew.smith5@mail.mcgill.ca", "herke.vanhoof@mail.mcgill.ca", "jpineau@cs.mcgill.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825723859, "id": "ICLR.cc/2018/Conference/-/Paper1136/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "rJIgf7bAZ", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper1136/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper1136/Authors|ICLR.cc/2018/Conference/Paper1136/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper1136/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper1136/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper1136/Reviewers", "ICLR.cc/2018/Conference/Paper1136/Authors", "ICLR.cc/2018/Conference/Paper1136/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825723859}}}], "count": 10}