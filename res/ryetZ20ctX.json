{"notes": [{"id": "ryetZ20ctX", "original": "HyeUeZaqYm", "number": 1189, "cdate": 1538087936527, "ddate": null, "tcdate": 1538087936527, "tmdate": 1550894778018, "tddate": null, "forum": "ryetZ20ctX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "B1lPQ2VreE", "original": null, "number": 1, "cdate": 1545059358805, "ddate": null, "tcdate": 1545059358805, "tmdate": 1545354483076, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Meta_Review", "content": {"metareview": "The reviewers agree the paper brings a novel perspective by controlling the conditioning of the model when performing quantization.  The experiments are convincing experiments. We encourage the authors to incorporate additional references suggested in the reviews. We recommend acceptance. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Good paper, accept."}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1189/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352932684, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352932684}}}, {"id": "r1l21JD9RQ", "original": null, "number": 9, "cdate": 1543298787611, "ddate": null, "tcdate": 1543298787611, "tmdate": 1543298787611, "tddate": null, "forum": "ryetZ20ctX", "replyto": "SJlagR_gR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "Re: Good research work with clear arguments well supported by rigorous experiments. ", "comment": "Thank you so much for the detailed feedback and advice.\n\n1. We conducted an empirical study to find the reason for inferior robustness in Section 3 and Figure 3.\n\n2. We appreciate the reviewer for the advice. The orthogonal regularization is an effective method to regularize the Lipschitz constant of the network, but indeed, it might not be the optimal strategy. We believe that the robustness of the quantized network is related to the specific form of Lipschitz regularization, and we will try the recommended term of Lipschitz regularization to see if it works better than orthogonal regularization.\n\nOn the other hand, one important reason why we used the orthogonal regularization is the computation efficiency. A simple way to speed up the calculation of orthogonal regularization is row sampling. In our experiments, we found that we can achieve similar regularization effect when sampling less than 30% of the weight matrix rows to conduct orthogonal regularization at each step, saving more than 90% of the computation of calculating regularization terms (which is already small compared to network training). In such case, the orthogonal regularization could be even more efficient than the power iteration.\n\n3. We have tried the weight clipping approach for controlling Lipschitz constant as proposed in Wasserstein GAN (Arjovsky et al.), but it performed worse than orthogonal regularization according to our experiments. We are interested to try out if penalizing the norm of the Jacobian will work better."}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "rJlCsNCx0X", "original": null, "number": 9, "cdate": 1542673574358, "ddate": null, "tcdate": 1542673574358, "tmdate": 1542673574358, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "I am trying to reproduce the results of this paper (mainly the claims that R+FGSM training is just as effective as PGD adversarial training, see the long discussion below regarding Table 3). I have tried taking the CIFAR-10 code from ( https://github.com/MadryLab/cifar10_challenge ) and implementing the R+FGSM attack during training as described in this paper. However, I have been unable to reproduce the claim of 43% robustness at eps=8.\n\nWould the authors be willing to release their source code?", "title": "Unable to reproduce R+FGSM results"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "SJlagR_gR7", "original": null, "number": 3, "cdate": 1542651380909, "ddate": null, "tcdate": 1542651380909, "tmdate": 1542651380909, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Review", "content": {"title": "Good research work with clear arguments well supported by rigorous experiments.", "review": "Summary of paper\nThis paper presents an approach  for quantising neural networks such that the resulting quantised model is robust to adversarial and random perturbations.\nThe core idea of the paper is to enforce the Lipschitz constant of each linear layer of the network approximately close to 1. Since the Lipschitz constant of the neural network is bounded by the product of the\nLipschitz constant of its linear layer (assuming Lipschitz 1 activation functions) the Lipschitz constant of the trained neural network is bounded by 1. This results in a model which is robust to adversarial and random noise ad all directions in the model space are non-expansive. Algorithmically, controlling the Lipschitz constant is achieved by using the orthogonal regulariser presented in the paper Cisse et.al which has the same motivation for this work but for standard neural network training but not quantising. The authors presents thorough experimental study showing why standard quantisation schemes are prone to adversarial noise and demonstrate clearly how this approach improves robustness of quantised network and sometimes even improve over the accuracy of original model. \n\nReview:\nThe paper is well written with clear motivation and very easy to follow. \nThe core idea of using orthogonal regulariser for improving the robustness of neural network models have been presented in Cisse et.al and the authors re-use it for improving the robustness of quantised models. The main contribution of this work is in identifying that the standard quantised models are very vulnerable to adversarial noise which is illustrated through experiments and then empirically showing that the regulariser presented in Cisse et. al improves the robustness of quantised models with rigorous experiments. The paper add value to the research community through thorough experimental study as well as in industry since quantised models are widely used and the presented model is simple and easy to use. \n\nSome suggestions and ideas:\n\n1. It will be great if the authors could add a simple analytical explanation why the quantised networks are not robust.                      \n\n2. The manifold of Orthogonal matrices does not include all 1 - Lipschitz matrices and also the Orthogonal set is not convex. I think a better strategy for this problem is to regularise the spectral norm to be 1.  Regularising the spectral norm is computationally cheaper than Orthogonal regulariser when combined with SGD using power iterations.  Moreover the regulariser part of the model becomes nice and convex.\n\n3. Another strategy to control the Lipschitz constant of the network is to directly penalise the norm of the Jacobian as explained in Improved Training of Wasserstein GANs (Gulrajani et. al).\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Review", "cdate": 1542234284951, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335892371, "tmdate": 1552335892371, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1lNfHKMTQ", "original": null, "number": 7, "cdate": 1541735691885, "ddate": null, "tcdate": 1541735691885, "tmdate": 1541735691885, "tddate": null, "forum": "ryetZ20ctX", "replyto": "SylvEsdV3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "Re: A simple regularization scheme that efficiently protects quantized models from adversarial attacks", "comment": "Thank you so much for the feedback. Below are our answers to the questions.\n\n- Lipschitz regularization has been proposed in previous papers, and the robustness of full-precision models can also be improved when combined with Lipschitz regularization. However, there is a significant difference between applying Lipschitz regularization term to full-precision model and quantized model: when the quantized model is trained with Lipschitz regularization, we found it more robust than the original full-precision model, and it is **even more robust** than the full-precision model trained with the same Lipschitz regularization. That is to say, when we introduce the Lipschitz regularization term, quantization itself can be used as an effective denoiser to reduce the adversarial perturbation, as much of the perturbation strength is smaller than the quantization bucket width. Thus we call the method Defensive Quantization.\nActually, we have already shown such an effect in Table 1 of the original paper. The column *Quantize Gain* shows the adversarially attacked accuracy improvement of the quantized model compared to the full-precision model, trained with exactly the same Lipschitz regularization term. Without Lipschitz regularization, quantized models are less robust than full-precision ones (-9.1%). While with Lipschitz quantization, the quantized models are consistently more robust.\nIn short, (1) conventional quantized models are less robust. (2) Lipschitz regularization makes the model robust. (3) Lipschitz regularization + quantization makes model even more robust. The reviewer has noticed (1)(2), but we also want to emphasize (3) in the paper. \n\n- Quantization has become an industry standard for deep learning hardware. Making quantized models robust is an important topic that concerns billions of AI devices. To deploy models on GPU/TPU/FPGA/Mobile phones, we need quantization to reduce the storage, inference time and energy. Many companies have provided both hardware and software to support quantized models. For example, TensorFlow-Lite supports running quantized models on mobile phones [1] to reduce inference time and storage. NVIDIA has released TensorCore [2] to support low-bit training like INT4, INT8, and binary precision for inference.\nThe performance of fully quantized models compared to full-precision counterparts is extensively studied in previous works like XNOR-Net [3], BNN [4], DoReFa-Net [5], where weights, activation (and even gradients) are quantized to low-bit representations. With quantization, we can achieve massive compression or speed-up at the cost of little or no accuracy lost. In short, quantization is an important topic widely adopted by industry. Making quantized models robust will benefit billions of AI devices. \n\n- For adversarial training, we used the quantized model itself to generate white-box adversarial samples. For white-box adversarial testing, we also used the quantized model, by definition. For black-box adversarial testing, we used another full-precision model. Since we used an STE (straight through estimator) y=x + clip_gradient(x_quant - x)  to compute the gradient of quantization operator, it behaves just like the normal full-precision model during backpropagation. Therefore we believe it does not make much difference to use full-precision or quantized models.\n\n- Our Lipschitz regularization term only applies to layers with parameters, i.e., convolution and fully connected layer, which make up the majority of modern networks. Our experiments also show that such a regularization term is enough to make quantized models robust. Also, since the size of parameters is much smaller compared to activations (we usually use a large batch size like 256 for training, while we only need to maintain a single copy of parameters), the cost for calculating the regularization is negligible according to our experiments.\nFurthermore, since non-linear layers like ReLU itself has a Lipschitz constant <=1, we do not need to take special care of them.\n\n[1] https://www.tensorflow.org/lite/performance/model_optimization#model_quantization\n[2] https://www.nvidia.com/en-us/data-center/tensorcore/\n[3] Rastegari et al., XNOR-Net: Imagenet classification using binary convolutional neural networks\n[4] Courbariaux et al., Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1\n[5] Zhou et al., DoReFa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "BkxPH1-xp7", "original": null, "number": 8, "cdate": 1541570367413, "ddate": null, "tcdate": 1541570367413, "tmdate": 1541570367413, "tddate": null, "forum": "ryetZ20ctX", "replyto": "rygxXn_fhQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "Reviewer 1, I'm sure you have a lot going on, and having only a few weeks to review several papers can be hard, but I would encourage you to re-read the paper carefully and write a more detailed review. This may not be your area -- that's okay. In fact, given that it's not your area of focus, your perspective could be very helpful in what could be improved on to make the paper more generally accessible to the broader community.\n\nUnfortunately, what you currently have written does not come close to resembling a complete paper review. It is disrespectful to both the authors of this paper (even if you do give it a high score, they don't get any comments on how to improve it) and the other reviewers (who put effort into writing a thorough review of the paper).", "title": "Please write a more thoughtful review"}, "signatures": ["~Nicholas_Carlini1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Nicholas_Carlini1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "SylvEsdV3m", "original": null, "number": 2, "cdate": 1540815662840, "ddate": null, "tcdate": 1540815662840, "tmdate": 1541533347472, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Review", "content": {"title": "A simple regularization scheme that efficiently protects quantized models from adversarial attacks", "review": "Summary: \nThe paper proposes a regualrization scheme to protect quantized neural networks from adversarial attacks. The authors observe that quantized models become less robust to adversarial attacks if the quantization includes the inner layers of the network. They propose a Lipschitz constant filtering of the inner layers' input-output to fix the issue.  \n\nStrengths:\nThe key empirical observation that fully quantized models are more exposed to adversarial attacks is remarkable in itself and the explanation given by the authors is reasonable. The paper shows how a simple regularization scheme may become highly effective when it is supported by a good understanding of the underlying process.\n\nWeaknesses:\nExcept for observing the empirical weakness of fully quantized models, the technical contribution of the paper seems to be limited to combining the Lipschitz-based regularization and quantization. Has the Lipschitz technique already been proposed and analysed elsewhere? If not, the quality of the paper would be improved by investigating a bit more the effects of the regularization from an empirical and theoretical perspective. If yes, are there substantial differences between applying the scheme to quantized models and using it on full-precision networks? It looks like the description of the Lipschitz method in Section 4 is restricted to linear layers and it is not clear if training is feasible/efficient in the general case.\n \nQuestions:\n- has the Lipschitz technique been proposed and analysed elsewhere? Is the robustness of full-precision models under adversarial attacks also improved by Lipschitz regularization?\n- how popular is the practice of quantizing inner layers? Has the performance of fully quantized models ever been compared to full-precision or partially quantized models in an extensive way (beyond adversarial attack robustness)? \n- are the adversarial attacks computed using the full-precision or the quantized models? would this make any difference?\n- the description of the Lipschitz regularization given in Section 4 assumes the layers to be linear. Does the same approach apply to non-linear layers? Would the training be feasible in this case? ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Review", "cdate": 1542234284951, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335892371, "tmdate": 1552335892371, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygxXn_fhQ", "original": null, "number": 1, "cdate": 1540684824224, "ddate": null, "tcdate": 1540684824224, "tmdate": 1541533347274, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Review", "content": {"title": "while i am no expert an adversarial attacks in deep learning, the results are compelling imho", "review": "imho, this manuscript is clearly written, addresses a confusing point in the current literature, clarifies some issues, and provides a novel and useful approach to mitigate those issues. \nreading the other comments online, the authors seem to have addressed those concerns as well.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Review", "cdate": 1542234284951, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335892371, "tmdate": 1552335892371, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1e96wfRoQ", "original": null, "number": 7, "cdate": 1540396994036, "ddate": null, "tcdate": 1540396994036, "tmdate": 1540396994036, "tddate": null, "forum": "ryetZ20ctX", "replyto": "BJeTRnNpjX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "In the paper you report 44% accuracy for PGD training and 43% accuracy for R+FGSM training (going up to 50% with DQ). The table above shows 36%. without DQ and 43% with DQ. Which is correct?\n\nAlso, Madry et al. uses 40 iterations of PGD to attack the trained model, not 7 (which is used during training).", "title": "Those results are different"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "BJeTRnNpjX", "original": null, "number": 6, "cdate": 1540340948812, "ddate": null, "tcdate": 1540340948812, "tmdate": 1540340997796, "tddate": null, "forum": "ryetZ20ctX", "replyto": "Hyly7UCijQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "Our attacker is not broken, and the results are correct", "comment": "We have confirmed with the author of Madry et al. (2018) about the details of R+FGSM adversarial training in their OpenReview reply. To make a fair comparison, we re-implemented their R+FGSM training and test the accuracy under PGD attack using their hyper-parameter (step_size=2, n_step=7, eps=8) compared to our R+FGSM adversarially trained model. The results are listed below:\n\n\t\t\t\t\t\t\t\toriginal\t\tPGD (Madry\u2019s) attacked accuracy\nMadry\u2019s adv. R+FGSM training\t\t93.130\t\t2.8\nOur adv. R+FGSM training\t\t\t91.61\t\t36.05\nOur adv. R+FGSM training + DQ\t94.0\t\t\t43.23\n\nWe can see that using R+FGSM (Madry\u2019s) adversarial training, the resulted model indeed reaches near 0% accuracy under PGD attack, while the model trained with our R+FGSM adversarial training is resistant to the attack. And DQ further improves the robustness. The reason is not due to the attacker, since we used the same attacker, but we have a stronger defense method. We think the difference is due to mainly 2 reasons:\n1. In Madry\u2019s R+FGSM adversarial training, they used a fixed eps same as testing (here they use eps=8). While in our method, the eps is sampled from a truncated normal distribution ranging (0, 16) during training, so that the model does not overfit to a certain epsilon. Our method can generate a noise with bigger infinity norm (up to 16) and thus provides better robustness. At the same time, it also reduces overfitting to R+FGSM adversarial samples itself and prevents gradient masking.\n2. In Madry\u2019s R+FGSM, they first sample a noise within [-eps, eps], and take a step with size 2*eps followed by clipping, so that the noise reaches a corner of the box. The value in the noise is one of {-eps, eps}. While with our implementation, the value of noise is one of {-eps, 0, eps}, which is more representative.\nIn conclusion, PGD attack can break Madry\u2019s R+FGSM adversarial training but cannot break our R+FGSM adversarial training. Therefore, we think our results are correct and the attacker is not broken.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "Hyly7UCijQ", "original": null, "number": 6, "cdate": 1540249111273, "ddate": null, "tcdate": 1540249111273, "tmdate": 1540249111273, "tddate": null, "forum": "ryetZ20ctX", "replyto": "S1gHry3ijQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "I'm sorry, but while technically you are right that they are different, for all intents and purposes, they are the same.\n\nAs you say in your paper, R+FGSM does the following: choose a constant e1, e2 so that e1<e2. For an image x first let x1 = x + e1*R where R is chosen to be randomly component-wise either -1 or 1. Then, let x2 = FGSM(x1, e).\n\nPGD, as described by Madry et al. (2018) does the following: choose a constant e. For an image x first let x1 = x + e*R where each entry of R is chosen *uniformly* from [-1,1]. Then, let x2 = FGSM(x1,e) (and repeat as necessary, but we're talking about 1 iteration here).\n\nWhile technically there is a difference (in how the initial noise sample is selected), they are essentially identical. But okay, technically different. To re-phrase my question then, your paper is claiming that if I do R+FGSM as described above, with epsilon=8, on CIFAR-10, the resulting model will have 43% accuracy? Is this the claim you are making?\n\n\nRegarding the comparisons: I understand you're not trying to compare the two approaches. However, in order to ensure that DQ is actually effective, it is important to also ensure that you get everything else right.\n\nAs of right now, it appears that your attack algorithm is somehow broken, because it should be possible to still attack a R+FGSM trained model and reduce its accuracy to near-0%. Because you can't attack this baseline that is known to be broken successfully, it raises doubts about the evaluation of your DQ defense.", "title": "R+FGSM is nearly the same as 1 step of PGD"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "H1lOYy3joQ", "original": null, "number": 5, "cdate": 1540239232503, "ddate": null, "tcdate": 1540239232503, "tmdate": 1540239232503, "tddate": null, "forum": "ryetZ20ctX", "replyto": "SyxwKb2hcX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "Not related to the claim of our paper", "comment": "In the link you provided, the author uses an iterative version of R+FGSM with multiple random starts, which is different from ours. \n\nAs mentioned in Section 2.2.1, we used the varying attack strength following Song et al. (2016), so that we can test the model\u2019s robustness under different strength. Although we used a smaller step size, we provided the results under eps=16 for PGD, which is a much stronger attack than Madry\u2019s. And our DQ method consistently outperforms normal network and VQ.\n\nFurthermore, all our experiments are conducted under the same setting for vanilla quantization and defensive quantization. Therefore, it does not affect the conclusion that DQ is more robust than VQ. What we try to demonstrate is that DQ is more robust than VQ, but not to compare adversarial R+FGSM training with adversarial PGD training. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "S1gHry3ijQ", "original": null, "number": 4, "cdate": 1540239165378, "ddate": null, "tcdate": 1540239165378, "tmdate": 1540239165378, "tddate": null, "forum": "ryetZ20ctX", "replyto": "BkgmUBVncQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "R+FGSM is different from 1-step PGD", "comment": "No. Even if you use n_step=1 for PGD, it is still different from R+FGSM. Please refer to equation (2) for details. Since we used an iterative PGD, we do not know how it behaves to use PGD with n_step=1.\n\nWe would like to stress that our paper aims to demonstrate the effectiveness of Defensive Quantization, not to compare R+FGSM adversarial training with PGD adversarial training. And all the experiments are conducted under the same setting for strict comparison. Comparison of other defend methods is beyond the scope of our paper.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "SyxwKb2hcX", "original": null, "number": 5, "cdate": 1539256702679, "ddate": null, "tcdate": 1539256702679, "tmdate": 1539257241691, "tddate": null, "forum": "ryetZ20ctX", "replyto": "BkgmUBVncQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "In this comment ( https://openreview.net/forum?id=rJzIBfZAb&noteId=HkRKZDTQM ), the authors claimed that when adversarially *training* the network using R+FGSM, it overfits to R+FGSM and completely vulnerable to PGD. \n\nHowever, the result at Table 3 (R+FGSM training without DQ under PGD attacks) is contradictory and I think that the difference is because of the parameters of PGD attacks which are different from Madry et al. (2018), especially the small step-size, alpha=1. See Section 2.2.1.", "title": "It seems that the difference is due to the different parameters in attack not training"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "BkgmUBVncQ", "original": null, "number": 4, "cdate": 1539224907376, "ddate": null, "tcdate": 1539224907376, "tmdate": 1539224907376, "tddate": null, "forum": "ryetZ20ctX", "replyto": "rkx4Xm72cm", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "To confirm then, the claim your paper makes is that if I take PGD adversarial training on CIFAR-10, and reduce the number of iterations to 1, that it is not less effective than before? (Before: Madry et al. use N=7 steps of PGD during training, a step size of 2/255, and a bound of eps=8/255. After: this paper proposes N=1 steps of PGD during training, a step size of 8/255, and a bound of eps=8/255.)\n\nLooking at Table 3: PGD adversarial training reaches 44% accuracy on eps=8 PGD attacks, and R+FGSM adversarial training reaches 43% accuracy accuracy on eps=8 PGD attacks.\n\nIs this correct?", "title": "Clarifying your claims"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "rkx4Xm72cm", "original": null, "number": 3, "cdate": 1539220252039, "ddate": null, "tcdate": 1539220252039, "tmdate": 1539220252039, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryly95Z29X", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "We used PGD (Mady et al., 2018) but not BIM for adversarial training", "comment": "No, we *strictly* used the PGD as proposed by Madry et al. (2018) for adversarial training and attack evaluation, *NOT* the BIM without a random start. We mentioned R+FGSM is less likely to cause gradient masking compared to *FGSM* because of the random start, but not compared to *PGD*.\n\nUnder stronger attack like PGD, our experiments have consistently shown that defensive quantization outperforms vanilla quantization, and bridges the gap between efficiency and robustness."}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "ryly95Z29X", "original": null, "number": 3, "cdate": 1539213959069, "ddate": null, "tcdate": 1539213959069, "tmdate": 1539213959069, "tddate": null, "forum": "ryetZ20ctX", "replyto": "Hkehlty29X", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "Reading the text more carefully, it appears you are not performing the strong version of adversarial training as proposed by Madry et al. (2018), but instead the BIM as used by Kurakin et al. (2016). The PGD implementation as proposed by Madry et al. (2018) take a single random step and then perform PGD from there. That is to say,  R-FGSM is identical to 1 step of PGD. So the extra random steps can not be the reason for preventing gradient masking. Calling this \"Adversarial PGD\" is therefore deceptive.\n", "title": "R-FGSM is strictly weaker than PGD"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "ByguRKJhqQ", "original": null, "number": 2, "cdate": 1539205583835, "ddate": null, "tcdate": 1539205583835, "tmdate": 1539205583835, "tddate": null, "forum": "ryetZ20ctX", "replyto": "HklDKcVZqQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "We already demonstrated the robustness of DQ under strong attack. We added strong attack results to Figure 5 and the result is consistent.", "comment": "We used FGSM in motivation, but we used PGD in experiments (Table 2/3). Even under weak attacks, vanilla quantization suffers. Even under strong attacks, defensive quantization is more robust. We have already shown the robustness of DQ under strong attack (PGD) in Table 2 and 3, both white and black box. \n \nWe choose FGSM attack for motivation because it is most transferable (Su et al., 2018) and thus best for mounting black-box attacks (Kurakin et al., 2017), while the black-box robustness is more essential for real deployed models. \nTo solve the reviewer\u2019s concern and make our claim stronger, we added the results of PGD attack (\u03b5=8) (Madry et al., 2018) to Figure 5. The corresponding results are listed below:\nAccuracy under PGD attack:\nn_bit\t1\t2\t3\t4\n----------- white-box -----------\nVQ\t\t1.6\t0.7\t0.2\t0.4\nDQ\t\t1.3\t1.0\t2.0\t1.1\n----------- black-box -----------\nVQ\t\t35.7\t59.2\t64.0\t65.0\t\nDQ\t\t69.7\t68.0\t68.9\t68.3\n\nFor black-box under PGD attack, our Defensive Quantization (DQ) consistently out-performed Vanilla Quantization (VQ) by a large margin. For example, under 1-bit quantization, VQ got only 35.7% accuracy, while DQ still maintains the accuracy at 69.7%. The trend is similar to the FGSM attacked results written in the paper.\nFor white-box, since the models are normally trained without adversarial training, the white-box accuracy is randomly near zero for both VQ and DQ, this is another reason why we use FGSM for the motivation. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "Hkehlty29X", "original": null, "number": 1, "cdate": 1539205363529, "ddate": null, "tcdate": 1539205363529, "tmdate": 1539205363529, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryxW9o4Z9m", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "content": {"title": "No Contradiction with Prior Conclusion. We Observed Consistent Trends Demonstrated by Other Paper", "comment": "Thanks. R+FGSM was proposed in Tramer et al. (2018), where the author used R+FGSM for the black-box based adversarial training. Previous work showed adversarial *FGSM* training is weaker than PGD, but not *R-FGSM*. There is no prior work conducted comparisons between white-box R+FGSM and PGD adversarial training, and thus no conclusion whether one of them is significantly better. In fact, compared with FGSM, R-FGSM introduced randomness, making it less likely to cause gradient masking than FGSM.\n\nWe do find a similar result (https://openreview.net/forum?id=ryxeB30cYX) where the author finds that randomized one-step adversarial training can achieve comparable and even better robustness than PGD adversarial training. R+FGSM adversarial training is also a randomized one-step adversarial training method, and therefore also achieves comparable or better robustness against PGD adversarial training. Since adversarial R+FGSM training is more stable, it would be possible to achieve better robustness under certain evaluations."}, "signatures": ["ICLR.cc/2019/Conference/Paper1189/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605314, "tddate": null, "super": null, "final": null, "reply": {"forum": "ryetZ20ctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1189/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1189/Authors|ICLR.cc/2019/Conference/Paper1189/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605314}}}, {"id": "ryxW9o4Z9m", "original": null, "number": 2, "cdate": 1538505608946, "ddate": null, "tcdate": 1538505608946, "tmdate": 1538505608946, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "Table 3 appears to show that training with R+FGSM is more robust than full PGD adversarial training, consistently across every entry entry, even without DQ. This contradicts much prior work, especially Madry et al. (2018). Do the authors believe there is something interesting going on to cause this?", "title": "Confusing Table 3 Results"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}, {"id": "HklDKcVZqQ", "original": null, "number": 1, "cdate": 1538505343296, "ddate": null, "tcdate": 1538505343296, "tmdate": 1538505343296, "tddate": null, "forum": "ryetZ20ctX", "replyto": "ryetZ20ctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "content": {"comment": "This paper makes many of its claims (e.g., Table 1, Figure 5) by evaluating against FGSM. Unfortunately, many times results that appear correct using FGSM do not hold true against the stronger attacks of Kurakin et al. (2017), Carlini & Wagner (2017), Madry et al. (2018). It would strengthen the paper to use one of these attacks consistently.", "title": "FGSM results are limiting"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1189/Reviewers/Unsubmitted"], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Defensive Quantization: When Efficiency Meets Robustness", "abstract": "Neural network quantization is becoming an industry standard to efficiently deploy deep learning models on hardware platforms, such as CPU, GPU, TPU, and FPGAs. However, we observe that the conventional quantization approaches are vulnerable to adversarial attacks. This paper aims to raise people's awareness about the security of the quantized models, and we designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models. We first conduct an empirical study to show that vanilla quantization suffers more from adversarial attacks. We observe that the inferior robustness comes from the error amplification effect, where the quantization operation further enlarges the distance caused by amplified noise. Then we propose a novel Defensive Quantization (DQ) method by controlling the Lipschitz constant of the network during quantization, such that the magnitude of the adversarial noise remains non-expansive during inference. Extensive experiments on CIFAR-10 and SVHN datasets demonstrate that our new quantization method can defend neural networks against adversarial examples, and even achieves superior robustness than their full-precision counterparts, while maintaining the same hardware efficiency as vanilla quantization approaches. As a by-product, DQ can also improve the accuracy of quantized models without adversarial attack. ", "keywords": ["defensive quantization", "model quantization", "adversarial attack", "efficiency", "robustness"], "authorids": ["jilin@mit.edu", "ganchuang1990@gmail.com", "songhan@mit.edu"], "authors": ["Ji Lin", "Chuang Gan", "Song Han"], "TL;DR": "We designed a novel quantization methodology to jointly optimize the efficiency and robustness of deep learning models.", "pdf": "/pdf/5e4d6bba768be6c31d29f850aa0ac1dacb7f461d.pdf", "paperhash": "lin|defensive_quantization_when_efficiency_meets_robustness", "_bibtex": "@inproceedings{\nlin2018defensive,\ntitle={Defensive Quantization: When Efficiency Meets Robustness},\nauthor={Ji Lin and Chuang Gan and Song Han},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=ryetZ20ctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1189/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311657686, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "ryetZ20ctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1189/Authors", "ICLR.cc/2019/Conference/Paper1189/Reviewers", "ICLR.cc/2019/Conference/Paper1189/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311657686}}}], "count": 22}