{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1363989120000, "tcdate": 1363989120000, "number": 4, "id": "JJQpYH2mRDJmM", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-AIqBI4_qZAQ1", "replyto": "-AIqBI4_qZAQ1", "signatures": ["Luis Gonzalo S\u00e1nchez"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The newest version of the  paper will appear on arXiv by Monday March 25th. \r\nIn the mean time the paper can be seen at the following link:\r\nhttps://docs.google.com/file/d/0B6IHvj9GXU3dMk1IeUNfUEpqSmc/edit?usp=sharing"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363799700000, "tcdate": 1363799700000, "number": 1, "id": "suhMsqNkdKs6R", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "-AIqBI4_qZAQ1", "replyto": "5pA7ERXu7H5uQ", "signatures": ["Luis Gonzalo S\u00e1nchez"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "This is the same comment from below, we just realized that this is the reply button for your comments.\r\nDear reviewer, we appreciate the comments and the effort put into reviewing our work. We believe you have made a very valid point by asking us about the role of alpha. The order of the matrix entropy acts as an Lp norm on the eigenvalues of the Gram matrix. The larger the entropy the more emphasis on the largest eigenvalues. This behaviour translates onto our metric learning algorithm as going from multimodal very flexible class structure towards a unimodal more constrained class structure as we increase alpha. We include an example that illustrates this behaviour. With respect to HSIC, it is true that for alpha =2 the trace of K^2 shares some resemblance with the criterion. However there are several differences that makes the connection hard to establish. First, when dealing with covariance operation it has been already assumed that the mean elements have been removed (covariance operator is centred) . As we see form the introductory motivation the second order entropy is the norm of the mean vector in the RKHS. If the mean is removed this vector has zero norm. We also require our Gram matrix to have non-negative entries so that our information theoretic interpretation makes sense. We have now included comparisons with NCA."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363775340000, "tcdate": 1363775340000, "number": 2, "id": "ssJmfOuxKafV5", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-AIqBI4_qZAQ1", "replyto": "-AIqBI4_qZAQ1", "signatures": ["Luis Gonzalo S\u00e1nchez"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The new version of the paper can be accessed through \r\nhttps://docs.google.com/file/d/0B6IHvj9GXU3dekxXMHZVdmphTXc/edit?usp=sharing\r\n\r\nuntil it is updated in arXiv"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363773600000, "tcdate": 1363773600000, "number": 1, "id": "nVC7VhbpFDnlL", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "-AIqBI4_qZAQ1", "replyto": "J04ah1kBas0qR", "signatures": ["Luis Gonzalo S\u00e1nchez"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks again for the good comments. We have worked hard on improving the presentation of the results. \r\nWith regard to your cons:\r\ni) We improve the presentation of the ideas by highlighting what are the contributions and why they are relevant. In section 3, where there was no clear delineation between what is know and what is new, we put our effort on  explaining the reason in including some well known results since they help understanding the role of the infinite divisible kernels in computing the proposed information theoretic quantities. We provide both a graphical and textual explanation of the main ideas that can be extracted from section 3.\r\nii)  Section 1 was revisited and redistributed to facilitate grasping the main ideas and contributions. We tried to emphasize more on the results obtained for the application to metric learning. We also motivate the proposed quantities from the point of view of computing high order descriptors of the data based on positive definite kernels.\r\niii) Section 4 was modified to convey the main result, which is the computation of the gradient of the proposed entropy, in a much simpler way. The technical details were moved to an appendix.\r\niv) we took care of the typos that have been pointed out by the reviewers as well as other we found during the paper improvement."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1363772280000, "tcdate": 1363772280000, "number": 1, "id": "hhNRhrYspih_x", "invitation": "ICLR.cc/2013/-/submission/reply", "forum": "-AIqBI4_qZAQ1", "replyto": "cUCwU-yxtoURe", "signatures": ["Luis Gonzalo S\u00e1nchez"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for the comments. We really appreciate the time you put into reviewing our paper. I agree that in the original presentation many of the main points  and contributions of the paper where hard to grasp. In the new version, we have made our contributions explicit. and some of the technical exposition was modified to avoid getting lost in t details.  We emphasized on the equations to and provide better motivations for the mathematical developments of each section. We agree that some of the details could be safely  moved to an appendix, without compromising the relevant results."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362276900000, "tcdate": 1362276900000, "number": 1, "id": "5pA7ERXu7H5uQ", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-AIqBI4_qZAQ1", "replyto": "-AIqBI4_qZAQ1", "signatures": ["anonymous reviewer 5093"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Information Theoretic Learning with Infinitely Divisible Kernels", "review": "This paper proposes a new type of information measure for positive semidefinte matrices, which is essentially the logarithm of the sum of powers of eigenvalues. Several entropy-like properties are shown based on properties of spectral functions. A notion of joint entropy is then defined through Hadamard products, which leads to conditional entropies.\r\n\r\nThe newly defined conditional entropy is finally applied to metric learning, leading naturally to a gradient descent procedure. Experiments show that the performance of the new procedure exceeds the state of the art (e.g., LMNN).\r\n\r\nI did not understand the part on infinitely divisible matrices and why Theorem 3.1 leads to a link with maximum entropy.\r\n\r\nTo the best of my knowledge, the ideas proposed in the paper are novel. I like the approach of trying to defining measures that have similar properties than entropies without the computational burden of computing densities. However, I would have like more discussion of the effect of alpha (e.g., why alpha=1.01 in experiments? does it make a big difference to change alpha? what does it corresponds to for alpha =2, in particular in relation fo HSIC?).\r\n\r\nPros:\r\n-New information measure with attractive properties\r\n-Simple algorithm for metric learning\r\n\r\nCons:\r\n-Lack of comparison with NCA which is another non-convex approach (J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov. (2005) Neighbourhood Component Analysis. Advances in Neural Information Processing Systems. 17, 513-520.\r\n-Too little discussion on the choice of alpha"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362229800000, "tcdate": 1362229800000, "number": 3, "id": "J04ah1kBas0qR", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-AIqBI4_qZAQ1", "replyto": "-AIqBI4_qZAQ1", "signatures": ["anonymous reviewer 4ccd"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Information Theoretic Learning with Infinitely Divisible Kernels", "review": "This paper introduces new entropy-like quantities on positive semi definite matrices. These quantities can be directly calculated from the Gram matrix of the data, and they do not require density estimation. This is an attractive property, because density estimation can be difficult in many cases. Based on this theory, the authors propose a supervised metric learning algorithm which achieves competitive results.\r\n\r\nPros: The problem studied in the paper is interesting and important. The empirical results are promising.\r\n\r\nCons: \r\ni) Although I believe that there are many great ideas in the paper, in my opinion the presentation of the paper needs significant improvement. It is very difficult to asses what exactly the novel contributions are in the paper, because the authors didn't separate their new results well enough from the existing results. For example, Section 3 is about infinitely divisible matrices, but I don't know what exactly the new results are in this section. \r\n\r\nii) The introduction and motivation could be improved as well. The main message and its importance is a bit vague to me. I recommend revising Section 1. The main motivation to design new entropy like quantities was that density estimation is difficult and we might need lots of sample points to get satisfactory results. That's true that the proposed approach doesn't require density estimation, but it is still not clear if the proposed approach works better than those algorithms that use density estimators. \r\nThe empirical results seem very promising, so maybe I would emphasize them more.\r\n\r\niii) There are a few places in the text where the presented idea is simple, but it is presented in a complicated way and therefore it is difficult to understand. For example Section 4.1 and 4.2 seem more difficult than they should be. The definition of function F is not clear either.\r\n\r\niv) There are  a few typos and grammatical mistakes in the paper that also need to be fixed before publication.\r\nFor example, on Page 1:\r\nPage 1: know as --> known as\r\nPage 1: jlearning"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1362176820000, "tcdate": 1362176820000, "number": 5, "id": "cUCwU-yxtoURe", "invitation": "ICLR.cc/2013/-/submission/review", "forum": "-AIqBI4_qZAQ1", "replyto": "-AIqBI4_qZAQ1", "signatures": ["anonymous reviewer 2169"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Information Theoretic Learning with Infinitely Divisible Kernels", "review": "The paper introduces a new approach to supervised metric learning. The\r\nsetting is somewhat similar to the information-theoretic approach of\r\nDavis et al. (2007). The main difference is that here the\r\nparameterized Mahalanobis distance is tuned by optimizing a new\r\ninformation-theoretical criterion, based on a matrix functional\r\ninspired by Renyi's entropy. Eqs. (5), (11) and (19) and their\r\nexplanations are basically enough to grasp the basic idea. In order to\r\nreach the above goal, several mathematical technicalities are\r\nnecessary and well developed in the paper. A key tool are infinitely\r\ndivisible matrices.\r\n\r\n  + New criterion for information-theoretic learning\r\n  + The mathematical development is sound\r\n+/- The Renyi-inspired functional could be useful in other contexts\r\n    (but details remain unanswered in the paper)\r\n\r\n  - The presentation is very technical and goes bottom-up making it\r\n    difficult to get the 'big picture' (which is not too complicated)\r\n    until Section 4 (also it's not immediately clear which parts\r\n    convey the essential message of the paper and which parts are just\r\n    technical details, for example Section 2.1 could be safely moved\r\n    into an appendix mentioning the result when needed).\r\n\r\n  - Experiments show that the method works. I think this is almost\r\n    enough for a conference paper. Still, it would improve the paper\r\n    to see a clear direct comparison between this approach and\r\n    KL-divergence where the advantages outlined in the conclusions\r\n    (quote: 'The proposed quantities do not assume that the density of\r\n    the data has been estimated, which avoids the difficulties related\r\n    to it.') are really appreciated. Perhaps an experiment with\r\n    artificial data could be enough to complete this paper but real\r\n    world applications would be nice to see in the future.\r\n\r\nMinor:\r\n\r\nSection 2. Some undefined symbols: $M_n$, $sigma(A)$ (spectrum of A?)\r\n\r\nPage 3: I think you mean \r\n   'where $n_1$ of the entries are 1$ -> $where $n_1$ of the entries of $mathbf{1}$ are 1$"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1358406900000, "tcdate": 1358406900000, "number": 23, "id": "-AIqBI4_qZAQ1", "invitation": "ICLR.cc/2013/conference/-/submission", "forum": "-AIqBI4_qZAQ1", "signatures": ["luisitobarcito@gmail.com"], "readers": ["everyone"], "content": {"title": "Information Theoretic Learning with Infinitely Divisible Kernels", "decision": "conferenceOral-iclr2013-conference", "abstract": "In this paper, we develop a framework for information theoretic learning based on infinitely divisible matrices. We formulate an entropy-like functional on positive definite matrices based on Renyi's entropy definition and examine some key properties of this functional that lead to the concept of infinite divisibility. The proposed formulation avoids the plug in estimation of density and brings along the representation power of reproducing kernel Hilbert spaces. We show how analogues to quantities such as conditional entropy can be defined, enabling solutions to learning problems. In particular, we derive a supervised metric learning algorithm with very competitive results.", "pdf": "https://arxiv.org/abs/1301.3551", "paperhash": "s\u00e1nchez|information_theoretic_learning_with_infinitely_divisible_kernels", "keywords": [], "conflicts": [], "authors": ["Luis Gonzalo S\u00e1nchez", "Jose C. Principe"], "authorids": ["luisitobarcito@gmail.com", "principe@cnel.ufl.edu"]}, "writers": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496673673639, "cdate": 1496673673639, "tcdate": 1496673673639, "id": "ICLR.cc/2013/conference/-/submission", "writers": ["ICLR.cc/2013"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717}}}], "count": 9}