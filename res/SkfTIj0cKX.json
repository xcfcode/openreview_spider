{"notes": [{"id": "SkfTIj0cKX", "original": "SJxq4k9ndQ", "number": 217, "cdate": 1538087765248, "ddate": null, "tcdate": 1538087765248, "tmdate": 1545355436006, "tddate": null, "forum": "SkfTIj0cKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkghMjKHgV", "original": null, "number": 1, "cdate": 1545079571893, "ddate": null, "tcdate": 1545079571893, "tmdate": 1545354480884, "tddate": null, "forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper217/Meta_Review", "content": {"metareview": "This paper addresses the problem of recommendations within user sessions from a reinforcement learning perspective. The problem is naturally modeled as an RL problem, given its sequential nature and inherent uncertainty of any model over user preferences. The problem suffers from delayed and sparse rewards, which the authors propose to address using self-supervised prediction. The approach is empirically validated in a simulated setting, using data from the 2015 ACM RecSys Challenge.\n\nThe reviewers and AC note that the problem studied is an important application area where RL has high potential to improve over current research results and industry practice. The proposed idea is interesting, and the strong empirical evaluation on a publicly available data set is highlighted. R1 also commends the authors' decision to address the challenging cold-start problem.\n\nThe reviewers and AC also note several potential weaknesses. The choice of addressing the problem from a reinforcement learning perspective is not clearly motivated. This is needed, as many supervised learning (and other types) approaches to the problem exist. A performance comparison to current state-of-the-art RL baselines is missing. The proposed approach is related to both imagination augmented (I2A, Racaniere et al. 2017) and agents with auxiliary rewards (UNREAL, Jaderberg et al. 2016), but does not compare to either method. Neither does the related work section sufficiently clarify why the proposed approach is expected to improve over these prior approaches. A thorough comparison to these baselines in a real-world application like session-based recommendation would be a strong contribution in itself, but without the contributions of the paper are hard to assess. Reviewers also noted lack of clarity. Some concerns are addressed by the authors, but the consensus is that the paper would benefit from a major revision to clearly work out the method, as well as it's conceptual and empirical differences from existing reinforcement learning approaches. R3 mentions missing related work, some of which the authors include in the revision. The AC recommends also following up on references in cited papers to ensure a future revision of the paper is well placed in the context of prior work on recommender systems, especially when modeled as a reinforcement learning problem.\n\nOverall, the paper was assessed as borderline by the reviewers. The ACs view is that there are too many concerns for acceptance at ICLR in the present form, and that the paper will benefit from a thorough revision.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "important application area, not sufficiently placed in the context of prior work (both conceptually and empirically)"}, "signatures": ["ICLR.cc/2019/Conference/Paper217/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper217/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper217/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353294209, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper217/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper217/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper217/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353294209}}}, {"id": "SyenKQGCA7", "original": null, "number": 4, "cdate": 1543541636487, "ddate": null, "tcdate": 1543541636487, "tmdate": 1543542896914, "tddate": null, "forum": "SkfTIj0cKX", "replyto": "B1lGkaXqnm", "invitation": "ICLR.cc/2019/Conference/-/Paper217/Official_Comment", "content": {"title": "Thank you for your appreciation of our contributions. We apologize if you had a hard time reading this paper.", "comment": "Q1: What is L_A3C in \u201cL = L_A3C + L_IRN\u201d in the first paragraph of session 4? It looks like a loss from a previous paper, but it\u2019s kind hard to track what it is exactly.\nA1: Thanks. Based on your comment, we have added detailed descriptions for both L_A3C and L_IRN. The L_A3C loss function is defined in Section 3 (\u201cAsynchronous Advantage Actor-Critic\u201d), which maximizes the external accumulated reward via policy gradient and value regression. \n\nQ2: \u201c\u2026Therefore, we use the one-hot transformation as \u03c6(\u00b7) and replace AE with the policy \u03c0 (excluding the final softmax function), and only back-propagate errors of non-zero entries.\u201d This seems one of the most important components of the proposed algorithm, but I found it\u2019s very hard to understanding what is done here exactly.\nA2: We have revised some flaws and supplemented detailed descriptions in Section 4 (especially \u201cImagination-augmented Executor\u201d).  For an input state s_t,  we use the imagination policy (or action policy \\pi for simplicity) to generate several imagined trajectories (each is a sequence of imagined items). To train the action policy \\pi, we introduce the loss function L_IRN and use these imagined trajectories as inputs and output targets to compute the backward error. Note that different imagined items are fed into different timestamps of the LSTM. For an imagined item (one time step), the mean squared error of Eq.4 is computed between its activation value through \\pi (prediction without softmax) and one (target label); errors for other items are turned to be zero. In other words, only errors for imagined items are back-propagated to optimize the policy network. Combined with A3C, the action policy \\pi is optimized not only to predict purchases accurately but also to minimize the reconstruction error of imagined items over time. Take a session for example, {i_0 , i_1 , ..., i_{q\u22121} , i_q } (i_q is the \ufb01nal purchased item), the policy network \\pi is trained t+1 times using imagination reconstruction and once using A3C updating (for the purchase event).\n\nQ3: It\u2019s interesting to see the comparison between different reward function.\nA3: For A3C-F and IRN-F (even GRU4Rec with different label weights), increasing the value of purchase reward can slightly improve the purchase performance at the cost of lower click performance; increasing the value of click reward degrades the purchase performance, due to the purchase reward sparsity. For IRN-P, only the purchase reward is used as supervision, the imagination reconstruction can be regarded as goal-oriented learning or semi-supervised learning. As a result, different purchase values give similar performance for IRN-P."}, "signatures": ["ICLR.cc/2019/Conference/Paper217/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper217/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper217/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605858, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkfTIj0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference/Paper217/Reviewers", "ICLR.cc/2019/Conference/Paper217/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper217/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper217/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper217/Authors|ICLR.cc/2019/Conference/Paper217/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper217/Reviewers", "ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference/Paper217/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605858}}}, {"id": "rye-UEDYC7", "original": null, "number": 1, "cdate": 1543234633442, "ddate": null, "tcdate": 1543234633442, "tmdate": 1543542008013, "tddate": null, "forum": "SkfTIj0cKX", "replyto": "Hkl4OErXa7", "invitation": "ICLR.cc/2019/Conference/-/Paper217/Official_Comment", "content": {"title": "Apologies for being unclear in some parts of our paper. We address your concerns below.  ", "comment": "Q1: The contributions of the paper in terms of theory are somewhat not significant. It seems that the proposed algorithm is built based on and combined by existing algorithms such as A3C. \nA1: IRN with GRU4Rec also significantly enhances the prediction performance. We would like to emphasize that the contributions of the paper lies primarily in two points: 1) a new application, i.e., optimizing the sparse and delayed purchase signal directly may be more business-critical, which is different from traditional CTR (Click-Through Rate Prediction) techniques; 2) the proposed imagination reconstruction module that is able to learn predictive representations even without any external reward (when training the action policy \\pi, inputs and output targets used for self-supervised reconstruction are all counterfactual predictions, i.e., the predictive items in imagined trajectories \\hat{\\mathcal{T}}_{j,\\tau}).\n\nQ2: The motivations of applying reinforcement learning. Why do the authors utilize RL to the task but not other supervised learning techniques? \nA2: We aim to solve the sequential recommendation problem, which meets the setting of MDP: the recommendation agent provides a list of candidate items to the user at a given timestamp, according to that observed state. Supervised learning techniques like GRU4Rec utilizes the session-parallel mini-batch training to handle the variable lengths of sessions. This training trick takes the current clicked item and the previous  (precomputed) hidden state as inputs of the RNN. As a result, GRU4Rec do not explicitly capture the sequential property of sessions, since the network is trained using the BP algorithm (not BPTT for RNNs). On the other hand, reinforcement learning alleviates this problem by encoding sequentiality of states into the value function (A3C-F performs better than GRU4Rec). Indeed, integrating the imagination reconstruction module into GRU4Rec also enhances the performance, but IRN with A3C achieves the best performance. \n\nQ3:  As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines.\nA3: We do not include other RL algorithms as baselines, which cannot further enhance the prediction performance. Different from Atari games, we do not have the environment simulator for recommendation systems. Therefore,  only a small fraction of real trajectories (i.e., offline session logs) is available, making it difficult to learn a better value function; A3C is enough to achieve a competitive result.\n\nQ4: How do the authors define the loss functions, i.e., \\mathcal{L}_{A3C} and \\mathcal{L}_{IRN}? What are the relationships among \\mathcal{L}_{A3C}, \\mathcal{L}_{IRN} and the one defined in equation (4)?\nA4: In section 5 (\u201carchitecture\u201d), we mentioned that the imagination reconstruction is performed every one environment step while the A3C updating is performed with immediate purchase reward (when found) or 3-step returns (when click reward is used). Take a session for example, {i_0, i_1, ..., i_{q-1},i_q} (here i_q is the purchased item), IRN-P is trained t+1 times using \\mathcal{L}_{IRN} and one time using \\mathcal{L}_{A3C} (for the final purchase).  Definitions of these loss functions are added to the revised paper (see Section 3, \"Asynchronous Advantage Actor-Critic\" and Section 4, \"Imagination-augmented Executor\"). \n\nQ5: In equations (2) and (3), what is theta_v? How is theta_v associated with the parameters in LSTM. Is theta_v denoted the parameters of LSTM? \nA5: A3C is an actor-critic approach that constructs a policy network \u03c0(a|s; theta) and a value function network V (s; theta_v ), with all non-output layers shared. Specifically, the output of the LSTM layer is fed into two separate fully connected layers to predict the state value and the probability of all actions, which is mentioned in section 5.1 (\u201cArchitecture\u201d) and section 3 (\u201cAsynchronous Advantage Actor-Critic\u201d). In other words, theta and theta_v share the same parameters except for separate weights of two output layers; the parameters of LSTM is the union set of theta and theta_v.\n\nQ6: It would be better if the authors can test the proposed model on more datasets. \nA6: Although there are many publicly available datasets for recommendation systems, we only found this ACM RecSys 2015 dataset that contains both click-streams and final purchase events."}, "signatures": ["ICLR.cc/2019/Conference/Paper217/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper217/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper217/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605858, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkfTIj0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference/Paper217/Reviewers", "ICLR.cc/2019/Conference/Paper217/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper217/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper217/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper217/Authors|ICLR.cc/2019/Conference/Paper217/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper217/Reviewers", "ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference/Paper217/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605858}}}, {"id": "BJlDlN9KRm", "original": null, "number": 2, "cdate": 1543246831060, "ddate": null, "tcdate": 1543246831060, "tmdate": 1543288342283, "tddate": null, "forum": "SkfTIj0cKX", "replyto": "r1e_dvHgnX", "invitation": "ICLR.cc/2019/Conference/-/Paper217/Official_Comment", "content": {"title": "Thank you for your detailed reviews. We address your concerns below.", "comment": "Q1: The motivation of using RL is missing from the technical contribution. Using LSTMs which already encode sequentiality of states in addition to another component for planning, seem to undermine the role of RL. \nA1: Similar to GRU4Rec, we also employ the session-parallel mini-batch trick to handle the variable lengths of sessions when training the LSTM. This training trick takes the current clicked item and the previous  (precomputed) hidden state as inputs of the RNN. As a result, GRU4Rec do not explicitly capture the sequential property of sessions, since the network is trained using the BP algorithm (not BPTT for RNNs). Details can be found in the author\u2019s paper (Session-based recommendations with recurrent neural networks, Hidasi et al., 2016) and implementation (https://github.com/hidasib/GRU4Rec). Therefore, we utilize RL to alleviate this problem by encoding sequentiality of states into the value function. That\u2019s why A3C-F performs better than GRU4Rec. Indeed, integrating the imagination reconstruction module into GRU4Rec also enhances the performance, but IRN with A3C achieves the best performance. \n\nQ2: The literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods.\nA2: We have added the review on MDP-based recommenders, including An MDP-based recommender system, Shani et al., 2015 and Factored MDPs for detecting topics of user sessions, Tavakol and Brefeld, 2014.\n\nQ3: The motivation of creating imagined trajectories instead of actual user trajectories is unclear. On the other hand, there are many traditional planning approaches which are not mentioned such as Monte Carlo Tree Search. \nA3: The IRN architecture is inspired by the theories of cognition science. Minimizing the reconstruction error of imagined trajectories adapts the agent to possible changes that arise from the ongoing exploratory action. When external reward is provided, the imagination-augmented executor (IAE) can be considered as a process of goal-oriented learning or semi-supervised learning. Introducing actual user trajectories into A3C-P (i.e., PRN-P, Table 1) also improves the prediction performance, but the proposed IRN-P significantly outperforms PRN-P, since PRN-P introduces stronger supervision and may not know what is the final goal. Besides, we do not employ MCTS for E&E, since we do not have the environment simulator like that of AlphaGo; a small fraction of real trajectories (i.e., offline session logs) is not enough to learn a value function that can approximate most state values for back-propagation (one step of MCTS).\n\nQ4: The length of the trajectories is only 2 and instead should be varied empirically to show the usefulness of the reconstruction network. \nA4: For IRN without any external reward (unsupervised predictive learning), longer imagined trajectories (>2) can further enhance the prediction performance (by 1%). For IRN with purchase reward (semi-supervised learning), the purchase performance cannot be improved using longer imagined trajectories. One possible reason is that two steps of imagination reconstruction is sufficient for learning to predict the future events recursively; the first step of IRN learns to capture the difference of adjacent input states, and the second step learns to look ahead the future purchase signal accurately.\n\nQ5: Why is there a strong focus on quick adaptation to user sessions? \nA5: In Introduction, we highlight that real-world recommender systems require (1) quick adaptation to user interest; (2) robustness to the cold-start scenario. Enhancing the purchase performance of short sessions is another expression for (2) but not for (1). Although longer sessions better reflect user interests, they would bias the training process of RL algorithms, which is unfair to short ones with the smaller accumulated reward. That's why we are interested in the case where only the purchase is used as reward. The proposed IRN achieves (1) and (2) by learning predictive representations. For (1), figure 4 shows that IRN-P-te generalizes much better to the test set (after 2000 iterations of online learning) compared to A3C-F-te and A3C-P-te. This demonstrates that the imagination reconstruction can promote quick adaptation and capture user\u2019s long-term interest ahead of time.\n\nQ6: How are cold-start situations encountered if items are one-hot encoded?\nA6: For a given session, the input of IRN is a set of one-hot vectors of clicked items, which are fed into different timestamps of LSTMs. The evaluation is done by sequentially adding the current clicked item to the set and calculating the next performance. For the cold-start situations, we only add the first |c| clicked items (or their corresponding one-hot vectors) to the set to evaluate the later purchase event, which is mentioned in section 5.2 (\u201cRobustness to the cold-start scenario\u201d)."}, "signatures": ["ICLR.cc/2019/Conference/Paper217/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper217/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper217/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605858, "tddate": null, "super": null, "final": null, "reply": {"forum": "SkfTIj0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference/Paper217/Reviewers", "ICLR.cc/2019/Conference/Paper217/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper217/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper217/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper217/Authors|ICLR.cc/2019/Conference/Paper217/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper217/Reviewers", "ICLR.cc/2019/Conference/Paper217/Authors", "ICLR.cc/2019/Conference/Paper217/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605858}}}, {"id": "Hkl4OErXa7", "original": null, "number": 3, "cdate": 1541784684282, "ddate": null, "tcdate": 1541784684282, "tmdate": 1541784684282, "tddate": null, "forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper217/Official_Review", "content": {"title": "The motivations of applying reinforcement learning to recommendation systems are not very convinced. Theory contributions may not be very significant", "review": "The paper aimed at improving the performance of recommendation systems via reinforcement learning. The author proposed an Imagination Reconstruction Network for the recommendation task, which implements an imagination-augmented policy via three components: (1)  the imagination core (IC) that predicts the next time steps conditioned on actions sampled from an imagination policy; (2) the trajectory manager (TM) that determines how to roll out the IC under the planning strategy and produces a set of imagined item trajectories; (3) the imagination-augmented executor (IAE) that aggregates the internal data resulting from imagination and external rewarding data to update its action policy.\n\nStrengths of the paper:\n(1) The research problem that the performance of recommendation systems needs to be improved is of great value to be investigated, as recommendation systems play crucial role in people\u2019s daily lives. \n(2) Experiments were conducted on a publicly available dataset. \n(3) Robustness to cold-start scenario was tested and evaluated in the experiments.\n\nWeaknesses of the paper:\n(1) The motivations of applying reinforcement learning techniques are not convinced to me. There are a lot of supervised learning algorithms to the task of recommendations. Why do the authors utilize reinforcement learning to the task but not other supervised learning techniques? Is it because reinforcement learning based methods work better than traditional machine learning based ones? The motivations of integrating A3C (Asynchronous Advantage Actor-Critic) but not other techniques into the proposed model are not convinced to me as well. \n(2) State-of-the-art reinforcement learning algorithms were not taken into account for baselines in the experiments. As the proposed method is built based on reinforcement learning, it would be better if the authors could include state-of-the-art reinforcement learning algorithms as their baselines.\n(3) Some details are missing, resulting in the fact that it is hard for other researchers to fully capture the mechanism of the proposed algorithm. In equations (2) and (3), what is theta_v? How is theta_v associated with the parameters in LSTM. Is theta_v denoted the parameters of LSTM? How do the authors define the loss functions, i.e., \\mathcal{L}_{A3C} and \\mathcal{L}_{IRN}? What are the relationships among \\mathcal{L}_{A3C}, \\mathcal{L}_{IRN} and the one defined in equation (4)?\n(4) The contributions of the paper in terms of theory are somewhat not significant. It seems that the proposed algorithm is built based on and combined by existing algorithms such as A3C. \n\nMinor comments:\n(1) It would be better if the authors can test the proposed model on more datasets. There are many publicly available datasets for testing the performance of recommendation systems.\n(2) Figure 2 is not straightforward. It would be better if the authors can draw the figure in other ways. (I am not sure if the authors have expressed the underlying ideas clearly with Figure 2).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper217/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper217/Official_Review", "cdate": 1542234512417, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper217/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335674672, "tmdate": 1552335674672, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper217/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1lGkaXqnm", "original": null, "number": 2, "cdate": 1541188826491, "ddate": null, "tcdate": 1541188826491, "tmdate": 1541534186436, "tddate": null, "forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper217/Official_Review", "content": {"title": "The main idea of the paper was very interesting, but the clarity of the paper needs to be improved significantly", "review": "The paper proposed a new framework for session-based recommendation system that can optimize for sparse and delayed signal like purchase. The proposed algorithm with an innovative IRN architecture was intriguing. \n\nThe writing of the paper was not very clear and pretty hard to follow. With this level of clarity, I don\u2019t think it\u2019s easy for other people to reproduce the results in this paper, especially in section 4, where I expect more details about the description of the proposed new architecture. Even though the author has promised to release their implementation upon acceptance, I still think the paper needs a major change to make the proposed algorithm more accessible and easier for reproduce.\n\nSome examples:\nWhat is L_A3C in \u201cL = L_A3C + L_IRN\u201d in the first paragraph of session 4? It looks like a loss from a previous paper, but it\u2019s kind hard to track what it is exactly.\n\n\u201cwhere Tj,\u03c4 is the \u03c4-th imagined item, \u03c6(\u00b7) is the input encoder shared by \u03c0 (for joint feature learning), AE is the autoencoder that reconstructs the input feature, and the discounting factor \u03b3 is used to mimic Bellman type operations. \u2026 Therefore, we use the one-hot transformation as \u03c6(\u00b7) and replace AE with the policy \u03c0 (excluding the final softmax function), and only back-propagate errors of non-zero entries.\u201d\nThis seems one of the most important components of the proposed algorithm, but I found it\u2019s very hard to understanding what is done here exactly.\n\nRegardless the sketchy description of the algorithm, the empirical results look good, with comprehensive baseline methods for comparison. It\u2019s interesting to see the comparison between different reward function. Maybe the author can also discuss on the impact of the new imagination module on the training time.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper217/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper217/Official_Review", "cdate": 1542234512417, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper217/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335674672, "tmdate": 1552335674672, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper217/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1e_dvHgnX", "original": null, "number": 1, "cdate": 1540540271726, "ddate": null, "tcdate": 1540540271726, "tmdate": 1541534186230, "tddate": null, "forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper217/Official_Review", "content": {"title": "I consider the proposed method interesting, although it is somewhat incremental. There are some conceptual issues with the proposed approach as well as missing related work. Motivation could be improved. The empirical evaluation is strong. ", "review": "Summary:\n\nThe paper presents a session-based recommendation approach by focusing on user purchases instead of clicks. The method is inspired from concepts of cognitive science which adds an imagination reconstruction network to an actor-critic RL framework in order to encourage exploration.\n\n\nComments:\n\nThe proposed architecture is an interesting inspiration from Neuroscience which fits into the sequential recommendation problem. However, the motivation of using RL is missing from the technical contribution. Considering a deterministic policy, using LSTMs which already encode sequentiality of states in addition to another component for planning, seem to undermine the role of RL. \n\nThe motivation of creating imagined trajectories instead of actual user trajectories is unclear. On the other hand, there are many traditional planning approaches which are not mentioned such as Monte Carlo Tree Search that simultaneously trade-off exploration and exploitation. \n\nThe literature review is incomplete and misses important contributions on session-based recommendation, particularly, MDP-based methods such as Shani et al., An MDP-based recommender system, 2005 and Tavakol and Brefeld, Factored MDPs for Detecting Topics of User Sessions, 2014 (also see references therein).\n\nEmpirically, the authors compare their method to several recent baselines. This renders the empirical part exceptionally strong. Nevertheless, the length of the trajectories is only 2 and instead should be varied empirically to show the usefulness of the reconstruction network. \n\n\nQuestions:\n\n-How are cold-start situations encountered if items are one-hot encoded?\n-Why is there a strong focus on quick adaptation to user sessions? Usually, users tend to search quite a lot before converging; hence, longer sessions possibly better reflect user interests.\n\n\nMinor:\n\n-Proofreading is necessary\n-Table 1 and 2 would be more readable if they were figures\n-Figure 3 seems to be taken from Tensorflow runtime convergence plots, which could be dropped given the limited space", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper217/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction", "abstract": "One of the key challenges of session-based recommender systems is to enhance users\u2019 purchase intentions. In this paper, we formulate the sequential interactions between user sessions and a recommender agent as a Markov Decision Process (MDP). In practice, the purchase reward is delayed and sparse, and may be buried by clicks, making it an impoverished signal for policy learning. Inspired by the prediction error minimization (PEM) and embodied cognition, we propose a simple architecture to augment reward, namely Imagination Reconstruction Network (IRN). Speci\ufb01cally, IRN enables the agent to explore its environment and learn predictive representations via three key components. The imagination core generates predicted trajectories, i.e., imagined items that users may purchase. The trajectory manager controls the granularity of imagined trajectories using the planning strategies, which balances the long-term rewards and short-term rewards. To optimize the action policy, the imagination-augmented executor minimizes the intrinsic imagination error of simulated trajectories by self-supervised reconstruction, while maximizing the extrinsic reward using model-free algorithms. Empirically, IRN promotes quicker adaptation to user interest, and shows improved robustness to the cold-start scenario and ultimately higher purchase performance compared to several baselines. Somewhat surprisingly, IRN using only the purchase reward achieves excellent next-click prediction performance, demonstrating that the agent can \"guess what you like\" via internal planning.", "keywords": ["recommender systems", "reinforcement learning", "predictive learning", "self-supervised RL", "model-based planning"], "authorids": ["qblee@zju.edu.cn", "xlzheng@zju.edu.cn"], "authors": ["Qibing Li", "Xiaolin Zheng"], "TL;DR": "We propose the IRN architecture to augment sparse and delayed purchase reward for session-based recommendation.", "pdf": "/pdf/ada2e2cf2042b3df487aebaef8a86246b1573bf2.pdf", "paperhash": "li|purchase_as_reward_sessionbased_recommendation_by_imagination_reconstruction", "_bibtex": "@misc{\nli2019purchase,\ntitle={Purchase as Reward : Session-based  Recommendation by Imagination Reconstruction},\nauthor={Qibing Li and Xiaolin Zheng},\nyear={2019},\nurl={https://openreview.net/forum?id=SkfTIj0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper217/Official_Review", "cdate": 1542234512417, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SkfTIj0cKX", "replyto": "SkfTIj0cKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper217/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335674672, "tmdate": 1552335674672, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper217/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 8}