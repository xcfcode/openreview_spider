{"notes": [{"id": "KmykpuSrjcq", "original": "l-AW39u1XnG", "number": 3143, "cdate": 1601308348629, "ddate": null, "tcdate": 1601308348629, "tmdate": 1612853877389, "tddate": null, "forum": "KmykpuSrjcq", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-qnS3o33fGe", "original": null, "number": 1, "cdate": 1610040418215, "ddate": null, "tcdate": 1610040418215, "tmdate": 1610474016616, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper proposes an extension to previous unsupervised feature learning work, with an EM-style latent variable model with momentum encoders. The paper is well-written and provides a nice read. It has been noted that it is easy to follow and provides good insights. On the experimental side, compared with MoCo, the proposed approach achieve noticeable improvements. One of the reviewers noted the easy reproducibility of the proposed approach.\n\nSome reviewers noted some comparisons were lacking from the original manuscript, but the authors have update the draft to include those. As noted in the reviews, the field of SSL in vision is moving at a very quick pace, making it hard to clearly state what is the SOTA at time t.\n\nOverall, most questions raised by the reviewers were properly addressed during rebuttal - and given the ratings, I suggest acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040418202, "tmdate": 1610474016600, "id": "ICLR.cc/2021/Conference/Paper3143/-/Decision"}}}, {"id": "PYhm7PJT1aF", "original": null, "number": 1, "cdate": 1603407807109, "ddate": null, "tcdate": 1603407807109, "tmdate": 1606751390451, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review", "content": {"title": "Review for \"Prototypical Contrastive Learning of Unsupervised Representations\"", "review": "### Summary\nIn this paper, the authors propose a method for contrastive learning inspired on current instance discrimination approaches. The propose method, called Prototypical Contrastive Learning, can be seen as an augmentation of MoCo where, besides discriminating each instance, they also introduce prototypes as latent variables. A EM-like algorithm is introduce to train the model with latent variables.\nThe model is trained by considering both instance discrimination (as in MoCo) and prototypical discrimination. The authors provide results in different setting (low-shot image classification, semi-supervised learning and image classification with fixed features).\n\n### Strengths\n+ The paper is nicely written and provides a nice read.\n+ The paper is well motivated and points to valid issue with current issues with (unsupervised) contrastive learning approaches. The idea of providing latent 'prototype' variables to capture the semantic structure of data.\n+ The model is built on top of MoCo and the experiments show constant improvement over MoCo on different range of experiments.\n\n### Weakness\n- My main issue with this paper is the lack of comparison with current similar model, SwAV (Caron et al., NeurIPS20). Although quickly mentioned on the related works section, the authors do compare with them on experimental section. \n- I feel that some ablation studies to better understand the components of the method is missing (the number of clusters, the importance of the concentration estimation and the choices of its parameters, etc.). The qualitative visualization from the appendix are very nice and it is unfortunate that they cannot go on main manuscript.\n\n### Comments\nAlthough I generally like the paper, i dont understand why the authors did not include most recent methods in the experimental results (eg. BYOL, SwAV, etc). In particular, no mention or comparison with SwAV---a method that is similar to the proposed approach---appears in experimental section. \nI don't particularly mind the fact that SwAV outperform the proposed approach (the fact the method is built on the top of MoCo AND outperforms it is not bad). However, I dont think the method should be omitted from the experimental results. I would be much more happy to see more comparison (similarities/differences) between the two approaches.\n\nBecause of this, my current rate is \"Marginally below acceptance threshold\"\n\n### Post-rebuttal updates\nGiven the answer of the authors. I will raise my reviews to 7 assuming the authors will add more about comapring with current sota methods, as discussed on this review.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081336, "tmdate": 1606915777696, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3143/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review"}}}, {"id": "C9X_KVpVUzj", "original": null, "number": 5, "cdate": 1605312581634, "ddate": null, "tcdate": 1605312581634, "tmdate": 1605313371562, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "PYhm7PJT1aF", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the insightful and valuable comments.  Here are our responses to the reviewers concerns.\n- Following the valuable suggestion, we have included results from BYOL and SwAV in Table 2 and Table 3. Note that both BYOL and SwAV use a huge computation resource with longer training epochs, larger batch size, and multi-crop augmentations (only SwAV). Moreover, the Arxiv versions of BYOL and SwAV were released after we released PCL online.  \nFrom a technical perspective, SwAV introduces an optimization objective per-minibatch which performs online clustering on a subset of training samples. Different from SwAV, PCL performs offline clustering on all samples. Hence, PCL would produce more semantically meaningful clusters that capture the global pattern, whereas SwAV would produce more flexible clusters that capture local patterns. Furthermore, PCL introduces the cluster-varying concentration estimation which replaces the fixed temperature in SwAV.\n- The concentration estimation is very important. With a fixed concentration, the representations would be skewed due to cluster imbalance (as shown in Figure 3a), and the performance would significantly decrease. Due to limitations in computation budget, we have not performed an extensive search for the hyper-parameters including number of clusters, parameters for concentration estimation, etc. We have provided the ablation study on the proposed loss in Table 7, and will try our best to conduct more ablation studies. We appreciate the reviewer\u2019s approval of our visualization, and have moved the t-SNE visualization to the main text due to the allowance of an additional page.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KmykpuSrjcq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3143/Authors|ICLR.cc/2021/Conference/Paper3143/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment"}}}, {"id": "0_NTjsgBk7c", "original": null, "number": 4, "cdate": 1605312481823, "ddate": null, "tcdate": 1605312481823, "tmdate": 1605312842265, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "iE6NtOKSXc6", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the insightful and positive comments. Here are our responses to the reviewers comments.\n\n1. Following the suggestion, we have included results from BYOL and SwAV in Table 2 and Table 3.\n2. SwAV introduces an online optimization which performs clustering on a subset of training samples. We perform k-means clustering on top of the representations, using the 200-epoch model released by the authors. The adjusted mutual information (AMI) score is 40.2, similar to 41.0 as produced by PCL. Hence we hypothesize that SwAV has a similar clustering effect. In terms of BYOL, since it does not optimize for a clustering objective, we hypothesize that the representations learned by BYOL will be more evenly distributed similar to MoCo.\n3. The cluster-varying temperature is crucial for good performance due to its ability to address the imbalance in cluster sizes. Using a global temperature results in more imbalanced clusters (as shown in Figure 3a) and a skewed distribution of representations. The final performance also drops by a considerable amount if a global temperature is used. The imbalance issue could be partially mitigated by a heuristic sampling strategy based on cluster size, but cluster-varying temperature is a more principled solution.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KmykpuSrjcq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3143/Authors|ICLR.cc/2021/Conference/Paper3143/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment"}}}, {"id": "Ck24_Xhq7HF", "original": null, "number": 3, "cdate": 1605312317604, "ddate": null, "tcdate": 1605312317604, "tmdate": 1605312317604, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "7hl4PIIA8L", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "We thank the reviewer for the insightful and valuable comments. Here are our responses to the reviewer's concerns.\n\n1. Implementation complexity.\n\\\nOur method is straightforward to implement, as suggested by Reviewer 3 who has re-implemented our method and achieved similar improvements.\n2. Comparison to MoCo-v2.\n\\\nAll of our experimental results on MoCo-v2 that are not reported in their original papers are produced using the officially released pre-trained models. The comparisons are done in a fair way with the same evaluation script as PCL. We would like to highlight that compared to the linear evaluation setting with abundant labels, PCL and PCL-v2 demonstrate much stronger advantages on low-resource transfer learning, as shown in the few-shot classification results and semi-supervised learning results in Table 1 and Table 2. We believe that low-resource transfer learning has high practical values for self-supervised representation learning and is commonly used in real applications. \n3. Clarification of the proposed loss.\n\\\nThe InfoNCE loss can be interpreted as a special case of our ProtoNCE loss, where each instance is a prototype, and the feature distribution around each instance has the same variance. Following the reviewer\u2019s suggestion, we have improved the paper clarity by adding the following sentence in introduction: \u201cProtoNCE also includes an InfoNCE term in which the instance embeddings can be interpreted as instance-based prototypes.\u201d, and will further revise it. \n4. Computation budget.\n\\\nWe appreciate the reviewer\u2019s understanding of the computation budget. We have updated the paper to specify this practical setting in Section 4.1. \n5. We appreciate the minor suggestions and have improved the paper accordingly. With the allowance of an additional page, we have included the data augmentation details in the main text. BYOL has also been added.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KmykpuSrjcq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3143/Authors|ICLR.cc/2021/Conference/Paper3143/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment"}}}, {"id": "W_3UXKi7u-f", "original": null, "number": 2, "cdate": 1605312040675, "ddate": null, "tcdate": 1605312040675, "tmdate": 1605312040675, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "WOa60tysaGF", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We thank the reviewer for the insightful and positive comments. Here are our responses.\n\nWe agree with the reviewer that joint clustering and representation learning has been proposed in the previous methods. Here we want to emphasize that the contribution of PCL is to bridge clustering with contrastive learning, and explicitly formulate the EM procedures. We appreciate the relevant online clustering paper and have discussed it in related works.\n\nWe observe that both terms in Eq. (11) are important to the performance. The inter-instance learning (first term) is more important at the beginning of training, while the intra-instance learning (second term) becomes more important as training progresses and the clusters become more meaningful. In Table 7 (appendix A), we report the experimental results with only the first or the second term. We observe that using only the second term achieves decent results, given that the first term is used for warm-up. We hypothesize that a better training curriculum could further improve the performance of PCL, but leave it for future work."}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KmykpuSrjcq", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3143/Authors|ICLR.cc/2021/Conference/Paper3143/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923840630, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Comment"}}}, {"id": "iE6NtOKSXc6", "original": null, "number": 2, "cdate": 1603668249320, "ddate": null, "tcdate": 1603668249320, "tmdate": 1605024058610, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review", "content": {"title": "A good exploration with a solid set of experiments. The major concern is significance as the field advances fast", "review": "The paper mainly injects clustering into the instance discrimination work, MoCo, for unsupervised representation learning. The main idea is to maintain a list of cluster centers (i.e., prototypes), and use them to estimate the similarity between a key and query. Because the cluster centers also need to be learned, the proposed method is an iterative, EM-like algorithm where the clusters are produced every now and then.\n\nFrom the arXiv upload time, this is an improved version of the manuscript submitted for NeurIPS 2020. Back then I was pretty interested in this work and was able to reproduce some of the improvements (e.g., some minor improvements on ImageNet linear evaluation) observed in the paper using my own implementation. So this paper did a pretty good exploration and I would trust the results in the paper. However, the main concern is on significance now, as there are concurrent submissions like BYOL (which claims to implicitly perform bootstrapping representation learning), or more closely SwAV (an online-clustering algorithm that also aims at unsupervised learning). Both BYOL and SwAV are accepted at NeurIPS already with better performance on several evaluation benchmarks (e.g. semi-supervised setting, BYOL reports 78.4 top-5 accuracy with 1% labels), yet unfortunately this work does not. Nevertheless, I would still give an acceptance rating as:\n\n1) This is solid work, I have personally implemented this work and the observation is quite consistent with what's described in the paper, at least for some major experiments.\n2) Doing an EM-style clustering with momentum encoder/key is indeed something that has never been explored to my knowledge, and there is some novelty to it.\n3) It also proposes a prototype-specific T, which is quite new and well-motivated.\n\nFor writing and organization: I think the paper is written quite clearly (as I was able to write code based on this), and since this is already an improved version for the NeurIPS submission, I believe the clarity/organization has improved more.\n\nSuggestions/questions:\n1) Please include more recent numbers from other papers like BYOL or SwAV, for completeness of the assessment of the value of the work.\n2) Does more recent method (BYOL/SwAV) also have \"clustering\" effect? Like the mutual information with class labels on ImageNet?\n3) How  much does the \"cluster-varying T\" help performance? Can a single, global T used for protoNCE?  ", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081336, "tmdate": 1606915777696, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3143/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review"}}}, {"id": "7hl4PIIA8L", "original": null, "number": 3, "cdate": 1603828939352, "ddate": null, "tcdate": 1603828939352, "tmdate": 1605024058544, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review", "content": {"title": "Interesting approach, empirical results not persuasive enough", "review": "##########################################################################\n\nSummary:\n\nThe paper proposes a new self-supervised representation learning loss which is a combination of InfoNCE and EM-clustering. Rather than encouraging similarity to another augmentation of the same image, it clusters images and encourages similarity of images to their cluster centers. The evaluated approach ProtoNCE actually consists of a sum of the standard InfoNCE loss and the proposed novel loss. The approach is evaluated on the most common ImageNet linear classification benchmark and a few others: ImageNet semi-supervised, VOC07, Places 205, including some detection tasks. The paper compares to reasonable baselines like MoCo-v2 and SimCLR.\n\n\n##########################################################################\n\nReasons for score:\n\nThe proposed approach seems relatively complicated to implement compared to baseline MoCo-v2 while not being sufficiently better on the most established ImageNet linear eval benchmark. MoCo-v2 produces 67.5\u00b10.1 (five runs from github repo: {67.7, 67.6, 67.4, 67.6, 67.3}, see https://github.com/facebookresearch/moco) while the proposed method gets 67.6 - not a very persuasive advantage. Moreover, the approach essentially compares with with 200-epoch ablation of MoCo-v2 without comparing to the main 800-epoch run. I couldn't find other numbers in MoCo-v2 paper https://arxiv.org/pdf/2003.04297.pdf surpassed by the numbers in this paper - although both papers evaluate on detection as well. \n\n\n##########################################################################\n\nPros:\n\n1. Interesting idea to compare images to EM cluster centers.\n2. Reasonable amount of experiments.\n\n##########################################################################\n\nCons:\n\n1. Results are within variance of MoCo-v2 number on ImageNet linear eval. Other numbers from MoCo-v2 paper are not directly compared against.\n2. It does not follow from the title/abstract/introduction that the evaluated approach is actually a sum of the standard InfoNCE and the novel loss. This is a clarity issue which needs to be addressed.\n3. The paper seems to make an implicit assumption about the computational budget under which it tries to obtain the best possible results. For example, it doesn't try to beat 1000-epoch-big-batch results of SimCLR. I don't have an issue with this assumption, but it needs to be directly articulated early on. Something like \"recent advances in self-supervised learning were propelled by huge compute, which is not available to the majority of labs; we instead target more commonly reproducible/achievable training regimes - X GPUs, Y days\". For example, MoCo-v2 paper makes these compute considerations clear upfront, as early as abstract - which is good for the reader.\n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above.\n\n#########################################################################\n\nMinor suggestions and typos: \n\n(1) c_s is not explained anywhere and it's a quite confusing notation for the center of the cluster which x_i belongs to\n\n(2) the fact that the paper uses data augmentations is not mentioned in the main text at all\n\n(3) \"where v'i is a positive embedding for instance i,\" - should be without '?\n\n(4) \"PCL not only learns low-level features for the task of instance discrimination, but\nmore importantly, it implicitly encodes semantic structures of the data into the\nlearned embedding space.\" - it is quite confusing to talk about semantic structures here, without clarifying what is meant by that. The clusters might have good mutual information with respect to unseen labels, but there is still no way to know that dog is a dog until an example of a dog is seen.\n\n(5) it might be nice (although not required by ICLR rules) to mention BYOL https://arxiv.org/abs/2006.07733 in something like Concurrent Work section", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081336, "tmdate": 1606915777696, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3143/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review"}}}, {"id": "WOa60tysaGF", "original": null, "number": 4, "cdate": 1604105961223, "ddate": null, "tcdate": 1604105961223, "tmdate": 1605024058407, "tddate": null, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "invitation": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review", "content": {"title": "A good paper presenting prototypical contrastive learning that combines deep clustering and momentum contrastive learning", "review": "[Overview]\nIn this paper, the authors tackle the problem of unsupervised visual representation learning by combining the deep clustering technique and momentum contrastive learning. Contrastive learning is widely used in instance-level SSL. However, considering it cannot model the inter-sample structures, the authors proposed a new method called prototypical contrastive learning (PCL), which boosts the original contrastive learning with clustering technique. It can be interpreted as an EM procedure. Though the high-level idea is similar to deep clustering and its variants, the authors argued that PCL can address the issue of reinitialization and potential cluster collapse, as demonstrated in the formulas. In the experiments, the authors performed extensive experiments to demonstrate the efficiency of PCL.\n\n[Strength]\n1. The authors presented a well-written paper with some theoretical explanations and extensive experiments. It is easy to follow with some good insights in the words.\n2. The proposed PCL is formulated as an EM procedure. Though see it as an EM algorithm at a high-level is not novel, this paper has a relatively thorough derivation and give a good intuition behind the proposed PCL method. \n3. The authors demonstrated the effectiveness of the proposed PCL on various datasets and settings. Compared with MoCo, its main counterpart, PCL achieve substantial improvement over it across the board. \n\n[Concerns]\n1. Introducing a EM-like procedure to mitigate the reinitialization issue has been proposed in previous works. Though not on large-scale dataset, JULE (Yang et al 2016) proposed to use a forward and backward pass for alternative learning, which corresponds to the E-step and M-step in this paper, respectively. To date, there are also some papers that proposed a similar strategy to address the issue of oscillation, such as \"Online Deep Clustering for Unsupervised Representation Learning. 2020\".  \n2. The proposed PCL combined momentum contrast and deep cluster technique, as formulated in Eq. (11). This formula prompts the interaction of intra-instance learning and inter-instance learning. From the experiments, it is not clear how they cooperate with each other during the training. Does the PCL (2nd term in Eq. (11)) solely perform poorly in representation learning?\n\n[Summary]\nOverall, I think this paper presents a clean formula to reconcile the intra-instance and inter-instance unsupervised learning methods, which seems to be a promising direction for further exploration. Based on my above comments, I think this is a good paper, and recommend acceptance. I would suggest the authors can address my above concerns and provide more insights about how these two losses interact with each other during the learning process in the rebuttal.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3143/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3143/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Prototypical Contrastive Learning of Unsupervised Representations", "authorids": ["~Junnan_Li2", "~Pan_Zhou3", "~Caiming_Xiong1", "~Steven_Hoi2"], "authors": ["Junnan Li", "Pan Zhou", "Caiming Xiong", "Steven Hoi"], "keywords": ["self-supervised learning", "unsupervised learning", "representation learning", "contrastive learning"], "abstract": "This paper presents Prototypical Contrastive Learning (PCL), an unsupervised representation learning method that bridges contrastive learning with clustering. PCL not only learns low-level features for the task of instance discrimination, but more importantly, it implicitly encodes semantic structures of the data into the learned embedding space. Specifically, we introduce prototypes as latent variables to help find the maximum-likelihood estimation of the network parameters in an Expectation-Maximization framework. We iteratively perform E-step as finding the distribution of prototypes via clustering and M-step as optimizing the network via contrastive learning. We propose ProtoNCE loss, a generalized version of the InfoNCE loss for contrastive learning, which encourages representations to be closer to their assigned prototypes. PCL outperforms state-of-the-art instance-wise contrastive learning methods on multiple benchmarks with substantial improvement in low-resource transfer learning. Code and pretrained models are available at https://github.com/salesforce/PCL.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|prototypical_contrastive_learning_of_unsupervised_representations", "one-sentence_summary": "We propose an unsupervised representation learning method that bridges contrastive learning with clustering in an EM framework.", "supplementary_material": "/attachment/7fe37b283f92372770869f2705675a8d0fff8ec4.zip", "pdf": "/pdf/601011be0933cda056049e8fd0b25a10bcfd4515.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021prototypical,\ntitle={Prototypical Contrastive Learning of Unsupervised Representations},\nauthor={Junnan Li and Pan Zhou and Caiming Xiong and Steven Hoi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=KmykpuSrjcq}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KmykpuSrjcq", "replyto": "KmykpuSrjcq", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3143/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538081336, "tmdate": 1606915777696, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3143/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3143/-/Official_Review"}}}], "count": 10}