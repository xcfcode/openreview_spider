{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582883585, "tcdate": 1520532299193, "number": 1, "cdate": 1520532299193, "id": "SJ7Isl1tz", "invitation": "ICLR.cc/2018/Workshop/-/Paper147/Official_Review", "forum": "Hy9haE1Df", "replyto": "Hy9haE1Df", "signatures": ["ICLR.cc/2018/Workshop/Paper147/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper147/AnonReviewer1"], "content": {"title": "Too packed, does not really explain the method", "rating": "5: Marginally below acceptance threshold", "review": "This paper is about a very interesting topic, but unfortunately, it does not work well in the workshop format -- the 3-pager simply appears to be too packed and doesn't leave me with an understanding of the proposed method.\nThe original conference submission was not invited to the workshop track (in which case there wouldn't even be a need for review, but the authors could just use the original submission). \n\nUnfortunately, condensing the paper to 3 pages did not make it clearer. Section 2 claims to prove something about Algorithm 1, but the theorem statement only is about the existence of \\phi^* and does not pertain to Algorithm 1. Furthermore, the theorem relies on assumptions that are not contained in the theorem statement, but only in its proof, e.g., that w_\\phi is a universal function approximator (which, as stated in the reviews of the conference version, only holds in the limit). The proof is also not clear to me, e.g., the step \"to satisfy Jensen's inequiality\". Rather than including this incomplete proof (which necessarily remains unclear given the space constraints), in a workshop submission I believe it would have been far better to explain the intuition behind Algorithm 1 and Algorithm 2 and relate & compare them to known methods.\n\nAs a reviewer of the conference submission already pointed out, the authors are treating argmin as a single element, which is actually a problem here. This is dirty notation that is unfortunately far too wide-spread in the ML community, but it is even more unfortunate that the authors did not change notation in response to the reviewer's comment (that reviewer was not me); simply writing w* \\in argmin_w [...] instead of w* = argmin_w [...] would really not be harder to understand and much cleaner. Writing \\in would also make it clearer that right after Equation 2, \"this function\" is imprecise.\n\nThe abstract ends with \"We compare this method to standard hyperparameter optimization strategies.\", but there is no empirical comparison. There is a section called \"related work\", but that section just lists a couple of methods, without stating differences/similarities, except in the case of the very closely related SMASH approach (which is not a standard hyperparameter optimization strategies). I think the paper would either have to empirically compare or drop that claim from the abstract.\n\nThe paper does not appropriately reflect the limitations of the method, as e.g., stated in the reply to the review of the conference submission: \"A good point is raised in that there are hyperparameters this algorithm can not optimize. We can not optimize hyperparameters about optimization, because there is no inner optimization loop.\"\n\nWhen I read the second paragraph, I was confused that this paper claims to propose to learn this function, since I thought the SMASH paper proposed this; I think the late mention of SMASH on page 2 is a bit unfortunate.\n\nDo I understand correctly that the experiments used MNIST with 10 training examples? Unfortunately, I cannot follow how the performed experiments demonstrate that the algorithm can be used for hyperparameter optimization. They don't even describe which hyperparameters are being optimized. The figures appear very useful, but without context I did not fully understand them.\n\nOverall, I believe that there is something interesting here, but I cannot extract it from this too packed 3-pager. Nevertheless, I strongly encourage the authors to continue this line of work and the full version of the paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Hyperparameter Optimization through Hypernetworks", "abstract": "Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters.  We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters.  Our process trains a neural network to output approximately optimal weights as a function of hyperparameters.  We show that our technique converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  We compare this method to standard hyperparameter optimization strategies.", "pdf": "/pdf/8bed9fc50c324fd21878ca25bf267e3ae5de9814.pdf", "TL;DR": "We train a neural network to output approximately optimal weights as a function of hyperparameters.", "paperhash": "lorraine|stochastic_hyperparameter_optimization_through_hypernetworks", "_bibtex": "@misc{\nlorraine2018stochastic,\ntitle={Stochastic Hyperparameter Optimization through Hypernetworks},\nauthor={Jonathan Lorraine, David Duvenaud},\nyear={2018},\nurl={https://openreview.net/forum?id=SJIA6ZWC-},\n}", "authorids": ["lorraine@cs.toronto.edu", "duvenaud@cs.toronto.edu"], "keywords": ["hypernetworks", "hyperparameter optimization", "metalearning", "neural networks", "Bayesian optimization", "game theory", "optimization"], "authors": ["Jonathan Lorraine", "David Duvenaud"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582883353, "id": "ICLR.cc/2018/Workshop/-/Paper147/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper147/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper147/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper147/AnonReviewer2"], "reply": {"forum": "Hy9haE1Df", "replyto": "Hy9haE1Df", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582883353}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582763649, "tcdate": 1520641478324, "number": 2, "cdate": 1520641478324, "id": "rJRTHsgYz", "invitation": "ICLR.cc/2018/Workshop/-/Paper147/Official_Review", "forum": "Hy9haE1Df", "replyto": "Hy9haE1Df", "signatures": ["ICLR.cc/2018/Workshop/Paper147/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper147/AnonReviewer2"], "content": {"title": "no title", "rating": "5: Marginally below acceptance threshold", "review": "[ Paper Summary ]\n\nThe paper proposes a simultaneous joint optimization of a parameter and a hyper-parameter in a network, in which ``hypernet'' learns a functional relation from hyper-parameter to network parameters.\n\n- novelty\n\nThe approach seems to be novel, though the topic is not my expertise.\n\n- clarity\n\nThe basic idea is clear.\n\n- significance\n\nThe problem setting would be significant. Technical significance of the proposed method might be slightly weak.\n\n- quality\n\nThe presentation of experimental results would be able to be improved. \n\n[ Comments ]\n\n- pros\n\nAutomatic tuning of network would be useful.\n\nThe method is simple to implement.\n\n- cons\n\nExperiments are not convincing. The efficiency of the proposed method is not clear. Results (Figure 1 and 2) are not enough to see effectiveness. \n\nConvergence of the algorithm 2 is not clear. In my understanding, Theorem 2.1 does not guarantee the convergence of the algorithm 2.\n\nhyper-net would also have hyper-parameters. \n\nDiscussion about hyper-parameters other than lambda is missing.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Hyperparameter Optimization through Hypernetworks", "abstract": "Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters.  We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters.  Our process trains a neural network to output approximately optimal weights as a function of hyperparameters.  We show that our technique converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  We compare this method to standard hyperparameter optimization strategies.", "pdf": "/pdf/8bed9fc50c324fd21878ca25bf267e3ae5de9814.pdf", "TL;DR": "We train a neural network to output approximately optimal weights as a function of hyperparameters.", "paperhash": "lorraine|stochastic_hyperparameter_optimization_through_hypernetworks", "_bibtex": "@misc{\nlorraine2018stochastic,\ntitle={Stochastic Hyperparameter Optimization through Hypernetworks},\nauthor={Jonathan Lorraine, David Duvenaud},\nyear={2018},\nurl={https://openreview.net/forum?id=SJIA6ZWC-},\n}", "authorids": ["lorraine@cs.toronto.edu", "duvenaud@cs.toronto.edu"], "keywords": ["hypernetworks", "hyperparameter optimization", "metalearning", "neural networks", "Bayesian optimization", "game theory", "optimization"], "authors": ["Jonathan Lorraine", "David Duvenaud"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582883353, "id": "ICLR.cc/2018/Workshop/-/Paper147/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper147/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper147/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper147/AnonReviewer2"], "reply": {"forum": "Hy9haE1Df", "replyto": "Hy9haE1Df", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582883353}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573585165, "tcdate": 1521573585165, "number": 180, "cdate": 1521573584812, "id": "BkYAR0RFf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Hy9haE1Df", "replyto": "Hy9haE1Df", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Stochastic Hyperparameter Optimization through Hypernetworks", "abstract": "Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters.  We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters.  Our process trains a neural network to output approximately optimal weights as a function of hyperparameters.  We show that our technique converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  We compare this method to standard hyperparameter optimization strategies.", "pdf": "/pdf/8bed9fc50c324fd21878ca25bf267e3ae5de9814.pdf", "TL;DR": "We train a neural network to output approximately optimal weights as a function of hyperparameters.", "paperhash": "lorraine|stochastic_hyperparameter_optimization_through_hypernetworks", "_bibtex": "@misc{\nlorraine2018stochastic,\ntitle={Stochastic Hyperparameter Optimization through Hypernetworks},\nauthor={Jonathan Lorraine, David Duvenaud},\nyear={2018},\nurl={https://openreview.net/forum?id=SJIA6ZWC-},\n}", "authorids": ["lorraine@cs.toronto.edu", "duvenaud@cs.toronto.edu"], "keywords": ["hypernetworks", "hyperparameter optimization", "metalearning", "neural networks", "Bayesian optimization", "game theory", "optimization"], "authors": ["Jonathan Lorraine", "David Duvenaud"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730166803, "tcdate": 1518452146882, "number": 147, "cdate": 1518452146882, "id": "Hy9haE1Df", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Hy9haE1Df", "original": "SJIA6ZWC-", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Stochastic Hyperparameter Optimization through Hypernetworks", "abstract": "Machine learning models are often tuned by nesting optimization of model weights inside the optimization of hyperparameters.  We give a method to collapse this nested optimization into joint stochastic optimization of weights and hyperparameters.  Our process trains a neural network to output approximately optimal weights as a function of hyperparameters.  We show that our technique converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  We compare this method to standard hyperparameter optimization strategies.", "pdf": "/pdf/8bed9fc50c324fd21878ca25bf267e3ae5de9814.pdf", "TL;DR": "We train a neural network to output approximately optimal weights as a function of hyperparameters.", "paperhash": "lorraine|stochastic_hyperparameter_optimization_through_hypernetworks", "_bibtex": "@misc{\nlorraine2018stochastic,\ntitle={Stochastic Hyperparameter Optimization through Hypernetworks},\nauthor={Jonathan Lorraine, David Duvenaud},\nyear={2018},\nurl={https://openreview.net/forum?id=SJIA6ZWC-},\n}", "authorids": ["lorraine@cs.toronto.edu", "duvenaud@cs.toronto.edu"], "keywords": ["hypernetworks", "hyperparameter optimization", "metalearning", "neural networks", "Bayesian optimization", "game theory", "optimization"], "authors": ["Jonathan Lorraine", "David Duvenaud"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730166803, "tcdate": 1509133773671, "number": 737, "cdate": 1518730166792, "id": "SJIA6ZWC-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SJIA6ZWC-", "original": "HJS0aWZ0Z", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Stochastic Hyperparameter Optimization through Hypernetworks", "abstract": "Machine learning models are usually tuned by nesting optimization of model weights inside the optimization of hyperparameters.  We give a method to collapse this nested optimization into joint stochastic optimization of both weights and hyperparameters.  Our method trains a neural network to output approximately optimal weights as a function of hyperparameters.  We show that our method converges to locally optimal weights and hyperparameters for sufficiently large hypernets.  We compare this method to standard hyperparameter optimization strategies and demonstrate its effectiveness for tuning thousands of hyperparameters.", "pdf": "/pdf/289f174f2e6165185b4a367d53128b7df0ac25d9.pdf", "TL;DR": "We train a neural network to output approximately optimal weights as a function of hyperparameters.", "paperhash": "lorraine|stochastic_hyperparameter_optimization_through_hypernetworks", "_bibtex": "@misc{\nlorraine2018stochastic,\ntitle={Stochastic Hyperparameter Optimization through Hypernetworks},\nauthor={Jonathan Lorraine and David Duvenaud},\nyear={2018},\nurl={https://openreview.net/forum?id=SJIA6ZWC-},\n}", "authorids": ["lorraine@cs.toronto.edu", "duvenaud@cs.toronto.edu"], "keywords": ["hypernetworks", "hyperparameter optimization", "metalearning", "neural networks", "Bayesian optimization", "game theory", "optimization"], "authors": ["Jonathan Lorraine", "David Duvenaud"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 4}