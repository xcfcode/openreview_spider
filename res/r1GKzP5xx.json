{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396545353, "tcdate": 1486396545353, "number": 1, "id": "rJYt3zUOg", "invitation": "ICLR.cc/2017/conference/-/paper370/acceptance", "forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Paper proposes a modification of batch normalization. After the revisions the paper is a much better read. However it still needs more diverse experiments to show the success of the method.\n \n Pros:\n - interesting idea with interesting analysis of the gradient norms\n - claims to need less computation\n \n Cons:\n - Experiments are not very convincing and only focus on only a small set of lm tasks.\n - The argument for computation gain is not convincing and no real experimental evidence is presented. The case is made that in speech domain, with long sequences this should help, but it is not supported.\n \n With more experimental evidence the paper should be a nice contribution.", "decision": "Invite to Workshop Track"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396545855, "id": "ICLR.cc/2017/conference/-/paper370/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396545855}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484585475116, "tcdate": 1478287994558, "number": 370, "id": "r1GKzP5xx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "r1GKzP5xx", "signatures": ["~C\u00e9sar_Laurent1"], "readers": ["everyone"], "content": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 13, "writable": false, "overwriting": ["r1hNW-MYg"], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1484340170744, "tcdate": 1484340170744, "number": 7, "id": "rymCi2IIe", "invitation": "ICLR.cc/2017/conference/-/paper370/public/comment", "forum": "r1GKzP5xx", "replyto": "SyeMVA-4x", "signatures": ["~C\u00e9sar_Laurent1"], "readers": ["everyone"], "writers": ["~C\u00e9sar_Laurent1"], "content": {"title": "Answer to the review", "comment": "Thanks for your comments!\n\nAbout data dependent initialization: As stated in the Weight Normalization paper, data dependent initialization can\u2019t be used for RNNs, because of the recurrent nature of RNNs. Instead, they simply propose to initialize gamma to 1, and we therefore use the same setup as in the Weight Normalization paper.\n\nAbout the novelty: We extended the Normalization Propagation framework to the LSTM, in order to be able to better preserve the variance through the recurrence (which is not the case in Weight Normalization). We empirically showed that preserving the variance seems to help with the optimization of the Norm-LSTM compared to the WN-LSTM, leading to slightly better results on both experiments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287604159, "id": "ICLR.cc/2017/conference/-/paper370/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1GKzP5xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper370/reviewers", "ICLR.cc/2017/conference/paper370/areachairs"], "cdate": 1485287604159}}}, {"tddate": null, "tmdate": 1484339996871, "tcdate": 1484339996871, "number": 5, "id": "HJrQi2L8g", "invitation": "ICLR.cc/2017/conference/-/paper370/public/comment", "forum": "r1GKzP5xx", "replyto": "BJg8pkV4g", "signatures": ["~C\u00e9sar_Laurent1"], "readers": ["everyone"], "writers": ["~C\u00e9sar_Laurent1"], "content": {"title": "Answer to the review", "comment": "Thanks for your comments.\n\nAbout the experiments: We do agree that the experimental setup could be improved.\n\nAbout the speed up: Since we are still computing an LSTM, we can\u2019t expect order of magnitude of speed up with such reparametrization, and can only wish for computation time close to the vanilla LSTM.\n\nFrom a computational point of view, the main difference between the BN-LSTM (or LN-LSTM) and the Norm-LSTM is that the BN-LSTM adds extra computations (means and variances) at each time step, where the Norm-LSTM only adds an overhead before the recurrence (the normalization of the weight matrices). Thus for long sequences, such as in speech recognition, the Norm-LSTM has a serious advantage over the BN-LSTM: Their performances are similar, while the Norm-LSTM is almost as cheap computationally as the vanilla LSTM.\n\nAbout the theoretical analysis: In our opinion, the theoretical analysis provides good insights on the role of gamma_x with respect to gamma_h, showing how we can play with their initialization to bias the network into a more short or long term memory."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287604159, "id": "ICLR.cc/2017/conference/-/paper370/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1GKzP5xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper370/reviewers", "ICLR.cc/2017/conference/paper370/areachairs"], "cdate": 1485287604159}}}, {"tddate": null, "tmdate": 1484339881640, "tcdate": 1484339881640, "number": 4, "id": "SyG3928Ix", "invitation": "ICLR.cc/2017/conference/-/paper370/public/comment", "forum": "r1GKzP5xx", "replyto": "S1Ffs_I4x", "signatures": ["~C\u00e9sar_Laurent1"], "readers": ["everyone"], "writers": ["~C\u00e9sar_Laurent1"], "content": {"title": "Answer to the review", "comment": "Thanks for your feedback.\n\n1) We updated the paper with better English.\n\n2) The main difference between the WN-LSTM and Norm-LSTM is the preservation of the variance through the recurrence. In both experiments that we conducted, we observed way better optimization (see for example figure 2 for the training performances on the PTB experiment), and slightly better generalization, at an almost identical computational cost. Moreover, having normalized hidden states is also nice when dealing with stacks of LSTMs. WN doesn\u2019t provide normalized hidden states, unless you perform a data dependent initialization (which is arguably more complex to implement). We also plan to release the code soon.\n\n3) Thank you!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287604159, "id": "ICLR.cc/2017/conference/-/paper370/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1GKzP5xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper370/reviewers", "ICLR.cc/2017/conference/paper370/areachairs"], "cdate": 1485287604159}}}, {"tddate": null, "tmdate": 1482226449447, "tcdate": 1482226449447, "number": 3, "id": "S1Ffs_I4x", "invitation": "ICLR.cc/2017/conference/-/paper370/official/review", "forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "signatures": ["ICLR.cc/2017/conference/paper370/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper370/AnonReviewer1"], "content": {"title": "Sloppy writing, unsufficient experimental validation", "rating": "4: Ok but not good enough - rejection", "review": "The authors show how the hidden states of an LSTM can be normalised in order to preserve means and variances. The method\u2019s gradient behaviour is analysed. Experimental results seem to indicate that the method compares well with similar approaches.\n\nPoints\n\n1) The writing is sloppy in parts. See at the end of the review for a non-exhaustive list.\n\n2) The experimental results show marginal improvements, of which the the statistical significance is impossible to asses. (Not completely the author\u2019s fault for PTB, as they partially rely on results published by others.) Weight normalisation seems to be a viable alternative in the: the performance and runtime are similar. The implementation complexity of weight norm is, however, arguably much lower. More effort could have been put in by the authors to clear that up. In the current state, practitioners as well as researchers will have to put in more effort to judge whether the proposed method is really worth it for them to replicate.\n\n3) Section 4 is nice, and I applaud the authors for doing such an analysis.\n\n\nList of typos etc.\n\n- maintain -> maintain\n- requisits -> requisites\n- a LSTM -> an LSTM\n- \"The gradients of ot and ft are equivalent to equation 25.\u201d Gradients cannot be equivalent to an equation.\n- \u201cbeacause\"-> because\n- One of the \u03b3x > \u03b3h at the end of page 5 is wrong.\n\n\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512607158, "id": "ICLR.cc/2017/conference/-/paper370/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper370/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper370/AnonReviewer3", "ICLR.cc/2017/conference/paper370/AnonReviewer2", "ICLR.cc/2017/conference/paper370/AnonReviewer1"], "reply": {"forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512607158}}}, {"tddate": null, "tmdate": 1482059079736, "tcdate": 1482059079736, "number": 2, "id": "BJg8pkV4g", "invitation": "ICLR.cc/2017/conference/-/paper370/official/review", "forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "signatures": ["ICLR.cc/2017/conference/paper370/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper370/AnonReviewer2"], "content": {"title": "incremental", "rating": "6: Marginally above acceptance threshold", "review": "I think this build upon previous works, in the attempt of doing something similar to batch norm specific for RNNs. To me the experiments are not yet very convincing, I think is not clear this works better than e.g. Layer Norm or not significantly so. I'm not convinced on how significant the speed up is either, I can appreciate is faster, but it doesn't feel like order of magnitude faster. The theoretical analysis also doesn't provide any new insights. \n\nAll in all I think is good incremental work, but maybe is not yet significant enough for ICLR.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512607158, "id": "ICLR.cc/2017/conference/-/paper370/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper370/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper370/AnonReviewer3", "ICLR.cc/2017/conference/paper370/AnonReviewer2", "ICLR.cc/2017/conference/paper370/AnonReviewer1"], "reply": {"forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512607158}}}, {"tddate": null, "tmdate": 1481921544156, "tcdate": 1481921544156, "number": 1, "id": "SyeMVA-4x", "invitation": "ICLR.cc/2017/conference/-/paper370/official/review", "forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "signatures": ["ICLR.cc/2017/conference/paper370/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper370/AnonReviewer3"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes an extension of weight normalization / normalization propagation to recurrent neural networks. Simple experiments suggest it works well.\n\nThe contribution is potentially useful to a lot of people, as LSTMs are one of the basic building blocks in our field.\n\nThe contribution is not extremely novel: the change with respect to weight normalization is minor. The experiments are also not very convincing: Layer normalization is reported to have higher test error as it overfits on their example, but in terms of optimization it seems to work better. Also the authors don't seem to use the data dependent parameter init for weight normalization as proposed in that paper.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512607158, "id": "ICLR.cc/2017/conference/-/paper370/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper370/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper370/AnonReviewer3", "ICLR.cc/2017/conference/paper370/AnonReviewer2", "ICLR.cc/2017/conference/paper370/AnonReviewer1"], "reply": {"forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512607158}}}, {"tddate": null, "tmdate": 1481911923480, "tcdate": 1481910888094, "number": 3, "id": "r1gu5jZ4g", "invitation": "ICLR.cc/2017/conference/-/paper370/public/comment", "forum": "r1GKzP5xx", "replyto": "r1obLUbEl", "signatures": ["~Nicolas_Ballas1"], "readers": ["everyone"], "writers": ["~Nicolas_Ballas1"], "content": {"title": "gamma", "comment": "Thanks for your question,\n\nGamma parameters are learned as it is the case in batch norm and weight norm, we will clarify this in the paper next update,"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287604159, "id": "ICLR.cc/2017/conference/-/paper370/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1GKzP5xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper370/reviewers", "ICLR.cc/2017/conference/paper370/areachairs"], "cdate": 1485287604159}}}, {"tddate": null, "tmdate": 1481889282590, "tcdate": 1481889282590, "number": 3, "id": "r1obLUbEl", "invitation": "ICLR.cc/2017/conference/-/paper370/pre-review/question", "forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "signatures": ["ICLR.cc/2017/conference/paper370/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper370/AnonReviewer3"], "content": {"title": "is gamma optimized during training or kept fixed?", "question": "The text suggests that the gamma parameters are treated as hyperparameters, and are kept fixed during training. Is that right? (For batch norm and weight norm these parameters are usually learned as well)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481889283100, "id": "ICLR.cc/2017/conference/-/paper370/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper370/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper370/AnonReviewer2", "ICLR.cc/2017/conference/paper370/AnonReviewer1", "ICLR.cc/2017/conference/paper370/AnonReviewer3"], "reply": {"forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481889283100}}}, {"tddate": null, "tmdate": 1481693303519, "tcdate": 1481693303513, "number": 2, "id": "B1yY_IA7l", "invitation": "ICLR.cc/2017/conference/-/paper370/public/comment", "forum": "r1GKzP5xx", "replyto": "Hyfyr-qmg", "signatures": ["~C\u00e9sar_Laurent1"], "readers": ["everyone"], "writers": ["~C\u00e9sar_Laurent1"], "content": {"title": "Hyperparameters tuning", "comment": "For Batch Normalization and Layer Normalization, we used the same initialization that their respective authors suggest. For Weight Normalization, we did a grid search for the value of gamma, since the authors propose the value of 1.0 kind of by default.\n\nThe main point of this paper is not about getting SOTA results, but more to propose a reparametrization similar to the BN-LSTM, but that doesn't suffer from the disadvantages of BN (computation of the population statistics, no clear generalization to variable length sequences, batch size big enough to have good statistical estimates,...) and is actually cheaper computationally (cf table 1)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287604159, "id": "ICLR.cc/2017/conference/-/paper370/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1GKzP5xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper370/reviewers", "ICLR.cc/2017/conference/paper370/areachairs"], "cdate": 1485287604159}}}, {"tddate": null, "tmdate": 1481409753622, "tcdate": 1481409753614, "number": 2, "id": "Hyfyr-qmg", "invitation": "ICLR.cc/2017/conference/-/paper370/pre-review/question", "forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "signatures": ["ICLR.cc/2017/conference/paper370/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper370/AnonReviewer1"], "content": {"title": "Significance of performance", "question": "In section 5.1 it is claimed that the proposed method compares favorably compared to other normalization techniques. To me, the results don't seem large. E.g. the improvements over batch normalization and weight normalization are about 1%. \n\nGiven that only a single set of hyper parameters was tried, I am unconvinced that a rigorous hyper parameter search and an evaluation featuring confidence intervals will lead to significant improvements; additionally, the proposed method is arguably more complex.\n\nA skeptical reader might attribute the results to cherry-picking. I think a more extensive experimental evaluation, including hyper parameter search and corresponding confidence intervals as well as learning curves would rule out those concerns."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481889283100, "id": "ICLR.cc/2017/conference/-/paper370/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper370/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper370/AnonReviewer2", "ICLR.cc/2017/conference/paper370/AnonReviewer1", "ICLR.cc/2017/conference/paper370/AnonReviewer3"], "reply": {"forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481889283100}}}, {"tddate": null, "tmdate": 1481268240262, "tcdate": 1481268240257, "number": 1, "id": "Sy_zhAP7g", "invitation": "ICLR.cc/2017/conference/-/paper370/public/comment", "forum": "r1GKzP5xx", "replyto": "B1VLKBEQe", "signatures": ["~C\u00e9sar_Laurent1"], "readers": ["everyone"], "writers": ["~C\u00e9sar_Laurent1"], "content": {"title": "Computation times and projected SGD", "comment": "Thanks for your questions!\n\nRegarding the computation time:\nFor Recurrent Batch Normalization, you have to compute 3 means and 3 variances **at each time step**. For Recurrent Normalization Propagation, you compute the matrix W'= W/||W_i|| **only once** before the recurrence. This is why Recurrent Normalization Propagation is cheaper computationally. Table 1 p. 6 gives the measured computation time per epochs for both variants.\n\nRegarding projected SGD:\nIf you have a layer of the form y_i = W/||W_i||*x, and the norm of the lines of W is 1, then the forward pass and the gradient of y_i with respect to x will be identical to projected SGD. However the update equation is different (see equation 18 p.4), than the one you would have with projected SGD. This difference is important, because we perform a reparametrization of the network, so the gradient computations take into account the fact that we performed the rescaling during the forward pass, which is not the case with projected SGD. This is similar to batch normalization: to make it work properly, you need to backpropagate through the computation of the means and variances to account for the reparametrization."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287604159, "id": "ICLR.cc/2017/conference/-/paper370/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "r1GKzP5xx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper370/reviewers", "ICLR.cc/2017/conference/paper370/areachairs"], "cdate": 1485287604159}}}, {"tddate": null, "tmdate": 1481034060449, "tcdate": 1481034060444, "number": 1, "id": "B1VLKBEQe", "invitation": "ICLR.cc/2017/conference/-/paper370/pre-review/question", "forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "signatures": ["ICLR.cc/2017/conference/paper370/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper370/AnonReviewer2"], "content": {"title": "Pre-review question", "question": "Hi, \n\n Sorry for the delay in posting this. One argument you make is that due to this new parametrization, the new method might be cheaper (computationally) then recurrent batch normalization. But you have to compute the norms of each column of the weight.. can you give a comparison about how much you save?\n\nAlso isn't there a clean connection between your normalization and projected SGD ? I think except the cell update (which I agree doesn't fit the bill), the use of the normalized columns, you get an identical effect if you just use projected SGD (whereby you project your weights after each update such that each column has unit norm). Can you compare this things? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Normalization Propagation", "abstract": "We propose a LSTM parametrization  that preserves the means and variances of the hidden states and memory cells across time. While having training benefits similar to Recurrent Batch Normalization and Layer Normalization, it does not need to estimate statistics at each time step,  therefore, requiring fewer computations overall. We also investigate the parametrization impact on the gradient flows and  present a way of initializing the weights accordingly.\n\nWe evaluate our proposal on language modelling and image generative modelling tasks. We empirically show that it performs similarly or better than other recurrent normalization approaches, while being faster to execute.", "pdf": "/pdf/f1b9905eba0f711fe3fe5a6596c33ce1d7141a74.pdf", "TL;DR": "Extension of Normalization Propagation to the LSTM.", "paperhash": "laurent|recurrent_normalization_propagation", "conflicts": ["umontreal.ca"], "authors": ["C\u00e9sar Laurent", "Nicolas Ballas", "Pascal Vincent"], "keywords": ["Deep learning", "Optimization"], "authorids": ["cesar.laurent@umontreal.ca", "nicolas.ballas@umontreal.ca", "pascal.vincent@umontreal.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481889283100, "id": "ICLR.cc/2017/conference/-/paper370/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper370/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper370/AnonReviewer2", "ICLR.cc/2017/conference/paper370/AnonReviewer1", "ICLR.cc/2017/conference/paper370/AnonReviewer3"], "reply": {"forum": "r1GKzP5xx", "replyto": "r1GKzP5xx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper370/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481889283100}}}], "count": 14}