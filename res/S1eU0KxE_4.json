{"notes": [{"id": "S1eU0KxE_4", "original": "r1eV0geoD4", "number": 6, "cdate": 1553365453762, "ddate": null, "tcdate": 1553365453762, "tmdate": 1562082913412, "tddate": null, "forum": "S1eU0KxE_4", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "content": {"title": "A Study of State Aliasing in Structured Prediction with RNNs", "authors": ["Layla El Asri", "Adam Trischler"], "authorids": ["layla.elasri@microsoft.com", "adam.trischler@microsoft.com"], "keywords": ["deep reinforcement learning", "structured prediction", "dialogue"], "abstract": "End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.", "pdf": "/pdf/e044c1dccc2ccc3b173a60ebabfbeb2c8672b009.pdf", "paperhash": "asri|a_study_of_state_aliasing_in_structured_prediction_with_rnns"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Blind_Submission", "cdate": 1552732853551, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*", "values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1552732853551, "tmdate": 1554911328507, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}, "tauthor": "OpenReview.net"}, {"id": "Skx4Ke55KV", "original": null, "number": 1, "cdate": 1554845819801, "ddate": null, "tcdate": 1554845819801, "tmdate": 1554910463424, "tddate": null, "forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Decision", "content": {"title": "Acceptance Decision", "decision": "Accept"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Study of State Aliasing in Structured Prediction with RNNs", "authors": ["Layla El Asri", "Adam Trischler"], "authorids": ["layla.elasri@microsoft.com", "adam.trischler@microsoft.com"], "keywords": ["deep reinforcement learning", "structured prediction", "dialogue"], "abstract": "End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.", "pdf": "/pdf/e044c1dccc2ccc3b173a60ebabfbeb2c8672b009.pdf", "paperhash": "asri|a_study_of_state_aliasing_in_structured_prediction_with_rnns"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Decision", "cdate": 1554496488470, "reply": {"forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554496488470, "tmdate": 1554910461501, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "SJgn2Yy_FV", "original": null, "number": 4, "cdate": 1554672051843, "ddate": null, "tcdate": 1554672051843, "tmdate": 1554910459811, "tddate": null, "forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "content": {"title": "Good starting point for a discussion of failure modes in RL-RNN training", "review": "In this well written paper the authors look into a particular failure mode of RNN models trained via RL: the case where the state representation fails to properly encode the history of previous observations because the next correct/optimal output is the same for different histories. Thus the learned model fails to distinguish the states and produces the same, potentially non-optimal policy.\n\nThe paper is quite specific as it addresses a problem that occurs only when training RNNs using RL methods such as REINFORCE to learn policies for a problem where states can be easily confused. Nevertheless, because of the prevalence of RL methods and the limited insight into failure modes this is a useful line of investigation.\n\nThe authors work through 3 examples with growing complexity: A simple 3-state problem where the agent has to visit 2 states and then gets a reward, another 3 state problem, where the agent has to perform two sub-actions to change the state and thus 4 actions to get a reward, and a text-based game to simulate a dialogue system where the agent has to learn to rank possible actions given a textual representation of the observations.\n\nAll these problems should be trivial to learn with even a tiny RNN and indeed the authors show that a supervised training never deteriorates. In contrast, RL often fails to learn the optimal policy. The authors investigave both REINFORCE and DQN, baseline and no baseline, as well as a loss that takes entropy into account. In all cases a number of training runs deteriorate. \n\nWhile I don't see any fundamental shortcomings of this paper I am a bit uneasy about the interpretation. Could it be that the observed phenomenon stems from a particular property of the error surface, where a few steps in the wrong direction might be enough to fall for some non-optimal local minimum? That being said I'd recommend this work for publication. The workshop should be the right place to discuss possible reasons for the observed outcome.\n\nSome smaller details: Tone down the first paragraph of the introduction a bit and define what you mean by a partially observable problem first.\nMention the number of runs (50) in the caption of Table 3.", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Study of State Aliasing in Structured Prediction with RNNs", "authors": ["Layla El Asri", "Adam Trischler"], "authorids": ["layla.elasri@microsoft.com", "adam.trischler@microsoft.com"], "keywords": ["deep reinforcement learning", "structured prediction", "dialogue"], "abstract": "End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.", "pdf": "/pdf/e044c1dccc2ccc3b173a60ebabfbeb2c8672b009.pdf", "paperhash": "asri|a_study_of_state_aliasing_in_structured_prediction_with_rnns"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "cdate": 1553778228482, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228482, "tmdate": 1554911862727, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "S1efcAsUK4", "original": null, "number": 3, "cdate": 1554591369928, "ddate": null, "tcdate": 1554591369928, "tmdate": 1554910458703, "tddate": null, "forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "content": {"title": "Questions regarding the simple maze", "review": "The authors investigate state aliasing of the learned representations of RNNs trained with policy gradient methods. They start with a simple maze example and claim to show that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. They find that this does not occur when the agent is trained with Q learning. Then they extend this to a more complex text based game and find that entropy bonuses and baselines improve performance.\n\nI had great difficulty understanding the conclusions stemming from the simple maze example, which is the core of the paper. McCallum introduces the example to demonstrate that when x1 & x2 are aliased, the optimal policy can be represented, but that learning when specifically using Q learning on the aliased states does not converge to the optimal policy. The issue stems from the fact that Q learning backs up the value from the aliased state. The authors claim that \"policy-gradient methods directly estimate a policy instead of estimating a state-action value, [so] we expect models trained with these methods to be sensitive to state aliasing.\" This is in opposition to the conclusions from (McCallum 1996).\n\nIt is my understanding that policy gradient methods would not suffer from the same issue.  Are the authors claiming that PG methods do? Then they should show that with aliased x1 & x2 in the tabular case, it converges to a suboptimal policy. \n\nFurthermore, in Fig 2 bottom, it appears that the distance between the representations for x1 and x2 converges to 0 and the policy converges to the optimal policy. This makes sense as the optimal policy can alias x1 & x2, but goes against the conclusions of the author.\n\nFinally, entropy bonuses and baselines are known to improve performance. The authors don't clearly explain the connection between them and their hypothesis that RNN state aliasing is a problem.", "rating": "1: Strong rejection", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Study of State Aliasing in Structured Prediction with RNNs", "authors": ["Layla El Asri", "Adam Trischler"], "authorids": ["layla.elasri@microsoft.com", "adam.trischler@microsoft.com"], "keywords": ["deep reinforcement learning", "structured prediction", "dialogue"], "abstract": "End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.", "pdf": "/pdf/e044c1dccc2ccc3b173a60ebabfbeb2c8672b009.pdf", "paperhash": "asri|a_study_of_state_aliasing_in_structured_prediction_with_rnns"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "cdate": 1553778228482, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228482, "tmdate": 1554911862727, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "HygpbQI4YE", "original": null, "number": 2, "cdate": 1554436869479, "ddate": null, "tcdate": 1554436869479, "tmdate": 1554910455253, "tddate": null, "forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "content": {"title": "Decent study, more experimental analysis needed", "review": "The paper considers reinforcement learning settings where the states are partially observable and an RNN is used as a reinforcement learning agent.  The paper studies the learned representations for the agent\u2019s policy. It shows that when trained with policy gradient methods, the learned policy could be suboptimal when encountering aliased states sharing the same optimal action.  To side step this problem, the following is suggested:\n\n1) Using value-based learning method such as DQN.\n2) Regularization through entropy based exploration and using a baseline for the REINFORCE algorithm.\n\nThe hypothesis is verified empirically on three simple, yet illustrative reinforcement learning environments: a simple maze, a structured prediction maze setting, and a text based game simulating a dialog setting.\n\nStrengths:\n========\n\nThe paper provides more insight into some of the failure cases for training RNN reinforcement learning agents. The paper shows that when using policy gradient methods, the agent may fail to learn the optimal policy perhaps due to state aliasing effect.  \n\n\nWeakness:\n=========\n\n1) Many experimental comparisons are missing:\n\na) DQN experiments are missing for the structured prediction task and the text-based  game setting;\nb) Results with entropy based regularization were not reported for the structured prediction task.\nc) What is the effect of different loss-weight settings on the results? Why are the failures in the first row of Table 7 fewer than the other two weight settings?\n\n2) The analysis considers a very controlled and simplified setting, but ignores several factors that might affect the hypothesis:\n\na) Does the choice of the optimizer affect the number of failures? \nb) In sequence to sequence tasks, does pre-training with MLE help in learning better state representations?\n\n\nClarity:\n======\n\nThe paper is mostly clear and well written. Some of the figure captions are short and non informative (e.g. table 7). The description for the text-based game starting from page 7 is not clear.  For example:\n\n\u201cThe retrieval model receives a list of valid actions at every time step \u201c -> where does this list come from? \n\nThis is only mentioned two paragraphs later: \u201cThe Text World game engine returns all valid actions given the current state of the game \u201d\n", "rating": "3: Marginally above acceptance threshold", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Study of State Aliasing in Structured Prediction with RNNs", "authors": ["Layla El Asri", "Adam Trischler"], "authorids": ["layla.elasri@microsoft.com", "adam.trischler@microsoft.com"], "keywords": ["deep reinforcement learning", "structured prediction", "dialogue"], "abstract": "End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.", "pdf": "/pdf/e044c1dccc2ccc3b173a60ebabfbeb2c8672b009.pdf", "paperhash": "asri|a_study_of_state_aliasing_in_structured_prediction_with_rnns"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "cdate": 1553778228482, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228482, "tmdate": 1554911862727, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}, {"id": "BJxd_c3AdV", "original": null, "number": 1, "cdate": 1554070127919, "ddate": null, "tcdate": 1554070127919, "tmdate": 1554910453274, "tddate": null, "forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "invitation": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "content": {"title": "Great paper", "review": "This paper investigates the representations learned by RNN-based RL agents for solving structured prediction problems when trained with both policy gradient and value-based methods. It studies the conditions leading to state aliasing and highlights strategies to prevent this situation. The hypothesis is that state aliasing happens when different states share the same optimal action and can result into a failure to converge to the optimal policy. The authors study this phenomenon using LSTM and GRU networks on synthetic (toy) environments, and validate their hypothesis.\n\nThe paper is clear, well written, and very relevant to the workshop.", "rating": "4: Top 50% of accepted papers, clear accept", "confidence": "2: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Study of State Aliasing in Structured Prediction with RNNs", "authors": ["Layla El Asri", "Adam Trischler"], "authorids": ["layla.elasri@microsoft.com", "adam.trischler@microsoft.com"], "keywords": ["deep reinforcement learning", "structured prediction", "dialogue"], "abstract": "End-to-end reinforcement learning agents learn a state representation and a policy at the same time. Recurrent neural networks (RNNs) have been trained successfully as reinforcement learning agents in settings like dialogue that require structured prediction. In this paper, we investigate the representations learned by RNN-based agents when trained with both policy gradient and value-based methods. We show through extensive experiments and analysis that, when trained with policy gradient, recurrent neural networks often fail to learn a state representation that leads to an optimal policy in settings where the same action should be taken at different states. To explain this failure, we highlight the problem of state aliasing, which entails conflating two or more distinct states in the representation space. We demonstrate that state aliasing occurs when several states share the same optimal action and the agent is trained via policy gradient. We characterize this phenomenon through experiments on a simple maze setting and a more complex text-based game, and make recommendations for training RNNs with reinforcement learning.", "pdf": "/pdf/e044c1dccc2ccc3b173a60ebabfbeb2c8672b009.pdf", "paperhash": "asri|a_study_of_state_aliasing_in_structured_prediction_with_rnns"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/drlStructPred/-/Paper6/Official_Review", "cdate": 1553778228482, "expdate": 1554526740000, "duedate": 1554526740000, "reply": {"forum": "S1eU0KxE_4", "replyto": "S1eU0KxE_4", "readers": {"values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/drlStructPred/Paper6/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553778228482, "tmdate": 1554911862727, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/drlStructPred"], "invitees": ["ICLR.cc/2019/Workshop/drlStructPred/Paper6/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/drlStructPred"]}}}], "count": 6}