{"notes": [{"id": "r1glDpNYwS", "original": "ByxLOE5wPH", "number": 583, "cdate": 1569439063820, "ddate": null, "tcdate": 1569439063820, "tmdate": 1577168285257, "tddate": null, "forum": "r1glDpNYwS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "WnQr_I9M1T", "original": null, "number": 1, "cdate": 1576798700436, "ddate": null, "tcdate": 1576798700436, "tmdate": 1576800935494, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Decision", "content": {"decision": "Reject", "comment": "Thanks for the discussion with reviewers, which improved our understanding of your paper significantly.\nHowever, we concluded that this paper is still premature to be accepted to ICLR2020. We hope that the detailed comments by the reviewers help improve your paper for potential future submission.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795723810, "tmdate": 1576800275351, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper583/-/Decision"}}}, {"id": "r1g-9ZHlsB", "original": null, "number": 3, "cdate": 1573044616548, "ddate": null, "tcdate": 1573044616548, "tmdate": 1574402665926, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #4", "review": "This paper proposes a method to create adversarial perturbations whose target labels are similar to their ground truth. The target labels are selected using an existing perceptual similarity measure for images.  Perturbations are generated using a DeepFool-like algorithm. Human evaluation supports that the pair of the generated images and target labels are more natural to humans than prior attack algorithms.\n\nThis paper should be rejected due to the lack of motivation to create adversarial examples less detectable by humans automatically. Attackers can manually select target labels and apply targeted attacks. In the target label selection, attackers can choose less detectable labels if necessary. It is encouraged to provide some applications where attackers want to create less detectable adversarial examples in label space without manually assigning target labels.\n\n==========\nUpdate:\n\nAfter reading the authors' responses, the motivation of the paper became clearer. I will not get surprised if this paper is accepted. However, all reviewers still share concerns about the importance of the problem tackled. I think the paper needs to suggest more applications and emphasize the value of the goal in the main paper before being published.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574735462971, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper583/Reviewers"], "noninvitees": [], "tcdate": 1570237750032, "tmdate": 1574735462986, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Review"}}}, {"id": "S1lFBoocjH", "original": null, "number": 11, "cdate": 1573727041244, "ddate": null, "tcdate": 1573727041244, "tmdate": 1573727041244, "tddate": null, "forum": "r1glDpNYwS", "replyto": "HJxDPHLcjS", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Let\u2019s clarify our motivation again. We think it is important for attackers to design less detectable adversarial examples. Are there any attackers who would like their attack to be easily detected by users? And if adversarial attacks are less likely to be \u201cinspected\u201d by humans, why many previous works generate adversarial examples \u201cimperceptible to human eyes\u201d in the image space? Therefore, we think it is common and natural for attackers to have this requirement where they would like to generate less detectable adversarial examples. In our opinion, subtle mistakes can last for a long time so that the attack can bring huge potential risks a human can\u2019t imagine, while obvious mistakes will be corrected in time so that the attack doesn\u2019t have time to cause damage to the user."}, "signatures": ["ICLR.cc/2020/Conference/Paper583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "HygViL9cjS", "original": null, "number": 10, "cdate": 1573721756308, "ddate": null, "tcdate": 1573721756308, "tmdate": 1573721756308, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1ggfNdqor", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response", "comment": "The review system does not provide options between weak reject and weak accept. Thus I cannot change it. I am sorry for that. However, I think AC will care about the opinion.\n\nSorry for confusing the authors. In comment 3), I meant both of them. \n\n3-1) Since the paper uses almost one page to introduce the variant of DeepFool, I wanted to see quantitative experiments that show the effectiveness of the proposed targeted attack (Sec. 3.2) over other targeted attack algorithms. I think it is interesting if the proposed targeted attack algorithm outperforms other targeted-attacks when the target labels are less-detectable by humans.\n3-2) The effectiveness of the proposed label selection algorithm was indeed not obvious. A comparison with the second-highest label will help as the authors mention. I think a comparison with choosing a label with second-largest d(x,y) will also help to prove the effectiveness of the additional complexity in the proposed method. Additionally, analysis of the effect of the hyperparameters \\delta_1 and \\delta_2 will help to justify the necessity of the division into cases."}, "signatures": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "r1ggfNdqor", "original": null, "number": 9, "cdate": 1573712904260, "ddate": null, "tcdate": 1573712904260, "tmdate": 1573712904260, "tddate": null, "forum": "r1glDpNYwS", "replyto": "BklW6d6tsB", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We thank the reviewer for increasing the score to borderline, but we could not see the rating changed in the system. We would like to answer the reviewer\u2019s questions as follows:\n\nWe are a little confused about \u201cshow that the label selection algorithm and targeted attack algorithm used in LabelFool performs better than other possible candidates\u201d. Do you mean we should use our label selection algorithm to choose target label for existing targeted attacks such as CW-target? If so, we don\u2019t think this experiment is meaningful. In this case, it is meaningless to do subjective experiments because the label for adversarial examples generated by different methods are all the same. It is also meaningless to calculate the attack rate in this case, because the attack rate doesn\u2019t matter to our label selection algorithm, it only matters to the existing targeted attack method itself.\n\nOr do you mean we should compare our label selection algorithm with some other na\u00efve selections such as random selection? If so, we didn\u2019t compare our selection with random selections because we think it is obvious that our carefully designed label selection algorithm is better than randomly chosen label. We also considered about using the second-highest label as the target label, but we found some drawbacks in doing so. On the one hand, when the network\u2019s confidence score for the highest class is higher than $\\delta_1$ (we use 0.8 in this paper), there is no significant difference in confidence scores for other classes, so choosing the second-highest label is not appropriate. On the other hand, when the network\u2019s confidence score for the highest class is lower than $\\delta_1$, the second-highest label is usually the ground truth label (In Figure 8 in Appendix D). It will reduce the attack rate if choosing the second-highest label as the target label. So before we read the reviews, we think it is obvious that our carefully designed label selection algorithm is better than second-highest label, too. However, it may be not so obvious. We will add this experiment in our future work.\n\nWhatever, we appreciate your suggestions, it may be more convincing if we show experimental results to illustrate LabelFool does better than na\u00efve label selections. If you still have questions, don\u2019t hesitate to let us know.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "HJxDPHLcjS", "original": null, "number": 8, "cdate": 1573705055069, "ddate": null, "tcdate": 1573705055069, "tmdate": 1573705055069, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1gvWsUYsB", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response", "comment": "The motivation in the Appendix does make things slightly more clear, although to me still quite contrived. \n\nStill, even if one was to accept the motivation, I think without explicitly comparing to a naive strategy where one just does a targeted attack towards the second label (or, more accurately, to the highest-confidence incorrect label), the study is critically incomplete. (I had initially misunderstood the paper as using this as a baseline, which is why I did not include it in the initial review.)"}, "signatures": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "BklW6d6tsB", "original": null, "number": 7, "cdate": 1573669049258, "ddate": null, "tcdate": 1573669049258, "tmdate": 1573672970805, "tddate": null, "forum": "r1glDpNYwS", "replyto": "SJgk3O6GoS", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you for your response. The authors' explanation of LabelFool's application did make the motivation easier to understand. It increased my score to borderline, but it is still negative.\n\nComments:\n1) (Comment to authors' response 1) I do not think that LabelFool should be categorized as an untargeted-attack. It will be more appropriate to classify it as a target-label selection algorithm for targeted-attack algorithms in (restricted) untargeted-attack settings. This categorization issue is not critical for my score.\n2) I think the key contribution of this paper is introducing the necessity of automatic and less-detectable target label selection algorithms. Thus, for my assessment, it is critical how much the problem will be important and how well the paper will motivate the machine learning community to discuss on the topic. The authors' response addressed to this point to some extent. Hence I slightly adjusted my score, but it was not so significant to flip my score into weak accept. Proposing more applications or performing evaluations in the settings of the proposed applications might have improved my score. I think these changes require significant modification of the paper, and I suggest for rejection this time.\n3) In terms of the significance of the proposed algorithm, I did not think it is beyond the bar of top-venues such as ICLR. It would have increased the significance if there were additional experiments that show that the label selection algorithm and targeted attack algorithm used in LabelFool performs better than other possible candidates. I am sorry that I could not provide this suggestion in the initial review."}, "signatures": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "r1gvWsUYsB", "original": null, "number": 6, "cdate": 1573640959119, "ddate": null, "tcdate": 1573640959119, "tmdate": 1573640959119, "tddate": null, "forum": "r1glDpNYwS", "replyto": "Ske39HWtiB", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the comments, and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, as last comment said, please read the example in Figure 9 in Appendix E in our new version. And we have illustrated a real life case where LabelFool is needed in Appendix E. We just give an example in Figure 1 to help the readers understand what LabelFool actually does in theory. We can change the examples in Figure 1 into the examples in Figure 9 if you think examples in Figure 9 are more helpful for readers to understand our motivation.\n\nSecond, we have considered about using the second-highest label as the target label, but we found some drawbacks in doing so. On the one hand, when the network\u2019s confidence score for the highest class is higher than $\\delta_1$ (we use 0.8 in this paper), there is no significant difference in confidence scores for other classes, so choosing the second-highest label is not appropriate. On the other hand, when the network\u2019s confidence score for the highest class is lower than $\\delta_1$, the second-highest label is usually the ground truth label (In Figure 8 in Appendix D). It will reduce the attack rate if we choose the second-highest label as the target label. So choosing the second-highest label is not appropriate in this case either. Therefore, we don\u2019t use the second-highest label as the target label, but design an explicit method instead.\n\nIf you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "Ske39HWtiB", "original": null, "number": 5, "cdate": 1573619092044, "ddate": null, "tcdate": 1573619092044, "tmdate": 1573619092044, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1e0X66zsr", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response", "comment": "I appreciate the authors clarification, but in the demonstrated settings, I can't imagine a case where perturbing an image of a \"church\" to be classified as a \"monastery\" is an adversarial example that one would care to generate. \n\nI'm also confused by how this approach meaningfully differs from just a targeted attack towards the second-highest label?"}, "signatures": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "r1e0X66zsr", "original": null, "number": 4, "cdate": 1573211429944, "ddate": null, "tcdate": 1573211429944, "tmdate": 1573214324590, "tddate": null, "forum": "r1glDpNYwS", "replyto": "B1g1QouqYB", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "We thank the reviewer for the time and some good suggestions, and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, there is some misunderstanding about \u201cadversarial examples\u201d. This concept was first proposed in 2014 [1]. It is defined as \u201cWe find that applying an imperceptible non-random perturbation to a test image, it is possible to arbitrarily Change the Network\u2019s Prediction. \u2026 We term the so perturbed examples \u2018adversarial examples\u2019.\u201d in work [1]. Many other works [2,3] follow this definition that adversarial examples are the ones misclassified by the network \uff08by adding imperceptible perturbations\uff09. Meanwhile, to the best of our knowledge, all works in adversarial attack community report the \u201csuccess rate of attack\u201d as the misclassification rate of their target model without checking whether the generated examples are reasonable to humans or not. As long as the prediction is different from the ground truth, the perturbed example is \u201cadversarial\u201d no matter it is reasonable to human or not.\n\nSecond, we think there are two evaluations of \u201cthe impact of the adversarial attack\u201d. One is whether the error happens, it is evaluated by the success rate of attack (misclassification rate of the target model). The other is how long the error lasts, it is evaluated by the time that a human user detects the attack. LabelFool does as well as other attacks on the first evaluation, but does better on the second evaluation because it is less detectable by a human user.\n\nThird, we only conduct experiments on the image classification task and you think this task is less likely to be \u201cinspected\u201d by humans. But the fact is, this task is widely used in many fields in our life. Please see Figure 9 in Appendix E in the new version. Taking face recognition system for entrance as an example, there will usually be a guard as \u201ca human inspector\u201d to check whether the man A who is entering the gate is the same as the system shows. In this case, if a model just misclassifies A and C, the person who looks totally different from A, the attack will be detected easily. But LabelFool aims to let the model misclassify A and B who looks like A as shown in Figure 9. If the guard doesn\u2019t identify carefully, he will let a fake B in and this error brings great potential security risks. So as an attacker, it is necessary to generate imperceptible examples in the label space for this task. \n\nFourth, we appreciate your suggestion about applying this method to other areas of ML security, we will do this in our future work, but now, we just introduce this idea, provide a tool and conduct experiments on simple tasks to prove the feasibility of this idea.\n\nIf we misunderstood you in this rebuttal or you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n\nReferences:\n[1] Szegedy et al. Intriguing properties of neural networks. (ICLR 2014)\n[2] Kurakin et al. Adversarial examples in the physical world[J]. arXiv preprint arXiv:1607.02533, 2016.\n[3] Goodfellow et al. Explaining and harnessing adversarial examples. (ICLR 2015)"}, "signatures": ["ICLR.cc/2020/Conference/Paper583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "r1xM_96for", "original": null, "number": 3, "cdate": 1573210730115, "ddate": null, "tcdate": 1573210730115, "tmdate": 1573211457955, "tddate": null, "forum": "r1glDpNYwS", "replyto": "HkgFLke2tr", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "We thank the reviewer for the time and concern, and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, let\u2019s clarify the concept of adversarial examples. The concept of \u201cadversarial examples\u201d was first proposed in 2014 [1] and defined as \u201cWe find that applying an imperceptible non-random perturbation to a test image, it is possible to arbitrarily Change the Network\u2019s Prediction. \u2026 We term the so perturbed examples \u2018adversarial examples\u2019.\u201d That is to say, adversarial examples are the ones misclassified by the network (by adding imperceptible perturbations). As long as the prediction is changed by the perturbation, the perturbed example is called \u201cadversarial example\u201d no matter how the prediction is far from the ground truth. Therefore, we think that each adversarial example should be treated equally. There is no \u201cgood\u201d or \u201cbad\u201d attack. There is only \u201csuccessful\u201d and \u201cfailed\u201d attack.\n\nSecond, to the best of our knowledge, most papers in adversarial attack community focus on untargeted attacks. Unfortunately, exiting untargeted attacks just focus on getting the networks failed (with imperceptible perturbations in the image space). They don\u2019t care about the misclassified-label in the label space so that the misclassified-label may be very unreasonable. However, many effective defense methods [2,3] have been published. Unreasonable labels will cause the users detect the attack and take defensive measures to let the attack fail (just as introduced in Section 1). So \u201ccreating an attack without regards to the target label\u201d is adverse for an attacker. Therefore, our method is applicable when an attacker conducts untargeted attack and hope it not to be detected. We recommend to create carefully automatically designed target label when doing untargeted attacks.\n\nThird, in Figure 7 in Appendix B of the original version, there are some figures as you suggested. In Appendix E of our new version, we add more examples in Figure 9. We also add true/target labels in Figure 5 as you suggested.\n\nIf you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n\nReferences:\n[1] Szegedy et al. Intriguing properties of neural networks. (ICLR 2014)\n[2] Jia X, et al. ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples. (CVPR 2019)\n[3] Dubey, Abhimanyu, et al. Defense against adversarial images using web-scale nearest-neighbor search. (CVPR 2019)\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "SJgk3O6GoS", "original": null, "number": 2, "cdate": 1573210278968, "ddate": null, "tcdate": 1573210278968, "tmdate": 1573210278968, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1g-9ZHlsB", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "We thank the reviewer for the time , and would like to answer the reviewer\u2019s questions as follows:\n\nFirst, we want to explain our motivation for this work. Our method is an untargeted attack, not a targeted attack. To the best of our knowledge, most papers in adversarial attack community focus on untargeted attacks. But exiting untargeted attacks don\u2019t care about the misclassified-label in the label space so that the misclassified-label may be very unreasonable. However, many effective defense methods [1,2] have been published. Unreasonable labels will cause the users detect the attack and take defensive measures to let the attack fail (just as introduced in Section 1). So creating an attack without regards to the target label is adverse for an attacker. Therefore, we propose this LabelFool method in order to help an untargeted attack not to be defensed. We recommend to create carefully automatically designed target label when doing untargeted attacks\n\nSecond, there are many applications where \u201cattackers want to create less detectable adversarial examples in label space without manually assigning target labels\u201d. Please see Figure 9 in Appendix E in the new version. Taking face recognition system for entrance as an example, there will usually be a guard as \u201ca human inspector\u201d to check whether the man A who is entering the gate is the same as the system shows. In this case, if the model just misclassifies A and C, the person who looks totally different from A, the attack will be detected easily. But LabelFool aims to let the model misclassify A and B who looks like A as shown in Figure 9. If the guard doesn\u2019t identify carefully, he will let a fake B in and this error brings great potential security risks. So as an attacker, it is necessary to generate imperceptible examples in the label space for this task. But how does the attacker know who is the one looks like A in such a huge face database? LabelFool can help him much in this application.\n\nThird, we appreciate your suggestion about applying this method to other applications, we will do this in our future work, but now, we just introduce this idea, provide a tool and conduct experiments on simple tasks to prove the feasibility of this idea.\n\nIf you still have questions about our motivation, please don\u2019t hesitate to let us know. We are here to answer your questions all the time.\n\nReferences:\n[1] Jia X, et al. ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples. (CVPR 2019)\n[2] Dubey, Abhimanyu, et al. Defense against adversarial images using web-scale nearest-neighbor search. (CVPR 2019)\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper583/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1glDpNYwS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper583/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper583/Authors|ICLR.cc/2020/Conference/Paper583/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504169281, "tmdate": 1576860533346, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper583/Authors", "ICLR.cc/2020/Conference/Paper583/Reviewers", "ICLR.cc/2020/Conference/Paper583/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Comment"}}}, {"id": "HkgFLke2tr", "original": null, "number": 2, "cdate": 1571712848991, "ddate": null, "tcdate": 1571712848991, "tmdate": 1572972577192, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper describes a technique for creating adversarial images where the added perturbations are not only imperceptible to machines, but also to human observers. The authors describe why this might be beneficial. The method works by finding labels that are not too far from the source image's ground-truth labels, and moving the source image in that direction. To find the target label, the authors use a threshold on the confidence of predicted ground-truth labels. The authors test their algorithm using a newly proposed metric of how much a method allows imperceptibility for a human observer. They show that their method creates images whose perturbations are more impercetible to humans, compared to other methods, but are also imperceptible to machines.\n\nMy concern is as follows: If the misclassification is between A and B, and they are related classes, is the attack so bad? And what are the scenarios in practice when a user simply wants to create an attack, without regards to the target label chosen? I imagine normally the attacker has a target label in mind, so the part of the paper that chooses a target label is not very useful; and this is the main element of novelty, since the rest of the method is from DeepFool, as the authors explain. Some specific use cases of this methods should be discussed. \n\nMinor suggestion: It would be useful to see examples like in Fig. 5 but with the classes (true/target) listed."}, "signatures": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574735462971, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper583/Reviewers"], "noninvitees": [], "tcdate": 1570237750032, "tmdate": 1574735462986, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Review"}}}, {"id": "B1g1QouqYB", "original": null, "number": 1, "cdate": 1571617559257, "ddate": null, "tcdate": 1571617559257, "tmdate": 1572972577158, "tddate": null, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "invitation": "ICLR.cc/2020/Conference/Paper583/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a method for constructing adversarial attacks that are less detectable by humans, by changing the target class to be a class similar to the original class of the image. The resulting attack methodology is then studied in terms of its imperceptibility in label space, and shown to be less perceptible in label space to human observers, while not coming at a cost in image space.\n\nThe paper presents compelling evaluation of the method and does seem to succeed in proving that their proposed attack satisfies the stated goal. However, it appears as though this goal is somewhat counter to the main point of adversarial examples---indeed, if the label is reasonable to a human, then what makes the adversarial example adversarial? The main threat in adversarial examples research seems to be that it is possible to induce predictions that are arbitrarily different from humans' on natural-looking in puts. Thus, changing the label to something that a human actually agrees with would actually reduce the impact of the adversarial attack.\n\nIn order to improve the paper, I would suggest applying the same (or similar) methodologies to other areas of ML security where imperceptibility in label space is commonly desired---for example, in data poisoning attacks or backdoor attacks. In general, such attacks are much more likely to be \"inspected\" by humans, and so imperceptibility in both label and image space is very desirable. However, I suspect that this would require significant effort and changes to the paper, and so for now I recommend rejection."}, "signatures": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper583/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LabelFool: A Trick in the Label Space", "authors": ["Yujia Liu", "Tingting Jiang", "Ming Jiang"], "authorids": ["yujia_liu@pku.edu.cn", "ttjiang@pku.edu.cn", "ming-jiang@pku.edu.cn"], "keywords": ["Adversarial attack", "LabelFool", "Imperceptibility", "Label space"], "TL;DR": "A trick on adversarial samples so that the mis-classified labels are imperceptible in the label space to human observers", "abstract": "It is widely known that well-designed perturbations can cause state-of-the-art machine learning classifiers to mis-label an image, with sufficiently small perturbations that are imperceptible to the human eyes. However, by detecting the inconsistency between the image and wrong label, the human observer would be alerted of the attack. In this paper, we aim to design attacks that not only make classifiers generate wrong labels, but also make the wrong labels imperceptible to human observers. To achieve this, we propose an algorithm called LabelFool which identifies a target label similar to the ground truth label and finds a perturbation of the image for this target label. We first find the target label for an input image by a probability model, then move the input in the feature space towards the target label. Subjective studies on ImageNet show that in the label space, our attack is much less recognizable by human observers, while objective experimental results on ImageNet show that we maintain similar performance in the image space as well as attack rates to state-of-the-art attack algorithms.", "pdf": "/pdf/00638871ccd861f8b3658a45ce5e8e9c61ba7b9c.pdf", "paperhash": "liu|labelfool_a_trick_in_the_label_space", "original_pdf": "/attachment/d736bdb47bd76e79eabb55b834d0802f67156d68.pdf", "_bibtex": "@misc{\nliu2020labelfool,\ntitle={LabelFool: A Trick in the Label Space},\nauthor={Yujia Liu and Tingting Jiang and Ming Jiang},\nyear={2020},\nurl={https://openreview.net/forum?id=r1glDpNYwS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1glDpNYwS", "replyto": "r1glDpNYwS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper583/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574735462971, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper583/Reviewers"], "noninvitees": [], "tcdate": 1570237750032, "tmdate": 1574735462986, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper583/-/Official_Review"}}}], "count": 15}