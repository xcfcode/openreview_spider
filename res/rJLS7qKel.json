{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487054591195, "tcdate": 1478234942149, "number": 116, "id": "rJLS7qKel", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJLS7qKel", "signatures": ["~Alexey_Dosovitskiy1"], "readers": ["everyone"], "content": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396365652, "tcdate": 1486396365652, "number": 1, "id": "B1IRiMIdg", "invitation": "ICLR.cc/2017/conference/-/paper116/acceptance", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper details the approach that won the VizDoom competition - an on-policy reinforcement learning approach that predicts auxiliary variables, uses intrinsic motivation, and is a special case of a universal value function. The approach is a collection of different methods, but it yields impressive empirical results, and it is a clear, well-written paper.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396366127, "id": "ICLR.cc/2017/conference/-/paper116/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJLS7qKel", "replyto": "rJLS7qKel", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396366127}}}, {"tddate": null, "tmdate": 1484259373088, "tcdate": 1484259373088, "number": 5, "id": "ryBEetBLg", "invitation": "ICLR.cc/2017/conference/-/paper116/public/comment", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["~Alexey_Dosovitskiy1"], "readers": ["everyone"], "writers": ["~Alexey_Dosovitskiy1"], "content": {"title": "Response to reviewers", "comment": "We thank the reviewers for their work and their comments. The reviews will help in further improving the paper. Some specific responses to individual reviewers are below.\n\n\nAnonReviewer1:\n\nThank you, we will further polish up the notation and the tables based on your suggestions.\n\n\nAnonReviewer2:\n\nWe used a very small replay buffer (2,500 frames per actor thread, 20,000 frames in total), and have not observed any significant changes in results when making it smaller or larger. This indicates that in practice the algorithm is not constrained to strict on-policy training. We will discuss this in more detail in the paper.\n\nWe agree that measurements that can be used for future prediction are not always available. (For example, as mentioned in the introduction, board games are an extreme example in which only a sparse scalar reward is provided.) We will further emphasize this in the paper to remind the reader that the presented approach adopts different modeling assumptions from most approaches in the literature.\n\n\nAnonReviewer3:\n\nThe simplicity of our model leads to several failure modes: the agent acts purely reactively, cannot construct a persistent representation of its environment (e.g., a cognitive map), and cannot plan its actions far into the future. For this reason its performance in Doom is at the level of a very inexperienced human player. Much exciting work remains to be done.\n\nWe are interested in sensorimotor control in immersive environments and chose Doom for this reason. We are looking forward to testing and further developing the approach in other simulators that are becoming available, such as DeepMind's new Lab."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287721496, "id": "ICLR.cc/2017/conference/-/paper116/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJLS7qKel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper116/reviewers", "ICLR.cc/2017/conference/paper116/areachairs"], "cdate": 1485287721496}}}, {"tddate": null, "tmdate": 1481962751525, "tcdate": 1481962751525, "number": 3, "id": "BJvWS_GVg", "invitation": "ICLR.cc/2017/conference/-/paper116/official/review", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["ICLR.cc/2017/conference/paper116/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper116/AnonReviewer2"], "content": {"title": "Compelling empirically driven result", "rating": "7: Good paper, accept", "review": "Deep RL (using deep neural networks for function approximators in RL algorithms) have had a number of successes solving RL in large state spaces. This empirically driven work builds on these approaches. It introduces a new algorithm which performs better in novel 3D environments from raw sensory data and allows better generalization across goals and environments. Notably, this algorithm was the winner of the Visual Doom AI competition.\n\nThe key idea of their algorithm is to use additional low-dimensional observations (such as ammo or health which is provided by the game engine) as a supervised target for prediction. Importantly, this prediction is conditioned on a goal vector (which is given, not learned) and the current action. Once trained the optimal action for the current state can be chosen as the action that maximises the predicted outcome according the goal. Unlike in successor feature representations, learning is supervised and there is no TD relationship between the predictions of the current state and the next state.\n\nThere have been a number of prior works both in predicting future states as part of RL and goal driven function approximators which the authors review in section 2. The key contributions of this work are the focus on Monte Carlo estimation (rather than TD), the use of low-dimensional \u2018measurements\u2019 for prediction, the parametrized goals and, perhaps most importantly, the empirical comparison to relevant prior work.\n\nIn addition to the comparison with Visual Doom AI, the authors show that their algorithm is able to learn generalizable policies which can respond, without further training, to limited changes in the goal.\n\nThe paper is well-communicated and the empirical results compelling and will be of significant interest.\n\nSome minor potential improvements:\nThere is an approximation in the supervised training as it is making an on-policy assumption but it learns from a replay buffer (with the Monte Carlo regression the expectation of the remainder of the trajectory is assumed to follow the current policy, but is being sampled from episodes generated by prior versions of the policy). This should be discussed.\nThe algorithm uses additional metadata (the information about which parts of the sensory input are worth predicting) that the compared algorithms do not. I think this, and the limitations of this approach (e.g. it may not work well in a sensory environment if such measurements are not provided) should be mentioned more clearly.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512692817, "id": "ICLR.cc/2017/conference/-/paper116/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper116/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper116/AnonReviewer1", "ICLR.cc/2017/conference/paper116/AnonReviewer3", "ICLR.cc/2017/conference/paper116/AnonReviewer2"], "reply": {"forum": "rJLS7qKel", "replyto": "rJLS7qKel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512692817}}}, {"tddate": null, "tmdate": 1481911452503, "tcdate": 1481911452503, "number": 2, "id": "ryVinjW4g", "invitation": "ICLR.cc/2017/conference/-/paper116/official/review", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["ICLR.cc/2017/conference/paper116/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper116/AnonReviewer3"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper presents an on-policy deep RL method with additional auxiliary intrinsic variables. \n\n- The method is a special case of an universal value function based approach and the authors do cite the correct references. Maybe the biggest claimed technical contribution of this paper is to distill many of the existing ideas to solve 3D navigation problems. I think the contributions should be more clearly stated in the abstract/intro\n\n- I would have liked to see failure modes of this approach. Under what circumstances does the model have problems generalizing to changing goals? There are other conceptual problems -- since this is an on-policy method, there will be catastrophic forgetting if the agent dosen't repeatedly train on goals from the distant past. \n\n- Since the main contribution of this paper is to integrate several key ideas and show empirical advantage, I would have liked to see results on other domains like Atari (maybe using the ROM as intrinsic variables)\n\nOverall, I think this paper does show clear empirical advantage of using the proposed underlying formulations and experimental insights from this paper might be valuable for future agents", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512692817, "id": "ICLR.cc/2017/conference/-/paper116/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper116/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper116/AnonReviewer1", "ICLR.cc/2017/conference/paper116/AnonReviewer3", "ICLR.cc/2017/conference/paper116/AnonReviewer2"], "reply": {"forum": "rJLS7qKel", "replyto": "rJLS7qKel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512692817}}}, {"tddate": null, "tmdate": 1481898605559, "tcdate": 1481898605559, "number": 1, "id": "BkBdc_ZEl", "invitation": "ICLR.cc/2017/conference/-/paper116/official/review", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["ICLR.cc/2017/conference/paper116/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper116/AnonReviewer1"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper presents an on-policy method to predict future intrinsic measurements. All the experiments are performed in the game of Doom (vizDoom to be exact), and instead of just predicting win/loss or the number of frags (score), the authors trained their model to predict (a sequence of) triplets of (health, ammunition, frags), weighted by (a sequence of) \"goal\" triplets that they provided as input. Changing the weights of the goal triplet is a way to perform/guide exploration. At test time, one can act by maximizing the long term goal only.\n\nThe results are impressive, as this model won the 2016 vizDoom competition. The experimental section of the paper seems sound:\n - There are comparisons of DFP with A3C, DQN, and an attempt to compare with DSR (a recent similar approach from Kulkarni et al., 2016). DFP outperforms other approaches (or equal them when they reach a ceiling / optimum, as for A3C in scenario D1).\n - There is an ablation study that supports the thesis that all the \"added complexity\" of the paper's model is useful.\n\nPredicting intrinsic motivation (Singh et al. 2004), auxiliary variables, and forward modelling, are well-studied domains of reinforcement learning. The version that I read (December 4th revision) adequately references prior work, even if it is not completely exhaustive. \n\nA few comments (nitpicks) on the form:\n - Doom is described as a 3D environment, whereas it is actually a 2D environment (the height is not a discriminative/actionable dimension) presented in (fake) 3D.\n - The use of \"P\" in (2) (and subsequently) may be misleading as it stands for prediction but not probability (as is normally the case for P).\n - The double use of \"j\" (admittedly, with different fonts) in (6) may be misleading.\n - Results tables could repeat the units of the measurements (in particular as they are heterogenous in Table 1).\n\nI think that this paper is a clear accept. One could argue that experiments could be conducted on different environments or that the novelty is limited, but I feel that \"correct\" (no-nonsense, experimentally sound on Doom, appendix providing details for reproducibility) and \"milestone\" (vizDoom winner) papers should get published.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512692817, "id": "ICLR.cc/2017/conference/-/paper116/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper116/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper116/AnonReviewer1", "ICLR.cc/2017/conference/paper116/AnonReviewer3", "ICLR.cc/2017/conference/paper116/AnonReviewer2"], "reply": {"forum": "rJLS7qKel", "replyto": "rJLS7qKel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512692817}}}, {"tddate": null, "tmdate": 1481149301902, "tcdate": 1481149301611, "number": 4, "id": "HkA_jbUXx", "invitation": "ICLR.cc/2017/conference/-/paper116/public/comment", "forum": "rJLS7qKel", "replyto": "rkvwg67Xl", "signatures": ["~Alexey_Dosovitskiy1"], "readers": ["everyone"], "writers": ["~Alexey_Dosovitskiy1"], "content": {"title": "Re: Meaning for reward, goal setting", "comment": "(1) The model is trained exactly as shown in the figure: both streams are trained jointly in such a way that the sum of their outputs approximates the target values. The training objective is given in equation (4). The separate training you described is hardly possible, since the only available training signal are the actual targets (that is, future measurements). Separate targets for the two streams are not available. Indeed, computing such a target for the expectation stream would require averaging future measurement values over all possible futures given a fixed current state, which is impossible to compute in realistic scenarios.\n\n(2) It will be easier to understand the approach if you do not think about the traditional notion of reward. Instead, think first about training the model to predict future measurements. Here the loss is equally distributed over all measurements and time steps. The model is trained to predict all measurements at all time steps specified in Appendix B. There is no temporal weighting (equation (4)). Now think about selecting the action at test time. Here different time steps can be prioritized via the goal vector (equation (3)). For example, for D3 and D4 we use a goal vector that only assigns non-zero weights to the farthest time step (t+32).\n\n(3) One of the main ideas of our approach is that the model predicts the future measurements f, and this prediction vector can then be flexibly combined with arbitrary goal vector g to form a test-time objective function g^T f . We call g a goal vector because it specifies the objective or, in other words, the goal of the agent. It is not the goal in the sense \u201ca specific location in space to be reached\u201d, but rather a certain general objective of the agent. Intuitively, the goal vector specifies how much to value each of the future measurements. For example, assume that the future measurements being predicted are (ammo, health, frags) 32 time steps in the future. Then g=(0,1,0) will specify the goal of maximizing health, while g=(-1,0,0) will specify the goal of pointlessly wasting ammo. In this way the system need not be limited to a single goal, and goals can be flexibly changed at test time, potentially as a reaction to the environment, other agents, a separate motivation module, etc.\n\nConcerning g also being an input of the network. In our model the behavior of the agent is defined by its goal vector g, since the actions are selected by maximizing g^T f. In order to allow the predictor to generalize to a variety of behaviors we condition the predictor on the goal vector. Intuitively, instead of predicting \u201cin 32 steps I will have 50 health points\u201d, the model predicts \u201cif I act according to goal vector (1,0,0), then in 32 steps I will have 75 health points\u201d. The conditioning is done by simply feeding the goal vector as one of the inputs to the network."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287721496, "id": "ICLR.cc/2017/conference/-/paper116/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJLS7qKel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper116/reviewers", "ICLR.cc/2017/conference/paper116/areachairs"], "cdate": 1485287721496}}}, {"tddate": null, "tmdate": 1480999007322, "tcdate": 1480999007313, "number": 3, "id": "rkvwg67Xl", "invitation": "ICLR.cc/2017/conference/-/paper116/public/comment", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Meaning for reward,  goal setting", "comment": "It's interesting to see that by purely using predictions over the differences between measurements for future step(s) and the presented step, the learning performance could surpass A3C and DSR in Doom. I have the following questions:\n\n(1) In Figure 1, why there is a \"Target f\" drawn to the right of \"Prediction P\"? Though this makes the model looks more end-to-end, I guess the \"Expectation\" component and \"Action\" component are updating their parameters according to their own regression losses, instead of back-propagating through a unified end. \n\n(2) The reward formulation in this work is similar to Monte-Carlo methods. In this work, the cumulative reward is represented by summing up the outputs of expectation stream and action stream *over each time steps*. If only those over the last time step is considered, it will become almost same as Monte-Carlo. However, the presented work accumulates the difference over multiple future steps. Based on the proposed form of *f* (in Section 3), the reward collected by near-future steps are weighted more. Or we could see the weight to be interpreted as linearly decaying based on the time distance from the future step to the current step. If this is the case, re-formulating the proposed approach with linearly discounted Monte-Carlo could be possible. You may also compare such Monte-Carlo as a compelling baseline. Also, it's better to put more discussion over your reward formulation, since it contributes a lot to the performance improvement and it's better to let people understand why it works well. \n\n(3) I don't quite understand what is the meaning of *goal*. In Section 3, *g* is proposed to be of the same dimension as  *f*. The maximization is over g^T*f, but what the architecture try to regress is over f, and g is designed as part of inputs to the deep neural network. And action-selection is over the maximum of summed E and A over all future steps. In this way, what the agent is trying to do is to maximize the future gain over those measurements, instead of performing according to the specified goal. Hope the authors could clarify on this.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287721496, "id": "ICLR.cc/2017/conference/-/paper116/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJLS7qKel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper116/reviewers", "ICLR.cc/2017/conference/paper116/areachairs"], "cdate": 1485287721496}}}, {"tddate": null, "tmdate": 1480885843295, "tcdate": 1480885624810, "number": 2, "id": "BJWKBWzXe", "invitation": "ICLR.cc/2017/conference/-/paper116/public/comment", "forum": "rJLS7qKel", "replyto": "BJaWZGCGl", "signatures": ["~Vladlen_Koltun1"], "readers": ["everyone"], "writers": ["~Vladlen_Koltun1"], "content": {"title": "Authors' response to AnonReviewer3", "comment": "We have uploaded a revision that relates our work in more detail to UVFA (Schaul et al., 2015). The modified text is at the end of the first paragraph on page 3. Our model is indeed related to the general family of models described by Schaul et al. However, the specifics are important.\n\nSchaul et al. present a very general framework, but none of its specific instantiations appear to have the effectiveness of our model. The closest instantiation presented by Schaul et al. is in Section 5.3 of their paper, yet it is only evaluated on a discrete grid-world with manually identified grid cells as goals (Figure 11). Furthermore, the authors report that training their model with function approximation is unstable and resort to a heuristic. In general, Schaul et al. use different locations in their 2D environments as goals. The locations essentially serve as \u201cbeacons\u201d placed by the designers in the environments. The goals do not emerge naturally and do not appear to be intrinsic to the agents.\n\nIn our work, goals are defined in terms of intrinsic measurements and control is based on direct prediction of future measurements. We provide a specific network architecture that handles realistic sensory and measurement streams (Figure 1 in our paper), to which there is no analogue in the work of Schaul et al. We show that this architecture trains extremely effectively. And we provide extensive experiments and results in immersive environments, to which there is also no analogue in the prior work.\n\nConcerning DSR, we used the code provided by the authors, spent time analyzing the code and attempting to speed it up, and corresponded with the authors to facilitate these attempts. Regarding the autoencoder specifically, it is presented in the technical report of Kulkarni et al. (2016b) as an integral component of the approach. During our correspondence, the lead author of the technical report wrote \u201cI do have an autoencoder loss, which does slow down the training significantly. That is currently my least favorite part of DSR and I am actively trying to change that to make DSR more scalable.\u201d This indicates that removing the autoencoder is not trivial, since the lead author himself is trying to do this but hasn\u2019t yet succeeded."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287721496, "id": "ICLR.cc/2017/conference/-/paper116/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJLS7qKel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper116/reviewers", "ICLR.cc/2017/conference/paper116/areachairs"], "cdate": 1485287721496}}}, {"tddate": null, "tmdate": 1480885388890, "tcdate": 1480885388882, "number": 1, "id": "HJSqEWf7g", "invitation": "ICLR.cc/2017/conference/-/paper116/public/comment", "forum": "rJLS7qKel", "replyto": "SJJYPkCfl", "signatures": ["~Vladlen_Koltun1"], "readers": ["everyone"], "writers": ["~Vladlen_Koltun1"], "content": {"title": "Authors' response to AnonReviewer2", "comment": "Regarding \u201cadditional information\u201d, note that all approaches were trained with the same weighting of ammo, health, and frags. That is, \u201chow much to relatively value ammo, health, frags\u201d (e.g., (0.5,0.5,1)) was provided in an identical form to both the presented approach and all of the baselines. This is stated at the bottom of page 6 and in Appendix C.\n\nBy \u201clearning the goals\u201d do you mean having measurements and a separate reward, and learning to express the reward via measurements? We have not tried this, but it is an interesting suggestion. It seems that in the case of goals being linear combinations of measurements it would be almost trivial, but in more complicated settings it can be interesting.\n\nMore broadly, note that the ability to change goals at test time is a key characteristic of the approach. It opens the possibility of using the model as part of a larger system in which goals are set dynamically by other modules, for example in response to interaction with other agents. Exposing the goal vector as part of the external interface of the model is thus deliberate."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287721496, "id": "ICLR.cc/2017/conference/-/paper116/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJLS7qKel", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper116/reviewers", "ICLR.cc/2017/conference/paper116/areachairs"], "cdate": 1485287721496}}}, {"tddate": null, "tmdate": 1480626437322, "tcdate": 1480626437317, "number": 2, "id": "BJaWZGCGl", "invitation": "ICLR.cc/2017/conference/-/paper116/pre-review/question", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["ICLR.cc/2017/conference/paper116/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper116/AnonReviewer3"], "content": {"title": "comparisons", "question": "- I would like the authors to relate formulations proposed in (1,2,3) to previous RL literature and models, and then adjust the claims. This approach could be crudely categorized as a UVFA (universal value function appx, Schaul et al) model trained with an on-policy method.\n\n- In order to make the comparisons with DSR fair, DSR could be learnt without the autoencoder loss (although the original DSR paper does make use of this loss throughout the paper)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959453896, "id": "ICLR.cc/2017/conference/-/paper116/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper116/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper116/AnonReviewer2", "ICLR.cc/2017/conference/paper116/AnonReviewer3"], "reply": {"forum": "rJLS7qKel", "replyto": "rJLS7qKel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959453896}}}, {"tddate": null, "tmdate": 1480615799169, "tcdate": 1480615799166, "number": 1, "id": "SJJYPkCfl", "invitation": "ICLR.cc/2017/conference/-/paper116/pre-review/question", "forum": "rJLS7qKel", "replyto": "rJLS7qKel", "signatures": ["ICLR.cc/2017/conference/paper116/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper116/AnonReviewer2"], "content": {"title": "Goals", "question": "The goal is always given to the agent, never learned. Is that correct? So it seems like this is providing additional information that other models didn't receive about the decomposition of the reward function (e.g. how much to relatively value ammo, health, frags).\n\nHave you tried learning the goal?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Act by Predicting the Future", "abstract": "We present an approach to sensorimotor control in immersive environments. Our approach utilizes a high-dimensional sensory stream and a lower-dimensional measurement stream. The cotemporal structure of these streams provides a rich supervisory signal, which enables training a sensorimotor control model by interacting with the environment. The model is trained using supervised learning techniques, but without extraneous supervision. It learns to act based on raw sensory input from a complex three-dimensional environment. The presented formulation enables learning without a fixed goal at training time, and pursuing dynamically changing goals at test time. We conduct extensive experiments in three-dimensional simulations based on the classical first-person game Doom. The results demonstrate that the presented approach outperforms sophisticated prior formulations, particularly on challenging tasks. The results also show that trained models successfully generalize across environments and goals. A model trained using the presented approach won the Full Deathmatch track of the Visual Doom AI Competition, which was held in previously unseen environments.", "pdf": "/pdf/f321f9cd51a309fb73de5972dec6e2bcee73b3db.pdf", "TL;DR": "We present an approach to sensorimotor control in immersive environments.", "paperhash": "dosovitskiy|learning_to_act_by_predicting_the_future", "keywords": [], "conflicts": ["intel.com", "cs.uni-freiburg.de", "stanford.edu"], "authors": ["Alexey Dosovitskiy", "Vladlen Koltun"], "authorids": ["adosovitskiy@gmail.com", "vkoltun@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959453896, "id": "ICLR.cc/2017/conference/-/paper116/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper116/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper116/AnonReviewer2", "ICLR.cc/2017/conference/paper116/AnonReviewer3"], "reply": {"forum": "rJLS7qKel", "replyto": "rJLS7qKel", "writers": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper116/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959453896}}}], "count": 12}