{"notes": [{"id": "J40FkbdldTX", "original": "F4YKOp1N89f", "number": 814, "cdate": 1601308094210, "ddate": null, "tcdate": 1601308094210, "tmdate": 1614985677053, "tddate": null, "forum": "J40FkbdldTX", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "quQ8KruzWM", "original": null, "number": 1, "cdate": 1610040477550, "ddate": null, "tcdate": 1610040477550, "tmdate": 1610474082217, "tddate": null, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This submission received reviews with a very wide range of scores (initially 3,5,5,9; then 5,5,5,9). In the discussion, all reviewers maintained their general position (although a private message by the reviewer giving a score of 9 said he/she would consider going down to an 8).\n\nBecause of the high variance, I read the paper in detail myself. I agree with all reviewers that NAS is a very important field of study, that the experiments are interesting, and that purely empirical papers studying what works and what doesn't work (rather than introducing a new method) are definitely needed in the NAS community. But overall, for this particular paper, I agree with the 3 rejecting reviewers. The paper presents a lot of experiments, but I am missing novel deep insights or lasting overarching take-aways. The papers reads a bit like a log book of all the experiments the authors did, before having gone through the next iteration in the process to consolidate findings and gain lasting insight.\n\nIn a bit more detail, half the results in Section 4 use medium-sized super networks, which seem broken to me, yielding much worse performance than small super networks. I did not find any motivation for studying these medium-sized networks, no reason given for them to perform poorly, and none stating why the results are still interesting when the networks perform so poorly (apologies if I overlooked these). The poor performance may be due to using a training pipeline that works poorly for these larger networks, but this is hard to know exactly without further experiments. I would either try to fix these networks' performance or drop them from the paper entirely, as I do not see any insights that can be reliable gained from the current results. As is, I believe these results (accounting for half the plots in the paper) only muddy the water and are preventing a crisp presentation of insightful results.\n\nAnother factor that I find unfortunate about the paper is that it only uses NAS-Bench-201 for its empirical study, and even for that dataset, mostly only the CIFAR-10 part. After getting rid of isomorphic graphs from the original 15625 architectures, NAS-Bench-201 only has 6466 unique architectures (see Appendix A of NAS-Bench-201), while, e.g., NAS-Bench-101 has 423k unique architectures. As the authors indicate themselves in their section \"Grains of Salt\", it is unclear whether insights gained on the very small NAS-Bench-201 space generalize to larger spaces. I therefore believe that there should also be some experiments on another, larger space, to study how well some of the findings generalize. An additional benchmark that the authors could have directly used without performing additional experiments themselves is the NAS benchmark NAS-Bench-1shot1 (ICLR 2020: https://openreview.net/forum?id=SJx9ngStPH), which studies 3 different subsets of NAS-Bench-101, and which was created to allow one-shot methods to use the larger space of evaluated architectures in NAS-Bench-101. \n\nMinor comments:\n- It reads as if the authors performed 5 runs, computed averages of the outcomes, and then computed correlation coefficients. That would be a suboptimal experimental setup, though; in practical applications, only one run of the super network would be run, and therefore, in order to assess performance reliably, one should compute correlation coefficients for one run at a time, and then obtain a measurement of reliability of these correlation coefficients across the 5 runs.\n- The y axis in Figure 2 appears to be broken: for example, in the left column it goes from 99.978 to 99.994, and the caption says these should be accuracy predictions of NAS-Bench201. However, even the best architectures in NAS-Bench201 only achieve around 95% accuracy.\n\n\nOverall, I recommend rejection for the current version of the paper.\nGoing forward, I encourage the authors to continue this line of work and recommend that they iterate over their experiments and extract crisp insights from their experiments. I also recommend performing experiments with a much larger search space than that of NAS-Bench-201 to assess whether the findings generalize."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040477537, "tmdate": 1610474082201, "id": "ICLR.cc/2021/Conference/Paper814/-/Decision"}}}, {"id": "RW2ClSyiPn0", "original": null, "number": 2, "cdate": 1603788853514, "ddate": null, "tcdate": 1603788853514, "tmdate": 1607420499091, "tddate": null, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Review", "content": {"title": "A timely analysis of the current NAS's ineffectiveness caused by the inaccurate architecture rating problem", "review": "\n+ This paper studies the single-path one-shot super-network predictions and ranking correlation throughout an entire search space, as all stand-alone model results are known in advance. This is a crucial step in NAS. As we know, inaccurate architecture rating is the cause of ineffective NAS in almost all existing NAS methods. It makes nearly all previous NAS methods not better the random architecture selection (suggested by two ICLR 2020 papers and many ICLR 2021 submissions). Therefore, analyzing the architecture rating problem is of most importance in NAS. This paper takes a deep insight into the architecture rating problem, which provides a timely metric for evaluating NAS's effectiveness. (+)\n\n\n- In the following text, another paper entitled \"Block-wisely Supervised Neural Architecture Search with Knowledge Distillation\" should be discussed: \"Recent efforts have shown improvements by strictly fair operation sampling in the super-network training phase (Chu et al. (2019b)), by adding a linear 1\u00d71 convolution to skip connections, improving training stability (Chu et al. (2019a)), or by dividing the search space (Zhao et al. (2020)),\" (-)\n\n\n- Kendall's Tau is a good metric. As shown in EagleEye, Spearman Correlation Coefficient(SCC) and Pearson Correlation Coefficient (PCC) are also good metrics. Could the authors also provide a comparison using these two metrics? (-)\n\nEagleEye: Fast Sub-net Evaluation for Efficient Neural Network Pruning\n\n\n- I think NAS-Bench-201 is not enough. As we know, CIFAR-10 is sometimes considered a toy benchmark, and the sole result on CIFAR-10 is not convincing. Could the authors provide more results in addition to CIFAR-10? (-)\n\n\n- As we know, there may be a gap between the small-channel supernet and the large-channel finally-adopted architecture. We are quite interested in the ranking correlations between a subnet obtained from the small-channel supernet and a channel-expanded version of the subnet trained from scratch. Could the authors provide such a ranking correlation analysis? (-)\n\n\n- Could the authors provide more details in Figure 3. Figure 3 shows that the lines on the top mean the operation is used more frequently. But I am not sure what the value of the y-axis means. (-)\n\n\n- Could the authors present some comments on \"Perhaps the most surprising is the low importance of Average Pooling, even lower than Zero, an operation that does absolutely nothing\"? (-)\n\n\n+ The following observation is believed to be crucial in NAS: \"The baseline for small networks (top left, red) has the same averaged prediction accuracy for the top 10 as for the top 500 networks\". This validates the inefficiency of SPOS in architecture search. (+)\n\n\n+ The following observation is also important in NAS: \"Masking Skip (blue, left) is the most harmful to \u03c4a (=1). As seen in Figure 4, the top-N networks have a worse average predicted accuracy than the top-M (for N < M) networks, and sometimes even below the random sample, which is terrible. Interestingly, \\tau may improve within the predictions for the top-N architectures.\" Especially, the phenomenon that masking skip connection reduces the ranking correlations is interesting. As is shown in SCARLET-NAS, the supernet training with skip connection is not fair. But in this paper, we can see that skip connection benefits the ranking correlation. We are interested in this opposite opinion. Specifically, it is fascinating to see that \"Although the additional transformers seem to stabilize training, as seen by the lower standard deviation, they also worsen the \u03c4a problem.\" Besides, the phenomenon of \"\\tao may improve within the predictions for the top-N architectures\" indicates that the metric for ranking correlations maybe not perfect. A more reasonable metric may be desirable. (+-)\n\n\n+ The following observation is important: \"medium-sized super-networks require additional care.\" As shown by Figure 4, the averaged predicted accuracy of top-N networks in several subsets is lower than that of a random subset of networks. This is consistent with previous work like DNA, which shows a large search space may be harmful to the architecture rating. Even if a medium-sized supernet has a bad architecture rating, the ranking correlation should be worse in a large-sized supernet. (+)\n\nDNA: Block-wisely Supervised Neural Architecture Search with Knowledge Distillation\n\n\n- The following description is questionable: \"After the architecture search, all Linear Transformers can safely be removed, as they do not impact the network capacity\". Actually, stacking many fully connected layers without non-linear activations could lead to only one fully connected layer. It is an open question of whether optimizing loss(ABCx, y) is as difficult as optimizing loss(Dx, y) using stochastic gradient descent. (-)\n\n\n+ The results providing evidence against disabling cell topology sharing during the training phase are exciting and new to the public. (+)\n\n\n+ The following observation is fascinating: \"The absolute validation accuracy value is increased by uniform sampling. However, this is not relevant, as only the correct ranking matters\". This is against FairNAS. (+)\n\n\n+ It is interesting and convincing that many tricks such as learning rate warm-up, gradient clipping, and regularization do not work to improve the ranking correlation. We are pleased that the authors provide so many experiments to point out some misleading approaches in NAS. I think this paper is very important in the context of AutoML. (+)\n\n\n- The analysis is based on medium-sized and small-sized search space. It would be good to see some analysis of large-sized search space. (-)\n\n\nOverall, this paper provides a timely analysis of the current NAS's ineffectiveness caused by the inaccurate architecture rating problem. As there are many NAS papers published every year and their ineffectiveness may still be not widely recognized by the reviewers and the public, I recommend a strong acceptance for this paper to promote the analysis of the NAS's architecture rating problem.\n\n\n\n\n------------------------------post rebuttal------------------------------------------\n\n\n-------------------Response to the authors' response----------------------\n\n\nThank you for the hard work in responding.\n\nI have read other reviewers' reviews and the response from the authors. The authors have addressed most of my concerns.\n\nI believe this paper deserves acceptance. As we know, variants of efforts have been made to improve NAS's effectiveness since 2016, and a great process has been reached. Despite the high expectation and solemn devotion, NAS's effectiveness is believed to be still low. This is inconsistent with many pioneer researchers' expectations four years ago, in which NAS is expected to be another revolutionary technique similar to 2012's deep learning. Currently, there are many NAS papers published every year. But their effectivenesses are unclear due to the lack of ranking correlation analysis. Differently, this paper comprehensively analyzes the architecture rating problem, which provides a timely analysis of the current NAS's ineffectiveness caused by inaccurate architecture rating. I think this paper can attract the community's attention, encouraging the community to pay attention to the architecture rating in NAS, especially when reviewing a NAS paper. Therefore, I recommend an acceptance for this paper to promote the analysis of the NAS's architecture rating problem.\n\nI agree with R2 that Yu et al. have proposed a similar idea (I assume R2 refers to \"Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, Mathieu Salzmann, Evaluating the Search Phase of Neural Architecture Search\"). But the analysis in this paper is more comprehensive than Yu et al.'s article. Many findings are new (at least they are not in published papers).\n\nI agree with R4 that the authors did not form a coherent logic flow to present these empirical findings, and the paper was similar to a technique report. However, many important articles, e.g., \"Designing Network Design Spaces,\" \"Exploring Simple Siamese Representation Learning,\" \"Is Faster R-CNN Doing Well for Pedestrian Detection?\" are also technique-report-like.\n\nI appreciate R1 for his devotion to finding similar observations in his experiments. I believe these observations are important and deserve publication. I agree with R1 that removing any operation leads to a smaller search space and a higher ranking.\n\nIn summary, I will keep my rating as an acceptance. \n\nUndoubtedly, I also believe the comments from other reviewers can benefit the improvement of your paper.\n\n\n\n\n\n\n\n\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134407, "tmdate": 1606915794071, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper814/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Review"}}}, {"id": "_tiCpFGx1NR", "original": null, "number": 4, "cdate": 1603985216006, "ddate": null, "tcdate": 1603985216006, "tmdate": 1606744964582, "tddate": null, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Review", "content": {"title": "Official Blind Review #2", "review": "This paper studies the relationship of correlation of ranking of networks sampled from SuperNet and that of stand-alone networks under various settings. They also study the how masking some operations in the search space and different ways of training effect the ranking correlation.\n\nPros:\nThe paper has a lot of experiments to substantiate the claims.\nFigure 3 where every operation is systematically masked, provides more insights about which operations are effective and how NAS behaves if one of the operation is masked.\n\nCons:\nSeveral other papers have already published similar findings. Overall the paper is very incremental.\nMore specifics in the questions\n\nQuestions\n\n1. How is the SuperNet trained?\n2. Figure2: Yu et al [1] have already explored the correlation of ranks of networks sampled from SuperNet and that of stand-alone networks. How is Figure 2 different from that? \n3. RobustDarts [2] has explored the possibility of how subset of NASBENCH search spaces behave. FAIRDarts [3] also explored the influence of skip connection by running DARTS without skip connection, running random search by limiting skip connection to 2 etc. Figure 4 seems to be inspired by that. While it is interesting, this might be a slight extension to the work done by Yu et al [1]\n4. Bender et al [4] postulate that the operations of a SuperNet are subject to co-adaptation and recommended techniques such as regularization, drop path etc to alleviate the same. RobustDarts also suggest some recommendations such as L2 regularization, drop path etc although in the context of DARTS. So while Figure 6 demonstrates this empirically, it is not a new finding.\n\nOverall, the empirical results in the paper are very useful for the NAS community. But the work is still very incremental. This might be better received as a workshop paper instead.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134407, "tmdate": 1606915794071, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper814/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Review"}}}, {"id": "olUO_IQr9f", "original": null, "number": 18, "cdate": 1606244796962, "ddate": null, "tcdate": 1606244796962, "tmdate": 1606244911105, "tddate": null, "forum": "J40FkbdldTX", "replyto": "NwfrAIDPVE", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Some examplar works for empirical study", "comment": "Thank you for your response. \n\nDuring the discussion among reviewers, R3 raised some very good previous works in formulating empirical study, which I believe can definitely inspire you in your reiteration: \n\n>\"Designing Network Design Spaces,\" \"Exploring Simple Siamese Representation Learning,\" \"Is Faster R-CNN Doing Well for Pedestrian Detection?\"\n\nI believe this work can hopefully be accepted in a future venue if you improve its organization towards these exemplars. For the current version, I would stand my initial rating. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "kfYm_du2y5", "original": null, "number": 14, "cdate": 1605772996515, "ddate": null, "tcdate": 1605772996515, "tmdate": 1605772996515, "tddate": null, "forum": "J40FkbdldTX", "replyto": "qLStF8NYOYv", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R3 part 1, number of normal cells", "comment": "We have added further plots for super-networks with N=1 to 5 normal cells per stage to Appendix D, and find that in the full search space, a smaller network is a better predictor.\n\nSincerely,\n\nFirst Author"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "lxe09zEY5w3", "original": null, "number": 9, "cdate": 1605272037627, "ddate": null, "tcdate": 1605272037627, "tmdate": 1605772751943, "tddate": null, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Changes to the paper", "comment": "Dear reviewers,\n\nthank you for evaluating our work and helping us in improving it. According to your feedback, we have made the following changes to the paper:\n\n---\n(Date:\u202fNovember 13, 2020) \n\n\nAbstract\n- minor rephrasing\n\nIntroduction:\n- added text about retraining with search-net weights\n- added citations for single-path methods\n- added overview/summary paragraph at the end\n\nRelated work\n- added DNA\n\nFigure 3\n- added clarification text\n\nSection 3.4\n- added formal description of \\tau_a\n\nSection 4.1\n- added text about Zero/Pool importance\n\nSection 4.2\n- removed that linear transformers can be \"safely\" removed, even though the capacity is the same \n\nAppendix D\n- added additional plots including the SCC and PCC metrics\n\nReferences\n- added venues\n\n\n---\n(Date:\u202fNovember 17, 2020) \n\nAppendix D\n- updated the plots\n\n\n---\n(Date:\u202fNovember 19, 2020) \n\n\nSection 3.1\n- mention Appendix D\n\nConclusions\n- mention Appendix D\n\nAppendix D\n- added plots for networks using a different number of normal cells\n\n\n---\n\nFurther changes to the paper will extend the above list, with a respective timestamp.\n\nWe have not yet updated our github repository (see the anonymized link in another comment), but plan to do so in the coming days.\n\nSincerely,\n\nFirst author\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "fHpjIe0oXmc", "original": null, "number": 13, "cdate": 1605623981704, "ddate": null, "tcdate": 1605623981704, "tmdate": 1605623981704, "tddate": null, "forum": "J40FkbdldTX", "replyto": "2-UnP-ieNp", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Code to reproduce", "comment": "We apologize for incautiousness and have uploaded the zipped code in the supplementary material instead.\n\nSincerely,\n\nFirst author"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "2-UnP-ieNp", "original": null, "number": 12, "cdate": 1605623164961, "ddate": null, "tcdate": 1605623164961, "tmdate": 1605623164961, "tddate": null, "forum": "J40FkbdldTX", "replyto": "an7xyQT_Lbm", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "non-anonymized  links are not permited", "comment": "Dear authors,\n\nLinks to unanonymized resources  are not permitted. \n\n- program chairs."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "an7xyQT_Lbm", "original": null, "number": 1, "cdate": 1604919740594, "ddate": null, "tcdate": 1604919740594, "tmdate": 1605623018006, "tddate": null, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Code to reproduce", "comment": "Dear reviewers and readers,\n\n\nthank You for the time already spent with our work. Since reproducing results is often an issue, we decided to make the the code available after some cleaning-up.\n\nThe link destination is not anonymized. \nThere are some instructions on the page to how to run the experiments.\n<removed>\n\n\nSincerely,\nFirst author"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Paper814/Reviewers", "ICLR.cc/2021/Conference/Paper814/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "qLStF8NYOYv", "original": null, "number": 11, "cdate": 1605612529606, "ddate": null, "tcdate": 1605612529606, "tmdate": 1605612529606, "tddate": null, "forum": "J40FkbdldTX", "replyto": "-_51MVAvsbZ", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R3 part 1, SSC/PCC", "comment": "We have updated the paper with the additional metrics SCC and PCC, please see the new Appendix D. \nThe experiments were selected from some prior ones, and additional small but wide super-networks (2 normal cells per stage, starting with 96 instead of 32 channels).\n\nOur github repository has been updated to include SCC and PCC.\n\n\n\nSincerely,\n\nFirst Author"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "-_51MVAvsbZ", "original": null, "number": 6, "cdate": 1605271614930, "ddate": null, "tcdate": 1605271614930, "tmdate": 1605612236129, "tddate": null, "forum": "J40FkbdldTX", "replyto": "RW2ClSyiPn0", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R3 part 1", "comment": "(due to the character limit, we have to split our reply into parts)\n\nThank you for your kind words of encouragement and the many great points that you make.\n\nIt is a hard task to keep track of the currently best NAS methods, given that they use different search spaces,\ndata sets, regularization methods, optimizers, learning rate schedules, batch sizes, training epochs,\nspecifics such as limiting the number of skip connections, and so on.\nWe therefore greatly appreciate benchmarks such as Bench-201, which, combined with the rather cheap evaluation of\nSPOS super-network predictions, enable a systematic and thorough comparison. \n\n\n- In the following text, another paper entitled\n\"Block-wisely Supervised Neural Architecture Search with Knowledge Distillation\"\nshould be discussed: \"Recent efforts have shown improvements by strictly fair\noperation sampling in the super-network training phase (Chu et al. (2019b)),\nby adding a linear 1\u00d71 convolution to skip connections, improving training stability (Chu et al. (2019a)),\nor by dividing the search space (Zhao et al. (2020)),\" (-)\n    - You are correct, in a sense DNA also divides the search space, albeit in another way than few-shot learning. \n    We have not considered using their technique, since that introduces the question how shared cell topologies should\n    be handled, which may be a topic on its own. However we added the reference to the text, which has been changed:\n        - *\"Recent efforts have\n        shown improvements by strictly fair operation sampling in the super-network training phase (Chu\n        et al. (2019b)) and adding a linear 1\u00d71 convolution to skip connections, improving training stability\n        (Chu et al. (2019a)). Other works divide the search space, exploring multiple models with different\n        operation-subsets (Zhao et al. (2020)), or one model with several smaller blocks that use a trained\n        teacher as a guiding signal (Li et al. (2020)).\"*\n\n\n- Kendall's Tau is a good metric. As shown in EagleEye, Spearman Correlation Coefficient(SCC)\nand Pearson Correlation Coefficient (PCC) are also good metrics.\nCould the authors also provide a comparison using these two metrics? (-)\n    - Absolutely. We have added Appendix D, where we selected some of the experiments and re-evaluated them,\n    adding SCC and PCC.\n\n\n- I think NAS-Bench-201 is not enough. As we know, CIFAR-10 is sometimes considered a toy benchmark,\nand the sole result on CIFAR-10 is not convincing. Could the authors provide more results in addition\nto CIFAR-10? (-)\n    - The computational cost of super-net training and especially acquiring ground-truth data on e.g. ImageNet is prohibitively\n    expensive, so that experiment typically only compare 10-20 models to measure \\tau (see e.g. FairNAS [A1]).\n    Such a comparison lacks information about the entire search space, such as knowing how good the discovered models\n    *actually* are - their ranking may be correct (high \\tau), but the models may still not be within the top 10%.\n    However, the recent idea of surrogate benchmarks [A2] may make such a study possible in the near future.\n\n\n- As we know, there may be a gap between the small-channel supernet and the large-channel\nfinally-adopted architecture. We are quite interested in the ranking correlations between a\nsubnet obtained from the small-channel supernet and a channel-expanded version of\nthe subnet trained from scratch. Could the authors provide such a ranking correlation analysis? (-)\n    - While we find this idea very tempting, we will not be able to train the amount of networks required for a good\n    analysis in time. Instead, we have trained small networks starting with 96 channels (as opposed to 32),\n    which is much closer to the final width.\n    These experiments also include the Pearson and Spearman metrics,\n    and are also listed in Appendix D.\n\n\n- Could the authors provide more details in Figure 3. Figure 3 shows that the lines on\nthe top mean the operation is used more frequently.\nBut I am not sure what the value of the y-axis means. (-)\n    - We have added the following text to the description of Figure 3., which hopefully answers the question:\n        - *\"As an example, if 3 of the top-10 networks use at least one Pool operation,\n    together a total of 4 Pool operations, and have all 5 operations available,\n    the usage is $3/10 = 0.3$ while the share is $4/(5\\cdot10) = 0.08$.\"*"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "XGm_ghs3tz9", "original": null, "number": 10, "cdate": 1605461141468, "ddate": null, "tcdate": 1605461141468, "tmdate": 1605461141468, "tddate": null, "forum": "J40FkbdldTX", "replyto": "npIVnLdSOWr", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R1 part 2", "comment": "(since we hit the character limit, this is part 2 of our reply)\n\n\n-  However, most of the findings are not new to me. They have been discussed more or less by previous works and discovered by my own experiments. So the contribution is not significant.\n    - We are happy to read that our results support your own findings, but it seems we failed to communicate\n    the novel parts in our work properly:\n         - we experiment with the idea of disabling the topology sharing only during training time\n         - the experiment design itself is novel, as we measure the ranking correlations in\n         many top-N network groups, which is only made possible by having extensive knowledge of the search space.\n         The thus obtained results give us insight not only in how well randomly sampled architectures are ranked,\n         but also if they still work in the subset of the top-500 networks (top <3% of the search space).\n    - And present interesting results, which we have not seen in literature yet:\n         - we study the effectiveness of method variations using the same environment (training schedule, network, data ...)\n         and in multiple search spaces. As many presented methods vary in this respect, or are limited to only one search\n         space, we saw a need in a broader and fair study.\n         - seconding the above point, we find that linear transformers [A1] only work in their proposed environment\n         and are otherwise harmful\n         - we also find that common training variations (regularization, learning rate warmup, gradient clipping)\n         do not improve the ranking correlations, independent of the search space\n    - We hope that the newly added paragraph in the introduction Section states our contribution more clearly, and that we could convince you, both of the necessity and the gained insights of a fair and broad study.\n\n\n - The paper is mostly clear, some paragraphs and references need to be polished, more related works should be added.\n    - We have added additional related work, references, some text (see our \"Changes to the paper\" post), and added\n    venues to the references in our newly uploaded version.\n    Please let us know if we neglected to mention further specific references or details.\n \n \n Sincerely\n \n First author\n \n [A1] SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search\n "}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "xyDyWUMIWAQ", "original": null, "number": 5, "cdate": 1605271525456, "ddate": null, "tcdate": 1605271525456, "tmdate": 1605456970413, "tddate": null, "forum": "J40FkbdldTX", "replyto": "npIVnLdSOWr", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R1", "comment": "\nWe thank you for the invested time in reviewing our work,\nand the suggestions that help us making the paper more accessible.\n\n\n\n- 'However, the aforementioned weight sharing xxx', there are a number of efficient multi-objective (Pareto front) NAS methods.\n    - That is absolutely correct, and the SPOS-based [A1] method that we use can be used for multi-objective optimization\n      with hardly any additional effort. However, if optimizing a single target is already hard\n      (accuracy is arguably the most important one), as we can see in our\n      experiments, adding additional targets will not give us any benefits. \n    - Edit to clarify (Nov. 15): \"no benefits\" with respect to the first target. Naturally, if the second target is e.g. latency,\n      a multi-objective search will choose fast architectures. But as the targets do not depend on each other, we can study\n      them in isolation. Furthermore, any improvement on the accuracy prediction also benefits multi-objective\n      methods.\n\n\n- 'However, since the single-path', please cite some literature related to the ranking issue.\n    - We added the SPOS [A1], FairNAS [A2], and SCARLET-NAS [A3], which we believe to be good examples.\n\n\n- Summarize the main findings in the introduction section.\n    - We added a paragraph at the end of the introduction section that briefly outlines the paper structure and includes\n    our most important findings. We feel that this is a strong improvement in accessibility and are grateful\n    for the suggestion.\n\n\n- Define \\tau_a in math. 'describing the ranking correlation of the average prediction accuracy depending on N',\nwhat is the average prediction accuracy for two lists?\n    - We have added a formal description that hopefully answers your question\n    (due to formatting we do not cite it here).\n\n\n- 4.1 'masking the Zero operation (bottom row) significantly reduces this portion and thus improves the ranking correlation\n(KT)'. If removing other operations other than zero, will the \\tau be lower?\nIn my opinion, removing any operation leads to smaller search space, and they all have a higher ranking.\n    - Given the Bench-201 set of operations, there are some rare exceptions. Masking e.g. the Zero operation\n    (see Figure 4, top left plot, top-10 networks) is actually detrimental.\n    However, observing \\tau in the same Figure seconds your point, a smaller search space is easier to rank\n    (the baseline \\tau is often worse than those of space subsets). \n    - We further want to point out that the search space subsets are a means to evaluate the robustness of search variations,\n    such as adding linear transformers [A3]. Being able to also validate the few-shot approach [A4] with these experiments\n    is a bonus to us, but not the main intention.\n    \n\n- 4.2 examine some of the training strategies proposed by previous works.\n    - The experiments cover SPOS [A1], FairNAS sampling [A2] (default in the experiments), linear transformers [A3],\n    multiple regularization techniques, the novel approach of disabling cell-topology-sharing during the search, and\n    indirectly also few-shot search [A4].\n    Please point us to any interesting or important topic that you feel we left uncovered.\n    - Please clarify if we understood you wrong, and you meant us to e.g. further elaborate on the super-net training process,\n    so that we can make the appropriate changes in time.\n    \n\n- Please make the references clear. Add the venues for all the papers.\n    - You are right, it is only good courtesy to add them as they are available. We have added all the venues we could\n    find via Google Scholar.\n\n\nSincerely,\n\nFirst author\n\n\n[A1] Single Path One-Shot Neural Architecture Search with Uniform Sampling\n\n[A2] FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search\n\n[A3] SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search\n\n[A4] Few-shot Neural Architecture Search\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "NwfrAIDPVE", "original": null, "number": 4, "cdate": 1605271435656, "ddate": null, "tcdate": 1605271435656, "tmdate": 1605276749236, "tddate": null, "forum": "J40FkbdldTX", "replyto": "vJzEWZvDffO", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R4", "comment": "Thank you for investing your time reviewing our work\nand pointing out many of our paper's shortcomings, which is a great help in improving it.\n\n\n- However, this respectable responsibility also comes with a higher standard to evaluate\nworks attempting to fulfill it. My major concern with this work is that the manuscript\nis not organized well. Although authors provide substantial details on their empirical study,\nthey did not form a coherent logic flow to present these empirical findings, which makes\nthis work more like a technical report than an academic paper.\n    - The lack of logic flow is a valid concern,\n    given that we explore multiple recent variations to SPOS [A1] and some regularization techniques,\n    rather than focusing on a single detail or method.\n    As we have added a small overview that includes an outlook to the results on behalf of another reviewer, we hope that\n    this particular concern has also been addressed.\n    If not, please provide us with feedback on how to improve the readability of this work.\n\n\n- Readers may find these phenomena interesting but may not get interesting insights after\nreading this paper. Hence the technical contribution, especially on novelty, seems quite limited,\neven if there may be some intriguing points in the authors' discovery.\n    - We explore many already-presented methods, which are naturally not novel by any means. However, we also\n        - use SPOS-based methods [A1] to study the ranking correlations in many top-N network groups, which is a novel\n        approach to evaluate the effectiveness of method variations\n        - experiment with disabling the topology sharing only during training time,\n        an attempt that we have not yet seen in literature\n        - find that linear transformers [A2] only work in their proposed environment and are otherwise harmful\n        - find that many commonly used training variations (regularization, learning rate warmup, gradient clipping)\n        actually have little to no benefit for the ranking correlation, which is a very interesting insight\n    - Nonetheless, it appears we failed to properly communicate the points above in our paper.\n    As we have now briefly included them in the introduction section as mentioned in the point above, we hope that\n    the paper is now easier to follow in structure more interesting to read.\n    Please do not hesitate to voice your concerns if you feel that we have not properly addressed your point.\n\n\n- I would recommend the authors to pick some phenomena e.g., masking Zero, masking Skip, etc.,\nas examples to provide more analysis, so as to demonstrate to colleagues in our community\nthat these findings can indeed lead to interesting research topics.\n    - Our main intent was to evaluate training variations that have been proposed across different search spaces\n    and under different conditions, but not actually been compared in effectiveness.\n    We fully agree that new research topics are often more exciting than an experimental study of known techniques,\n    nevertheless we hope that our findings have sparked further interest comparing some of the many recently proposed\n    approaches in a thorough and fair way.\n\n\n- For example, the authors did not give adequate credits to colleagues\nwho pioneered in using Kendall Tau to evaluate NAS training.\nAs far as I know, Sciuto et al., 2019 was one of the earliest works.\n    - You are correct that we should add credit where it is due and\n    add Sciuto et al. and FairNAS [A3] as references, as we are not aware of any other work that predates\n    Sciuto et al. using Kendall Tau for NAS ourselves.\n\n\n- In the third paragraph, when reviewing recent progress,\nthe authors did not distinguish the ranking correlation between NAS searching and retraining\nfrom the correlation between NAS searching results and stand-alone training.\nThe former one was discussed and addressed in Hu et al., 2020.\nAs the community has not fully realized the subtle but crucial difference between\nthese two correlations, I believe a better framing of this work can be more helpful\nto other colleagues, especially those new comers.\n    - We have added the following sentence that clarifies that reusing search network weights is not a given:\n        - *\"Although the standalone network is often trained from scratch, reusing the search network weights can\n        increase both training speed and final accuracy (citing [A4], DSNAS).\"*\n\n\nSincerely,\n\nFirst author\n\n\n[A1] Single Path One-Shot Neural Architecture Search with Uniform Sampling\n\n[A2] SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search\n\n[A3] FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search\n\n[A4] HM-NAS: Efficient Neural Architecture Search via Hierarchical Masking\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "_3FVh40pxP", "original": null, "number": 3, "cdate": 1605271317613, "ddate": null, "tcdate": 1605271317613, "tmdate": 1605272653904, "tddate": null, "forum": "J40FkbdldTX", "replyto": "_tiCpFGx1NR", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R2", "comment": "We thank you for the time invested in providing the comment,\nand in pointing out many interesting papers that investigate similar properties in gradient-based algorithms.\nWe hope that our answers to your concerns can clarify the issues at hand. If not, please feel free to ask.\n\nPlease add a link for reference [1], as we can not see it and are uncertain which paper it refers to.\nIf you feel that we have missed the point of your related questions, we will then clarify the issues.\n\n\n- How is the SuperNet trained?\n    - You can find a description in Section 3.3 and in Appendix A.\n    We have also recently released the code base used for all experiments, which hopefully answers your question.\n    If not, do not hesitate to ask us about further specifics.\n\n\n- Figure2: Yu et al [1] have already explored the correlation\nof ranks of networks sampled from SuperNet and that of stand-alone networks.\nHow is Figure 2 different from that?\n    - You are correct that Figure 2. is not novel by any means,\n    but only intended to give a visual intuition about the experiment method\n    and what Figures 4. to 6. condense to single data points.\n    \n\n- RobustDarts [2] has explored the possibility of how subset of NASBENCH\nsearch spaces behave. FAIRDarts [3] also explored the influence of skip connection\nby running DARTS without skip connection, running random search by limiting skip\nconnection to 2 etc. Figure 4 seems to be inspired by that. While it is interesting,\nthis might be a slight extension to the work done by Yu et al [1]\n    - Both RobustDARTS and FairDARTS are using a gradient-based architecture search method, which has different properties\n    than the SPOS-based [A1] strategies that we experiment with.\n        - While both works are very interesting in the former domain, the design of SPOS permits us to evaluate many more networks\n        than we actually have to train, which is very different from gradient-based methods that converge to exactly one solution.\n        - In our current understanding, a gradient-trained network can not be used in the way that we experiment with:\n        estimating the super-network accuracy for any particular architecture.\n        Instead, gradient-based ranking comparisons are often limited to a small number of trained super-networks and their\n        respective stand-alone results, where hopefully the ranking of super-network accuracies and stand-alone accuracies match.\n        This does still not examine the ranking correlation over an entire search space (or top-N subsets thereof),\n        even with ground-truth stand-alone results we can only estimate how the results of these methods are distributed\n        (i.e. the probability to converge to a top-1% model).\n\n\n- Bender et al [4] postulate that the operations of a SuperNet are subject\nto co-adaptation and recommended techniques such as regularization,\ndrop path etc to alleviate the same. RobustDarts also suggest some\nrecommendations such as L2 regularization, drop path etc although in\nthe context of DARTS. So while Figure 6 demonstrates this empirically, it is not a new finding.\n    - Figure 6. actually highlights the ineffectiveness of many regularization techniques.\n    We can see that, almost independent of the search space and technique, the top-N network predictions in small search networks\n    (for N=10, 25, ...500) behave nearly the same.\n    We have not experimented with path dropout, given that the search space already has a Zero operation and this\n    technique is probably much less useful in more linear cell-designs, where it is equivalent to block-drop [A2].\n    - [A3] experimented with adding drop-path to only Skip operations.\n    Systematically exploring the effectiveness of such an approach,\n    given different operations of which only certain ones can be dropped,\n    may provide interesting insights in future works.\n\n\nSincerely,\n\nFirst author\n\n\n[A1] Single Path One-Shot Neural Architecture Search with Uniform Sampling\n\n[A2] BlockDrop: Dynamic Inference Paths in Residual Networks\n\n[A3] Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "dpGmPX5qGsL", "original": null, "number": 8, "cdate": 1605271677230, "ddate": null, "tcdate": 1605271677230, "tmdate": 1605271677230, "tddate": null, "forum": "J40FkbdldTX", "replyto": "RW2ClSyiPn0", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R3 part 3", "comment": "\n- It is interesting and convincing that many tricks such as learning rate warm-up,\ngradient clipping, and regularization do not work to improve the ranking correlation.\nWe are pleased that the authors provide so many experiments to point out some misleading approaches in NAS.\nI think this paper is very important in the context of AutoML. (+)\n    - This also surprised us, given that regularization is very common addition to super-network training.\n    In search spaces where all operations can benefit from a given technique, they are probably not harmful though\n    (but not necessarily improving the ranking).\n\n\n- The analysis is based on medium-sized and small-sized search space.\nIt would be good to see some analysis of large-sized search space. (-)\n    - Unfortunately, models with 6 normal cells per stage and 64 channels require too much GPU memory under the same\n    training schedule (i.e. batch size and channel count) on our Nvidia 1080 Ti GPUs. \n    However, we can evaluate models with 1 to 5 normal cells per stage if you are interested in such a comparison.\n\n\nSincerely,\n\nFirst author\n\n\n[A1] FairNAS: Rethinking Evaluation Fairness of Weight Sharing Neural Architecture Search\n\n[A2] NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search\n\n[A3] SCARLET-NAS: Bridging the gap between Stability and Scalability in Weight-sharing Neural Architecture Search\n\n[A4] Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "WRMg71HsgW4", "original": null, "number": 7, "cdate": 1605271651904, "ddate": null, "tcdate": 1605271651904, "tmdate": 1605271651904, "tddate": null, "forum": "J40FkbdldTX", "replyto": "RW2ClSyiPn0", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment", "content": {"title": "Rebuttal reply R3 part 2", "comment": "\n- Could the authors present some comments on \"Perhaps the most surprising is\nthe low importance of Average Pooling, even lower than Zero,\nan operation that does absolutely nothing\"? (-)\n    - We have altered the text to the following:\n        - *\"Perhaps the most surprising is the low importance of Average Pooling, even lower than Zero.\n    It appears that all the benefits of Pool are already covered by the $3\\times3$ Convolution,\n    so that using the unnecessary operation now decreases the network accuracy.\"*\n\n\n- The following observation is believed to be crucial in NAS:\n\"The baseline for small networks (top left, red) has the same averaged\nprediction accuracy for the top 10 as for the top 500 networks\".\nThis validates the inefficiency of SPOS in architecture search. (+)\n    - Yes and no. We can also see that the top-500 models have a much better average super-net accuracy than a\n    random pick over the entire search space, and \\tau is between 0.2 and 0.4 in most sub-spaces.\n    It is not trivial to answer whether that is also accurate in e.g. the SPOS or FairNAS ImageNet search spaces,\n    which lack harmful operations such as Zero or Pool, which are also rarely used in the given top-500 networks -\n    this remains an interesting question for future research.\n\n\n- Especially, the phenomenon that masking skip connection reduces the ranking correlations is interesting. \ns is shown in SCARLET-NAS, the supernet training with skip connection is not fair.\nBut in this paper, we can see that skip connection benefits the ranking correlation.\nWe are interested in this opposite opinion.\n    - We believe this to be the result of the different search space. By removing the skip connection, many top-N \n    Bench-201 networks make use of Average Pooling (Figure 3., top right, top plot), likely since the fully sized\n    standalone model training benefits from the non-parametrized paths.\n    In contrast, the super-networks have troubles with ranking these models correctly.\n\n\n- Specifically, it is fascinating to see that\n\"Although the additional transformers seem to stabilize training,\nas seen by the lower standard deviation, they also worsen the \u03c4a problem.\"\nBesides, the phenomenon of \"\\tao may improve within the predictions for the top-N architectures\"\nindicates that the metric for ranking correlations maybe not perfect.\nA more reasonable metric may be desirable. (+-)\n    - We also consider the behavior of linear transformers very interesting, especially that it's not advisable to\n    simply add them regardless of the search space\n    (and the same is true for regularization methods, as pointed out further down).\n    Granted, they do work decently within their proposed domain [A3],\n    it is simply that experiments in other search spaces were not considered.\n\n\n- The following observation is important: \"medium-sized super-networks require additional care.\"\nAs shown by Figure 4, the averaged predicted accuracy of top-N networks in several\nsubsets is lower than that of a random subset of networks.\nThis is consistent with previous work like DNA,\nwhich shows a large search space may be harmful to the architecture rating.\nEven if a medium-sized supernet has a bad architecture rating,\nthe ranking correlation should be worse in a large-sized supernet. (+)\n    - Interestingly, this conflicts with P-DARTS [A4], in which the authors find that a larger super-network bridges\n    the evaluation gap between super-network and standalone model.\n    However, that is quite likely due to the difference of single-path and gradient-based training, as a gradient-trained\n    NAS algorithm already converges into the final architecture during training, which single-path methods do not\n    (except for operation pruning). \n\n\n- The following description is questionable: \"After the architecture search,\nall Linear Transformers can safely be removed, as they do not impact the network capacity\".\nActually, stacking many fully connected layers without non-linear activations\ncould lead to only one fully connected layer.\nIt is an open question of whether optimizing loss(ABCx, y)\nis as difficult as optimizing loss(Dx, y) using stochastic gradient descent. (-)\n    - Good catch. While the capacity remains the same, your point about the network training is valid.\n    We have rephrased the sentence the avoid a misleading interpretation:\n        - *\"All Linear Transformers are removed after the search, resulting in a standalone\n        network with the same capacity.\"*\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "J40FkbdldTX", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper814/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper814/Authors|ICLR.cc/2021/Conference/Paper814/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866913, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Comment"}}}, {"id": "vJzEWZvDffO", "original": null, "number": 3, "cdate": 1603926558891, "ddate": null, "tcdate": 1603926558891, "tmdate": 1605028493188, "tddate": null, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Review", "content": {"title": "An empirical study on the ranking correlation of several NAS mechanisms in the singe-path setup", "review": "This paper introduces an empirical study on the ranking correlation in the singe-path setup. Following the paradigm of NAS-Bench-201,  the authors test the Kendall Tau correlation of networks from NAS training and networks from standalone training. \n\nIn general, I appreciate the authors' effort in bringing more infrastructure to the community of NAS. As a recently emerged community, we do need works like this one, as well as previous ones such as NAS-Bench-101 and NAS-Bench-201, to make the evaluation protocol more scientific. NAS problems are non-trivial as the search space is notoriously large. Colleagues who would like to invest their time and resources in exploring and manifesting this search space to uncover more phenomena are thus worth respect. \n\nHowever, this respectable responsibility also comes with a higher standard to evaluate works attempting to fulfill it. My major concern with this work is that the manuscript is not organized well. Although authors provide substantial details on their empirical study, they did not form a coherent logic flow to present these empirical findings, which makes this work more like a technical report than an academic paper. Readers may find these phenomena interesting but may not get interesting insights after reading this paper. Hence the technical contribution, especially on novelty, seems quite limited, even if there may be some intriguing points in the authors' discovery. I would recommend the authors to pick some phenomena e.g., masking Zero, masking Skip, etc., as examples to provide more analysis, so as to demonstrate to colleagues in our community that these findings can indeed lead to interesting research topics. \n\nSome minors: There are some works missed in the literature review. For example, the authors did not give adequate credits to colleagues who pioneered in using Kendall Tau to evaluate NAS training. As far as I know, Sciuto et al., 2019 was one of the earliest works. In the third paragraph, when reviewing recent progress, the authors did not distinguish the ranking correlation between NAS searching and retraining from the correlation between NAS searching results and stand-alone training. The former one was discussed and addressed in Hu et al., 2020. As the community has not fully realized the subtle but crucial difference between these two correlations, I believe a better framing of this work can be more helpful to other colleagues, especially those new comers. \n\nSciuto et al. 2019, Evaluating the search phase of neural architecture search.\n\nHu et al. 2020, DSNAS: Direct Neural Architecture Search without Parameter Retraining", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134407, "tmdate": 1606915794071, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper814/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Review"}}}, {"id": "npIVnLdSOWr", "original": null, "number": 1, "cdate": 1602570526589, "ddate": null, "tcdate": 1602570526589, "tmdate": 1605024599487, "tddate": null, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "invitation": "ICLR.cc/2021/Conference/Paper814/-/Official_Review", "content": {"title": "Review", "review": "In this paper, the authors proposed several findings on the single-path training strategy. The ranking correlation is the main issue. The experiments are conducted on NASBench201. \n\nIntroduction.\n\n'However, the aforementioned weight sharing xxx', there are a number of efficient multi-objective (Pareto front) NAS methods.\n\n'However, since the single-path', please cite some literature related to the ranking issue.\n\nSummarize the main findings in the introduction section.\n\nMethod.\n\nDefine $\\tau_{\\alpha}$ in math. 'describing the ranking correlation of the average prediction accuracy depending on N', what is the average prediction accuracy for two lists?\n\nExperiments.\n\n4.1 'masking the Zero operation (bottom row) significantly reduces this portion and thus improves the ranking correlation $\\tau$ (KT)'. If removing other operations other than zero, will the $\\tau$ be lower? In my opinion, removing any operation leads to smaller search space, and they all have a higher ranking.\n\n4.2, 4.2 examine some of the training strategies proposed by previous works.\n\nReferences.\n\nPlease make the references clear. Add the venues for all the papers. \n\n\n\n\nThis paper tries to explore the single-path training strategy by studying the search space, the supernet, the linear transformer, the strict uniform sampling, the topology sharing, the LR warmup, the regularization, the clipping. The authors have done lots of experiments to clarify the important reasons for the ranking. However, most of the findings are not new to me. They have been discussed more or less by previous works and discovered by my own experiments. So the contribution is not significant. The paper is mostly clear, some paragraphs and references need to be polished, more related works should be added.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper814/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper814/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Exploring single-path Architecture Search ranking correlations", "authorids": ["~Kevin_Alexander_Laube1", "~Andreas_Zell1"], "authors": ["Kevin Alexander Laube", "Andreas Zell"], "keywords": ["Neural Architecture Search", "AutoML", "Neural Networks"], "abstract": "Recently presented benchmarks for Neural Architecture Search (NAS) provide the results of training thousands of different architectures in a specific search space, thus enabling the fair and rapid comparison of different methods.\nBased on these results, we quantify the ranking correlations of single-path architecture search methods\nin different search space subsets and under several training variations;\nstudying their impact on the expected search results.\nThe experiments support the few-shot approach and Linear Transformers,\nprovide evidence against disabling cell topology sharing during the training phase or using strong regularization in the NAS-Bench-201 search space,\nand show the necessity of further research regarding super-network size and path sampling strategies.", "one-sentence_summary": "An empirical study of how several method variations affect the quality of the architecture ranking prediction.", "pdf": "/pdf/5d9b39e4650306f0a69f439331527e51b4dc8401.pdf", "supplementary_material": "/attachment/735bf091f7d64b75f2be16b590bcac601c1be822.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "laube|exploring_singlepath_architecture_search_ranking_correlations", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=ajZ1hA570j", "_bibtex": "@misc{\nlaube2021exploring,\ntitle={Exploring single-path Architecture Search ranking correlations},\nauthor={Kevin Alexander Laube and Andreas Zell},\nyear={2021},\nurl={https://openreview.net/forum?id=J40FkbdldTX}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "J40FkbdldTX", "replyto": "J40FkbdldTX", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper814/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538134407, "tmdate": 1606915794071, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper814/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper814/-/Official_Review"}}}], "count": 20}