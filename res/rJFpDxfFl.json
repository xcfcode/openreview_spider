{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028558259, "tcdate": 1490028558259, "number": 1, "id": "r1LfdKaoe", "invitation": "ICLR.cc/2017/workshop/-/paper27/acceptance", "forum": "rJFpDxfFl", "replyto": "rJFpDxfFl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Adversarial Gaussian Mixture Auto-Encoder for Clustering", "abstract": "Feature representation for clustering purposes consists in building an explicit or implicit mapping of the input space onto a feature space that is easier to cluster or classify. This paper relies upon an adversarial auto-encoder as a means of building a code space of low dimensionality suitable for clustering. We impose a tunable Gaussian mixture prior over that space allowing for a simultaneous optimization scheme. We arrive at competitive unsupervised classification results on hand-written digits images (MNIST) that is customarily classified within a supervised framework.", "pdf": "/pdf/0a738c1cea6783fea1d479e2f84ae8013db8d273.pdf", "TL;DR": "We simultaneously train both an auto-encoder and a Gaussian mixture model for clustering in an adversarial fashion", "paperhash": "harchaoui|deep_adversarial_gaussian_mixture_autoencoder_for_clustering", "conflicts": ["uw.edu", "parisdescartes.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Warith Harchaoui", "Pierre-Alexandre Mattei", "Charles Bouveyron"], "authorids": ["warith.harchaoui@oscaro.com", "pierre-alexandre.mattei@parisdescartes.fr", "charles.bouveyron@parisdescartes.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028558802, "id": "ICLR.cc/2017/workshop/-/paper27/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJFpDxfFl", "replyto": "rJFpDxfFl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028558802}}}, {"tddate": null, "tmdate": 1489501486688, "tcdate": 1489501486688, "number": 2, "id": "SkP4puSix", "invitation": "ICLR.cc/2017/workshop/-/paper27/official/review", "forum": "rJFpDxfFl", "replyto": "rJFpDxfFl", "signatures": ["ICLR.cc/2017/workshop/paper27/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper27/AnonReviewer1"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes to use adversarial autoencoders with a Gaussian mixture prior on the latent space for unsupervised clustering. The parameters of the Gaussian mixture are also optimized by maximum likelihood. Training the prior by maximum likelihood while training the rest of the model with the AAE objective appears to be the main novelty of the work. \n\nThe use of a tuned prior appears to give significantly better results on unsupervised clustering than the previous work of Makhzani et al on adversarial autoencoders. Training the prior by maximum likelihood (rather than using the adversarial loss) might be helpful in avoiding optimization difficulties, but there are no experiments exploring this.\n\nThe paper shows good results on unsupervised clustering of MNIST digits, but the proposed algorithm is a standard adversarial autoencoder with a learned prior term. The main concern is that the algorithm does not seem sufficiently novel to justify inclusion in the workshop.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Adversarial Gaussian Mixture Auto-Encoder for Clustering", "abstract": "Feature representation for clustering purposes consists in building an explicit or implicit mapping of the input space onto a feature space that is easier to cluster or classify. This paper relies upon an adversarial auto-encoder as a means of building a code space of low dimensionality suitable for clustering. We impose a tunable Gaussian mixture prior over that space allowing for a simultaneous optimization scheme. We arrive at competitive unsupervised classification results on hand-written digits images (MNIST) that is customarily classified within a supervised framework.", "pdf": "/pdf/0a738c1cea6783fea1d479e2f84ae8013db8d273.pdf", "TL;DR": "We simultaneously train both an auto-encoder and a Gaussian mixture model for clustering in an adversarial fashion", "paperhash": "harchaoui|deep_adversarial_gaussian_mixture_autoencoder_for_clustering", "conflicts": ["uw.edu", "parisdescartes.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Warith Harchaoui", "Pierre-Alexandre Mattei", "Charles Bouveyron"], "authorids": ["warith.harchaoui@oscaro.com", "pierre-alexandre.mattei@parisdescartes.fr", "charles.bouveyron@parisdescartes.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489501487304, "id": "ICLR.cc/2017/workshop/-/paper27/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper27/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper27/AnonReviewer2", "ICLR.cc/2017/workshop/paper27/AnonReviewer1"], "reply": {"forum": "rJFpDxfFl", "replyto": "rJFpDxfFl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper27/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489501487304}}}, {"tddate": null, "tmdate": 1488791482279, "tcdate": 1488791482279, "number": 1, "id": "S1zTPj5qx", "invitation": "ICLR.cc/2017/workshop/-/paper27/public/comment", "forum": "rJFpDxfFl", "replyto": "rk_MvSwql", "signatures": ["~Pierre-Alexandre_Mattei1"], "readers": ["everyone"], "writers": ["~Pierre-Alexandre_Mattei1"], "content": {"title": "Answer to AnonReviewer2's review", "comment": "Thank you for your careful reading of our manuscript.\n\nAs you point out, our main contribution is to provide a simple way to automatically choose the prior distribution of an adversarial auto-encoder (AAE) for the purpose of clustering.\n\nHowever, we want to stress that, within the original AAE framework of Makhzani et al., no automatic way of tuning this prior is provided. Indeed, the prior is arbitrarily designed to take into account the problem at hand. For example, it is assumed that, for MNIST, the prior is composed of a standard multivariate Gaussian modeling the style of the digits concatenated with a fixed categorical distribution modeling the classes.\n\nWe believe that prior design is a difficult task and that automatic procedures should be studied. Our work, by simply assuming that the prior belongs to a clustering-oriented parametric family (GMM), is a  step in that direction.\n\nNote that Makhzani et al. also cluster MNIST. They select 16 clusters (which should lead to a smaller clustering error than choosing 10 clusters) and obtain an accuracy of 90.45%, which is substantially smaller than our approach. This suggests the potential superiority of automatic priors over handcrafted priors."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Adversarial Gaussian Mixture Auto-Encoder for Clustering", "abstract": "Feature representation for clustering purposes consists in building an explicit or implicit mapping of the input space onto a feature space that is easier to cluster or classify. This paper relies upon an adversarial auto-encoder as a means of building a code space of low dimensionality suitable for clustering. We impose a tunable Gaussian mixture prior over that space allowing for a simultaneous optimization scheme. We arrive at competitive unsupervised classification results on hand-written digits images (MNIST) that is customarily classified within a supervised framework.", "pdf": "/pdf/0a738c1cea6783fea1d479e2f84ae8013db8d273.pdf", "TL;DR": "We simultaneously train both an auto-encoder and a Gaussian mixture model for clustering in an adversarial fashion", "paperhash": "harchaoui|deep_adversarial_gaussian_mixture_autoencoder_for_clustering", "conflicts": ["uw.edu", "parisdescartes.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Warith Harchaoui", "Pierre-Alexandre Mattei", "Charles Bouveyron"], "authorids": ["warith.harchaoui@oscaro.com", "pierre-alexandre.mattei@parisdescartes.fr", "charles.bouveyron@parisdescartes.fr"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487173570029, "tcdate": 1487173570029, "id": "ICLR.cc/2017/workshop/-/paper27/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper27/reviewers"], "reply": {"forum": "rJFpDxfFl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487173570029}}}, {"tddate": null, "tmdate": 1488570127858, "tcdate": 1488570127858, "number": 1, "id": "rk_MvSwql", "invitation": "ICLR.cc/2017/workshop/-/paper27/official/review", "forum": "rJFpDxfFl", "replyto": "rJFpDxfFl", "signatures": ["ICLR.cc/2017/workshop/paper27/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper27/AnonReviewer2"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "CONTRIBUTIONS\n\nThis paper applies adversarial auto-encoders to unsupervised classification of MNIST digits.\n\nThe proposed model, named DAC for \"deep adversarial clustering\", optimizes a loss with three components:\n- A reconstruction term encourages reconstructions to be close to the original.\n- A log-likelihood term on the latent representation's prior distribution forces the prior to track the encoder's distribution.\n- An adversarial loss forces the encoder to have a distribution close to the prior distribution.\n\nThe paper claims state-of-the-art on the unsupervised MNIST clustering task.\n\nNOVELTY, CLARITY, SIGNIFICANCE, QUALITY\n\nThe proposed method is well-explained.\n\nIt seems to me that the proposed loss is almost identical to an adversarial auto-encoder loss where the prior would be learned, the difference being that here the prior is learned through maximum likelihood as opposed to using the adversarial term to guide the prior. Is there any reason to choose one over the other?\n\nIn my opinion, this work lacks the novelty sought by this workshop: unless something escapes me, the main point of the paper is that a very straightforward adaptation of adversarial auto-encoders performs well at clustering MNIST digits.\n\nPROS (+), CONS (-)\n\n+ Proposed method is well-explained\n- Paper offers little novelty", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Deep Adversarial Gaussian Mixture Auto-Encoder for Clustering", "abstract": "Feature representation for clustering purposes consists in building an explicit or implicit mapping of the input space onto a feature space that is easier to cluster or classify. This paper relies upon an adversarial auto-encoder as a means of building a code space of low dimensionality suitable for clustering. We impose a tunable Gaussian mixture prior over that space allowing for a simultaneous optimization scheme. We arrive at competitive unsupervised classification results on hand-written digits images (MNIST) that is customarily classified within a supervised framework.", "pdf": "/pdf/0a738c1cea6783fea1d479e2f84ae8013db8d273.pdf", "TL;DR": "We simultaneously train both an auto-encoder and a Gaussian mixture model for clustering in an adversarial fashion", "paperhash": "harchaoui|deep_adversarial_gaussian_mixture_autoencoder_for_clustering", "conflicts": ["uw.edu", "parisdescartes.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Warith Harchaoui", "Pierre-Alexandre Mattei", "Charles Bouveyron"], "authorids": ["warith.harchaoui@oscaro.com", "pierre-alexandre.mattei@parisdescartes.fr", "charles.bouveyron@parisdescartes.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489501487304, "id": "ICLR.cc/2017/workshop/-/paper27/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper27/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper27/AnonReviewer2", "ICLR.cc/2017/workshop/paper27/AnonReviewer1"], "reply": {"forum": "rJFpDxfFl", "replyto": "rJFpDxfFl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper27/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper27/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489501487304}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487174583803, "tcdate": 1487173569417, "number": 27, "id": "rJFpDxfFl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJFpDxfFl", "signatures": ["~Pierre-Alexandre_Mattei1"], "readers": ["everyone"], "content": {"title": "Deep Adversarial Gaussian Mixture Auto-Encoder for Clustering", "abstract": "Feature representation for clustering purposes consists in building an explicit or implicit mapping of the input space onto a feature space that is easier to cluster or classify. This paper relies upon an adversarial auto-encoder as a means of building a code space of low dimensionality suitable for clustering. We impose a tunable Gaussian mixture prior over that space allowing for a simultaneous optimization scheme. We arrive at competitive unsupervised classification results on hand-written digits images (MNIST) that is customarily classified within a supervised framework.", "pdf": "/pdf/0a738c1cea6783fea1d479e2f84ae8013db8d273.pdf", "TL;DR": "We simultaneously train both an auto-encoder and a Gaussian mixture model for clustering in an adversarial fashion", "paperhash": "harchaoui|deep_adversarial_gaussian_mixture_autoencoder_for_clustering", "conflicts": ["uw.edu", "parisdescartes.fr"], "keywords": ["Deep learning", "Unsupervised Learning"], "authors": ["Warith Harchaoui", "Pierre-Alexandre Mattei", "Charles Bouveyron"], "authorids": ["warith.harchaoui@oscaro.com", "pierre-alexandre.mattei@parisdescartes.fr", "charles.bouveyron@parisdescartes.fr"]}, "writers": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}