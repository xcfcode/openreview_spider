{"notes": [{"id": "SJeQEp4YDH", "original": "HklsWtMvDH", "number": 480, "cdate": 1569439019038, "ddate": null, "tcdate": 1569439019038, "tmdate": 1588996849361, "tddate": null, "forum": "SJeQEp4YDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "ryloEnTVKS", "original": null, "number": 1, "cdate": 1571245106653, "ddate": null, "tcdate": 1571245106653, "tmdate": 1576860961223, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment", "content": {"title": "Source code available", "comment": "Dear reviewers and readers,\n\nOur source code is available at https://github.com/xuwangyin . Please leave a comment if you have any problem with the code. \n\nThanks!"}, "signatures": ["ICLR.cc/2020/Conference/Paper480/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeQEp4YDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper480/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper480/Authors|ICLR.cc/2020/Conference/Paper480/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170863, "tmdate": 1576860532705, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment"}}}, {"id": "CyBEqwmqV", "original": null, "number": 1, "cdate": 1576798697745, "ddate": null, "tcdate": 1576798697745, "tmdate": 1576800938022, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This work addresses the problem of detecting an adversarial attack. This is a challenging problem as the detection mechanism itself is also vulnerable to attack. The paper proposes asymmetrical adversarial training as a robust solution. This approach partitions the feature space according to the output of the robust classifier and trains an adversarial example detector per partition. The paper demonstrates improvements over state-of-the-art detection techniques. \n\nAll three reviewers recommend acceptance of this work. Some positive points include the paper being well-written with strong experimental evidence. One potential difficulty with the proposed approach is the additional computational cost associated with a per class adversarial attack detector. The authors have responded to this concern by claiming that the straightforward version of their approach is K times slower (10 in the case of 10 classes), but their integrated version is 2x slower as they only run the detector associated with the example-specific class prediction. We encourage the authors to include a discussion on computational cost in the final version. In addition, there was a community comment about black-box testing which will be of relevance to many in the community. The authors have already provided additional experiments to address this question as well as code to reproduce the new experiment. \n\nOverall, the paper addresses an important problem with a two-step solution of training a robust model and detecting potentially perturbed samples per class. This is a novel solution with comprehensive experiments and therefore recommend acceptance. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705578, "tmdate": 1576800253394, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper480/-/Decision"}}}, {"id": "H1lI1gUwiB", "original": null, "number": 6, "cdate": 1573507038468, "ddate": null, "tcdate": 1573507038468, "tmdate": 1573854768612, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment", "content": {"title": "Overview of Revisions", "comment": "Dear Reviewers,\n\nThank you again for your thought-provoking comments and your valuable time. We have responded to all comments in detail and uploaded a revision. Major changes of the manuscript includes:\n1) Revised the 3rd contribution (at the end of Introduction section) to incorporate a brief discussion about important properties and relative merits of the proposed generative modeling objective.\n2) Revised Section 3.2 to reflect a reformulation of the generative classification/detection approach under the energy-based learning framework. (Experimental results won\u2019t' be affected.)\n3) Added Appendix A. Provide an intuitive explanation of asymmetrical adversarial training from the perspective of energy-based learning.  Demonstrate AAT's capability to do (implicit) density estimation with experimental results on 1D and 2D benchmarking datasets. Update CIFAR10 and ImageNet image generation results to backup our claim that our generative modeling performance rivals that of state-of-the-art GANs.\n4) Added appendix E (responding to R1). Use Gaussian noise attack to further clarify how our generative classification model is more interpretable in the sense that it offers a probabilistic view of the problem.\n5) Added appendix F to provide a discussion about the computational issue (responding to R2).\n\nIn addition to the above, we would like to emphasize on the generative modeling capability of our method and that it allows for (implicit) density estimation. To that end, in addition to the high-dimensional experiments in our manuscript, we are providing experimental results on 2D benchmarking datasets. We emphasize that our novel generative modeling technique achieves state-of-the-art image generation performance (comparable to that of GANs), and to our knowledge, it's the first [1] generative modeling technique that also provides performance guarantee for classifying and detecting (norm-constrained) adversarial examples. \n\nWe would appreciate any further feedback and discussion regarding our responses. We certainly welcome any additional requests to help enhancing the quality of our paper.\n\nRegards\n\n[1] Fetaya, Ethan, J\u00f6rn-Henrik Jacobsen, and Richard Zemel. \"Conditional Generative Models are not Robust.\" arXiv preprint arXiv:1906.01171 (2019)."}, "signatures": ["ICLR.cc/2020/Conference/Paper480/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeQEp4YDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper480/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper480/Authors|ICLR.cc/2020/Conference/Paper480/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170863, "tmdate": 1576860532705, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment"}}}, {"id": "r1emkzy8iB", "original": null, "number": 3, "cdate": 1573413338899, "ddate": null, "tcdate": 1573413338899, "tmdate": 1573847499036, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "rklkefNcFr", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment", "content": {"title": "Improved interpretability", "comment": "We thank the reviewer for a positive evaluation of our contributions and for raising a thought-provoking question. We agree that we didn't provide a proper discussion about the interpretability of our method. Here are our thoughts and some additional experimental results.\n\nWe start with a discussion about how these two approaches - our generative classification approach, and the discriminative robust classification approach - determine the posterior probabilities of inputs.\n\nIn our approach, the posterior is computed in two steps. In the first step, we train our base detectors, which is the process of solving the inference problem of determining the joint probability $p(x,k)$. Unfortunately, the detectors don't define explicit density functions. We could, however, use the logit output of the detector to define an energy function [1] $E_{\\theta_k}(x) = -z(h_k(x))$, and then use the Gibbs distribution to obtain the joint probability $p(x, k) = \\frac{\\exp(-E_{\\theta_k}(x))}{Z_\\Theta}=\\frac{\\exp(z(h_k(x)))}{Z_\\Theta}$, where  $Z_\\Theta$ is a normalizing constant known as the partition function, and could be computed in our case as $Z_\\Theta = \\sum_k\\int\\exp(-E_{\\theta_k}(x))dx$. (We note that a concurrent ICLR2020 submission https://openreview.net/forum?id=Hkxzx0NtDB uses a similar formulation.) In the second step we use Bayes rule to compute the posterior probability $p(k|x) = p(x,k)/p(x) = \\frac{\\exp(z(h_k(x)))}{\\sum_{j=1}^K \\exp(z(h_j(x)))}$. Although here $Z_\\Theta$ is intractable, it doesn't appear in the posterior, so knowing the logit outputs of the base detectors will suffice. (The readers may have noticed that the above formulation is different from the one presented in our manuscript. It's because after we later realized that we are not able to do explicit density estimation, we resorted to the energy-based learning framework [1], and worked out an energy-based solution. Note that our experimental results are not affected by the reformulation. In our updated manuscript we provide more data and discussions to justify the energy-based formulation.)\n\nIn the discriminative classification approach, the posterior probability is computed from the logit outputs of the classifier using the softmax function $p(k|x) = \\frac{\\exp(z(f(x))_k)}{\\sum_{j=1}^K \\exp(z(f(x))_j)}$. \n\nCoincidentally, the formulas for computing the posterior probabilities take the same form. But in our approach, the exponential of the logit output of a detector (i.e., $\\exp(z(h_k(x)))$) has a clear probabilistic interpretation: it's the *unnormalized joint probability* of the input and the corresponding class category. We now demonstrate that this probabilistic interpretation is consistent with our visual perception. \n\nWe start from a Gaussian noise image, and gradually perturb it to cause higher and higher logit outputs. This is implemented by targeted PGD attack against logit outputs of corresponding models. The resulting images are in the last figure of this README file https://github.com/nhLKeO/AAT-CIFAR10/blob/master/README.md . It's clear that for our model, the logit output increase direction is the semantic changing direction; while for the discriminative robust model, the perturbed image computed by increasing logit outputs are not as clearly interpretable. In particular, the perturbed images that cause high logit outputs of the robust classifiers are not recognizable.\n\nWe hope that our explanation could convince you that our approach improves the interpretability by providing a probabilistic view of the decision process of the classification problem. \n\n[1] LeCun, Yann, et al. \"A tutorial on energy-based learning.\" *Predicting structured data* 1.0 (2006)."}, "signatures": ["ICLR.cc/2020/Conference/Paper480/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeQEp4YDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper480/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper480/Authors|ICLR.cc/2020/Conference/Paper480/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170863, "tmdate": 1576860532705, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment"}}}, {"id": "rJeXAGbLoH", "original": null, "number": 4, "cdate": 1573421770596, "ddate": null, "tcdate": 1573421770596, "tmdate": 1573846999470, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "B1xdR_S6YH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #2", "comment": "Thank you very much for your insightful review! We provide our answers below.\n\n>While the idea to partition into subspaces and learn a different detection for each of them is novel, it involves \n>training multiple detectors one by one that can be computationally expensive. The loss function for different \n>attacking scenarios uses the outputs from all the detectors, which can also be expensive, especially when there \n>are a lot of classes.\n\nWe agree with the reviewer that having a detector per class could could incur high computational cost, especially for problems with a large number of classes. Your thoughtful question has motivated us to think deeper about the computational issue and do additional benchmarking. Below we summarize our points.  \n\nIn terms of memory requirements, if we assume the softmax classifier and the detectors use the same architecture (i.e., only defer in the final layer) then the detector-based generative classifier is approximately K times more expensive than the  K-class softmax classifier. This also means that the computational graph of the generative classifier is K times larger than the softmax classifier. Indeed, in the CIFAR10 task, on our Quadro M6000 24GB GPU (TensorFlow 1.13.1), the inference speed of the generative classifier is roughly ten times slower than the softmax classifier.  However, we would like to point out that for our \"integrated detection\" approach, inference only requires two times computation (first obtain the ordinary classifier's prediction, then run the corresponding detector).\n\nWe next benchmark the training speed of these two types of classifiers.\n\nThe generative classifier has K logit outputs, with each one defined by the logit output of a detector. Same with the softmax classifier, except that the K outputs share the parameters in the convolutional part. Now consider ordinary adversarial training on the softmax classifier and asymmetrical adversarial training on the generative classifier. To train the softmax classifier, we use batches of N samples. For the generative classifier, we train each detector with batches of 2*M samples (M positive samples + M negative samples). At each iteration, we need to respectively compute N and M*K adversarial examples for these two classifiers. Now we test the speed of the following two scenarios:  1) compute the gradient w.r.t. to N samples on a single computational graph, and 2) compute the gradient w.r.t to M*K samples on K computational graphs, with each graph working on M samples. We assume in scenario 2 that all the computational graphs are loaded to GPUs, and their computations are in parallel.\n\nIn our CIFAR10 experiment, we used batches consisting of 30 positive samples and 30 negative samples to train each ResNet50 based detector. In Madry et al., the softmax classifier was trained with batches of 128 samples. In this case, K=10, M=30, and N=128.  On our GPU, scenario 1 took 683 ms \u00b1 6.76 ms per loop, while scenario 2 took 1.85 s \u00b1 42.7 ms per loop. So in this case, we could expect asymmetrical adversarial training to be about 2.7 times slower than ordinary adversarial training, if not considering parameter gradient computation. (If we choose to use a large batch size for the sake of more stable training, the computational cost will increase accordingly.)\n\nAnother factor to take into consideration is that we extensively use adversarial finetuning (training starts from a naturally trained model). We are not sure why this practice is not exercised in ordinary adversarial training, but we found it to be an effective trick for speeding up training (see Appendix \"Effects of perturbation limit\").\n\nA straightforward solution to the computational issue would be to let detectors share their convolutional kernels. If we allow our detectors to share the same trunk and allocate different heads for the detectors, we will enhance the computational cost of the generative classifier.  The architecture of the generative classifier could also be made to be precisely the same as the softmax classifier (imagine replacing cross-entropy loss with binary cross-entropy loss).  We are currently looking into this new direction and will update our paper if we find this to be a viable solution.\n\nNotebook to produce these results: https://github.com/nhLKeO/AAT-CIFAR10/blob/master/speed_test.ipynb\n\n>To deal with extremely unbalanced data sets when training the detector, the solutions used in the paper \n>resamples to balanced the positive and negative class data. This would mean throwing off most of the data, I \n>would see how it affects the training.\n\nIt's true that in each batch we drop some negative samples (more specifically, we only utilize the first n negative sample where n is the number of positive samples in the current batch), but at the same time we randomly shuffle the whole training set at the beginning of each epoch. So the overall effect is that each negative sample has equal probability of being dropped (or visited)."}, "signatures": ["ICLR.cc/2020/Conference/Paper480/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeQEp4YDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper480/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper480/Authors|ICLR.cc/2020/Conference/Paper480/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170863, "tmdate": 1576860532705, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment"}}}, {"id": "S1eVLOH8jB", "original": null, "number": 5, "cdate": 1573439563540, "ddate": null, "tcdate": 1573439563540, "tmdate": 1573846743727, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SklzDtn-9H", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #3", "comment": "Thank you for your helpful feedback, and we certainly appreciate your time evaluating our work! Below we provide our specific answers to your comments.\n\n1) Since $D^{'^{f}}_k \\subset D^f_k$, would the model minimize w.r.t. both the loss of $L(h(x, \\theta), 0)$ and $L(h(x, \\theta), 1)$? Would this cause unstable training?\n\nIn practice we use the objective in Equation (5), which does not involve $f$ to train detectors. In other words, we treat the classifier $f$ to be fixed, and therefore we only train the detectors $h_k$s. Regarding the overlap of sets, we first clarify that $D^{'^{f}}_k$ is the set of adversarially attacked samples from other classes that fool the classifier network $f$ to think the samples belong to class $k$. And, $D^f_k$ is the set of un-attacked/clean samples that the classifier, $f$, identifies (correctly or incorrectly) as class $k$. In theory, it could happen that the set $\\mathcal{D'}_{\\backslash k}=\\{x_i+\\delta_i: x_i\\in\\mathcal{D}_{\\backslash k}\\}$ and $\\mathcal{D}_k$ have common elements. As an example, consider two data points  $x_i\\in \\mathcal{D'}_{\\backslash k}$ and  $x_j\\in\\mathcal{D}_k$, and consider the unlikely case that $x_i$ would be transformed to $x_j$ using the perturbation $\\delta=x_j-x_i$, if $\\delta\\in \\mathcal{S}$. Now the binary cross-entropy loss for these two points,i.e., $x_j$ and $x_i+\\delta$, are $\\text{BCE}(h(x_i+\\delta), 0) + \\text{BCE}(h(x_j), 1) =  - \\log(1-h(x_j))-\\log(h(x_j))$ (assume $h(x_j)$ to be the sigmoid output), where we used the fact that $x_i+\\delta=x_j$. This is in fact a convex function of $h(x_j)$ and has the global minimum at point $h(x_j)=0.5$. But in practice this is highly unlikely to happen, especially when we are using numerical optimization and have constraints on the perturbation limit.\n\n2) Maybe I missed it, but it seems that the objective function in Eq. (5) is based on the adversarial detector. How could the classification performance of classifier $f$ be guaranteed in training?\n\nFollowing our previous comment, in practice, we use the objective in Eq. (5), which treats the classifier $f$ to be unchanging. To further elaborate on this, we first train the classifier $f$ and then partition the training data using $f$'s outputs. Now assuming that $f$ is complex enough to fit the training set reliably, the partition based on $f$ would be equivalent to (or very close to)  the partition based on the ground-truth labels. Therefore, we can omit $f$ from objective (4) and use the alternative objective (5) to train our detectors. In this case, the training of detectors is entirely independent of the classifier, and the classification performance of the classifier won't be affected.\n\n3) What does the cross mark mean in Figures 2(b) and 4(b)?\n\nThe cross marks indicate the performances of the \"robust classifier\" trained with standard adversarial training [1] (in Section 4.3 we provide a discussion about the performance metric). In Fig. 2(b) the cross mark indicates the performances of a robust classifier under eps=0.3 and eps=0.4 perturbations on the MNIST dataset, and in Fig. 4(b) the performances under eps=8 and eps=12 perturbations on the CIFAR10 dataset. For both classifiers, we used the pre-trained models provided by Madry et al. [1] to compute the performances.\n\n[1] Madry, Aleksander, et al. \"Towards deep learning models resistant to adversarial attacks.\" arXiv preprint arXiv:1706.06083 (2017)."}, "signatures": ["ICLR.cc/2020/Conference/Paper480/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeQEp4YDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper480/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper480/Authors|ICLR.cc/2020/Conference/Paper480/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170863, "tmdate": 1576860532705, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment"}}}, {"id": "rklkefNcFr", "original": null, "number": 1, "cdate": 1571598823315, "ddate": null, "tcdate": 1571598823315, "tmdate": 1572972590155, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary:\nThis paper studies the adversarial detection problem within the robust optimization framework. They propose an adversarial detection and a generative modeling technique called asymmetrical adversarial training (AAT). With one detector for each class discriminating natural data from adversarially perturbed data, AAT can learn class-conditional distributions, which further result in generative detection/classification methods with competitive performance. Experimental results are provided on MNIST, CIFAR10 and Restricted ImageNet, compared with CW method as baseline.\n\nThe paper is well written with detailed experimental results. I'd suggest accepting the paper.\n\nTo my understanding, the objective function of AAT is similar to GAN's, while there is a detector for each class discriminating natural data from adversarially perturbed data instead of generated data. They incorporate the attack into the training objective with three attacking scenarios: classifier attack, detectors attack, and combined attack. They also introduce integrated classification of the classifier and detectors with the reject option. Further, they demonstrate ATT promotes the learning of class-conditional distributions and leads to generative classifiers. They claim in addition to more robust classification, ATT also gives rise to improved interpretability, which I'm not convinced of with given experimental results."}, "signatures": ["ICLR.cc/2020/Conference/Paper480/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575227308166, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper480/Reviewers"], "noninvitees": [], "tcdate": 1570237751519, "tmdate": 1575227308177, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Review"}}}, {"id": "B1xdR_S6YH", "original": null, "number": 2, "cdate": 1571801296497, "ddate": null, "tcdate": 1571801296497, "tmdate": 1572972590120, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Review: The paper addresses the adversarial example detection problem. The framework proposed in the paper divides the input space into subspaces based on a classifier\u2019s output and trains detectors on these subspaces to classify a natural sample (classified as that class) from an adversarial one fooling the network. The goal is to use a robust optimization approach to enable detection methods to withstand adaptive/dynamic attacks. Hence, an asymmetrical adversarial training (AAT) regime is employed which presents solving a min-max problem. AAT supports the detectors to learn class conditional distributions, motivating generative detection/classification approaches. There are three different attacking scenarios and evaluation shows that the combined attack turns out to be most effective (as it fools both the classifier and detectors) against integrated detection. The paper also demonstrates empirical improvements over state of the art detection techniques with higher L2 distortion of perturbed samples.\n\nPros:\n- With the vulnerabilities associated with neural networks, the motivation behind building defense mechanisms against adversarial attacks has been well-justified.\n- Most of the evaluation metrics look appropriate and well-defined.\n- It was interesting to observe the perturbed samples produced by attacking generative classifier. While they exhibited visible features of the target class, these perturbations had to be different on a sematic level to be distinguished from the natural samples. \n\n\nCons:\n- While the idea to partition into subspaces and learn a different detection for each of them is novel, it involves training multiple detectors one by one that can be computationally expensive. The loss function for different attacking scenarios uses the outputs from all the detectors, which can also be expensive, especially when there are a lot of classes.\n- To deal with extremely unbalanced data sets when training the detector, the solutions used in the paper resamples to balanced the positive and negative class data. This would mean throwing off most of the data, I would see how it affects the training. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper480/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575227308166, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper480/Reviewers"], "noninvitees": [], "tcdate": 1570237751519, "tmdate": 1575227308177, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Review"}}}, {"id": "SklzDtn-9H", "original": null, "number": 3, "cdate": 1572092249905, "ddate": null, "tcdate": 1572092249905, "tmdate": 1572972590077, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a method for adaptive adversarial example detection. The authors propose to construct the adversarial subspace detector based on Asymmetrical Adversarial Training (AAT). The proposed model is composed of both classifier and adversarial detector, where the classifier makes the classification prediction and the adversarial detector evaluate if the input sample is natural of adversarial. The goal of the objective function is to minimize the adversarial detector error given large enough perturbation budget.\n\nThe authors provide extensive experimental results showing the promising performance of the model in detecting various types of adversarial attack. I have several concerns regarding the model and experiments:\n\n1) Since D^{'^{f}}_k \\subset D^f_k, would the model minimize w.r.t. both the loss of L(h(x, \\theta), 0) and L(h(x, \\theta), 1)? Would this cause unstable training?\n\n2) Maybe I missed it, but it seems that the objective function in Eq. (5) is based on the adversarial detector. How could the classification performance of classifier f be guaranteed in training?\n\n3) What does the cross mark mean in Fig. 2(b) and 4(b)?"}, "signatures": ["ICLR.cc/2020/Conference/Paper480/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575227308166, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper480/Reviewers"], "noninvitees": [], "tcdate": 1570237751519, "tmdate": 1575227308177, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Review"}}}, {"id": "S1lmGGSatS", "original": null, "number": 2, "cdate": 1571799563154, "ddate": null, "tcdate": 1571799563154, "tmdate": 1571800721190, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "S1eCbL0IKr", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment", "content": {"comment": "Hi Anthony, \n\nThanks for the suggestion. Black-box testing is indeed missing in our experiments. Here we report the black-box test results using the CIFAR10 class 0 base detector. We used PGD attack and Nattack to optimize (maximize) the logit output of the base detector. We used 100 test samples of classes from 1 to 9 to run the test, and the average logit outputs achieved by PGD and Nattack are respectively -17.53 and -17.73 (higher is better). In particular, Nattack outperforms PGD in 11 out of the 100 test cases. So in most cases PGD performs better, which indicates that the proposed objective didn't cause obfuscated gradients. Speed-wise, the 1000-iteration based Nattack runs about 150 times slower than 100-iteration based PGD attack. The script for reproducing these results is at  https://github.com/nhLKeO/AAT-CIFAR10/blob/master/eval_base_detector_Nattack.py . P.S. we  updated some image generation results on https://github.com/nhLKeO/AAT-CIFAR10 . ", "title": "Nattack result"}, "signatures": ["ICLR.cc/2020/Conference/Paper480/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeQEp4YDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper480/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper480/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper480/Authors|ICLR.cc/2020/Conference/Paper480/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170863, "tmdate": 1576860532705, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper480/-/Official_Comment"}}}, {"id": "S1eCbL0IKr", "original": null, "number": 1, "cdate": 1571378693911, "ddate": null, "tcdate": 1571378693911, "tmdate": 1571378753576, "tddate": null, "forum": "SJeQEp4YDH", "replyto": "SJeQEp4YDH", "invitation": "ICLR.cc/2020/Conference/Paper480/-/Public_Comment", "content": {"comment": " Hi, thanks for this contribution.\n\nIt seems that the evaluation on the black-box attack is missing in this paper, which is important, because that if a model causes obfuscated gradients, black-box attacks perform better than white-box attacks[1]. \n\n[1] Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples. ICML 2018", "title": "About the evaluation on the black-box attack"}, "signatures": ["~Anthony_Wittmer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Anthony_Wittmer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["xy4cm@virginia.edu", "skolouri@hrl.com", "gustavo@virginia.edu"], "title": "GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification", "authors": ["Xuwang Yin", "Soheil Kolouri", "Gustavo K Rohde"], "pdf": "/pdf/7dea0414ad25ca0d21744619b376b9d45dd8743e.pdf", "TL;DR": "We propose an objective that could be used for training adversarial example detection and robust classification systems.", "abstract": "The vulnerabilities of deep neural networks against adversarial examples have become a significant concern for deploying these models in sensitive domains. Devising a definitive defense against such attacks is proven to be challenging, and the methods relying on detecting adversarial samples are only valid when the attacker is oblivious to the detection mechanism. In this paper we present an adversarial example detection method that provides performance guarantee to norm constrained adversaries. The method is based on the idea of training adversarial robust subspace detectors using generative adversarial training (GAT). The novel GAT objective presents a saddle point problem similar to that of GANs; it has the same convergence property, and consequently supports the learning of class conditional distributions. We demonstrate that the saddle point problem could be reasonably solved by PGD attack, and further use the learned class conditional generative models to define generative detection/classification models that are both robust and more interpretable. We provide comprehensive evaluations of the above methods, and demonstrate their competitive performances and compelling properties on adversarial detection and robust classification problems.", "keywords": ["adversarial example detection", "adversarial examples classification", "robust optimization", "ML security", "generative modeling", "generative classification"], "paperhash": "yin|gat_generative_adversarial_training_for_adversarial_example_detection_and_robust_classification", "code": "https://github.com/xuwangyin/GAT-Generative-Adversarial-Training", "_bibtex": "@inproceedings{\nYin2020GAT:,\ntitle={GAT: Generative Adversarial Training for Adversarial Example Detection and Robust Classification},\nauthor={Xuwang Yin and Soheil Kolouri and Gustavo K Rohde},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJeQEp4YDH}\n}", "original_pdf": "/attachment/3b0fcc4e7e3717de38dd2d5d87f3698f2f0a7135.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJeQEp4YDH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504208514, "tmdate": 1576860566365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper480/Authors", "ICLR.cc/2020/Conference/Paper480/Reviewers", "ICLR.cc/2020/Conference/Paper480/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper480/-/Public_Comment"}}}], "count": 12}