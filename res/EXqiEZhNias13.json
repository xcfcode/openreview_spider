{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1393501440000, "tcdate": 1393501440000, "number": 1, "id": "ytLTwU0sTRwkv", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "EXqiEZhNias13", "replyto": "n7hyv-DWypM1K", "signatures": ["Michael Tetelman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "To correctly consider r going to infinity you have to look at the contribution to integral from the point w=0. For example for L2 case P0(w=0|r) ->inf when r->inf, nevertheless Integral_by_w[P0(w|r)]=1, because single point w=0 does not contribute to the integral. \r\n\r\nSame is true for the Bayesian integral with training data.  For the case of L2 regularization, simply rescale w->w/sqrt(r), then P0(w|r)dw->P0(v|1)dv will not depend on r at all and the whole likelihood cannot be arbitrary large when r goes to infinity. - We can always rescale w because w is an integration variable."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393474440000, "tcdate": 1393474440000, "number": 1, "id": "n7hyv-DWypM1K", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "EXqiEZhNias13", "replyto": "7OTyOhG2uU7l6", "signatures": ["Olivier Delalleau"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "It seems to me that in your example arbitrarily large values of your criterion (in #3) could be obtained with w=0 and r going to infinity. But I agree this is a minor point and it's not worth spending more time discussing it..."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393370460000, "tcdate": 1393370460000, "number": 1, "id": "7OTyOhG2uU7l6", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "EXqiEZhNias13", "replyto": "Auc96IjxzaAB5", "signatures": ["Michael Tetelman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Please see the latest version of the paper, where I added some clarifications about finding the optimal regularization and the regularized solution w*. This is not a major result of the paper and I will publish detailed analysis of the regularization method in a separate publication somewhere else.\r\n\r\nHere I am going to give a few points on the subject.\r\n1. P(y|x,w) is a model definition. For example, linear regression, logistic regression or some other probability distribution that depends on data (y,x) and parameters w.\r\n2. P0(w|r) is a prior probability distribution of parameters that depends also on regularization parameters r. It does not depend on data.\r\nFor example, for L2 regularization P0(w|r)=exp(-r*R(w))/NormFactor(r),\r\nwhere R(w)=sum(w^2) and NormFactor(r)=Integral_by_w[exp(-r*R(w))].\r\n3. Log-likelihood depends on both w and r, it is given by following expression \r\nL(w,r) = sum_by_training_set[ln(P(y|x,w))]-r*R(w)-ln(NormFactor(r));\r\nThen, MLA solution is obtained by maximizing L(w,r) by w with fixed r. As stated in the paper, the optimal regularization r and the corresponding solution w* is found by maximizing L(w,r) w.r.t. both w and r.\r\n4. The solution found in the approach above in (3) will avoid over-fitting. Also, it is possible to show that this method is equivalent to finding the optimal regularization by using the cross-validation, which is maximizing a likelihood of validation data set by r on solutions w(r) obtained by maximizing the likelihood in (3) above by w at a given r."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1393351380000, "tcdate": 1393351380000, "number": 1, "id": "Auc96IjxzaAB5", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "EXqiEZhNias13", "replyto": "33wYiE_DWa1DA", "signatures": ["Olivier Delalleau"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thanks for the clarifications. Just one comment on point 7: maybe I'm reading it wrong (and the paper should be clearer), but my understanding of eq.1 is that P(w) is the prior on parameters. The regularization parameters define this P(w), and if you optimize eq.1 w.r.t. P, you could very easily overfit (e.g. concentrating all probability mass on the w* that maximizes the likelihood of training data)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392770400000, "tcdate": 1392770400000, "number": 4, "id": "f2gY2I9y3V2Jt", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "EXqiEZhNias13", "replyto": "EXqiEZhNias13", "signatures": ["Michael Tetelman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "The revised version of the paper is available in arxiv: at http://arxiv.org/abs/1312.5398"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392269880000, "tcdate": 1392269880000, "number": 1, "id": "kcguUL7qOvexy", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "EXqiEZhNias13", "replyto": "XrvOXmJEOQXE3", "signatures": ["Michael Tetelman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I would like to thank the reviewer for commenting the paper.\r\n\r\n1. I will publish corrected version of the paper that will address grammatical and other issues asap.\r\n2. I agree that experiments must be done, however, at this point it is not within scope of the paper and will be published somewhere else.\r\n3. I agree that PCA is not the best way to reduce dimension of parameter space and some other method is worth to consider. However, PCA may work for many cases when there is a well defined global maximum of likelihood in parameter space."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392269760000, "tcdate": 1392269760000, "number": 1, "id": "48VR5H0ILApmy", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "EXqiEZhNias13", "replyto": "FmlK4JzwtCFMj", "signatures": ["Michael Tetelman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I would like to thank the reviewer for commenting the paper. My answers are given in same order as reviewers notes.\r\n\r\n1. I believe the method for constructing features that form an algebra could be useful for finding computable efficient representations for a given data, especially for images and other data that have natural symmetries.\r\n\r\n2. I will publish corrected version of the paper that will address grammatical and other issues asap."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392269280000, "tcdate": 1392269280000, "number": 1, "id": "33wYiE_DWa1DA", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "EXqiEZhNias13", "replyto": "gOfXgzVJmQgc2", "signatures": ["Michael Tetelman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I would like to thank the reviewer for the comments. My answers are given in the same order as reviewer\u2019s items.\r\n\r\n1. I agree that not presenting experiments in the paper is a big downside. I will present experiments somewhere else.\r\n2. I agree that convergence requires a special investigation. However, for a limited training data set and with constraints on number of parameters (PCA) I believe it it is reasonable to expect that the iterative process described in paper will converge to polynomials of some finite degree.\r\nThis argument seems to be sufficient for the purpose of the paper. The formal proof will be published somewhere else.\r\n3. Because each iteration consists of solving a series of convex problems that contain all previous solutions, when parameters for products of features are zero, new solutions must have higher likelihoods or be the same as already found solutions (excluding some degenerate cases when multiple solutions have same likelihood).\r\n4. Cost of the algorithms is contained because \r\na. for each iteration most of samples of solutions could be obtained from small subsets of training data - number of reads of training data is expected to have a highest cost;\r\nb. number of iterations is limited because after N iterations features become 2^N degree polynomials, which is reasonable to be limited - please see the convergence argument above.\r\n5. The ideas presented in the paper are not based on any specifics of logistic regression.  \r\nThe only important property is that models are presented by probabilities that are predefined functions of scalar products of parameters and features.\r\nThe variety of model that has this property is much bigger set than a logistic regression case.\r\n6. PCA is selected as a simple and reliable method of dimensionality reduction. Other methods could be used as well. \r\n7. Regularization parameters _could_ be found by maximizing Bayes integral in Eq.1, because it is not just a likelihood - it is a probability of data for a given model and regularization parameters,  that contains a _normalization_ _factor_ from prior probability of parameters (that is important!). That normalization factor cannot be estimated by MLA-type method and must be computed exactly or with high enough accuracy for any possible values of regularization parameters.\r\nThen, maximizing probability of data by regularization parameters will produce regularization solutions that are equivalent to regularization parameters found by maximizing likelihood of a validation set.\r\n8. The iterative procedure in paper does not depend on finding best parameter-solutions. It depends on obtaining samples in parameter space with high likelihood. MLA is a reasonable method to get these samples. There is no need for iterative process to have very good estimates for Bayesian integrals in Eq.1.\r\n9. Eq.5 is an approximation. It is selected due its simplicity for practical computations. It may need a revision.\r\n10. Sampling with replacement is a reasonable approach. The whole idea of iterative approach in the paper is to find a set of features that minimally dependent on selection of training set and representative for any sampled set from available training data.\r\n11. I will clarify the equation for the algebra in a revised version of the paper.\r\nThat will look like the following:\r\n\r\nIterative process will converge when set of super-features: F_a(x), a=1..NumFeatures, with bias super-feature F_0(x) will satisfy algebra equations:\r\n\r\nF_a(x) F_b(x) = sum_d C_a,b,d F_d(x) + F_0(x)G_a,b , where C_a,b,d and G_a,b are constants in regard to x.\r\n\r\n12. Eq.16 shows a property of feature algebra space: any function on feature space that could be represented by power series due to algebra is a linear function of features."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391858760000, "tcdate": 1391858760000, "number": 3, "id": "XrvOXmJEOQXE3", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "EXqiEZhNias13", "replyto": "EXqiEZhNias13", "signatures": ["anonymous reviewer b2f7"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Continuous Learning: Engineering Super Features With Feature Algebras", "review": "> - A brief summary of the paper's contributions, in the context of prior work.\r\n\r\nThe way we represent data -- the features we use -- has proven essential to a wide variety of tasks in machine learning. Often, the trivial features we naturally get from the data perform poorly. One approach is to hand-engineer features, having humans carefully construct features based on their understanding of the data and the task. Another approach, taken in deep learning, is to learn features as part of the optimization process of a multi-layer model. This paper proposes an alternative approach in which 'super-features' are generated by an iterative exploration process.\r\n\r\nAs the reviewer understands it, this iterative process begins with random sigmoidial features. Features are evaluated on several subsets of the dataset and good features are selected. Principal component analysis is performed on the parameters of the good features in order to define a lower-dimensional space of good features. The next iteration proceeds on products of the principal components discovered in the previous step. In the limit, the discovered features form a space with nice algebraic properties.\r\n\r\n> - An assessment of novelty and quality.\r\n\r\nThe reviewer is not an expert but to the best of their knowledge this approach is novel.\r\n\r\nThe reviewer is concerned that this paper may be a bit rushed: it suffers from many grammatical errors, and some parts of the paper are hard to follow. The reviewer thinks that a bit of further revision would greatly benefit this paper.\r\n\r\n> - A list of pros and cons (reasons to accept/reject)\r\n\r\nPros:\r\n* The paper introduces some interesting, novel ideas. In particular, attempting to do dimensionality reduction in parameter space seems like a really interesting strategy. (I'm concerned that PCA may not be a very good way to do this, but the idea is very interesting.)\r\n\r\nCons:\r\n* The paper does not do any experiments to test the efficacy of the proposed approach. Given how radically different the proposed approach is from things that have been previously tried, it seems really difficult to have any significant confidence in it without experimental results. (Not having experimental results for this would seem more appropriate if this were a workshop track submission.)\r\n* The paper suffers from many grammatical issues. These really need to be fixed in a final version of this paper.\r\n* The paper is quite hard to follow at some points.\r\n\r\nTesting this approach on a standard dataset would also provide an opportunity for the author to walk the reader through a concrete application of this approach, in addition to providing experimental validation of it."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391811180000, "tcdate": 1391811180000, "number": 2, "id": "FmlK4JzwtCFMj", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "EXqiEZhNias13", "replyto": "EXqiEZhNias13", "signatures": ["anonymous reviewer 1483"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Continuous Learning: Engineering Super Features With Feature Algebras", "review": "This paper presents an iterative way of defining increasingly complex feature spaces for prediction model selection. \r\nIt is argued that in the limit, this sequence of features results in a feature algebra, meaning that products of features are linear combinations of features within the feature space. \r\n\r\nThe mathematical ideas could be interested if explained in more clarity. \r\nThe nonlinear super features look like an interesting way of reasoning of feature compositions, however, \r\n\r\n* I am not sure about the usefulness of the construction. \r\n\r\n* Version 1 of the paper is quite unpolished. \r\n\r\nMINOR:\r\n\r\nIn eq. (4) `maxarg' should be `argmax'. \r\nIn eq. (9) use `langle' and `\rangle' instead of `<' and `>'. \r\nEq. (6) seems not to be used in Section 3."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391496960000, "tcdate": 1391496960000, "number": 1, "id": "gOfXgzVJmQgc2", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "EXqiEZhNias13", "replyto": "EXqiEZhNias13", "signatures": ["anonymous reviewer 93f6"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Continuous Learning: Engineering Super Features With Feature Algebras", "review": "The idea presented in this paper consists in iteratively building new features\r\nfrom products of so-called 'super features', obtained from a PCA in parameter\r\nspace of multiple models trained on a subset of the data. The theoretical\r\nanalysis is done with logistic regression, and no experiments are performed.\r\n\r\nIn its current form, this paper is definitely not ready for publication. The\r\nfact that there is no experiment is already a pretty big downside, and the\r\ntheory does not seem complete enough to me to justify it. In particular:\r\n- There is no proof or even intuition for the convergence of the procedure. To\r\n  be fair, there is no claim that it will converge either, but what is the\r\n  point of Section 5 if this is not the case?\r\n- The abstract mentions a guarantee of increasing likelihood which is not shown \r\n  in the paper.\r\n- There is no analysis of the complexity of the algorithm (it seems costly, \r\n  especially since each iteration involves sampling multiple datasets).\r\n- There is no discussion of extensions beyond logistic regression.\r\n- There is no justification for using PCA in parameter space (which works best \r\n  with Gaussian-like unimodal distributions, but may fail in other situations:\r\n  how can we tell it makes sense for a specific application / model?)\r\n\r\nThat being said, this is an intriguing idea, and I am hoping the author will be able to investigate it more thoroughly so as to be able to present it in a more\r\nfleshed out paper in the future.\r\n\r\nThere are several points that were unclear to me and may be worth mentioning in \r\naddition to the above high level comments:\r\n- The equation below eq. 1 does not seem to be used anywhere.\r\n- 'Values of regularization parameters must be found by maximizing Bayesian \r\n  integral in Equation 1.' => this integral is the likelihood -- optimizing\r\n  regularization parameters to maximize the likelihood seems wrong to me.\r\n- '[eq. 1] is estimated by maximum likelihood approximation': I do not see how \r\n  MLA is a good approximation here.\r\n- eq. 5 seems arbitrary to me, especially since the datasets are of different\r\n  sizes,  so each L_w_s involves a different number of terms.\r\n- It is not mentioned exactly how the 'sample data' step works (for instance in\r\n  bootstrap one typically samples t_max points with replacement, but it does\r\n  not seem to be the case here).\r\n- I do not understand how G_alpha,beta can be non-zero in eq. 16, and still\r\n  lead to a linear combination.\r\n- Not clear what are func and F in eq. 19."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387520220000, "tcdate": 1387520220000, "number": 12, "id": "EXqiEZhNias13", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "EXqiEZhNias13", "signatures": ["michael.tetelman@gmail.com"], "readers": ["everyone"], "content": {"title": "Continuous Learning: Engineering Super Features With Feature Algebras", "decision": "submitted, no decision", "abstract": "In this paper we consider a problem of searching a space of predictive models for a given training data set. We propose an iterative procedure for deriving a sequence of improving models and a corresponding sequence of sets of non-linear features on original input space. After finite number of iterations $N$ the non-linear features become $2^N$-degree polynomials on original space. We show that in a limit of infinite number of iterations derived non-linear features must form an algebra, so for any given input point a product of two features is a linear combination of features from same feature space. Due to convexity of each iteration and its ability to fall back to solutions found in previous iteration the models in the sequence have always increasing likelihood with each iteration while dimensionality of each model parameter space is set to a limited controlled value.", "pdf": "https://arxiv.org/abs/1312.5398", "paperhash": "tetelman|continuous_learning_engineering_super_features_with_feature_algebras", "authors": ["Michael Tetelman"], "authorids": ["michael.tetelman@gmail.com"], "keywords": [], "conflicts": []}, "writers": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 12}