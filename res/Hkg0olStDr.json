{"notes": [{"id": "Hkg0olStDr", "original": "HJegkgWFPB", "number": 2520, "cdate": 1569439910382, "ddate": null, "tcdate": 1569439910382, "tmdate": 1577168233856, "tddate": null, "forum": "Hkg0olStDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5ic6a2vjtv", "original": null, "number": 1, "cdate": 1576798751074, "ddate": null, "tcdate": 1576798751074, "tmdate": 1576800884628, "tddate": null, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a solution to the decentralized privacy preserving domain adaptation problem. In other words, how to adapt to a target domain without explicit data access to other existing domains. In this scenario the authors propose MDDA which consists of both a collaborator selection algorithm based on minimal Wasserstein distance as well as a technique for adapting through sharing discriminator gradients across domains. \n\nThe reviewers has split scores for this work with two recommending weak accept and two recommending weak reject. However, both reviewers who recommended weak accept explicitly mentioned that their recommendation was borderline (an option not available for ICLR 2020). The main issues raised by the reviewers was lack of algorithmic novelty and lack of comparison to prior privacy preserving work. The authors agreed that their goal was not to introduce a new domain adaptation algorithm, but rather to propose a generic solution to extend existing algorithms to the case of privacy preserving and decentralized DA.  The authors also provided extensive revisions in response to the reviewers comments. Though the reviewers were convinced on some points (like privacy preserving arguments), there still remained key outstanding issues that were significant enough to cause the reviewers not to update their recommendations. \n\nTherefore, this paper is not recommended for acceptance in its current form. We encourage the authors to build off the revisions completed during the rebuttal phase and any outstanding comments from the reviewers. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705869, "tmdate": 1576800253752, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Decision"}}}, {"id": "ByxBx9sFOS", "original": null, "number": 1, "cdate": 1570515436835, "ddate": null, "tcdate": 1570515436835, "tmdate": 1574323291573, "tddate": null, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "The paper focuses on the problem of domain adaptation among multiple domains when some domains are not available on the same machine. The paper builds a decentralized algorithm based on previous domain adaptation methods.\n\nPros: \n1. The problem is novel and practical. Previous domain adaptation assumes that source and target domains are available but it can happen when the source and target domains have connection issues.\n2. The method exploits asynchronizing accumulating and synchronizing update, which reduces the cost of communication between domains.\n3. The paper proposes to use Wasserstein distance to select the optimal domain as the source domain for the target domain. \n4. The experimental results show that the proposed method outperforms baselines.\n\nCons:\n1. The asynchronizing accumulating and synchronizing update is not novel. It has been used in other communities such as reinforcement learning.\n\nOverall, the paper is good and it is technically sound. The contribution is not significant to the community but providing a new perspective for domain adaptation. I vote for weak accept.\n\nThank Reviewer1 for reminding. I think the paper still has some novelty and the comments address my concerns. I do  not change my score. Also, I'm not unhappy if the paper is rejected. It is more like a borderline paper.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842292154, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Reviewers"], "noninvitees": [], "tcdate": 1570237721692, "tmdate": 1575842292168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review"}}}, {"id": "rylZ6nz59B", "original": null, "number": 4, "cdate": 1572641977200, "ddate": null, "tcdate": 1572641977200, "tmdate": 1574082502939, "tddate": null, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "I read the authors response. I am satisfied with the explanations on the privacy party. However, decentralized training part is still unsatisfactory since the empirical evaluation is not really decentralized. Back of the envelope calculations are at best correlated to the actual times spent by each node.  Hence, the numbers in Table 3 are not physical numbers, rather result of an idealized network. Moreover, the x-axis of Figure 2 being training step is still not acceptable. Decentralized and centralized methods should be compared in terms of time which is the only fair metric. I stick to my original decision.\n\n-----\n\nThe manuscript is proposing a method for domain adaptation in a private and distributed setting where there are multiple target domains and they are added in a sequential manner. The proposed method considers only the domain adaptation methods in which the source model training and the target model training are done separately. In this setting, existing adapted models can be used as a source domain since a trained model suffices for adaptation. One major contribution of the paper is proposing a straightforward but successful method to choose which domain to adapt from. The main algorithmic tool is estimating Wasserstein distance and choosing the closest domain. The second contribution is distributed training setting for privacy and decentralization.\n\nChoosing which model to adapt from is an interesting contribution. The proposed setting is definitely sensible and the proposed method is theoretically sound. Hence, I consider this as a valuable contribution to the domain adaptation literature. Moreover, results suggest that it also results in significant performance improvement.\n\nPrivacy and decentralized learning part has major issues. First of all, the privacy and learning private models is a sub-field of machine learning with a large literature. Authors do not discuss any of these existing work. Second of all, authors do not specify the definition of privacy they are using. Only guarantee the  algorithm provides is not passing data around. However, this is clearly not enough. Passing gradients might result in sharing sensitive data. The actual data can be reconstructed (upto some accuracy) using the gradients passed between nodes. Therefore, either a justification or a privacy guarantee result is needed. Both of these are major issues which need to be fixed.\n\nDecentralized learning is also an important problem which have been studied significantly. Related work section is missing majority of recent and existing work on distributed learning and federated learning. Moreover, empirical study is very counter intuitive. Results are given in terms of accuracy vs number of training steps. The important metrics are amount of massages passed and the total time of the distributed training. Many distributed algorithms trade off having less accurate gradients (consequently having higher number of gradient updates) with less message passing. Hence, I am not sure how to understand the distributed domain adaptation experiments. I am not even sure what the time in Table 3 actually means since it is clearly not even experimented in a distributed setting.\n\nIn summary, the submission is addressing an important problem. Moreover, the contribution on collaborator selection is interesting and seems to be working well. However, the private and decentralized learning parts are rather incomplete from related work and experiment sense. Moreover, I am also not sure can we call this method private or not.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842292154, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Reviewers"], "noninvitees": [], "tcdate": 1570237721692, "tmdate": 1575842292168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review"}}}, {"id": "HJx6LU_nor", "original": null, "number": 8, "cdate": 1573844564856, "ddate": null, "tcdate": 1573844564856, "tmdate": 1573844564856, "tddate": null, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment", "content": {"title": "Summary of changes ", "comment": "We thank the reviewers for finding our problem setting as sensible, well-motivated, and practical [R2, R3, R4]; our proposed solution as interesting [R1], novel [R2], technically solid [R3], theoretically sound and a valuable contribution to the DA literature [R4], and for considering our experimental analysis as extensive and detailed [R1]. We also appreciate the feedback and constructive critique from all the reviewers, which has helped us in significantly improving our paper. \n\nBelow is a summary of the major changes we have made in the paper: \n\n***Based on the feedback from R1***\n\n- We have substantially improved the description of our distributed training mechanism in Section 3.2. \n- We have described the adversarial learning formulations used in our experiments in the Appendix.  \n- We conducted a larger literature survey and extended the related work section.\n- We have added a new domain adaptation technique in Table 2 to demonstrate that our approach can work with the latest DA techniques. This specific baseline called CADA is from a work published at AAAI 2019 and the authors show that it outperforms many recent uDA techniques.  \n\n***Based on the feedback from R2***\n\n- In Section 4, we have improved the writing to ensure that the objective of various evaluations done in the paper is clear. \n- We have added the limitations of our techniques while describing them, as well as summarized them in a separate section (Limitations and Conclusion). \n\n***Based on the feedback from R3***\n\n- We have improved the description of our distributed training algorithm in Section 3.2 and also added a figure of our training architecture in the Appendix. \n- We have added prior works on decentralized and distributed training in the Related Work and highlighted that our contribution lies in enabling adversarial domain adaptation to work in a distributed manner.  \n\n***Based on the feedback from R4***\n\n- We added a detailed discussion about the scope of privacy guarantees of our approach in Section 3.\n- We have substantially enhanced the related work section by adding literature on decentralized training and privacy in ML. \n- We have clarified the goals of our evaluation in Section 4.2 and clearly explained the metrics used to evaluate our approach. - We have also updated the training time in Table 3 and explained it in the text.\n- We have acknowledged the limitations of our approach while introducing them, as well as briefly in Section 5. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg0olStDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2520/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2520/Authors|ICLR.cc/2020/Conference/Paper2520/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140141, "tmdate": 1576860553700, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment"}}}, {"id": "Hkg7iGi9iS", "original": null, "number": 7, "cdate": 1573724827342, "ddate": null, "tcdate": 1573724827342, "tmdate": 1573724827342, "tddate": null, "forum": "Hkg0olStDr", "replyto": "ByefrXPFiS", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment", "content": {"title": "On Decentralization and Distributed Training", "comment": "With regards to the term \u2018decentralization\u2019 used in the paper, we would like to clarify that \u2018decentralized domain adaptation\u2019 means that target domains can adapt from not only the source domain, but also other target domains, which eliminates the strict dependency on the source domain (which can be seen as a \u2018central node\u2019). This is different from the well-known \u2018decentralized training\u2019 of machine learning models where the idea is that the information from all distributed workers won\u2019t be aggregated globally by a central node (like parameter server) or message passing process (like All-Reduce). We understand that the different definition of \u2018decentralization\u2019 used in the paper might confuse a reader, as such, we have added more clarity about it in the text.\n\nTo your second question about the metrics used to evaluate our distributed training strategy, we note that our evaluation is done through the lens of domain adaptation as opposed to a general-purpose evaluation of a decentralized training algorithm. In that aspect, our evaluation has two objectives:\n\na) After splitting the discriminator into two replicas (source and target discriminator) and applying lazy-synchronization, can we still achieve similar convergence rate and target domain accuracy as the case where the source and target discriminators reside on the same node (the convention, non-distributed DA case). Figure 2 and Table 3, in fact, confirm that even with a distributed adversarial training mechanism, we can reach similar levels of target domain accuracy as the non-distributed case.\n\nb) what is the total time taken for domain adaptation training by our lazy-synchronization approach which exchanges the gradients across nodes for every p batches, when compared to fully-synchronized distributed training (which exchange gradients for each batch) and the non-distributed case (which is conventional in DA literature)? The total training time is the sum of local computation time and communication time across nodes. In the non-distributed case, there is no communication at all during the training, so the training time is just the computation time. But it requires the data from all domains on one node. In the fully synchronized case, the amount of communication equals to the size of discriminator gradients multiplied by the number of training steps, whereas in the lazy-synchronized case, the communication amount is one *p*th of the amount of fully synchronized case, where p is the sync-up step. Based on the amount of data communicated across nodes, we estimate the communication time by assuming a bandwidth of 40Mbps [1] which is roughly the global average upload speed. The computation time is measured for each training step and aggregated to get the total computation time. By adding the total computation time with the total communication time, we obtain the total training time, which we report in Table 3 as one of metrics to evaluate our distributed method.   \n\nWe acknowledge that this description was not adequately provided in the first draft, and based on your feedback, we have now incorporated it in the paper. \n\nWe would also like to highlight that our approach of estimating training time in a distributed setting is not ideal as it does not take into account the effect of real-world factors such as network setup latency, network congestion etc. However, it does provide a reasonable estimate to compare different domain adaptation training approaches, which can further be verified through a real-world deployment of domain adaptation algorithms on user devices in future work.\n\n[1] https://www.speedtest.net/global-index"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg0olStDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2520/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2520/Authors|ICLR.cc/2020/Conference/Paper2520/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140141, "tmdate": 1576860553700, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment"}}}, {"id": "Bkl3MaIFsr", "original": null, "number": 2, "cdate": 1573641491952, "ddate": null, "tcdate": 1573641491952, "tmdate": 1573643101852, "tddate": null, "forum": "Hkg0olStDr", "replyto": "SyehsyHxKr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment", "content": {"title": "Response to Review 1 (part 1)", "comment": "\n\nThanks for providing a very nice summary of our work and for your insightful comments. We are glad that you found our paper interesting and considered our experimental analysis as extensive and detailed.  \n \n###Novelty###:\n \nWe agree with your point that our paper is not proposing a new domain adaptation algorithm to boost the accuracy of the model in the target domain. Instead, our contribution operates one layer above the adaptation algorithm and can be utilized with many existing domain adaptation techniques as we demonstrate in Table 2.  \n\nWe would like to argue that our proposed contribution is novel \u2013 both from a problem formulation and a solution perspective. To the best of our knowledge, no prior domain adaptation paper has looked at the problems of domain selection with distributed domain datasets, which, as we highlight in the paper, are of practical significance but have been overlooked. Our proposed solution consists of the Wasserstein-distance collaborator selection algorithm to find the best possible collaborator for adaptation, and the Lazy-Synchronized DA algorithm to reduce the communication between nodes to merely the gradients of the discriminator (summarized in Algorithm 3).  The former algorithm is particularly noteworthy because the question of \u2018which domain should I select to adapt from\u2019 is no less important than \u2018how to adapt between two selected domains (which has been mainly studied before)\u2019 in multiple domains adaptation. Overall, we believe our submission provides a brand-new perspective and novel contribution to the domain adaptation literature.\n \n#### Clarity (1) ####\n\nPlease correct us if we misunderstood your question -- our interpretation is that you are asking how to train D_s and D_t in a distributed way, when we are not sharing the features extracted from source and target domains across nodes. Indeed, this is a valid question and touches the core of our contribution on distributed training of the discriminators. \n\nTo answer this question, let us first have a brief look of how the Discriminator D gets trained in the non-distributed case (Equation 1), where the data from source domain and target domain are fed into their extractors respectively, then the input of D is the concatenation of the output of source extractor (E_s) and target extractor (E_t). \n\nBy contrast, as described in Section 3.2 and Algorithm 1, in the distributed case, D is split across nodes as D_s and D_t, and the outputs of E_s and E_t are fed to the respective discriminators separately. As you rightly questioned, it is not possible to update a discriminator without seeing the features from the other domain -- as such, our idea is to \n\ni) compute local gradients for D_s and D_t on their respective nodes (Algo 1, line 4), \nii) exchange and average the gradients during the sync-up step (Algo 1, line 10), \niii) update D_s and D_t with the averaged gradients (Algo 1, line 11). \n\nIn effect, this guarantees the following: \n\na) We are able to exchange knowledge between the two domains through sharing gradients of D_s and D_t, without requiring the raw data or extracted features to be shared across nodes. \n\nb) Weights of D_s and D_t always remain identical as they get updated with the same averaged gradients. As such, both the discriminators are always in sync with each other. This means that we are able to keep domain datasets private, and yet perform the adversarial training as given in Equation 1 and 2.\n\nWe have added this clarification in the paper in Section 3.2 and also added a figure in the Appendix as a visual aid to explain our distributed training mechanism. \n\n#### Clarity (2) ####\n\nWe follow the convention used in past DA papers (e.g. [1]) wherein the target classifier C_t is initialized with C_s (C_t <-- C_s) and is not updated in the training process. The intuition is that if the feature space of the target domain can be successfully aligned with the source domain through adversarial training, i.e., the outputs of E_s and E_t become close enough, then C_s can directly be used in the target domain without any adaptation. Recall that the classifier takes the outputs of the feature extractor as inputs, therefore if feature extractor outputs become similar across the two domains, then the same classifier can be shared by the two domains.\n\nAs such, during adversarial training, we only adapt the feature extractor E_t with the goal of aligning the target feature space with the source. In practice, this also means that the classifier C_s or C_t is very simple (e.g., it could be just a softmax layer or one fully-connected layer) and E_t does the bulk of the work for the classification task. \n\nWe have added this clarification to the paper in Section 3.2. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg0olStDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2520/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2520/Authors|ICLR.cc/2020/Conference/Paper2520/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140141, "tmdate": 1576860553700, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment"}}}, {"id": "BylUbevFiB", "original": null, "number": 4, "cdate": 1573642237994, "ddate": null, "tcdate": 1573642237994, "tmdate": 1573643086025, "tddate": null, "forum": "Hkg0olStDr", "replyto": "rkgK7eQYcH", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "\n\nThanks for the review, and for finding our paper well-motivated, novel for unsupervised domain adaptation, and well-supported by theoretical analysis. Here are our responses with respect to your questions:\n\n### Question 1 ###\n\nWe would like to humbly submit that our results indeed demonstrate the superiority of our technique. As you have correctly identified in your review, our paper proposes two techniques: a) *collaborator selection*, i.e., which existing domain should be picked as a collaborator for a new target domain and b) *distributed domain adaptation*, i.e., how to conduct adaptation between two distributed computing nodes without sharing domain data. \n\nFor the evaluation of the first technique, we show in Table 1 that in majority of the cases, our collaborator-selection algorithm (MDDA) outperforms conventional strategies used in existing DA works such as always selecting the labeled-source domain as collaborator, selecting multiple collaborators etc. \n\nFor example, in the case of RMNIST (O1), MDDA outperforms the top baseline by as much as *33%* in target domain accuracy. There are indeed some cases where our performance is lower than the best performing baseline, but not by much (e.g., 0.84% in Mic2Mic (O2)). On average, it can be easily seen that MDDA significantly outperforms other strategies.  Further, in Table 2, we extend our analysis to demonstrate that our collaborator selection algorithm can work in conjunction with three different DA techniques (ADDA, GRL, Wasserstein DA), and also outperform the existing baselines in all cases.  \n\nOnce we demonstrated the efficacy of our collaborator selection (our first algorithmic contribution), we proceeded to evaluate the performance of distributed domain adaptation (our second contribution). \n\nIt is important to clarify the goal of this evaluation. It is obvious that if we can perform domain adaptation in a distributed manner without requiring the domain datasets to be shared across nodes, it provides certain privacy benefits over non-distributed training. At the same time, there is a tradeoff between the adaptation accuracy and communication costs in distributed DA \u2014 if we are willing to incur high communication costs, we can achieve the same accuracy as non-distributed training (by exchanging gradients of each batch). In order to save communication costs, we proposed the Lazy Synchronized DA algorithm which instead exchanges gradients after every p batches. \n\nAs such, the primary evaluation goal for our distributed DA algorithm is to reach as close as possible to the accuracy of a conventional non-distributed DA algorithm, with significantly lower communication costs. We have also added this explanation in a concise form in Section 4.  \n\nAs can be seen in Fig 2 and Table 3, we are indeed able to reach significantly close to the target accuracy obtained with non-distributed DA approaches, even when we synchronize the gradients after every 4 batches. In other words, we incur only 25% of the communication costs of a fully-synchronized algorithm and yet are able to achieve similar levels of target domain accuracy, on top of the privacy benefits that are inherent in a distributed training technique. \n\nTherefore, in summary - we show that our collaborator selection algorithm provides significant accuracy gains over existing baselines. On the other hand, our Lazy-Synchronized training approach -- in addition to enhancing user privacy and saving significant communication costs -- can achieve similar levels of accuracy as conventional non-distributed DA algorithms. We believe that both these results are significant and novel contributions to the DA literature. \n\n### Question 2 ###\n\nOne limitation of our approach is the overhead associated with collaborator selection algorithm. While we showed that selecting an optimal collaborator can significantly boost the target domain accuracy, the selection of such collaborator requires computing the Wasserstein distance for different domain pairs which adds a one-time overhead before the adversarial training begins. Given the benefits associated with collaborator selection (as shown in Table 1 and 2), we believe it is worth paying a small price associated with running the selection algorithm. \n\nAnother weakness is that of indirect information leakage in distributed training. In conventional non-distributed DA, users are clearly aware that they are releasing their data over the network, hence giving up on their privacy. However, in distributed training, although there is an impression that the user data is private, prior works have shown the possibility of indirect information leakage [1] and privacy attacks. While a detailed analysis of privacy attacks on our method and their mitigation is beyond the scope of this paper, we have added it as an avenue for future research in the paper. \n\nWe hope our responses have answered your questions and convinced you about the significance of our experimental findings. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg0olStDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2520/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2520/Authors|ICLR.cc/2020/Conference/Paper2520/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140141, "tmdate": 1576860553700, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment"}}}, {"id": "ByefrXPFiS", "original": null, "number": 6, "cdate": 1573643065804, "ddate": null, "tcdate": 1573643065804, "tmdate": 1573643065804, "tddate": null, "forum": "Hkg0olStDr", "replyto": "rylZ6nz59B", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment", "content": {"title": "Related Work and Privacy", "comment": "\n\nThanks for your comments and for finding our problem setting sensible, our approach as theoretically sound and a valuable contribution to the DA literature. Based on your feedback, we have revamped the related work section to include more literature on privacy and decentralized learning.\n \nWith regard to privacy in the distributed setting, we highlight that our approach (algorithm 1) only shares the gradients of the **domain discriminators** between nodes. The raw data from both domains as well as its corresponding gradients from the feature extractor are completely private and are NEVER shared between nodes. \n\nThis, in turn, reduces the possibility of reconstructing the training data as was demonstrated in earlier private-ML works such as [1], which used the gradients of the classifier to reconstruct the raw data. We will clarify in the paper that this is the level of privacy provided by MDDA in its current form.\n\nTo the best of our knowledge, no prior work has shown that discriminator gradients can leak the raw data training data. However, we accept and acknowledge your point and do not discount the possibility that privacy attacks could be developed even on discriminator gradients in the future. While a detailed analysis of privacy and security attacks and their mitigation is out of scope for this paper, we have added a discussion on the possibility of information leakage with our method and pointed to relevant work in the ML privacy literature on the attack defenses. \n\n[1] Aono, Yoshinori, et al. \"Privacy-preserving deep learning via additively homomorphic encryption.\" IEEE Transactions on Information Forensics and Security 13.5 (2017): 1333-1345.  \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg0olStDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2520/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2520/Authors|ICLR.cc/2020/Conference/Paper2520/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140141, "tmdate": 1576860553700, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment"}}}, {"id": "H1gdhxwFjr", "original": null, "number": 5, "cdate": 1573642416237, "ddate": null, "tcdate": 1573642416237, "tmdate": 1573642432924, "tddate": null, "forum": "Hkg0olStDr", "replyto": "ByxBx9sFOS", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thanks for finding the problem novel and practical, and for considering our contribution as technically sound. \n\n### Cons (1) ### \n\nIndeed, as you mentioned, asynchronized accumulating and synchronized updating are variants of existing techniques in distributed training. However, to the best of our knowledge, these techniques have not been applied to adversarial domain adaptation, as such we adopted them to reduce the communication costs associated with sharing discriminator gradients between nodes. We have added this explanation in the paper in the related work section. \n\nMoreover, different from other works and specific to adversarial training, we only accumulate the gradients of parts of the model (source and target discriminators), while the remaining model (target encoder) is updated after every batch without any gradient accumulation (please refer to 3.2 for details). Our experimental evaluation shows a novel finding that this partial accumulation of gradients does not hurt the convergence rate as compared to conventional training of DA algorithms. \n\nFinally, we would like to highlight that the core contribution and novelty of our work lies in the problem formulation and our proposed collaborator selection algorithm. To the best of our knowledge, both these perspectives are missing from current DA literature and therefore, we believe that our work provides a significant contribution to the ML community. These contributions, in combination with our lazy-synchronized distributed training strategy, make our proposed framework useful for real-world deployments of domain adaptation algorithms. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg0olStDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2520/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2520/Authors|ICLR.cc/2020/Conference/Paper2520/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140141, "tmdate": 1576860553700, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment"}}}, {"id": "S1lSfA8Fsr", "original": null, "number": 3, "cdate": 1573641740897, "ddate": null, "tcdate": 1573641740897, "tmdate": 1573641740897, "tddate": null, "forum": "Hkg0olStDr", "replyto": "Bkl3MaIFsr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment", "content": {"title": "Response to Reviewer 1 (part 2)", "comment": "### Clarity (3) ###\nAlthough the adversarial training process is provided in Equation 1 and 2 in Section 3.1, we acknowledge that we did not explain it in detail in the text. \n\nBroadly, to train the domain discriminator, we follow the formulation given in equation 1. The key difference is that in our setting, since the source (X_s) and target (X_t) datasets reside in different nodes, we compute the gradients of the discriminators (D_s and D_t) separately and then update them based on their average gradients as discussed in Algorithm 1. \n \nThereafter, discriminators and the target extractor E_t play the adversarial game depending on the underlying domain adaptation algorithm. As shown in Table 2, we evaluated our approach in conjunction with three domain adaptation algorithms: \n\ni) ADDA [1] which implements adversarial training by reversing domain labels, \nii) GRL [2] which uses a Gradient Reversal Layer between the feature extractor and the discriminator to enable adversarial learning, \niii) Wasserstein DA [3] which uses the Wasserstein loss for adversarial training.  \n\nWe have added the details of the adversarial formulation of each algorithm in Section A.3 in the Appendix.   \n\n\n### Cons (2) ###\n\nWe have polished the paper writing by adding more details and explanations of the design choices and algorithms. We have also revamped the related work section. \n\n### Cons (3) ###\n\nCertainly, we will be happy to add results for more DA algorithms in the paper. We however note that due to the novelty of our problem setting and solution, there are no baselines which do collaborator selection, against which we can directly compare our results. Therefore, in this setting, we chose baselines as \u201cRandom selecting collaborator (no strategy)\u201d, \u201cAlways selecting the first source domain (majority of the prior DA algorithms)\u201d and \u201cSelecting multiple collaborators (a recent work [4])\u201d.  Besides, we have shown the efficacy of our technique with 3 DA [1,2,3] techniques in Table 2. \n\nWe can extend our analysis (in Table 2) to include an additional (latest) DA algorithm. Would that be sufficient from your perspective? \n\n[1] Tzeng et al. Adversarial discriminative domain adaptation. CVPR 2017.\n[2] Ganin et al. Domain-adversarial training of neural networks.JMLR, 2016.\n[3] Shen J. et al. Wasserstein distance guided representation learning for domain adaptation. AAAI 2018\n[4] Zhao et al. Adversarial Multi-Source Domain Adaptation. NeurIPS 2018.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Hkg0olStDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2520/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2520/Authors|ICLR.cc/2020/Conference/Paper2520/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140141, "tmdate": 1576860553700, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Authors", "ICLR.cc/2020/Conference/Paper2520/Reviewers", "ICLR.cc/2020/Conference/Paper2520/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Comment"}}}, {"id": "SyehsyHxKr", "original": null, "number": 2, "cdate": 1570946980164, "ddate": null, "tcdate": 1570946980164, "tmdate": 1572972327895, "tddate": null, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\n\n###Summary###\n\nThis paper tackles unsupervised domain adaptation in a decentralized setting.  The high-level observation is that the conventional unsupervised domain adaptation has two bottlenecks, namely excessive centralization and poor support for distributed domain datasets. The paper proposes Multi-Step Decentralized Domain Adaptation (MDDA) to transfer the knowledge learned from the source domain to the target domain without sharing the data.  \n\nThe paper also explores explore a proposition: in addition to adapting from the labeled source, can uDA leverage the knowledge from other target domains, which themselves may have undergone domain adaptation in the past. \n\nThe proposed MMDA method contains a feature extractor (E), a domain discriminator (D) and task classifier (C) for each domain. The target domain components are initialized with the respective source components. The source domain discriminator D_s target domain discriminator D_t are synchronized by exchanging and averaging the gradients. The paper also proposes Lazy Synchronization to reduce the communication cost of the algorithm.\n\nThe paper also proposes Wasserstein distance guided collaborator selection schema to perform the domain adaptation task. \n\nThe paper performs experiments on five image and audio datasets: Rotated MNIST, Digits, and Office-Caltech, DomainNet and Mic2Mic. \n\nThe baselines used in this paper include \"Labeled Source\", \"Random Collaborator\", and \"Multi-Collaborator\". The experimental results demonstrate that the proposed method can outperform the baselines on some of the experimental settings. The paper also provides a detailed analysis of the model and experimental results. \n\n### Novelty ###\n\nThis paper does not propose a new domain adaptation algorithm. However, the paper introduces some interesting tricks to solve the MMDA task such as the lazy synchronization between the source domain discriminator and the target domain discriminator. \n\n###Clarity###\n\nSeveral critical explanations are missing from the paper:\n1) When training the source domain discriminator D_s and target domain discriminator D_t, if the features between the source domain and target domain cannot be shared with each other, how to train the D_s and D_t. For example, the D_s cannot get access to the features from the target domain, how to train D_s? \n2) How is the target classifier C_t updated when there are no labels for the target domain?\n3) As far as I understand, the domain discriminator is this paper is trained adversarially. The detailed adversarial training step is unclear.  \n\n###Pros###\n\n1) The paper proposes an interesting transfer learning schema where the data between the source and target domain can not be shared with each other to protect the data-privacy.\n\n2) The paper provides extensive experiments on multiple standard domain adaptation benchmarks, especially the most recent dataset such as the DomainNet. \n\n3) The paper provides detail empirical analysis to demonstrate the effectiveness of the proposed methods. \n\n###Cons###\n\n1) The most critical issue of this paper is that some explanations are missing, e.g. how are D_s, D_t, C_t trained? Refer to the #Clarity.\n\n2) The presentation and writing of this paper need polish. The author should do more relative surveys to motivate the authors. One critical relevant reference of this paper is:\n\"Secure Federated Transfer Learning\", Yang Liu et al \n\nhttps://arxiv.org/pdf/1812.03337.pdf\n\n3) The baselines used in this paper is also trivial. It is desirable to compare the proposed method with state-of-the-art domain adaptation methods.\n\nBased on the summary, cons, and pros, the current rating I am giving now is \"reject\". I would like to discuss the final rating with other reviewers, ACs.\nTo improve the rating, the author should explain the questions I proposed in the #Clarity"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842292154, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Reviewers"], "noninvitees": [], "tcdate": 1570237721692, "tmdate": 1575842292168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review"}}}, {"id": "rkgK7eQYcH", "original": null, "number": 3, "cdate": 1572577312751, "ddate": null, "tcdate": 1572577312751, "tmdate": 1572972327860, "tddate": null, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "invitation": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper focuses on the problem of unsupervised domain adaptation in practical machine learning systems. To address the problems in current unsupervised domain adaptation methods, the authors propose to a novel multi-step framework which works in a decentralized and distributed manner. Experimental results show the effectiveness of the proposed approach. \n\nThis paper is well-motivated and the proposed method is novel for unsupervised domain adaptation. The paper is well-supported by theoretical analysis, however, the improvements are not that significant on some experimental results. For the above reasons, I tend to accept this paper but wouldn't mind rejecting it. \n\nQuestions: \n1. The experiments do not really show the superiority of the proposed method compared to the common centralized approaches as they have similar performances on both collaborator selection and distributed domain adaptation. Can you convince the readers more with some other experiments?\n2. What is the weakness of such decentralization models?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2520/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["akhilmathurs@gmail.com", "sgan@inf.ethz.ch", "anton.isopoussu@gmail.com", "fahim.kawsar@gmail.com", "nadia.berthouze@gmail.com", "nicholasd.lane@gmail.com"], "title": "Multi-Step Decentralized Domain Adaptation", "authors": ["Akhil Mathur", "Shaoduo Gan", "Anton Isopoussu", "Fahim Kawsar", "Nadia Berthouze", "Nicholas D. Lane"], "pdf": "/pdf/9e4b2c75e86a97daca93b88055bb6be6b7574c44.pdf", "TL;DR": "A novel method for decentralized and distributed domain adaptation, as a way to make these methods more practical in real ML systems.", "abstract": "Despite the recent breakthroughs in unsupervised domain adaptation (uDA), no prior work has studied the challenges of applying these methods in practical machine learning scenarios. In this paper, we highlight two significant bottlenecks for uDA, namely excessive centralization and poor support for distributed domain datasets. Our proposed framework, MDDA, is powered by a novel collaborator selection algorithm and an effective distributed adversarial training method, and allows for uDA methods to work in a decentralized and privacy-preserving way.  \n", "keywords": ["domain adaptation", "decentralization"], "paperhash": "mathur|multistep_decentralized_domain_adaptation", "original_pdf": "/attachment/d2c4f349492687c98d50a7eb1b9596a8007ba761.pdf", "_bibtex": "@misc{\nmathur2020multistep,\ntitle={Multi-Step Decentralized Domain Adaptation},\nauthor={Akhil Mathur and Shaoduo Gan and Anton Isopoussu and Fahim Kawsar and Nadia Berthouze and Nicholas D. Lane},\nyear={2020},\nurl={https://openreview.net/forum?id=Hkg0olStDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Hkg0olStDr", "replyto": "Hkg0olStDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2520/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575842292154, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2520/Reviewers"], "noninvitees": [], "tcdate": 1570237721692, "tmdate": 1575842292168, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2520/-/Official_Review"}}}], "count": 13}