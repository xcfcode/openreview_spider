{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396702239, "tcdate": 1486396702239, "number": 1, "id": "S1IXpz8Og", "invitation": "ICLR.cc/2017/conference/-/paper605/acceptance", "forum": "ry_4vpixl", "replyto": "ry_4vpixl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Paper has an interesting idea, but isn't quite justified, as pointed out by R2. Very minimal experiments are presented in the paper.\n \n pros:\n - interesting idea\n \n cons:\n - insufficient experiments with no real world problems.\n - no rebuttal either :(."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "pdf": "/pdf/652d9439cff6afd96bc9a6060100a6c12d9982a9.pdf", "TL;DR": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "paperhash": "mccarthy|rotation_plane_doubly_orthogonal_recurrent_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["berkeley.edu", "openai.com"], "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396703255, "id": "ICLR.cc/2017/conference/-/paper605/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ry_4vpixl", "replyto": "ry_4vpixl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396703255}}}, {"tddate": null, "tmdate": 1482057333374, "tcdate": 1482057268964, "number": 3, "id": "BJp48JVNx", "invitation": "ICLR.cc/2017/conference/-/paper605/official/review", "forum": "ry_4vpixl", "replyto": "ry_4vpixl", "signatures": ["ICLR.cc/2017/conference/paper605/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper605/AnonReviewer2"], "content": {"title": "not ready yet", "rating": "4: Ok but not good enough - rejection", "review": "My main objection with this work is that it operates under a hypothesis (that is becoming more and more popular in the literature) that all we need is to have gradients flow in order to solve long term dependency problems. The usual approach is then to enforce orthogonal matrices which (in absence of the nonlinearity) results in unitary jacobians, hence the gradients do not vanish and do not explode. However this hypothesis is taken for granted (and we don't know it is true yet) and instead of synthetic data, we do not have any empirical evidence that is strong enough to convince us the hypothesis is true. \n\nMy own issues with this way of thinking is: a) what about representational power; restricting to orthogonal matrices it means we can not represent the same family of functions as before (e.g. we can't have complex attractors and so forth if we run the model forward without any inputs). You can only get those if you have eigenvalues larger than 1. It also becomes really hard to deal with noise (since you attempt to preserve every detail of the input, or rather every part of the input affects the output). Ideally you would want to preserve only what you need for the task given limited capacity. But you can't learn to do that. My issue is that everyone is focused on solving this preserved issue without worrying of the side-effects. \n\nI would like one of these papers going for jacobians having eigenvalues of 1 show this helps in realistic scenarios, on complex datasets.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "pdf": "/pdf/652d9439cff6afd96bc9a6060100a6c12d9982a9.pdf", "TL;DR": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "paperhash": "mccarthy|rotation_plane_doubly_orthogonal_recurrent_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["berkeley.edu", "openai.com"], "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512526878, "id": "ICLR.cc/2017/conference/-/paper605/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper605/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper605/AnonReviewer3", "ICLR.cc/2017/conference/paper605/AnonReviewer1", "ICLR.cc/2017/conference/paper605/AnonReviewer2"], "reply": {"forum": "ry_4vpixl", "replyto": "ry_4vpixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512526878}}}, {"tddate": null, "tmdate": 1481969448576, "tcdate": 1481969448576, "number": 2, "id": "SJbVk5M4x", "invitation": "ICLR.cc/2017/conference/-/paper605/official/review", "forum": "ry_4vpixl", "replyto": "ry_4vpixl", "signatures": ["ICLR.cc/2017/conference/paper605/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper605/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper discusses recurrent networks with an update rule of the form h_{t+1} = R_x R h_{t}, where R_x is an embedding of the input x into the space of orthogonal or unitary matrices, and R is a shared orthogonal or unitary matrix.    While this is an interesting model, it is by no means a *new* model:  the idea of using matrices to represent input objects (and multiplication to update state) is often used in the embedding-knowledge-bases or embedding-logic literature (e.g. Using matrices to model symbolic relationships by Ilya Sutskever and Geoffrey Hinton, or Holographic Embeddings of Knowledge Graphs by Maximillian Nickel et al.).  I don't think the experiments or analysis in this work add much to our understanding of it.    In particular, the experiments are especially weak, consisting only of a very simplified version of the copy task (which is already very much a toy).  I know several people who have played with this model in the setting of language modeling, and as the other reviewer notes, the inability of the model to forget is an actual annoyance.   \n\nI think it is incumbent on the authors to show how this model can be really useful on a nontrivial task; as it is we should not accept this paper.\n\nSome questions:  is there any reason to use the shared R instead of absorbing it into all the R_x?  Can you find any nice ways of using the fact that the model is linear in h or linear in R_x ?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "pdf": "/pdf/652d9439cff6afd96bc9a6060100a6c12d9982a9.pdf", "TL;DR": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "paperhash": "mccarthy|rotation_plane_doubly_orthogonal_recurrent_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["berkeley.edu", "openai.com"], "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512526878, "id": "ICLR.cc/2017/conference/-/paper605/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper605/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper605/AnonReviewer3", "ICLR.cc/2017/conference/paper605/AnonReviewer1", "ICLR.cc/2017/conference/paper605/AnonReviewer2"], "reply": {"forum": "ry_4vpixl", "replyto": "ry_4vpixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512526878}}}, {"tddate": null, "tmdate": 1481821329603, "tcdate": 1481821329598, "number": 1, "id": "rk5c3reEe", "invitation": "ICLR.cc/2017/conference/-/paper605/official/review", "forum": "ry_4vpixl", "replyto": "ry_4vpixl", "signatures": ["ICLR.cc/2017/conference/paper605/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper605/AnonReviewer3"], "content": {"title": "my review", "rating": "5: Marginally below acceptance threshold", "review": "This is a nice proposal, and could lead to more efficient training of\nrecurrent nets. I would really love to see a bit more experimental evidence.\nI asked a few questions already but didn't get any answer so far.\nHere are a few other questions/concerns I have:\n\n- Is the resulting model still a universal approximator? (providing large enough hidden dimensions and number of layers)\n- More generally, can one compare the expressiveness of the model with the equivalent model without the orthogonal matrices? with the same number of parameters for instance?\n- The experiments are a bit disappointing as the number of distinct input/output\nsequences were in fact very small and as noted by the authr, training\nbecomes unstable (I didn't understand what \"success\" meant in this case).\nThe authors point that the experiment section need to be expanded, but as\nfar as I can tell they still haven't unfortunately.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "pdf": "/pdf/652d9439cff6afd96bc9a6060100a6c12d9982a9.pdf", "TL;DR": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "paperhash": "mccarthy|rotation_plane_doubly_orthogonal_recurrent_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["berkeley.edu", "openai.com"], "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512526878, "id": "ICLR.cc/2017/conference/-/paper605/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper605/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper605/AnonReviewer3", "ICLR.cc/2017/conference/paper605/AnonReviewer1", "ICLR.cc/2017/conference/paper605/AnonReviewer2"], "reply": {"forum": "ry_4vpixl", "replyto": "ry_4vpixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512526878}}}, {"tddate": null, "tmdate": 1481013950502, "tcdate": 1481013950494, "number": 2, "id": "Sy8acg4Qx", "invitation": "ICLR.cc/2017/conference/-/paper605/pre-review/question", "forum": "ry_4vpixl", "replyto": "ry_4vpixl", "signatures": ["ICLR.cc/2017/conference/paper605/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper605/AnonReviewer2"], "content": {"title": "the pre-review questions", "question": "Hi, \n\nsorry for the delay in reviewing the paper. Interesting paper, here are a few questions that I'd love to hear back on: \n (1) Saxe et al work was on deep linear feedforward models. I'm wondering why would expect his findings to be mirrored exactly in your setup (given, for e.g. that you have an input at every time step, issue that he doesn't have in this model, shared weights, etc.). I'm mostly referring to sentences like we would expect that learning time is constant regardless of sequence length.\n\n (2) More importantly maybe, most of the work I've seen on orthogonal matrices for RNNs focus only on the learning dynamics of this model. What about the representational power? The main advantage of not having an orthogonal rotation matrix is that you can *forget*, which seems a fundamental property to dealing with information. Otherwise dealing with noise becomes really hard. Do you have any intuition on what restrictions on the representational power to you have due to your parametrization choices? Isn't it true that while the gradients might not vanish, because of noise, learning could actually still be extremely hard? In particular, the only choice the model has is to designated a particular dimension for noise and make sure that puts all the noise in the input in this dimension (or dimensions). Otherwise accumulated noise, would make solving the task impossible. To me this seems a harder learning problem. IMHO this all boils down to: have you experimented with this model in more realistic task, maybe ones where you have large amounts of noise (or irrelevant features) ? One simple step forward that you might have time to do is to check the task of Pascanu et al 2013 ( https://arxiv.org/abs/1211.5063) which is identical to the one you proposed except that you have noise between the two symbols (I think is called the temporal order task). I'd love to see how the model performs on that, though I think even that task is very simplistic at the end of the day, and the ability of the model to deal with noise will still not be fully answered. Btw, that paper also seems relevant, though not cited (this however is maybe just a minor detail). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "pdf": "/pdf/652d9439cff6afd96bc9a6060100a6c12d9982a9.pdf", "TL;DR": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "paperhash": "mccarthy|rotation_plane_doubly_orthogonal_recurrent_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["berkeley.edu", "openai.com"], "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481013951070, "id": "ICLR.cc/2017/conference/-/paper605/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper605/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper605/AnonReviewer3", "ICLR.cc/2017/conference/paper605/AnonReviewer2"], "reply": {"forum": "ry_4vpixl", "replyto": "ry_4vpixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481013951070}}}, {"tddate": null, "tmdate": 1480524300830, "tcdate": 1480524300825, "number": 1, "id": "HyrfzFhfe", "invitation": "ICLR.cc/2017/conference/-/paper605/pre-review/question", "forum": "ry_4vpixl", "replyto": "ry_4vpixl", "signatures": ["ICLR.cc/2017/conference/paper605/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper605/AnonReviewer3"], "content": {"title": "Some questions", "question": "- You said the model trained after 5000 time steps was unstable but still able to succeed. How do we measure this when the curves go up and down like this. What does \"success\" mean precisely?\n- Have you compared this to other approaches (the document says more results will appear soon)?\n- Have you also done experiments with the more standard setting of the task, so that it's easier to compare with previous results?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "pdf": "/pdf/652d9439cff6afd96bc9a6060100a6c12d9982a9.pdf", "TL;DR": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "paperhash": "mccarthy|rotation_plane_doubly_orthogonal_recurrent_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["berkeley.edu", "openai.com"], "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481013951070, "id": "ICLR.cc/2017/conference/-/paper605/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper605/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper605/AnonReviewer3", "ICLR.cc/2017/conference/paper605/AnonReviewer2"], "reply": {"forum": "ry_4vpixl", "replyto": "ry_4vpixl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper605/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481013951070}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478379312473, "tcdate": 1478379312461, "number": 605, "id": "ry_4vpixl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ry_4vpixl", "signatures": ["~Zoe_McCarthy1"], "readers": ["everyone"], "content": {"title": "Rotation Plane Doubly Orthogonal Recurrent Neural Networks", "abstract": "Recurrent Neural Networks (RNNs) applied to long sequences suffer from the well known vanishing and exploding gradients problem.  The recently proposed Unitary Evolution Recurrent Neural Network (uRNN) alleviates the exploding gradient problem and can learn very long dependencies, but its nonlinearities make it still affected by the vanishing gradient problem and so learning can break down for extremely long dependencies. We propose a new RNN transition architecture where the hidden state is updated multiplicatively by a time invariant orthogonal transformation followed by an input modulated orthogonal transformation. There are no additive interactions and so our architecture exactly preserves forward hid-den state activation norm and backwards gradient norm for all time steps, and is provably not affected by vanishing or exploding gradients.  We propose using the rotation plane parameterization to represent the orthogonal matrices. We validate our model on a simplified memory copy task and see that our model can learn dependencies as long as 5,000 timesteps.", "pdf": "/pdf/652d9439cff6afd96bc9a6060100a6c12d9982a9.pdf", "TL;DR": "Recurrent equation for RNNs that uses the composition of two orthogonal transitions, one time invariant and one modulated by input, that doesn't suffer from vanishing or exploding gradients.", "paperhash": "mccarthy|rotation_plane_doubly_orthogonal_recurrent_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["berkeley.edu", "openai.com"], "authors": ["Zoe McCarthy", "Andrew Bai", "Xi Chen", "Pieter Abbeel"], "authorids": ["zmccarthy@berkeley.edu", "xiaoyang.bai@berkeley.edu", "c.xi@berkeley.edu", "pabbeel@berkeley.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 7}