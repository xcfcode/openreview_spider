{"notes": [{"id": "uqD-un_Mzd-", "original": "hXzSV7pJ_EH", "number": 3073, "cdate": 1601308340895, "ddate": null, "tcdate": 1601308340895, "tmdate": 1614985741055, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1u9CcVooG7Y", "original": null, "number": 1, "cdate": 1610040397080, "ddate": null, "tcdate": 1610040397080, "tmdate": 1610473992425, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "All four reviewers expressed significant concerns on this submission during review. None of them is willing to change their evaluations and supports this work during discussions. Thus a reject is recommended."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040397066, "tmdate": 1610473992408, "id": "ICLR.cc/2021/Conference/Paper3073/-/Decision"}}}, {"id": "srTgOEkK8YW", "original": null, "number": 2, "cdate": 1603374570812, "ddate": null, "tcdate": 1603374570812, "tmdate": 1606763352922, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review", "content": {"title": "Official Blind Review #4", "review": "Summary: The paper proposes Polynomial Graph Convolution (PGC), which enjoys a larger-than-one-hop receptive field within a single layer. This is done by first propagating information with a fixed (not learned) propagation matrix (e.g. adjacency matrix or graph Laplacian), and then projecting the information from different topological distances with a learned linear layer. PGC is shown to be theoretically more expressive than linearly stacking simple graph convolutions; experiments on several graph classification tasks show good performance.\n\nRecommendation: Overall, I am slightly leaning to reject. The paper is interesting, but the theoretical contribution is rather straight-forward, and to warrant acceptance I would look for strong empirical performance together with an analysis of what is contributing to the gains. While some of the gains PGCN obtains look promising, others do not seem statistically significant; some results and baselines from related works are not compared against, and the effect of a strong node aggregation scheme is not discussed. Finally, it is unclear if the proposed method can be generalized to use edge types.\n\nMain pros:\n1. The paper is clear and easy to understand; even relatively small claims are backed up by proofs.\n2. The relationship between linearly stacking k GraphConvs and using a single k-PGC layer is cleanly explained and motivates the work well. Decoupling receptive field size from network depth is a natural goal.\n3. I appreciate the authors taking care to fairly evaluate their PGCN following [1]. This is crucial as many GNN works are iterating on metrics computed on the evaluation set despite it being available to the model selection procedure; as noted by authors, this includes the GIN results from [2].\n\nMain cons:\n1. I have several concerns about the results (Table 1):\n- Best mean performance is bolded, but many of the gains (esp. PTC, PROTEINS, IMDB-B, IMDB-M) seem within noise levels due to high variance. Instead of saying \"PGCN shows a slight improvement\", it would be better to perform a statistical significance test, and clearly mark which improvements are significant (by bolding several joint-top values).\n- Many results are taken from [1], but simple baselines of [1] that do not consider graph structure are omitted. These baselines outperform all GNNs in [1] for some benchmarks, e.g. getting 78.4 for D&D and 65.2 for ENZYMES (which is between the results of the best baseline and PGCN); to get a full picture these baselines should be included.\n- [1] also shows that adding simple node features can make a big difference for IMDB-B and IMDB-M; for example, adding node degree features improves GIN on IMDB-B from 66.8 to 71.2. While PGCN is not using node degree features, for IMDB it's using the Graph Laplacian as T(A) (Appendix H), which implicitly encodes some degree information. As [1] already has results with node degree features included, it would be nice to see how PGCN compares.\n- PGCN uses a powerful node aggregation scheme (Page 6). What aggregation methods are used in the baselines? What is the contribution of using this stronger aggregation scheme instead of a more standard weighted sum?\n2. Section 3.3 and Appendix E mention that PGCN can be more efficient to train than standard GNNs because the propagation (i.e. multiplying node features by powers of T(A)) can be computed once. This only applies to the very first PGCN layer, and as authors state, before using a non-trivial (k > 1) PGC layer, one would typically use either 1-PGC (as done in the paper) or a learned linear projection, in order to avoid propagating highly sparse raw node features. It's therefore unclear if this training speedup can be really obtained in practice without sacrificing accuracy; one could use a random (not learned) linear projection as the initial \"densification\", but it's hard to say if that would work. Either way, if this speedup can really be obtained, it would be good to see supporting results (preferably on a large dataset where training time is indeed an issue).\n3. Can PGC take edge types into account? I understand some of the benchmark datasets contain molecules, and thus have typed edges (single, double, triple); was that information utilized by PGCN? Considering edge types is straight-forward for GNNs that only perform a single learned propagation step, since the propagation matrix can be selected based on the edge type; it's unclear what to do for PGC which uses a fixed propagation matrix.\n\nOther comments:\n- The abstract and introduction say \"interactions among GC parameters at different levels pose a bias on the flow of topological information\". I'm assuming this refers to entanglement of weights as in Equation 8, but when reading the intro this got me confused, since there was no reference, and the claim did not seem obvious.\n- Propagation in PGCN does not use any non-linearity or normalization, so it seems values can grow exponentially with k, especially on dense graphs. Was this observed to be a problem?\n- \"GraphConv is more expressive than GCN and GIN\" - this is not fully true, since GCN as defined in the paper uses the Laplacian instead of the adjacency matrix.\n- The analysis shows the relation between PGC and linearly stacking GraphConv's, but in practice one wouldn't linearly stack them, and rather use non-linearities in between. Can anything be said in that case?\n- \"they (...) consider shortest paths, (...) that choice limits significantly the expressiveness\" - I'm not fully convinced: PGC cannot be stronger than the WL test, while using shortest path information allows to exceed that (since it makes it possible to tell apart two disconnected cycles from a single larger cycle).\n- The paper compares only against GraphConv-like layers; it would be interesting to compare (empirically) to a larger class of GNNs, such as GAT [3], GGNN [4] or GNN-FiLM [5].\n\nTypos and small formatting issues (did not influence my rating recommendation):\n- Page 1: \"faces the problem\" -> \"addresses the problem\"\n- Page 1: \"being able to\" -> \"by being able to\"\n- Page 3: \"start showing\" -> \"start by showing\"\n- Page 6: In equation for y_j, I believe j should start with 1\n- Pages 6 & 14: \"PCG\" -> \"PGC\"\n\n\nReferences:\n- [1] A Fair Comparison of Graph Neural Networks for Graph Classification\n- [2] How Powerful are Graph Neural Networks?\n- [3] Graph Attention Networks\n- [4] Gated Graph Sequence Neural Networks\n- [5] GNN-FiLM: Graph Neural Networks with Feature-wise Linear Modulation\n\n----------------------------------------------------------------------------------------------------\n\nComments after rebuttal:\n\nI have reviewed the response from the authors, and I decided to keep my score. Although the paper is certainly interesting, for ICLR it is borderline. While I agree that many works in the literature choose to treat their network as a monolithic architecture, and thus do not explore the effects of different components, I would still encourage the authors to add an ablation of the readout method, as that would help assess the interaction between that and the choice of the graph propagation scheme.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082820, "tmdate": 1606915771764, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3073/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review"}}}, {"id": "wfX447KggK", "original": null, "number": 4, "cdate": 1603928518215, "ddate": null, "tcdate": 1603928518215, "tmdate": 1606756256902, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review", "content": {"title": "Recommendation to Reject", "review": "##########################################################################\n\nSummary:\n\nThis work proposes the Polynomial Graph Convolutional Networks (PGCNs), which is built upon the Polynomial Graph Convolution (PGC). The PGC is able to aggregate k-hop information in a single layer and comes with the hyper-parameter k. The PGCNs are composed of a PGC with k=1, followed by a PGC with a chosen k (usually > 1), and a complex readout layer using avg, max, and sum over all nodes. Theoretically, the proposed PGC has two major benefits as claimed: 1) Common graph convolution operators can be represented as special cases of the PGC; 2) A PGC with k = q (q > 1) is more expressive than linearly stacked q PGCs with k=1. The PGCNs are thus more general, expressive, and efficient than existing GNNs. Experimental studies are conducted on common graph classification benchmarks, showing the improved performances of the PGCNs.\n\n##########################################################################\n\nReasons for score:\n\nOverall, I vote for rejection. My major concerns lie in three aspects, as detailed in Cons below: 1) Some statements to motivate this work are not well explained and supported. 2) The proposed idea seems quite similar to several previous studies; 3) The experimental studies are not convincing to me in order to show the effectiveness of the proposed PGC.\n\n##########################################################################\n\nPros:\n\n1. The presentation is good and the proposed method is clearly described. I like the theoretical analysis in 3.2, which explains clearly how to build PGCNs with PGC.\n\n2. This paper targets at two interesting problems in the literature: 1) designing more powerful graph convolution operator; 2) building GNNs with similar power as deeper GNNs. The deeper version of existing GNNs is usually not powerful as expected. As a result, building GNNs to achieve the expected power is an important topic.\n\n3. The experimental details are provided in the appendix.\n\n##########################################################################\n\nCons:\n\n1. Some statements to motivate this work are not well explained and supported. In particular, in the abstract and section 1, the authors indicate that interactions among non-linearly stacked graph convolution (GC) parameters at different levels pose a bias on the flow of topological information. This statement is not well explained and supported with analysis or experimental results.\n\n2. The proposed idea seems quite similar to several previous studies. The proposed PGC is quite similar (or equivalent, with minor differences) to concatenating outputs of linearly stacked GCs and then going through a linear transformation. First, such concatenation is proposed in [1]. Second, linearly stacking GCs, or equivalently using a polynomial powers of the adjacency matrix A (possibly with some transformations), is studies in multiple studies like [2,3,4]. I didn't capture the key differences between this work and these previous studies. Only [2] is discussed in the appendix. [4] is mentioned in the experiments, but no discussion in terms of method differences is provided.\n\n[1] Xu et al. \"Representation Learning on Graphs with Jumping Knowledge Networks\", ICML 2018\n\n[2] Wu et al. \"Simplifying Graph Convolutional Networks\", ICML 2019\n\n[3] Liu et al. \"Towards Deeper Graph Neural Networks\", KDD 2020\n\n[4] Chen et al. \"Are Powerful Graph Neural Nets Necessary? A Dissection on Graph Classification\", ICLR 2019 RLGM workshop\n\n3. The experimental studies are not convincing to me in order to show the effectiveness of the proposed PGC. As mentioned in Summary, the PGCNs use a quite complex readout layer, which is different from all the GNNs in comparison. Ablation studies on this readout layer should be provided in order to show that the improvement is indeed from the PGC.\n\n##########################################################################\n\nQuestions during rebuttal period: \n\nPlease address and clarify the cons above \n\n\n##########################################################################\n\nComments after the rebuttal period: \n\nI will keep my score. The authors' responses addresses my first concern. However, the differences from previous studies are still not significant to me. In addition, no matter the authors claim the proposal of a complete architecture or a new operator, ablation studies are necessary to backup the designing of each part.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082820, "tmdate": 1606915771764, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3073/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review"}}}, {"id": "lOTGbX8zcb3", "original": null, "number": 10, "cdate": 1606240961469, "ddate": null, "tcdate": 1606240961469, "tmdate": 1606240961469, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Revised version of the paper", "comment": "We just submitted the revised version of the manuscript, where we addressed the issues reported by the reviewes concerning the originality of the model with respect to the GCN architectures in literature, the speed of convergenece, and the statistical significance of the experimental results. \nWe would like to stress that, even though the idea of exploiting the powers of the adjacency matrix is similar to other works in literature, our formulation differs enough to produce a *statistically significant* difference in the experimental results. By the way, also the differences among the works pointed out by reviewers are relatively small. Nonetheless, the works have been published anyway, based on improved (but not proven to be statistically significant) experimental results. Thus, we consider unfair to penalize our paper based mainly on this kind of argument. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "5rGh8FyTdQ0", "original": null, "number": 6, "cdate": 1605654982726, "ddate": null, "tcdate": 1605654982726, "tmdate": 1605655524429, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "srTgOEkK8YW", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response part 1", "comment": "We thank the reviewer for writing this detailed review. In the following, we respond to the various cons highlighted by the reviewer. In the next few days we will upload the revised version of the paper.\n\nComment:\nI have several concerns about the results (Table 1):\nBest mean performance is bolded, but many of the gains (esp. PTC, PROTEINS, IMDB-B, IMDB-M) seem within noise levels due to high variance. Instead of saying \"PGCN shows a slight improvement\", it would be better to perform a statistical significance test, and clearly mark which improvements are significant (by bolding several joint-top values).\nMany results are taken from [1], but simple baselines of [1] that do not consider graph structure are omitted. These baselines outperform all GNNs in [1] for some benchmarks, e.g. getting 78.4 for D&D and 65.2 for ENZYMES (which is between the results of the best baseline and PGCN); to get a full picture these baselines should be included.\n[1] also shows that adding simple node features can make a big difference for IMDB-B and IMDB-M; for example, adding node degree features improves GIN on IMDB-B from 66.8 to 71.2. While PGCN is not using node degree features, for IMDB it's using the Graph Laplacian as T(A) (Appendix H), which implicitly encodes some degree information. As [1] already has results with node degree features included, it would be nice to see how PGCN compares.\nPGCN uses a powerful node aggregation scheme (Page 6). What aggregation methods are used in the baselines? What is the contribution of using this stronger aggregation scheme instead of a more standard weighted sum?\n\nAnswer:\nOur experimental setting is the same used in [1] . Unfortunately we do not have access to the results of every single run for the baselines we report, but only to the final averages. Due to this issue, we are strongly limited in the statistical significance tests that can be computed. Note that this is a common issue in the field, and for this reason seldom statistical significance tests are computed. However we completely agree with the reviewer about the importance of performing statistical significance tests. Luckily we were able to compute the two-tailed Wilcoxon Signed-Ranks test between our proposed PGCN and competing methods. This test considers all the results for the different datasets at the same time. According to this test, PGCN performs significantly better (p-value<0.05) than PSCN, DGCNN, GIN, DIFFPOOL, and GraphSAGE. Meanwhile, for FGCNN and DGCNN, four datasets are not enough to conduct the test.\nWe will report the results of the statistical significance test in the revised version of the paper.\n\nIn the proposed comparison, we considered only the methods that exploit the topological information of the graph. The baseline in [1] is structure-agnostic and is used as a tool to understand the effectiveness of GNNs and extract useful insights. \nIn the revised version of the paper, we will also report the results of this particular baseline. Note that the only dataset where the baseline outperforms the PGCN is PROTEINS, but with a significantly higher variance.\n\nFor what concerns the use of node features in the social network dataset, we decided not to use them to evaluate the capability of the model when focusing on learning just from the graph structure.\nUsing the normalized Laplacian (as we did for social datasets) is very different from using the degree as a node label, and indeed several approaches reported in the comparison use the Laplacian as well. Note that also the matrix A embeds the degree information, and thus the model can extract this information both when using T(A)=A and T(A)=L.\nWe will include experiments also using the node degree as a label in a revision of the paper.\nFor what concerns the PGC readout please note that we propose an architecture and not just a convolutional layer. The aggregation that we use is the same used in FGCNN, and it turns out to be less complex than the ones exploited by some other models in the comparison. For instance, the DIFFPOOL uses a particular pooling method, and the DGCNN uses the SortPooling layer followed by a 1D-convolutional layer followed by several dense layers (MLP). \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "S1xv6pBSIiN", "original": null, "number": 9, "cdate": 1605655114571, "ddate": null, "tcdate": 1605655114571, "tmdate": 1605655465173, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "IAFCJs-8yST", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response", "comment": "We thank the reviewer for writing this detailed review. In the following, we try to address the various highlighted cons. In the next few days we will upload the revised version of the paper.\n\nComment:\nThe core idea of PGCN mainly relies on the exponential power of different order applying on adjacent matrix A. It is easy to prove that by exponentiating the adjacency matrix to the K-th power, the node's receptive field can be extended to the K-th nearest neighboring nodes. However, this approach has been successfully adopted in different articles, as a result, the proposed PGCN is of limited originality without further analysis, either in theoretical aspect or technical aspect.\n\nAnswer:\nIn order to clarify the difference between our approach and other methods exploiting the exponentiations of the adjacency matrix recently proposed in the literature, in the revised version of the paper we will add a section where we report a complete review and comparison with these methods. However, we would like to stress that the majority of these methods are developed to perform nodes classification, while we propose an architecture that is designed to perform a different task (graph classification).\n\n--\n\nComment:\nThe mathematical proof in section 3.1 and section 3.2 shows that the stronger expressive ability comes from the less constraints of learnable parameters W. However, reducing the constraints may lead to the training process more difficult than other graph convolution. To dispel this concern, the author needs to provide more detailed experimental results, especially the training convergence curves of different graph convolutions.\n\nAnswer:\nThe experimental results show that the convergence of the PGCN is even faster than the common GC-based method in the literature. In order to clarify this point in the revised version of the paper we will add an appendix with some of the training curve plots for the various datasets. For sake of comparison, we will also report the convergence graph of the FGCNN that shares the same readout.\nWe are sorry but we think we did not completely understand what the reviewer means with \u201ctraining process more difficult\u201d. In the case of  the reviewer referring to the complexity of the optimization, we point out that thanks to the constraint reduction introduced by the PGC layer, the optimization is less complex compared to stacked GC layers. If the reviewer refers to the efficacy of the training phase, the result obtained by PGCN in various datasets show the benefits of having a less constrained GC layer.\n\n--\n\nComment:\nIn terms of efficiency, the K-th order exponential method in equation (1) can improve the nodes' receptive field but brings heavy computational burden. In Table 3, All the evaluated dataset has a limited number of nodes in each graph. The maximum average number is 284.32 while the minimum average number is 13.00. The generalization of PGCN on large scale graph remains a serious concern. In Appendix E, the author has also noticed the computing efficiency and adopted a pre-computing strategy, which means the proposed PGCN is not an out-of-box model. In a word, although the proposed model shows stronger expression ability, the generalization and efficiency still remains a big concern.\n\nAnswer:\nWe respectfully disagree with the reviewer, since some of the datasets considered (e.g. COLLAB) have a significant amount of nodes per graph (and they are the most used dataset in the literature to benchmark GNNs). For what concerns the use of a pre-computing strategy, it is already used in other methods proposed in the literature (e.g. [1]), and in our opinion, this does not make the PGCN \u201cnot an out-of-box model\u201d, since this step is automatically run when a new graph is given in input. To clarify this point and give more information about the computational efficiency of the PGCN,in the revised version of the papers, we will report the training time and a comparison with the training time required by the FGCNN. \n\n---\n\n[1] Simplifying Graph Convolutional Networks\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "MC4AzfezyAY", "original": null, "number": 7, "cdate": 1605655045290, "ddate": null, "tcdate": 1605655045290, "tmdate": 1605655390815, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "5rGh8FyTdQ0", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response part 2", "comment": "Comment:\nSection 3.3 and Appendix E mention that PGCN can be more efficient to train than standard GNNs because the propagation (i.e. multiplying node features by powers of T(A)) can be computed once. This only applies to the very first PGCN layer, and as authors state, before using a non-trivial (k > 1) PGC layer, one would typically use either 1-PGC (as done in the paper) or a learned linear projection, in order to avoid propagating highly sparse raw node features. It's therefore unclear if this training speedup can be really obtained in practice without sacrificing accuracy; one could use a random (not learned) linear projection as the initial \"densification\", but it's hard to say if that would work. Either way, if this speedup can really be obtained, it would be good to see supporting results (preferably on a large dataset where training time is indeed an issue).\n\nAnswer:\nNote that in the case of the first PGC layer with k=1 and a stacked Convolutional layer with k>1 it is still possible to precompute the power series of the adjacency matrix. We will update the paper to reflect this consideration.\nNote that the choice of using a PGC layer instead of a random projection is due to the fact that using a random projection would have made it difficult to compare our results with other models considered in the comparison, since the same random projection of the input could have been applied to other models as well. In order to clarify this point we will add a  section in the revised version of the paper where we report some evaluation of the time required to train the PGCN compared with the one of other Graph Convolutional networks.\n\n--\n\nComment:\nCan PGC take edge types into account? I understand some of the benchmark datasets contain molecules, and thus have typed edges (single, double, triple); was that information utilized by PGCN? Considering edge types is straight-forward for GNNs that only perform a single learned propagation step, since the propagation matrix can be selected based on the edge type; it's unclear what to do for PGC which uses a fixed propagation matrix.\n\nAnswer:\nWe focused our attention on graphs where edges do not have a type, similarly to the other models considered in the comparison that do not consider edge types. An extension of our approach allowing us to consider the type of edges is possible but not trivial and highly depends on the nature of the information related to the edges. For instance, in the specific case of molecules where the edge type denotes if the edge is single, double or triple, it is possible to represent this information using a weighted adjacency matrix. However, the extension to typed edges is outside the scope of our paper.\nNote that the extension to typed edges is not trivial for the other models as well, since in many cases such an extension was published as a separate paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "xIcz7tMEHc", "original": null, "number": 4, "cdate": 1605654899263, "ddate": null, "tcdate": 1605654899263, "tmdate": 1605655249949, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "pzMAoE468Lb", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response part 1", "comment": "We thank the reviewer, whose suggestions helped us improve the clarity and quality of the paper. In the following we respond to the various weaknesses reported by the reviewer. In the next few days we will upload the revised version of the paper.\n\n\nComment:\nNovelty seems limited. It is unclear how the proposed method/framework is different from the Krylov subspace based work of (Luan et al. 2019) and also the Chebyshev polynomials approach of (Defferrard et al., 2016). Both these methods boil down to considering powers (polynomials) of the Laplacian, i.e., different topological distances in the model. Authors should clarify the key differences.\n\nAnswer:\nIn order to give a complete answer to this weakness we will add a dedicated section in the revised version of the paper, where we highlight the difference between our proposed approach and the various architecture cited by the reviewer. Briefly we would like to highlight that in [2] the authors propose a model that stacks various layers and exploits nonlinear activation functions. Both these aspects make the gradient flow more complex compared to the PGCN. In [3] the model exploits the Chebyshev polynomials, and sums them over k. Note that the graph model proposed in [3] is an instance of the PGC, since the PGC is more general (as we show in section 3.1). \n\n--\n\nComment:\nExperimental results section can be enhanced. Currently, the paper considers only the graph classification task, where only global information suffices. GCN are more interesting in local, graph level tasks, in particular for prediction tasks on nodes and edges, e.g., link prediction, node and edge classification. It is unclear why these tasks were not considered. Moreover, methods of (Kipf and Welling, 2016), (Luan et al. 2019) and (Defferrard et al., 2016), that are most relevant to this work are not considered for experimental comparisons.\n\nAnswer:\nPlease note that the architecture that we propose is specifically designed for graph classification. The similar models pointed out by the reviewer exploits the idea of using the power series of the adjacency matrix only to perform node classification. We exploited a similar base intuition and developed a model specific for the graph classification task. Thus, it is not possible to compare to those models since they are designed for a different learning task.\nWe respectfully disagree with the reviewer\u2019s statement that GCNs are more interesting in local tasks.\nGraph-level tasks such as graph classification/regression or generation are extremely important e.g. in chemoinformatics, where predicting properties of chemical compounds can significantly speed up the development of new drugs. For instance, we do not consider the JEDI Grand Challenge of virtual screening of a billion molecules for effectiveness against SARS-CoV-2 not interesting (https://www.covid19.jedi.group).\nMoreover, we used datasets that are largely used in all state-of-the-art papers in the field using an experimental validation very close to the one used in [1] (that provides an extensive validation via these datasets) so to have a solid placement of the performance of the proposed approach versus other SOTA competing approaches.\n\n--\n\nComment:\nThe claim in Theorem 2, that PGC with k =2 is more expressive than two stacked PGC with k = 1, is du to that fact that the former has number of parameters higher than the latter. Is this correct? (Due to the structure of weight matrix W)\n\nAnswer:\nTheorem 2 proves that the PGCN is more expressive using the same amount of  parameters. The gain of expressiveness comes from the particular structure of the PGC that allows us to have a less constrained combination of the embedding computed for the various k values. The proof is in the appendix."}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "oOX1gKTsZI", "original": null, "number": 8, "cdate": 1605655068696, "ddate": null, "tcdate": 1605655068696, "tmdate": 1605655068696, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "MC4AzfezyAY", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response part 3", "comment": "Comment:\nThe abstract and introduction say \"interactions among GC parameters at different levels pose a bias on the flow of topological information\". I'm assuming this refers to entanglement of weights as in Equation 8, but when reading the intro this got me confused, since there was no reference, and the claim did not seem obvious.\nPropagation in PGCN does not use any non-linearity or normalization, so it seems values can grow exponentially with k, especially on dense graphs. Was this observed to be a problem?\n\"GraphConv is more expressive than GCN and GIN\" - this is not fully true, since GCN as defined in the paper uses the Laplacian instead of the adjacency matrix.\nThe analysis shows the relation between PGC and linearly stacking GraphConv's, but in practice one wouldn't linearly stack them, and rather use non-linearities in between. Can anything be said in that case?\n\"they (...) consider shortest paths, (...) that choice limits significantly the expressiveness\" - I'm not fully convinced: PGC cannot be stronger than the WL test, while using shortest path information allows to exceed that (since it makes it possible to tell apart two disconnected cycles from a single larger cycle).\nThe paper compares only against GraphConv-like layers; it would be interesting to compare (empirically) to a larger class of GNNs, such as GAT [3], GGNN [4] or GNN-FiLM [5].\n\n\nAnswer:\nIn the abstract when we discuss the bias on the flow of topological information posed by the interactions among non-linearly stacked graph convolution (GC) parameters at different levels we refer to the behavior highlighted (and proved) by theorem 2. Indeed in the theorem we prove that stacking 2 PGC layers with k=1 is less expressive than having a single PGC layer with k=2. Since we show in the paper (before the theorem) that the most common GC layers are just particular instances of the PGC layer with k=1, that means that stacking 2 or more (see corollary 2.1 in the appendix) GC layers is less expressive than a single PGC layer with k equal to the number of GC layers. Therefore it is straightforward to conclude that staking several GC layers poses a bias. We will clarify the sentence in the abstract.\n\nActually, as we highlighted in appendix G, our model uses batch normalization, in particular after the aggregation layer. During the experiments we did not have any issue related with exponential growth of the nodes embeddings. Indeed it is important to note that without stacking several layers, and having independent weights for each k, the embedding values do not tend to significantly grow.\n\nSince the proposed PGC layer abstracts from the specific transformation of the adjacency matrix that is considered, the proof of theorem 2 is independent from the specific operator to represent the node connections. This is obvious from the proof (in Appendix).\n\nWe agree that current state of the art models non-linearly stack GCN layers. In this paper we show that using a linear operator, and a more expressive architecture, helps the gradient flow and allows us to obtain better results than models explaining non-linearly stacked GCs (i.e. the state-of-the-art models we consider in our experimental comparison)\nWhen we discuss the expressiveness of the model proposed in [2] we refer to the use of a fixed transformation of the adjacency matrix. We completely agree with the reviewer and we will rewrite the discussion of the related paper.\nNote that we focus our work on the graph classification task, and the papers pointed out by the reviewer developed models for node classification, classification of sequences of graphs, or particular regression on the QM9 dataset (the exploits type of the edges). That poses a problem related to how to manage the readout part. Please note that the readout is a crucial part of our architecture. We focus our comparison on models that are developed and tested for the same purpose of the PGCN, and we decided to use the results available in the literature, adapting our validation method to perform a fair comparison.\n\n---\n\n[1] A Fair Comparison of Graph Neural Networks for Graph Classification\n[2]On filter size in graph convolutional networks\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "7pxlTickWzk", "original": null, "number": 5, "cdate": 1605654926509, "ddate": null, "tcdate": 1605654926509, "tmdate": 1605654926509, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "xIcz7tMEHc", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response part 2", "comment": "Comment:\nThere are few statements that need to be substantiated and explained. E.g., (a) It is claimed there is a bias in deeper GCNs, but no explanation is given. (b) Similarly, PGC being more expressive (for the asme number of parameters) needs to be explained.\n\nAnswer:\nBoth these claims are related to theorem 2. When we discuss the bias on the flow of topological information posed by the interactions among non-linearly stacked graph convolution (GC) parameters at different levels we refer to the behavior highlighted (and proven) by theorem 2. Indeed in the theorem, we prove that stacking 2 PGC layers with k=1 is less expressive than having a single PGC layer with k=2. Since we show in the paper (before the theorem) that the most common GC layers are just particular instances of the PGC layer with k=1, that means that stacking 2 or more (see corollary 2.1 in the appendix) GC layers is less expressive than a single PGC layer with k equal to the number of GC layer. Therefore it is straightforward to conclude that staking several GC layers pose a bias. Note that Theorem 2 proves that the PGC with k=2 is more expressive than 2 staked PGC layers, in the case where the number of the parameters of the two configurations is the same. \nTo avoid confusion, we will clarify the sentence in the abstract.\nFor point (b), please see the previous answer.\n\n\n\n---\n[1] A Fair Comparison of Graph Neural Networks for Graph Classification\n[2] Break the Ceiling: Stronger Multi-scale Deep Graph Convolutional Networks\n[3] Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "U8u3JrAMY81", "original": null, "number": 3, "cdate": 1605654826034, "ddate": null, "tcdate": 1605654826034, "tmdate": 1605654826034, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "0g5GjuFpqB", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response part 2", "comment": "Comment:\nThe experimental studies are not convincing to me in order to show the effectiveness of the proposed PGC. As mentioned in Summary, the PGCNs use a quite complex readout layer, which is different from all the GNNs in comparison. Ablation studies on this readout layer should be provided in order to show that the improvement is indeed from the PGC.\n\nAnswer:\nWe respectfully disagree since the readout of the proposed architecture is, in our opinion, quite simple. We use a simple MLP, experimenting with two alternatives: 1)  a single layer and 2) 3 fully connected readout layers. For what concerns the aggregation step, we use three simple element-wise operations (sum, mean, and max). Moreover, many of the architectures considered in the comparison use a more complex readout. For instance, the DIFFPOOL uses a pretty complex node pooling method,  while DGCNN uses the SortPooling layer followed by a 1D-convolutional layer followed by several dense layers (MLP). Finally, the aggregation method exploited in the PGCN is the same used by the FGCNN.\nAs mentioned before, we clarify that we propose a complete architecture to perform graph classification that performs significantly better (see our answer to AnonReviewer4) than state-of-the-art methods."}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "0g5GjuFpqB", "original": null, "number": 2, "cdate": 1605654791186, "ddate": null, "tcdate": 1605654791186, "tmdate": 1605654791186, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "wfX447KggK", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment", "content": {"title": "Authors' Response part 1", "comment": "We sincerely thank the reviewer for constructive criticism. In the following, we respond to the various cons raised by the reviewer. In the next few days we will upload the revised version of the paper.\n\nComment:\nSome statements to motivate this work are not well explained and supported. In particular, in the abstract and section 1, the authors indicate that interactions among non-linearly stacked graph convolution (GC) parameters at different levels pose a bias on the flow of topological information. This statement is not well explained and supported with analysis or experimental results.\n\nAnswer:\nIn the abstract when we discuss the bias on the flow of topological information posed by the interactions among non-linearly stacked graph convolution (GC) parameters at different levels we refer to the behavior highlighted (and proved) by theorem 2. Indeed in the theorem we prove that stacking 2 PGC layers with k=1 is less expressive than having a single PGC layer with k=2. Since we show in the paper (before the theorem) that the most common GC layers are just particular instances of the PGC layer with k=1, that means that stacking 2 or more (see corollary 2.1 in the appendix) GC layers is less expressive than a single PGC layer with k equal to the number of GC layers. Therefore it is straightforward to conclude that staking several GC layers poses a bias. We will clarify the sentence in the abstract.\n\nComment:\nThe proposed idea seems quite similar to several previous studies. The proposed PGC is quite similar (or equivalent, with minor differences) to concatenating outputs of linearly stacked GCs and then going through a linear transformation. First, such concatenation is proposed in [1]. Second, linearly stacking GCs, or equivalently using a polynomial powers of the adjacency matrix A (possibly with some transformations), is studies in multiple studies like [2,3,4]. I didn't capture the key differences between this work and these previous studies. Only [2] is discussed in the appendix. [4] is mentioned in the experiments, but no discussion in terms of method differences is provided.\n\nAnswer:\nWe respectfully disagree and in order to better highlight the difference between the architecture we propose and the models proposed in literature, in the revised version of the paper we will add a section discussing these differences. \nBriefly, for what concerns the papers cited by the reviewer: in [1] the authors proposed to modify the common aggregation stage in such a way that, for each node, the model aggregates all the  intermediate representations computed in the previous GC-layers. Note that differently from PGCN, the model proposed in [1] exploits the message passing method introducing the bias in the flow of the topological information. We would like to stress the fact that a PGC layer of degree k is not equalivalent to concatenate the output of k stacked GC layers, even though the PGC layer can also learn to represent this particular architecture.\nIn [2] the model proposed is basically a stack of linear GC operators. Note that in theorem 2 we prove that staking k GC layers is less expressive that using a single PGC layer of degree k.\nWe thank the reviewer for reporting the work [3] of which we were not aware of when writing our manuscript, since [3] has been published only at the end of August 2020. We will discuss the differences with [3] in the revised version of the manuscript.\nIn general, the majority of the models that concatenate polynomial powers of the adjacency matrix A are designed to perform node classification, while the proposed PGCN is developed to perform graph classification. In this regard, we want to point out that our proposal is not limited to a novel GC layer, but we propose a complete architecture to perform graph classification that performs significantly better (see our answer to AnonReviewer4) than state-of-the-art methods."}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "uqD-un_Mzd-", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3073/Authors|ICLR.cc/2021/Conference/Paper3073/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923841399, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Comment"}}}, {"id": "IAFCJs-8yST", "original": null, "number": 1, "cdate": 1602834952434, "ddate": null, "tcdate": 1602834952434, "tmdate": 1605024073523, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review", "content": {"title": "Owning more expressive ability but still lacking generalization", "review": "This paper analyzes the current graph convolution and proposes a novel polynomial graph convolutional network (PGCN). Under the framework of PGCN, several other graph convolutions can be seen as a special case.\n\nQuality:\nThe paper is technically sound and maintains a high presentation quality. \n\nClarity:\nThe mathematical proof is clear and the whole paper is easy to follow.\n\nOriginality:\nThe paper is somewhat original but the proposed PGCN still lacks generalization, which will be pointed out in the following part.\n\nSignificance:\nThe significance is not very prominent.\n\n\nPros: \n1. The paper proposes a novel graph convolution named with PGCN. Under the framework of PGCN, several other graph convolutions can be regarded as a special case. \n2. The whole paper is described clearly and easy to follow.\n\n\nCons:\n1. The core idea of PGCN mainly relies on the exponential power of different order applying on adjacent matrix A. It is easy to prove that by exponentiating the adjacency matrix to the K-th power, the node's receptive field can be extended to the K-th nearest neighboring nodes. However, this approach has been successfully adopted in different articles, as a result, the proposed PGCN is of limited originality without further analysis, either in theoretical aspect or technical aspect.\n2. The mathematical proof in section 3.1 and section 3.2 shows that the stronger expressive ability comes from the less constraints of learnable parameters W. However, reducing the constraints may lead to the training process  more difficult than other graph convolution. To dispel this concern, the author needs to provide more detailed experimental results, especially the training convergence curves of different graph convolutions.\n3. In terms of efficiency, the K-th order exponential method in equation (1) can improve the nodes' receptive field but brings heavy computational burden. In Table 3, All the evaluated dataset has a limited number of nodes in each graph. The maximum average number is 284.32 while the minimum average number is 13.00. The generalization of PGCN on large scale graph remains a serious concern. In Appendix E, the author has also noticed the computing efficiency and adopted a pre-computing strategy, which means the proposed PGCN is not an out-of-box model. In a word, although the proposed model shows stronger expression ability, the generalization and efficiency still remains a big concern.", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082820, "tmdate": 1606915771764, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3073/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review"}}}, {"id": "pzMAoE468Lb", "original": null, "number": 3, "cdate": 1603893736205, "ddate": null, "tcdate": 1603893736205, "tmdate": 1605024073461, "tddate": null, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "invitation": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review", "content": {"title": "Interesting work, certain details missing", "review": "The article presents a novel framework for Graph Convolutional Neural Networks (GCNs). The method called  Polynomial Graph Convolution (PGC) is based on concatenating the powers of a transformed adjacent matrix in a given layer. The paper shows that various popular variants of GNNs can be expressed using the PGC framework.  Theoretical results presented show that PGC with higher degree is more expressive that deeper std. GNNs. Numerical results are presented on graph classification task that illustrate the performance of the method.\n\n--------------\nStrengths:\n1. Paper presents a novel framework.\n2. Numerical results look promising.\n3. The method presents a more expressive method.\n\n--------------\nWeakness:\n1. Novelty seems limited.\n2. Node and edge task experiments are missing.\n3. Certain claims need to be substantiated.\n\n--------------\nDetails:\nI have the following comments:\n1. Novelty seems limited. It is unclear how the proposed method/framework is different from the Krylov subspace based work of (Luan et al. 2019) and also the Chebyshev polynomials approach of (Defferrard et al., 2016). Both these methods boil down to considering powers (polynomials) of the Laplacian, i.e., different topological distances in the model. Authors should clarify the key differences.\n\n2. Experimental results section can be enhanced. Currently, the paper  considers only the graph classification task, where only global information suffices. GCN are more interesting in local, graph level tasks, in particular for prediction tasks on nodes and edges, e.g., link prediction, node and edge classification. It is unclear why these tasks were not considered. Moreover, methods of  (Kipf and Welling, 2016), (Luan et al. 2019) and (Defferrard et al., 2016), that are most relevant to this work are not considered for experimental comparisons.\n\n3. The claim in Theorem 2, that PGC with k =2 is more expressive than two stacked PGC with k = 1, is du to that fact that the former has number of parameters higher than the latter. Is this correct? (Due to the structure of weight matrix W)\n\n4.  There are few statements that need to be substantiated and explained. E.g.,\n(a) It is claimed there is a bias in deeper GCNs, but no explanation is given.\n(b) Similarly, PGC being more expressive (for the asme number of parameters) needs to be explained.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3073/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3073/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Polynomial Graph Convolutional Networks", "authorids": ["~Luca_Pasa1", "~Nicol\u00f2_Navarin1", "~Alessandro_Sperduti1"], "authors": ["Luca Pasa", "Nicol\u00f2 Navarin", "Alessandro Sperduti"], "keywords": ["Graph Convolutional Networks", "Graph Neural Network", "Deep Learning", "Structured Data", "Machine Learning on Graphs"], "abstract": "Graph Convolutional Neural Networks (GCNs) exploit convolution operators, based on some neighborhood aggregating scheme, to compute representations of graphs. The most common convolution operators only exploit local topological information. To consider wider topological receptive fields, the mainstream approach is to non-linearly stack multiple Graph Convolutional (GC) layers. In this way, however, interactions among GC parameters at different levels pose a bias on the flow of topological information. In this paper,  we propose a different strategy, considering a single graph convolution layer that independently exploits neighbouring nodes at different topological distances, generating decoupled representations for each of them. These representations are then  processed by subsequent readout layers. We implement this strategy introducing the Polynomial Graph Convolution (PGC) layer, that we prove being more expressive than the most common convolution operators and their linear stacking. Our contribution is not limited to the definition of a convolution operator with a larger receptive field, but we prove both theoretically and experimentally that the common way  multiple non-linear graph convolutions are stacked limits the neural network expressiveness. Specifically, we show that a Graph Neural Network architecture with a single PGC layer achieves state of the art performance on many commonly adopted graph classification benchmarks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pasa|polynomial_graph_convolutional_networks", "one-sentence_summary": "The contribution of this paper is to\u00a0propose the Polynomial Graph Convolutional\u00a0Network, where the depth of the network is decoupled from the receptive field size, allowing to build deep Graph Neural Networks avoiding the oversmoothing problem.", "supplementary_material": "/attachment/d40df7340683613c1ab00a762261a8f647cb0455.zip", "pdf": "/pdf/a283f246173c9bf9b4113142d66312ed82d1d3a1.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=jZkYadMATf", "_bibtex": "@misc{\npasa2021polynomial,\ntitle={Polynomial Graph Convolutional Networks},\nauthor={Luca Pasa and Nicol{\\`o} Navarin and Alessandro Sperduti},\nyear={2021},\nurl={https://openreview.net/forum?id=uqD-un_Mzd-}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "uqD-un_Mzd-", "replyto": "uqD-un_Mzd-", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3073/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538082820, "tmdate": 1606915771764, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3073/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3073/-/Official_Review"}}}], "count": 15}