{"notes": [{"id": "rJxpuoCqtQ", "original": "H1glnwHctQ", "number": 396, "cdate": 1538087797002, "ddate": null, "tcdate": 1538087797002, "tmdate": 1545355400157, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Syem4KElgE", "original": null, "number": 1, "cdate": 1544730923403, "ddate": null, "tcdate": 1544730923403, "tmdate": 1545354512348, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Meta_Review", "content": {"metareview": "This paper proposes a new permutation invariant loss (where the order doesn't matter), motivated by set autoencoding settings. This is an important problem, and the authors' solution is interesting.  The reviewers, however, found the exposition to be unclear, in particular the explanation on how the loss function is derived was confusing for two of the reviewers. Reviewers also found the experimental results to be not convincing, even after the revision. This is a borderline paper: the idea is valuable and I'd encourage the authors to develop it further, improving exposition and including additional experiments as suggested by the reviewers.\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Borderline paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper396/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353232029, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353232029}}}, {"id": "SyeqWmt9CX", "original": null, "number": 13, "cdate": 1543308033715, "ddate": null, "tcdate": 1543308033715, "tmdate": 1543308033715, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "H1gg6JZ9RX", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "last comment", "comment": "Because when I looked at the paper the last time there was a sentence: \" Out of 30 instances, Latplan+Set Avg returned XX  plans, out of which XX plans were correct, while Latplan+SCE managed to solve all instances correctly.\" and I didn't realise that those 30 come from Table 7.\n\nIn the latest version, there are now like 20 pages of plots in the appendix, which I don't think are interesting and that is not what I requested. Section 6.5 of the appendix is quite enough, I think.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "H1gg6JZ9RX", "original": null, "number": 12, "cdate": 1543274424352, "ddate": null, "tcdate": 1543274424352, "tmdate": 1543274424352, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "SyeTgUjt0Q", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "All experiments are finished", "comment": "It is unclear why the reviewer believe that they are \"still running\".\n\nIt seems that the reviewer is requesting to include all planning results in the paper, which greatly increases the paper length , but we updated the paper to satisfy that anyways.\n\nPlease check the revised paper, where all visualized results are added in Section 6.6.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "rJe3Vj9tA7", "original": null, "number": 10, "cdate": 1543248691609, "ddate": null, "tcdate": 1543248691609, "tmdate": 1543248691609, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "S1xEuWr4AX", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "Paper updated", "comment": "Please check the updated paper for the additional results."}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "rkxXX1XYRm", "original": null, "number": 9, "cdate": 1543216923002, "ddate": null, "tcdate": 1543216923002, "tmdate": 1543217067736, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "S1xEuWr4AX", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "Planning with Plannable Representation, i.e., Discrete Latent Binary Representation + Latplan system (AAAI18)", "comment": "  > I still think that evaluating the quality of representations learnt\n    with an SCE loss is not a very surprising thing to ask for.\n\n  To address the reviewer's request, we are running a new experiment. We\n  modified Latplan (Asai, Fukunaga AAAI18) neural-symbolic classical\n  planning system, a system that operates on a discrete symbolic latent\n  space of the real-valued inputs and runs dijkstra/A* search using a\n  state-of-the-art symbolic classical planning solver. We modified\n  Latplan to take the set-of-object-feature-vector input rather than\n  images.  It is a high-level task planner (unlike motion planning /\n  actuator control) that has implications on robotic systems.\n\n  To briefly describe the Latplan system, it learns the discrete binary\n  latent space of an arbitrary raw input (e.g. images) with a\n  Gumbel-Softmax variational autoencoder, learns a discrete transition\n  model in the state space from the transition examples, and runs a\n  systematic, complete search algorithm such as Dijkstra search or A*\n  which gurantee the optimality of the solution.  Unlike RL-based\n  planning systems, the search agent does not contain the learning\n  aspects.  The discrete plan in the latent space is mapped back to the\n  raw image visualization of the plan execution.  A similar system\n  replacing Gumbel Softmax VAE with Causal InfoGAN was later proposed\n  (Kurutach et. al., NIPS18).\n\n  We replaced Latplan's Gumbel-Softmax VAE with our autoencoder used in\n  the 8-Puzzle and the Blocksworld experiments (Appendix, Sec 6.1). Our\n  autoencoder also uses Gumbel Softmax in the latent layer, but it uses\n  (Zaheer 2017) encoder and is trained with Set Cross Entropy in order\n  to encode permutation-invariant information.\n\n  When the network learned the representation, it guarantees that the\n  planner finds a solution because the search algorithm being used\n  (e.g. Dijkstra) is a complete, systematic, symbolic search algorithm,\n  which guarantees to find a solution whenever it is reachable in the\n  state space.  If the network cannot learn the permutation-invariant\n  representation, the system cannot solve the problem and/or return the\n  human-comprehensive visualization. This makes the specific\n  permutation-invariant representation using (Zaheer 2017) and the\n  proposed Set Cross Entropy necessary when the input is given as a set\n  of feature vectors.\n\n  The new results will be found in the appendix section of the revision.\n  The reason for putting it in the appendix is that the value of the new\n  experiment added to our main contribution could be debatable.\n  Our MAIN claim is a loss function that can train a NN to reconstruct a\n  set, and the importance of better handling a set in the context of\n  deep learning is already widely recognized in the ML community (see\n  the existing work cited by our paper).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "S1xEuWr4AX", "original": null, "number": 8, "cdate": 1542898027958, "ddate": null, "tcdate": 1542898027958, "tmdate": 1542971227132, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "BygnoGpIpX", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "reply to rebuttal", "comment": "Thank you for the rebuttal! It clarified some things, but there are still parts I'm very confused about.\n\n>> The usual notation of the probability distribution, such as P(x), is an abbreviation of a random variable X taking a particular value x, i.e. P(X=x)\n\nI agree with that, but the confusion is coming from the fact that here x is a random variable and x also denotes a probability distribution starting from Eq. 1 if I'm not mistaken.\n\n>> The input is not assumed to be discrete. \"binomial distribution\" might be the more appropriate term. We rephrased them in the revision.\n\nI don't see where the binomial distribution comes from.\n\n>> Regarding the value of reconstructing a set, existing work (Vinyal NIPS 2015, ICLR 2016) already showed that once we can train a NN to output a set (by whatever means), it allows a variety of tasks to be\n  solved. \n\nI don't know about pointer nets, but I think models like seq2seq for sets struggle more with encoding a set in a permutation-invariant manner. I still think that evaluating the quality of representations learnt with an SCE loss is not a very surprising thing to ask for. \n\nI'm sure that SCE loss function is a great idea, but I agree with Reviewer2 about the clarity. So I'd keep my score for now. \n\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "H1ezDoAkCQ", "original": null, "number": 7, "cdate": 1542609754370, "ddate": null, "tcdate": 1542609754370, "tmdate": 1542609754370, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "BkgusNjaam", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "reply", "comment": "  Thank you for the further comments.\n\n  Getting back to the first comment,\n\n  > The logsumexp formulation of the proposed measure is unsatisfactory\n  > to me as it directly averages over each of the independent\n  > probabilities that a given element is a member of the target set,\n  > rather than integrating over the combinatorial set of probabilities\n  > for each set of complete possible matches.\n\n  Since every sets have the equal number of permutations (assuming they\n  are of the same size and every elements are distinct), the individual\n  probability and such an integrated sum over every permutations of the\n  probabilities will only differ by a constant factor scaling.\n\n  For example, P([x,y,z]=[1,2,3]) vs sum of all permutations,\n  P([x,y,z]=[1,2,3])+\u2026+P([x,y,z]=[3,2,1]) differs by 6, the number of\n  permutations for 3 elements.  Since other patterns, such as\n  P([x,y,z]=[4,5,6]), also have 6 permutations, considering just one\n  ordering and considering all ordering makes essentially no difference.\n\n  Informally speaking, the key to understand our method is to notice\n  that considering the exponential/combinatorial number of possible\n  permutations (matches) is equivalent to treating every permutation as\n  a distinct event, which is not necessary when the output order is\n  ignored.  It is not something that is asked for in this\n  problem setting.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "BkgusNjaam", "original": null, "number": 6, "cdate": 1542464671548, "ddate": null, "tcdate": 1542464671548, "tmdate": 1542464671548, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "BJlQWHaL67", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "some improvements to clarity", "comment": "The authors have clarified a number of the confusing points; however, I'm still not satisfied with the objective: the representation of the logsumexp that comes after the proof on page 3, namely the manipulations done in equation (4)."}, "signatures": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "BJlQWHaL67", "original": null, "number": 4, "cdate": 1542014202720, "ddate": null, "tcdate": 1542014202720, "tmdate": 1542014202720, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "S1lS16Ohsm", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "reply", "comment": "\n  > only sets of equal size are under consideration ...\n\n  See the overall reply. It is not only for sets of equal size.\n\n\n  > The logsumexp formulation of the proposed measure is unsatisfactory\n  > to me as it directly averages over each of the independent\n  > probabilities that a given element is a member of the target set,\n  > rather than integrating over the combinatorial set of probabilities\n  > for each set of complete possible matches.\n\n  We have shown (the proof in page.3, sec.3, par.7) that our method\n  guarantees that, at the global minima, every element x of the dataset\n  X is matched by some element y of the output Y exactly once, just as\n  in Hausdorff measure.\n\n  We would like to hear more details about why it is unsatisfactory despite the guarantee.\n\n  > The definition of the Hausdorff distance given is directional and is\n  > therefore not a metric, contrary to what is stated on page 2.\n\n  We already clarified this in page 2: \"Note that, in this work, we use\n  the informal usage of the terms \u201cdistance\u201d or \u201cmetric\u201d ...\"  We\n  also only stated that Hausdorff distance is a metric, and not that the\n  directed Hausdorff distance is a metric.  We moved the clarification\n  to the beginning of the section to avoid confusion.\n\n  > the space [0,1]^NxF is described as binary...\n\n  See the overall reply. [0,1] is a closed set of reals, not discrete\n  values.  The input is not assumed to be discrete. \"binomial\n  distribution\" might be the more appropriate term. We rephrased them in the revision.\n\n  > what is the objective of each task and what are the 'ingredients' we\n    begin with.\n\n  The detailed descriptions are in the appendix.  The first two tasks (8\n  puzzle, Blocksworld) are the autoencoding task, but without\n  considering the ordering in the first axis of the data point (i.e. the order of the elements).\n  In 8-puzzle, the object representation (an element of the set) is\n  hand-crafted as in Fig.1.  In Blocksworld, a set of objects are\n  extracted from the image, and the image patch and the bounding box\n  information are compressed into 1224-D vector by a feature engineering\n  using Conv-AE (Appendix, 6.3).\n\n  The last task (rule learning task) is to predict a set of terms from a\n  single term, where each term is a n-hot vector representing a\n  first-order logic term (Section 4.2, Appendix 6.4).  We moved the\n  description in the appendix to the main text.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "r1g0z4pU6m", "original": null, "number": 3, "cdate": 1542013974039, "ddate": null, "tcdate": 1542013974039, "tmdate": 1542013974039, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "SklYVU2hs7", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "Overall comment", "comment": "  > The use of log likelihoods to metrize distances between sets,\n    although not new, ...\n\n  Log likelihood itself is a general notion and is not new.  However,\n  our contribution is formulating the log likelihood for sets, with a\n  proof that it converges to the correct answer at the global minima.\n\n  > The question of comparing sets of different sample sizes would be a\n  > valuable extension to the work.\n\n  See the overall reply. We added some explanations and also a\n  new experiment that shows that it can model the sets with the different number of elements.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "BygnoGpIpX", "original": null, "number": 2, "cdate": 1542013603879, "ddate": null, "tcdate": 1542013603879, "tmdate": 1542013716737, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "HJglfr-CiQ", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "Overall reply", "comment": "  > Actually, I\u2019ve never seen cross entropy written with P(x=y).\n\n  The usual notation of the probability distribution, such as P(x), is\n  an abbreviation of a random variable X taking a particular value x,\n  i.e. P(X=x), following the notation in Ian Goodfellow's Deep Learning\n  textbook, chap.3\n  ([http://www.deeplearningbook.org/contents/prob.html]). We just kept\n  it expanded for denoting the cross entropy.\n\n  > The phrase \u201cThe purpose of this experiment is to reproduce the\n  > results from (Zaheer et al., 2017)\u201d makes little sense to me.\n  > In Deep Sets, there are many experiments and it\u2019s not clear which\n  > experiment is meant here.\n\n  By \"reproduce\" we did not mean we run the same experiment in the prior\n  work; What we did is that we confirmed their general claim (their\n  particular network structure is able to encode an input in a\n  permutation invariant manner) in *our* experimental setting.\n\n  We rephrased it in the revision in order to avoid the confusion.\n\n  > Table 1 gives test error statistics for 10 runs. What is changed in\n    every run?\n\n  The purpose of running the experiments 10 times is to address the\n  potential concern about the stability of the training.\n\n  We kept the same set of training/testing data, the only difference is\n  the random seed.  In one of the 10 runs, A1H (set average pseudo\n  metric) did not converge, showing that the A1H (baseline) could be\n  unstable, possibly due to the issue explained in the example at the\n  end of section 3. # , though this is a speculation from the empirical\n  result.  This shows another empirical evidence that our proposed\n  method is superior.  We clarified these points in the revision.\n\n  > Blocksworld: the reconstructions are nice, but the numbers in Table\n    2 are difficult to interpret.\n\n  We agree with this point. To address it, we added the RMSE between the\n  visualized pictures to give more insights.  Note that these pictures\n  are not the direct output of the neural network; However, comparing\n  the reconstructed pictures by RMSE should give some intuitive sense\n  since the error directly translates to the pixel value.  A new table\n  is added in the revision.\n\n  Furthermore, we compared the visualized results between the networks\n  trained with a different loss formulation.\n\n  Since we believe the same issue applies to 8-puzzles, we also added a\n  new evaluation metric for 8-puzzles: Since we know 8 puzzle feature\n  vectors are discrete (a domain knowledge, not the assumption in our\n  proposed method), we can directly compare the output reconstruction\n  with the input by rounding the continuous output to 0/1 and comparing\n  whether all elements are correctly reconstructed, and count the rate\n  of the successful reconstructions across the dataset.  Another new\n  table is added in the revision.\n\n  > Rule learning ILP tasks: the concept of 10 runs is still unclear.\n\n  This is same as the previous experiments; The only difference is the\n  random seed.\n\n  > I think an important goal of any autoencoder is to learn a\n  > representation that can be useful in other tasks. There is even an\n  > example in the paper: \u201cset representation of the environment is\n  > crucial in the robotic systems\u201d. Thus, the experiments I would like\n  > to see are about evaluating the quality of a representation from an\n  > SCE-trained autoencoder compared to other training methods.\n  > Without those experiments, I cannot estimate how valuable the SCE\n  > loss function is.\n\n  We were surprised by this question.\n\n  First, we do not focus only on the autoencoding task. In the ILP task,\n  the neural network learns to predict a set from the single\n  element. For example, in the last `neighbor5` experiment in Table 6,\n  the task is to predict 5 elements from 1 element.  Our contribution is\n  the method for training a NN to output a set, not limited\n  to autoencoding.\n\n  Regarding the value of reconstructing a set, existing work (Vinyal\n  NIPS 2015, ICLR 2016) already showed that once we can train a NN to\n  output a set (by whatever means), it allows a variety of tasks to be\n  solved.  The problem is that existing methods relied on an ad-hoc\n  preprocessing and/or a careful tuning that depends on the domain\n  knowledge, or a sequential process such as Gale-Shapley.\n  Our contribution is to completely remove these assumptions motivated\n  by the theoretical formulation, NOT by an empirical success\n  that may occur by chance in a particualr problem setting.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "rkxJ7-a8a7", "original": null, "number": 1, "cdate": 1542013207185, "ddate": null, "tcdate": 1542013207185, "tmdate": 1542013207185, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "content": {"title": "Overall reply to all reviewers", "comment": "Thanks very much for the thoughtful comments.  We first discuss the\ngeneral response to the reviewers, then in the threads we reply to the\nspecific concerns raised by each reviewer.\n\nWhile ICLR may not mandate it, we would appreciate it if you confirm in\nthe reply that you read all of the rebuttals, including the replies to\nthe other reviewers.\n\nTo Reviewer #1 and #3:\n\nThe input is *not* assumed to be discrete.  [0,1] is meant to be a\nclosed set of reals between 0 and 1 (while an open set is denoted as\n(0,1)).  We clarified this in the revision.  By \"binary representation\"\nin page 3, we meant a binomial probability distribution (thus can be a\ncontinuous value) in contrast to the multinomial\nprobability distribution.\n\nIn fact, each input feature vector in the Blocksworld experiment are\ngenerated from the individual image patches by an additional autoencoder\nwith a sigmoid latent activation (Appendix 6.3, Figure 4), thus they are\nin fact the continuous vectors.\n\nTo Reviewer #2 and #3:\n\n> #2 The question of comparing sets of different sample sizes would be a\n> valuable extension to the work.\n\n> #3 only sets of equal size are under consideration ...\n\n(Sec 3, paragraph 3) When the size of the set varies, we can add an\narbitrary number of artificially generated distinct dummy elements to\nthe set in order to keep the size of the set equal throughout\nthe dataset.\n\nFor example, when there is a set of N objects of F features and we want\nto normalize the size of the set to N' (>N), one way is to add an\nadditional axis to the feature vector (F+1 features) where the\nadditional F+1-th feature is 0 for the real data and 1 for the dummy\ndata, and the distinct N'-N objects are generated in an arbitrary way\n(e.g. as a binary sequence 100000, 100001, 100010, 100011, ... for F=5).\nDuring the inference, the dummy vectors in the output can be removed.\n\nWe added this explanation in the revision.\n\nThe ability to learn from such an augmented dataset is only a matter of\nthe neural network topology and the hyperparameter tuning, which is not\nthe main topic of this paper.  Whether using an LSTM, a CNN or a fully\nconnected network does not affect the loss formulation presented in this\nwork.  Given enough capability for tuning and network engineering (which\nincludes the choise of NN), they should also be able to learn from such\nan augmented dataset, as the augmented dataset has the\nsame characteristics.\n\nHowever, to address this concern, we ran an additional experiment where\nsome of the tiles are randomly missing in 8 puzzles.  The feature\nvectors are extended from 15 to 16 dimensions and the dummy elements are\ninserted as explained above.  The system reconstructs the elements\nincluding the dummy ones, which means it can identify how many and which\nelements are missing for the given augmented output. Moreover, the\nproposed Set Cross Entropy outperformed Set Average and other loss\nfunctions.  See Section 4.1 for the additional details and the results.\n\nFinally, we noticed that the evaluation metric for the\nILP task was not explained properly; Originally, we wrote:\n\n> We counted the ratio of the clauses across the test set where every\n  output term matches against one of the body terms.\n\nThis is wrong: It is an improper way to measure the correct matching, as\nthere could be duplicates.  Our implementation did not measured the\nsuccess rate in this way.  The correct description that reflects our\nimplementation is:\n\n> We counted the ratio of the clauses across the test set where every\n  *body* term matches against one of the *output* terms.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607583, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJxpuoCqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper396/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper396/Authors|ICLR.cc/2019/Conference/Paper396/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers", "ICLR.cc/2019/Conference/Paper396/Authors", "ICLR.cc/2019/Conference/Paper396/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607583}}}, {"id": "HJglfr-CiQ", "original": null, "number": 3, "cdate": 1540392200421, "ddate": null, "tcdate": 1540392200421, "tmdate": 1541534030875, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Review", "content": {"title": "new loss function for set autoencoders; experiments are not sufficient", "review": "This paper proposes an objective function for sets autoencoders such that the loss is permutation invariant with respect to the order of reconstructed inputs. I think that the problem of autoencoding sets is important and designing custom loss functions is a good way to approach it. Thus, I quite like the idea of SCE  from that point of view. However, I find the experiments not convincing for me to accept the paper. \n\nWhile reading Section 3, I found it hard to keep in mind that x and y are discrete probability distributions and the notation like P(x=y) is not making things easier. Actually, I\u2019ve never seen cross entropy written with P(x=y). Though is my personal opinion and I don\u2019t have a suggestion on how to improve the explanations in Eq. 1-8. However, I\u2019m glad there is an example at the end of Section 3.\n\nI have some comments on the Experiments section. \n\n* Puzzles:\n(1) Figure 1 could have been prettier. \n(2) The phrase \u201cThe purpose of this experiment is to reproduce the results from (Zaheer et al., 2017)\u201d makes little sense to me.  In Deep Sets, there are many experiments and it\u2019s not clear which experiment is meant here.\n(3) Table 1 gives test error statistics for 10 runs. What is changed in every run? Does the test set stay the same in every run or is a kind of a cross-validation? Or is it just a different random seed for the initial weights? I could not find an explanation in the text, so there is no way I can interpret the results.\n\n* Blocksworld: the reconstructions are nice, but the numbers in Table 2 are difficult to interpret. \nFor example, I cannot estimate how important the difference of 10 points in SH scores is.\n\n* Rule learning ILP tasks: I don\u2019t know enough about learning logic rules tasks to comment on those experiments, but Table 3 seems overwhelming and the concept of 10 runs is still unclear.\n\n--- General comment on the experiments ---\n\nI think an important goal of any autoencoder is to learn a representation that can be useful in other tasks. There is even an example in the paper: \u201cset representation of the environment is crucial in the robotic systems\u201d. Thus, the experiments I would like to see are about evaluating the quality of a representation from an SCE-trained autoencoder compared to other training methods.  Without those experiments, I cannot estimate how valuable the SCE loss function is.  ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Review", "cdate": 1542234470934, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714641, "tmdate": 1552335714641, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklYVU2hs7", "original": null, "number": 2, "cdate": 1540306481123, "ddate": null, "tcdate": 1540306481123, "tmdate": 1541534030675, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Review", "content": {"title": "Extension to case of sets with different sample sizes?", "review": "The paper is understandable and the question addressed is interesting. The use of log likelihoods to metrize distances between sets, although not new, is used quite effectively to address the issue of label switching in sets. Although the run time is O(N^2), the metric can be computed in a parallelized manner. The question of comparing sets of different sample sizes would be a valuable extension to the work. Although I think the proposed loss function addresses some important issues, would like to defer the question of acceptance/rejection to other reviewers due to lack of expertise in related areas.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Review", "cdate": 1542234470934, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714641, "tmdate": 1552335714641, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1lS16Ohsm", "original": null, "number": 1, "cdate": 1540291805483, "ddate": null, "tcdate": 1540291805483, "tmdate": 1541534030472, "tddate": null, "forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper396/Official_Review", "content": {"title": "Lacks clarity", "review": "In the manuscript entitled \"Likelihood-based Permutation Invariant Loss Function for Probability Distributions\" the authors propose a loss function for training against instances in which ordering within the data vector is unimportant.  I do not find the proposed loss function to be well motivated, find a number of confusing points (errors?) in the manuscript, and do not easily follow what was done in the examples.\n\nFirst, it should be noted that this is a very restricted consideration of what it means to compare two sets since only sets of equal size are under consideration; this is fundamentally different to the ambitions of e.g. the Hausdorff measure as used in analysis.  The logsumexp formulation of the proposed measure is unsatisfactory to me as it directly averages over each of the independent probabilities that a given element is a member of the target set, rather than integrating over the combinatorial set of probabilities for each set of complete possible matches.  Moreover, the loss function H() is not necessarily representative of a generative distribution.\n\nThe definition of the Hausdorff distance given is directional and is therefore not a metric, contrary to what is stated on page 2.\n\nI find the description of the problem domain confusing on page 3: the space [0,1]^NxF is described as binary, but then values of log y_i and log (1-y_i) are computed with y in [0,1] so we must imagine these are in fact elements in the open set of reals: (0,1).\n\nClarity of the examples could be greatly improved, in particular by explaining precisely what is the objective of each task and what are the 'ingredients' we begin with.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper396/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Likelihood-based Permutation Invariant Loss Function for Probability Distributions", "abstract": "We propose a permutation-invariant loss function designed for the neural networks reconstructing a set of elements without considering the order within its vector representation. Unlike popular approaches for encoding and decoding a set, our work does not rely on a carefully engineered network topology nor by any additional sequential algorithm. The proposed method, Set Cross Entropy, has a natural information-theoretic interpretation and is related to the metrics defined for sets. We evaluate the proposed approach in two object reconstruction tasks and a rule learning task.", "keywords": ["Set reconstruction", "maximum likelihood", "permutation invariance"], "authorids": ["masataro.asai@ibm.com"], "authors": ["Masataro Asai"], "TL;DR": "The proposed method, Set Cross Entropy, measures the information-theoretic similarity of sets in a permutation-invariant manner.", "pdf": "/pdf/fa484e5647445a2ab14945244ba698ffa29febb1.pdf", "paperhash": "asai|likelihoodbased_permutation_invariant_loss_function_for_probability_distributions", "_bibtex": "@misc{\nasai2019likelihoodbased,\ntitle={Likelihood-based Permutation Invariant Loss Function for Probability Distributions},\nauthor={Masataro Asai},\nyear={2019},\nurl={https://openreview.net/forum?id=rJxpuoCqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper396/Official_Review", "cdate": 1542234470934, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJxpuoCqtQ", "replyto": "rJxpuoCqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper396/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714641, "tmdate": 1552335714641, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper396/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 16}