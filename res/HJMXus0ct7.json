{"notes": [{"id": "HJMXus0ct7", "original": "r1erddFqF7", "number": 341, "cdate": 1538087787188, "ddate": null, "tcdate": 1538087787188, "tmdate": 1545355422656, "tddate": null, "forum": "HJMXus0ct7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "iRDA Method for Sparse Convolutional Neural Networks", "abstract": "We propose a new approach, known as the iterative regularized dual averaging (iRDA), to improve the efficiency of convolutional neural networks (CNN) by significantly reducing the redundancy of the model without reducing its accuracy.  The method has been tested for various data sets, and proven to be significantly more efficient than most existing compressing techniques in the deep learning literature.  For many popular data sets such as MNIST and CIFAR-10, more than 95% of the weights can be zeroed out without losing accuracy. In particular, we are able to make ResNet18 with 95% sparsity to have an accuracy that is comparable to that of a much larger model ResNet50 with the best 60% sparsity as reported in the literature.", "keywords": ["sparse convolutional neural networks", "regularized dual averaging"], "authorids": ["jiaxiaodong1994@gmail.com", "zhaoliang14@lsec.cc.ac.cn", "lzhangay@ust.hk", "juncaihe@pku.edu.cn", "xu@math.psu.edu"], "authors": ["Xiaodong Jia", "Liang Zhao", "Lian Zhang", "Juncai He", "Jinchao Xu"], "TL;DR": "A sparse optimization algorithm for deep CNN models.", "pdf": "/pdf/f780ef1735a07486e81b419351a80040b7c51fc7.pdf", "paperhash": "jia|irda_method_for_sparse_convolutional_neural_networks", "_bibtex": "@misc{\njia2019irda,\ntitle={i{RDA} Method for Sparse Convolutional Neural Networks},\nauthor={Xiaodong Jia and Liang Zhao and Lian Zhang and Juncai He and Jinchao Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMXus0ct7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SylJxelmlV", "original": null, "number": 1, "cdate": 1544908774713, "ddate": null, "tcdate": 1544908774713, "tmdate": 1545354492808, "tddate": null, "forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper341/Meta_Review", "content": {"metareview": "This paper proposes an \u201citerative\u201d regularized dual averaging method to sparsify CNN weights during learning. The main contribution seems to be in an iterative procedure where the weights are pruned out greedily by observing the sparsity of the averaged gradients. The reviewers agree that the idea seems straightforward and novelty is limited. For this reason, I recommend to reject this paper.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Limited novelty."}, "signatures": ["ICLR.cc/2019/Conference/Paper341/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper341/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "iRDA Method for Sparse Convolutional Neural Networks", "abstract": "We propose a new approach, known as the iterative regularized dual averaging (iRDA), to improve the efficiency of convolutional neural networks (CNN) by significantly reducing the redundancy of the model without reducing its accuracy.  The method has been tested for various data sets, and proven to be significantly more efficient than most existing compressing techniques in the deep learning literature.  For many popular data sets such as MNIST and CIFAR-10, more than 95% of the weights can be zeroed out without losing accuracy. In particular, we are able to make ResNet18 with 95% sparsity to have an accuracy that is comparable to that of a much larger model ResNet50 with the best 60% sparsity as reported in the literature.", "keywords": ["sparse convolutional neural networks", "regularized dual averaging"], "authorids": ["jiaxiaodong1994@gmail.com", "zhaoliang14@lsec.cc.ac.cn", "lzhangay@ust.hk", "juncaihe@pku.edu.cn", "xu@math.psu.edu"], "authors": ["Xiaodong Jia", "Liang Zhao", "Lian Zhang", "Juncai He", "Jinchao Xu"], "TL;DR": "A sparse optimization algorithm for deep CNN models.", "pdf": "/pdf/f780ef1735a07486e81b419351a80040b7c51fc7.pdf", "paperhash": "jia|irda_method_for_sparse_convolutional_neural_networks", "_bibtex": "@misc{\njia2019irda,\ntitle={i{RDA} Method for Sparse Convolutional Neural Networks},\nauthor={Xiaodong Jia and Liang Zhao and Lian Zhang and Juncai He and Jinchao Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMXus0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper341/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353250895, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper341/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper341/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper341/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353250895}}}, {"id": "HJxBByxuaX", "original": null, "number": 3, "cdate": 1542090556883, "ddate": null, "tcdate": 1542090556883, "tmdate": 1542090556883, "tddate": null, "forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper341/Official_Review", "content": {"title": "The paper is not written well and needs major modifications. The contribution of the paper which is analyzing RDA with arbitrary init point is a small incremental contribution. ", "review": "iRDA Method for sparse convolutional neural networks \n\nThis paper considers the problem of training a sparse neural network. The main motivation is that usually all state of the art neural network\u2019s size or the number of weights is enormous and saving them in memory is costly. So it would be of great interest to train a sparse neural network. To do so, this paper proposed adding l1 regularizer to RDA method in order to encourage sparsity throughout training. Furthermore, they add an extra phase to  RAD algorithm where they set the stochastic gradient of zero weights to be zero. They show experimentally that the method could give up to 95% sparsity while keeping the accuracy at an acceptable level. \nMore detail comments: \n\n1- In your analysis for the convergence, you totally ignored the second step. How do you show that with the second step still the method converge? \n\n2- \\bar{w} which is used in the thm 1, is not introduced. \n\n3- In eq 5, you say g_t is subfunction. What is it? \n\n4- When does the algorithm switch from step 1 to step 2? \n\n5- In eq 35 what is \\sigma? \n\n6- What is the relation between eq 23 and 24? The paper says 23 is an approximation for 24 but the result of 23 is a point and 24 is a function. \n\n7- What is MRDA in the Fig 1? \n\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper341/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "iRDA Method for Sparse Convolutional Neural Networks", "abstract": "We propose a new approach, known as the iterative regularized dual averaging (iRDA), to improve the efficiency of convolutional neural networks (CNN) by significantly reducing the redundancy of the model without reducing its accuracy.  The method has been tested for various data sets, and proven to be significantly more efficient than most existing compressing techniques in the deep learning literature.  For many popular data sets such as MNIST and CIFAR-10, more than 95% of the weights can be zeroed out without losing accuracy. In particular, we are able to make ResNet18 with 95% sparsity to have an accuracy that is comparable to that of a much larger model ResNet50 with the best 60% sparsity as reported in the literature.", "keywords": ["sparse convolutional neural networks", "regularized dual averaging"], "authorids": ["jiaxiaodong1994@gmail.com", "zhaoliang14@lsec.cc.ac.cn", "lzhangay@ust.hk", "juncaihe@pku.edu.cn", "xu@math.psu.edu"], "authors": ["Xiaodong Jia", "Liang Zhao", "Lian Zhang", "Juncai He", "Jinchao Xu"], "TL;DR": "A sparse optimization algorithm for deep CNN models.", "pdf": "/pdf/f780ef1735a07486e81b419351a80040b7c51fc7.pdf", "paperhash": "jia|irda_method_for_sparse_convolutional_neural_networks", "_bibtex": "@misc{\njia2019irda,\ntitle={i{RDA} Method for Sparse Convolutional Neural Networks},\nauthor={Xiaodong Jia and Liang Zhao and Lian Zhang and Juncai He and Jinchao Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMXus0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper341/Official_Review", "cdate": 1542234483284, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper341/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335702662, "tmdate": 1552335702662, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper341/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklGAR9ah7", "original": null, "number": 2, "cdate": 1541414602487, "ddate": null, "tcdate": 1541414602487, "tmdate": 1541534076967, "tddate": null, "forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper341/Official_Review", "content": {"title": "Algorithm and presentation are flawed", "review": "The submission made a few modifications to the RDA (regularized dual averaging) optimization solver to form the proposed \"iterative RDA (iRDA)\" algorithm, and shows that empirically the proposed algorithm could  reduce the number of non-zero parameters by an order of magnitude on CIFAR10 for a number of benchmark network architectures (Resnet18, VGG16, VGG19).\n\nThe experimental result of the paper is strong but the algorithm and also a couple of statements seem flawed. In particular:\n\n* For Algorithm 1, consider the case when lamda=0 and t -> infinity,  the minimization eq (28) goes to negative infinity for any non-zero gradient, which corresponds to an update of infinitely large step size. It seems something is wrong.\n\n* Why in Step 2 the algorithm sets both g_t and w_t to 0 during each iterate? It looks so wrong.\n\n*The whole paper did not mention batch size even once. Does the algorithm apply only with batch size=1? \n\n*What is the \"MRDA\" method in the figure? Is it mentioned anywhere in the paper?\n\n*What are \"k\", \"c\"  in eq (25)? Are they defined anywhere in the paper?\n\n*Theorem states 1/sqrt(t) convergence but eq (28), (31) have updates of unbounded step size. How is this possible?", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper341/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "iRDA Method for Sparse Convolutional Neural Networks", "abstract": "We propose a new approach, known as the iterative regularized dual averaging (iRDA), to improve the efficiency of convolutional neural networks (CNN) by significantly reducing the redundancy of the model without reducing its accuracy.  The method has been tested for various data sets, and proven to be significantly more efficient than most existing compressing techniques in the deep learning literature.  For many popular data sets such as MNIST and CIFAR-10, more than 95% of the weights can be zeroed out without losing accuracy. In particular, we are able to make ResNet18 with 95% sparsity to have an accuracy that is comparable to that of a much larger model ResNet50 with the best 60% sparsity as reported in the literature.", "keywords": ["sparse convolutional neural networks", "regularized dual averaging"], "authorids": ["jiaxiaodong1994@gmail.com", "zhaoliang14@lsec.cc.ac.cn", "lzhangay@ust.hk", "juncaihe@pku.edu.cn", "xu@math.psu.edu"], "authors": ["Xiaodong Jia", "Liang Zhao", "Lian Zhang", "Juncai He", "Jinchao Xu"], "TL;DR": "A sparse optimization algorithm for deep CNN models.", "pdf": "/pdf/f780ef1735a07486e81b419351a80040b7c51fc7.pdf", "paperhash": "jia|irda_method_for_sparse_convolutional_neural_networks", "_bibtex": "@misc{\njia2019irda,\ntitle={i{RDA} Method for Sparse Convolutional Neural Networks},\nauthor={Xiaodong Jia and Liang Zhao and Lian Zhang and Juncai He and Jinchao Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMXus0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper341/Official_Review", "cdate": 1542234483284, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper341/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335702662, "tmdate": 1552335702662, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper341/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJetJMY9hQ", "original": null, "number": 1, "cdate": 1541210592915, "ddate": null, "tcdate": 1541210592915, "tmdate": 1541534076757, "tddate": null, "forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "invitation": "ICLR.cc/2019/Conference/-/Paper341/Official_Review", "content": {"title": "Not enough novelty", "review": "This paper claims to propose a new iRDA method. Essentially, it is just dual averaging with \\ell_1 penalty and an \\ell_2 proximal term. The O(1/\\sqrt{t}) rate is standard in literature. This is a clear rejection.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper341/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "iRDA Method for Sparse Convolutional Neural Networks", "abstract": "We propose a new approach, known as the iterative regularized dual averaging (iRDA), to improve the efficiency of convolutional neural networks (CNN) by significantly reducing the redundancy of the model without reducing its accuracy.  The method has been tested for various data sets, and proven to be significantly more efficient than most existing compressing techniques in the deep learning literature.  For many popular data sets such as MNIST and CIFAR-10, more than 95% of the weights can be zeroed out without losing accuracy. In particular, we are able to make ResNet18 with 95% sparsity to have an accuracy that is comparable to that of a much larger model ResNet50 with the best 60% sparsity as reported in the literature.", "keywords": ["sparse convolutional neural networks", "regularized dual averaging"], "authorids": ["jiaxiaodong1994@gmail.com", "zhaoliang14@lsec.cc.ac.cn", "lzhangay@ust.hk", "juncaihe@pku.edu.cn", "xu@math.psu.edu"], "authors": ["Xiaodong Jia", "Liang Zhao", "Lian Zhang", "Juncai He", "Jinchao Xu"], "TL;DR": "A sparse optimization algorithm for deep CNN models.", "pdf": "/pdf/f780ef1735a07486e81b419351a80040b7c51fc7.pdf", "paperhash": "jia|irda_method_for_sparse_convolutional_neural_networks", "_bibtex": "@misc{\njia2019irda,\ntitle={i{RDA} Method for Sparse Convolutional Neural Networks},\nauthor={Xiaodong Jia and Liang Zhao and Lian Zhang and Juncai He and Jinchao Xu},\nyear={2019},\nurl={https://openreview.net/forum?id=HJMXus0ct7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper341/Official_Review", "cdate": 1542234483284, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJMXus0ct7", "replyto": "HJMXus0ct7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper341/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335702662, "tmdate": 1552335702662, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper341/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}