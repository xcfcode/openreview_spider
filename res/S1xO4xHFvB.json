{"notes": [{"id": "S1xO4xHFvB", "original": "SylURUeYPH", "number": 2253, "cdate": 1569439791737, "ddate": null, "tcdate": 1569439791737, "tmdate": 1577168277744, "tddate": null, "forum": "S1xO4xHFvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fflH5pKQq", "original": null, "number": 1, "cdate": 1576798744429, "ddate": null, "tcdate": 1576798744429, "tmdate": 1576800891733, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposed a very general idea called Atomic Compression Networks (ACNs) to construct neural networks. The idea looks simple and effective.  However, the reason why it works is not well explained.  The experiments are not sufficient enough to convince the reviewers.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795707874, "tmdate": 1576800256159, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Decision"}}}, {"id": "rke2IBwPtS", "original": null, "number": 1, "cdate": 1571415380376, "ddate": null, "tcdate": 1571415380376, "tmdate": 1573918362332, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "This paper explores the use of replicating neurons across and within layers to compress fully connected neural networks. The idea is simple, and is evaluated on a number of datasets and compared with fully connected, single layer, and several compression schemes. \n\nStrengths: a lot of nice experiments with clearly advantageous results are given.\n\nWeaknesses: One obvious baseline missing is sparse compression, which can be achieved using either l1 regularization, or hard thresholding + fine tuning, both of which are easy to implement and appear in several works, e.g.\n\nScalable Neural Network Compression and Pruning Using Hard Clustering and L1 Regularization (Yang, Ruozzi, Gogate)\nTraining skinny deep neural networks with iterative hard thresholding methods (Yin, Yuan, Feng, Yan)\n\n... many others just via googling ... \n\nAlso, I think this work should be compared with compression schemes that work via kronecker product, which seem very similar to this scheme (but where the kronecker matrix is binary to produce replication)\n\nCompression of Fully-Connected Layer in Neural Network by Kronecker Product (Zhou, Wu)\n(more via google)\n\nOne obvious advantage of replication over kronecker product is lower complexity, but nonetheless, the methods belong in a similar family.\n\nOtherwise, I think the work makes sense, the idea is nice, and the results show promise!\n\nAfter rebuttal: I have read the rebuttal and the authors have basically addressed all my concerns. It is a bit disappointing that simple L1 regularization can give competitive results, but the fact that the authors are willing to do the experiment and incorporate the results convinces me that there's nothing being hidden here, and the reader can make a fair and informed conclusion, so I have no more complaints.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2253/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2253/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783640111, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2253/Reviewers"], "noninvitees": [], "tcdate": 1570237725492, "tmdate": 1574783640127, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Official_Review"}}}, {"id": "H1eZ4aN2jS", "original": null, "number": 4, "cdate": 1573829929307, "ddate": null, "tcdate": 1573829929307, "tmdate": 1573829929307, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment", "content": {"title": "Updated Version for Rebuttal", "comment": "Many thanks to all the reviewers for providing valuable feedback and insights regarding our work.\n\nConsequently we updated our paper to clarify the following key points in the review: \n\n> Reviewer 2 rightfully proposed the additional comparison to sparsification methods employing L1-regularization and hard-thresholding. We added the respective experiment results as table 6 in the appendix. Furthermore we added the paper regarding compression with the Kronecker product to our related work.\n\n> To empasize the importance of model compression which Reviewer 1 legitimately questioned, we added more respective references to our motivation in the introduction. \n\n> In response to the points correctly brought up by Reviewer 3  we more extensively elaborate on the intuition behind the general model idea and the observed effects and competitive results. Furthermore we clarify the points regarding algorithm 1.\n\nBesides the aforementioned points, we answered the raised questions in more detail in the direct comments to the reviews. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xO4xHFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2253/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2253/Authors|ICLR.cc/2020/Conference/Paper2253/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144090, "tmdate": 1576860536419, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment"}}}, {"id": "ryguvqJjiB", "original": null, "number": 1, "cdate": 1573743199543, "ddate": null, "tcdate": 1573743199543, "tmdate": 1573773446194, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "SJgIDBwJ9B", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment", "content": {"title": "Answers to clarification questions", "comment": "Thank you for your time and the thorough evaluation of our paper. \n\nIn the following we want to clarify the points brought up in your review:\n\n1) There is no delta missing. Line 6 in Algorithm 1 is meant as 2 consecutive lines since we return the two sets m and delta at the end of the alogirthm. We will separate it to make it more clear.\nHowever as you suggest it would also be possible to absorb the mask delta into m_i and only return m.\n\n2) No, we do not select the best sample architecture. In the experiments we run Algorithm 1 only once per seed for 3 seeds and average the performance of all 3 resulting architectures. We do not apply any selection regarding the sampled architectures. All other models are also initialized, trained and evaluated over 3 different seeds and the results are averaged as well. \n\n3) To motivate the benefit of recursively repeating neurons, we would like to present an example from function composition. Let us consider a simple function $f(x) = (\\alpha x +\\beta)^2, f: \\mathbb{R} \\rightarrow \\mathbb{R}$ which has only two parameters $\\alpha, \\beta$. By applying the composition $f(f(x)) = \\left(  \\alpha^3 x^2 + 2 \\alpha^2 \\beta x + \\alpha \\beta^2 + \\beta \\right)^2$ we get a more complex function, but still having just two parameters $\\alpha, \\beta$. We can keep composing $f(\\dots f( \\dots f(x)))$ and achieve a very complex function, yet with only two parameters. Notice that the intuition of repeating neurons is equivalent to that of achieving a higher non-linear expressivity by composing functions, for instance composing a set of functions $f(x), g(x), h(x), \\dots$ yields very deep representations, e.g. $f(g(f(h(g(h(f(x)))))$. Please consider that each $f, g, h$ can be a neuron, therefore our atomic networks are special cases of recursive function compositions from a set of base functions (a.k.a. repeating neurons in our paper). In our assessment, we are the first to consider adding non-linear expressivity by recursively applying the same set of neurons (a.k.a. functions).\n\nTherefore ACN achieves much deeper architectures with the same number of parameters compared to a standard FCN, what could further improve the fitting capability. Finally we see the expected trend that the fit for both models increases respectively when increasing the number of parameters when going from left to right in both rows of figure 2.\n\n4) The main focus of this work is showing the advantage of ACN compared to MLP baselines on vector data. The image datasets were added for experimental diversity as special case of high dimensional vector data (the images were flattened) with an explicit structure. In the same way as MLPs, ACNs are not able to levarage the spatial information in image data compared to a specialized architecture like ConvNets. Furthermore, although ConvNets share parameters of their filters over the image, they do not share parameters between layers. Since the extension of the underlying idea of our ACNs to ConvNets is not feasible within the short time of the rebuttal periode, we plan to explore that direction in future work.\n\n5) In our experiments we follow the established trend of comparing the compression rate w.r.t. a large standard model. However contrary to most work we also introduce a small, tuned FCN of comparable size to compressed networks, which is shown to be a very strong baseline [2].\nIn general our experiments confirm the findings of [1], that with a very large and comprehensive hyperparameter search including the general network architecture, one can find very shallow and small FC networks which perform on par or even better than most networks produced by compression techniques. The inherent advantage of the compression techniques is however that in most cases they lead more reliably and with less computational effort to relatively small and well performing architectures. \nThe 528 times is a typo in the text, it should be 218 times as reported in table 2. Furthermore the compression rates are achieved on different datasets e.g. the 1115 times of ACN compared to 133 of small FC on the internetAds dataset.\n\n[1] Liu, Zhuang, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. 2018. \u201cRethinking the Value of Network Pruning.\u201d ArXiv:1810.05270 [Cs, Stat], October. http://arxiv.org/abs/1810.05270.\n\n[2] Chen, Wenlin, James Wilson, Stephen Tyree, Kilian Weinberger, and Yixin Chen. \"Compressing neural networks with the hashing trick.\" In International Conference on Machine Learning, pp. 2285-2294. 2015."}, "signatures": ["ICLR.cc/2020/Conference/Paper2253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xO4xHFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2253/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2253/Authors|ICLR.cc/2020/Conference/Paper2253/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144090, "tmdate": 1576860536419, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment"}}}, {"id": "Bklvp51joB", "original": null, "number": 2, "cdate": 1573743294808, "ddate": null, "tcdate": 1573743294808, "tmdate": 1573772607528, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "ByxlTSdttS", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment", "content": {"title": "Answers to clarification questions", "comment": "Thank you for your time and the valuable feedback and insights regarding our paper. \n\nWe would like to clarify some points mentioned in your review:\n\n> \u201eIs it just randomly constructed network also perform well?\u201c\n\n1) No, naively randomly constructed networks do not perform well, as can be seen with the RER [1] baseline (section 4.2.2, figure 3 and table 5 in the appendix). The proposed method works well compared to the other baselines because the special weight sharing architecture enables the model to use the available capacity given by the number of its parameters more efficiently. As we show in the experimental section this applies to randomly constructed networks, where the shared weights are trained end-to-end what leads to an effective fine-tuned collective network. However we want to emphasize that only the distribution and connections of the neurons are random, while the number of layers and number of neurons per layer is predefined. Furthermore as we point out in the conclusion, the proposed method could be combined with smarter approaches to construct even more powerful networks, e.g. by using  NAS methods (cp. [7]).\n\n\n> \u201eThe model size is small, but in what cases this small model size matters?\u201c\n\n2) The small model size achieved by the presented methods matters in different theoretical and real world scenarios. An increasing number of recent publications are concerned with network compression approaches to improve scalability and minimize the required and utilized memory of originally huge models to run them on edge devices with restricted resources (IoT devices, smartphones, etc.) [2,3,4,5,6].\n\n\n> \u201eIs this a reliable way to create useful models?\u201c\n\n3) The random construction of ACN is reliable and produces useful models, what is demonstrated by the reasonable variances shown in table 5 in the appendix. In the performed experiments on 9 diverse real world datasets with a different number of instances, features and classes as well as on 3 image datasets, the results show that the performance and gains of the proposed method are significant.\n\n\n> \u201eOn page 7, in Figure 3, why logistic regression only has a single point in some of the plots?\u201c\n\n4) Since logistic regression has a constant number of parameters and in figure 3 we compare models for different numbers of parameters, there can only be one point for logistic regression in all plots.\n\n\n\n[1] Cire\u015fan, Dan C., Ueli Meier, Jonathan Masci, Luca M. Gambardella, and J\u00fcrgen Schmidhuber. \"High-performance neural networks for visual object classification.\" arXiv preprint arXiv:1102.0183 (2011).\n\n[2] Cheng, Yu, Duo Wang, Pan Zhou, and Tao Zhang. 2017. \u201cA Survey of Model Compression and Acceleration for Deep Neural Networks.\u201d ArXiv:1710.09282 [Cs], October. http://arxiv.org/abs/1710.09282.\n\n[3] Kim, Yong-Deok, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. 2015. \u201cCompression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications.\u201d ArXiv:1511.06530 [Cs], November. http://arxiv.org/abs/1511.06530.\n\n[4] Han, S., X. Liu, H. Mao, J. Pu, A. Pedram, M. A. Horowitz, and W. J. Dally. 2016. \u201cEIE: Efficient Inference Engine on Compressed Deep Neural Network.\u201d In 2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA), 243\u201354. https://doi.org/10.1109/ISCA.2016.30.\n\n[5] Samie, Farzad, Vasileios Tsoutsouras, Lars Bauer, Sotirios Xydis, Dimitrios Soudris, and J\u00f6rg Henkel. 2016. \u201cComputation Offloading and Resource Allocation for Low-Power IoT Edge Devices.\u201d In 2016 IEEE 3rd World Forum on Internet of Things (WF-IoT), 7\u201312. https://doi.org/10.1109/WF-IoT.2016.7845499.\n\n[6] Mehta, Sachin, Mohammad Rastegari, Anat Caspi, Linda Shapiro, and Hannaneh Hajishirzi. 2018. \u201cESPNet: Efficient Spatial Pyramid of Dilated Convolutions for Semantic Segmentation.\u201d In Computer Vision \u2013 ECCV 2018, edited by Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, 11214:561\u201380. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-01249-6_34.\n\n[7] Elsken, Thomas, Jan Hendrik Metzen, and Frank Hutter. 2018. \u201cNeural Architecture Search: A Survey.\u201d ArXiv:1808.05377 [Cs, Stat], August. http://arxiv.org/abs/1808.05377.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xO4xHFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2253/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2253/Authors|ICLR.cc/2020/Conference/Paper2253/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144090, "tmdate": 1576860536419, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment"}}}, {"id": "SkxlZokior", "original": null, "number": 3, "cdate": 1573743352469, "ddate": null, "tcdate": 1573743352469, "tmdate": 1573743352469, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "rke2IBwPtS", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment", "content": {"title": "Answers to points brought up in review", "comment": "Thank you very much for the feedback and the positive evaluation of our paper. \n\n> \u201eOne obvious baseline missing is sparse compression...\u201c\n\n1) Regarding the sparse compression baseline we want to point out, that the Bayesian Compression baseline [1] in our paper is implicitly sparsifying the network. Furthermore the authors compare their method against the sparsifying variational dropout proposed by [2] and show that they achieve better results. \nWe performed some additional experiments employing simple L1 regularization and  L1 regularization combined with iterative hard thresholding (cp.  [3]) but without explicit cardinality constraint [4]. The results show that both methods in general perform a bit worse than the small FC baseline, beating our ACN in some of the cases where the small FC baseline is also stronger, especially for the two last parameter bins (with the highest number of parameters). However it doesn\u2018t change the overall impression and results. We will add the additional results to the appendix to clarify the points made.\n\n\t\tSparseL1\tSparseL1+HT\t\n\t\tAccuracy\tAccuracy\t\nhar\t\t\t\t\t\n< 500\t&\t0.000\t&\t0.000\t\\\\\n< 1000\t&\t0.181\t&\t0.193\t\\\\\n< 2500\t&\t0.181\t&\t0.959\t\\\\\n< 5000\t&\t0.981\t&\t0.967\t\\\\\n>= 5000&\t0.981\t&\t0.975\t\\\\\nnomao\t\t\t\t\t\n< 250\t&\t0.000\t&\t0.000\t\\\\\n< 500\t&\t0.000\t&\t0.000\t\\\\\n< 1000\t&\t0.718\t&\t0.718\t\\\\\n< 2500\t&\t0.952\t&\t0.949\t\\\\\n>= 2500&\t0.952\t&\t0.951\t\\\\\ninternetAds\t\t\t\t\t\n< 1000\t&\t0.916\t&\t0.913\t\\\\\n< 2500\t&\t0.916\t&\t0.966\t\\\\\n< 5000\t&\t0.969\t&\t0.966\t\\\\\n< 10000&\t0.977\t&\t0.966\t\\\\\n>= 10000&\t0.977\t&\t0.966\t\\\\\nisolet\t\t\t\t\t\n< 2500\t&\t0.033\t&\t0.488\t\\\\\n< 5000\t&\t0.033\t&\t0.933\t\\\\\n< 7500\t&\t0.938\t&\t0.933\t\\\\\n< 10000&\t0.938\t&\t0.953\t\\\\\n>= 10000&\t0.938\t&\t0.953\t\\\\\nspambase\t\t\t\t\t\n< 250\t&\t0.000\t&\t0.000\t\\\\\n< 500\t&\t0.000\t&\t0.000\t\\\\\n< 1000\t&\t0.733\t&\t0.739\t\\\\\n>= 1000&\t0.919\t&\t0.922\t\\\\\nsplice\t\t\t\t\t\n< 500\t&\t0.000\t&\t0.000\t\\\\\n< 1000\t&\t0.533\t&\t0.533\t\\\\\n< 2500\t&\t0.889\t&\t0.974\t\\\\\n< 5000\t&\t0.971\t&\t0.974\t\\\\\n>= 5000&\t0.976\t&\t0.977\t\\\\\ntheorem\t\t\t\t\t\n< 250\t&\t0.000\t&\t0.000\t\\\\\n< 500\t&\t0.000\t&\t0.000\t\\\\\n< 1000\t&\t0.422\t&\t0.422\t\\\\\n>= 1000&\t0.488\t&\t0.493\t\\\\\nbioresponse\t\t\t\t\t\n< 1000\t&\t0.548\t&\t0.548\t\\\\\n< 2500\t&\t0.548\t&\t0.777\t\\\\\n< 5000\t&\t0.767\t&\t0.791\t\\\\\n< 10000&\t0.788\t&\t0.791\t\\\\\n>= 10000&\t0.788\t&\t0.791\t\\\\\noptdigits\t\t\t\t\t\n< 250\t&\t0.000\t&\t0.000\t\\\\\n< 500\t&\t0.000\t&\t0.000\t\\\\\n< 750\t&\t0.000\t&\t0.104\t\\\\\n< 1000\t&\t0.104\t&\t0.961\t\\\\\n>= 1000&\t0.979\t&\t0.985\t\\\\\n\n> \u201e...this work should be compared with compression schemes that work via kronecker product, ...\u201c\n\n2) The proposed paper employing the Kronecker product is quite interesting. We will add it to the related work. Our 3rd baseline \u201eTensorNet\u201c [5] (see section 4.2.2) employs the Tensor-Train (TT) format [6]. The TT format is itself a special case of a Nested Kronecker Tensor Decomposition [7].\nFurthermore [5] is well known in the network compression literature and comes with available code which simplifies the experiments. Therefore we argue that the TensorNet baseline is a good representation of compression methods based on layer-wise matrix decomposition and low-rank approximations. Furthermore our model does not only focus on layer-wise decompositions but takes the whole (deep) network structure into account. \n\n\n\n[1] Louizos, Christos, Karen Ullrich, and Max Welling. 2017. \u201cBayesian Compression for Deep Learning.\u201d In Advances in Neural Information Processing Systems 30, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 3288\u20133298. Curran Associates, Inc. http://papers.nips.cc/paper/6921-bayesian-compression-for-deep-learning.pdf.\n\n[2] Molchanov, Dmitry, Arsenii Ashukha, and Dmitry Vetrov. 2017. \u201cVariational Dropout Sparsifies Deep Neural Networks.\u201d ArXiv:1701.05369 [Cs, Stat], June. http://arxiv.org/abs/1701.05369.\n\n[3] Han, Song, Jeff Pool, John Tran, and William Dally. 2015. \u201cLearning Both Weights and Connections for Efficient Neural Network.\u201d In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1135\u20131143. Curran Associates, Inc. http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network.pdf.\n\n[4] Jin, Xiaojie, Xiaotong Yuan, Jiashi Feng, and Shuicheng Yan. 2016. \u201cTraining Skinny Deep Neural Networks with Iterative Hard Thresholding Methods.\u201d ArXiv:1607.05423 [Cs], July. http://arxiv.org/abs/1607.05423.\n\n[5] Novikov, Alexander, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. 2015. \u201cTensorizing Neural Networks.\u201d ArXiv:1509.06569 [Cs], September. http://arxiv.org/abs/1509.06569.\n\n[6] Oseledets, Ivan V. \"Tensor-train decomposition.\" SIAM Journal on Scientific Computing 33, no. 5 (2011): 2295-2317.\n\n[7] Cichocki, Andrzej, Namgil Lee, Ivan V. Oseledets, A-H. Phan, Qibin Zhao, and D. Mandic. \"Low-rank tensor networks for dimensionality reduction and large-scale optimization problems: Perspectives and challenges part 1.\" arXiv preprint arXiv:1609.00893 (2016).\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2253/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1xO4xHFvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2253/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2253/Authors|ICLR.cc/2020/Conference/Paper2253/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504144090, "tmdate": 1576860536419, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2253/Authors", "ICLR.cc/2020/Conference/Paper2253/Reviewers", "ICLR.cc/2020/Conference/Paper2253/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Official_Comment"}}}, {"id": "ByxlTSdttS", "original": null, "number": 2, "cdate": 1571550648368, "ddate": null, "tcdate": 1571550648368, "tmdate": 1572972363002, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper describes a new method called Atomic Compression Network for constructing neural networks. The idea is straightforward. Basically, firstly create some neurons in random fashion, then reuse a subset of those neurons in each layer. The experiments shows ACN produces better accuracy than baseline models including a FC network, a Baysesina compression method, etc. for MINIST, etc. The paper also show ACN uses much less numbers of parameters and achieves similar accuracy when comparing with a large optima FC network on a set of datasets. \n\nOverall, I don\u2019t support accepting this paper. First, I don\u2019t think the proposed idea is very innovative. Please elaborate why this method seems to work well when comparing baseline models. Is it just randomly constructed network also perform well?  Secondly, I\u2019m not convinced we will use this method to build network in real world applications. The model size is small, but in what cases this small model size matters? Is this a reliable way to create useful models? \n\nOn page 7, in Figure 3, why logistic regression only has a single point in some of the plots?\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2253/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2253/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783640111, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2253/Reviewers"], "noninvitees": [], "tcdate": 1570237725492, "tmdate": 1574783640127, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Official_Review"}}}, {"id": "SJgIDBwJ9B", "original": null, "number": 3, "cdate": 1571939678277, "ddate": null, "tcdate": 1571939678277, "tmdate": 1572972362966, "tddate": null, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "invitation": "ICLR.cc/2020/Conference/Paper2253/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new way to create compact neural net, named Atomic Compression Networks (ACN). An immediate related work is LayerNet, where a deep neural net is created by replicating the same layer. Here, this paper extends replication down to the neuron level. \n\nI am leaning towards rejecting this paper because the experimental setup is not well justified and a few important details are missing before conclusions can be drawn. I would like to ask a few clarification questions. Depending on the authors\u2019 answers, I might be willing to adjust my rating. \n\n(1) Is there missing a delta in the first half of line 6 in Algorithm 1? \n\n(2) Throughout the experiments, for the same hyperparameter (e.g. Table 4 in A.2) do you run Algorithm 1 more than once and select the best sample architecture? If the answer is yes, summarizing all masks as one parameter will not be reasonable. Given a yes answer, I would also like to ask if the same number of samples have been considered for FC (for the same hyperparameter). \n\n(3) Is there any intuition behind why FC does a much worse job of fitting curves than ACN with much less parameters? This refers to Fig. 2, if we compare FC with 41 parameters to ACN with 18 parameters. I am confused because MSE on sampled points often goes down when we increase the number of parameters for the application of curve fitting.\n\n(4) Convolution can be thought of as a special case of ACN. ConvNet is the default architecture for working on image datasets. Since MNIST and CIFAR are considered, why not also compare to ConvNet?\n\n(5) The claims that \u201cACNs achieve compression rates of up to three orders of magnitudes compared to fine-tuned fully-connected neural networks with only a fractional deterioration of classification accuracy\u201d is quite misleading. Given fully-connected neural networks achieve up to 528 times with also a fractional deterioration (Sec. 4.3), by presumably having a shallower architecture. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2253/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2253/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Atomic Compression Networks", "authors": ["Jonas Falkner", "Josif Grabocka", "Lars Schmidt-Thieme"], "authorids": ["falkner@ismll.uni-hildesheim.de", "josif@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de"], "keywords": ["Network Compression"], "TL;DR": "We advance the state-of-the-art in model compression by proposing Atomic Compression Networks (ACNs), a novel architecture that is constructed by recursive repetition of a small set of neurons.", "abstract": "Compressed forms of deep neural networks are essential in deploying large-scale\ncomputational models on resource-constrained devices. Contrary to analogous\ndomains where large-scale systems are build as a hierarchical repetition of small-\nscale units, the current practice in Machine Learning largely relies on models with\nnon-repetitive components. In the spirit of molecular composition with repeating\natoms, we advance the state-of-the-art in model compression by proposing Atomic\nCompression Networks (ACNs), a novel architecture that is constructed by recursive\nrepetition of a small set of neurons. In other words, the same neurons with the\nsame weights are stochastically re-positioned in subsequent layers of the network.\nEmpirical evidence suggests that ACNs achieve compression rates of up to three\norders of magnitudes compared to fine-tuned fully-connected neural networks (88\u00d7\nto 1116\u00d7 reduction) with only a fractional deterioration of classification accuracy\n(0.15% to 5.33%). Moreover our method can yield sub-linear model complexities\nand permits learning deep ACNs with less parameters than a logistic regression\nwith no decline in classification accuracy.", "code": "https://drive.google.com/open?id=1weAzCzlI4p0L9vXsBI12LfNMlfGZdjd_", "pdf": "/pdf/2796e7b556b2bc5b2b32b87d3dc4532b731988a5.pdf", "paperhash": "falkner|atomic_compression_networks", "original_pdf": "/attachment/2d1a718f4b52b882001bce344af281d563b9101e.pdf", "_bibtex": "@misc{\nfalkner2020atomic,\ntitle={Atomic Compression Networks},\nauthor={Jonas Falkner and Josif Grabocka and Lars Schmidt-Thieme},\nyear={2020},\nurl={https://openreview.net/forum?id=S1xO4xHFvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1xO4xHFvB", "replyto": "S1xO4xHFvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2253/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574783640111, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2253/Reviewers"], "noninvitees": [], "tcdate": 1570237725492, "tmdate": 1574783640127, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2253/-/Official_Review"}}}], "count": 9}