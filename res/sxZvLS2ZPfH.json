{"notes": [{"id": "sxZvLS2ZPfH", "original": "D1xEfoaITJp", "number": 528, "cdate": 1601308065564, "ddate": null, "tcdate": 1601308065564, "tmdate": 1614985624013, "tddate": null, "forum": "sxZvLS2ZPfH", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining", "authorids": ["~Wei_Zhu5"], "authors": ["Wei Zhu"], "keywords": ["pretrained language models", "multi-vocab pretraining", "Chinese BERT"], "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert, which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, $seg\\_tok$, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness.   Experiments show that: (a) compared with char based vocabulary, $seg\\_tok$ does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs' downstream performance, especially it can improve $seg\\_tok$'s performances on sequence labeling tasks. ", "one-sentence_summary": "Redesign Chinese BERT's vocab and propse multi-vocab pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|mvpbert_redesigning_vocabularies_for_chinese_bert_and_multivocab_pretraining", "pdf": "/pdf/b116eb1fc9d82b7721f2ed9cfd8ee801ea015ec8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cx_iRenSB5", "_bibtex": "@misc{\nzhu2021mvpbert,\ntitle={{\\{}MVP{\\}}-{\\{}BERT{\\}}: Redesigning Vocabularies for Chinese {\\{}BERT{\\}} and Multi-Vocab Pretraining},\nauthor={Wei Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=sxZvLS2ZPfH}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "QBdxGa4A-3", "original": null, "number": 1, "cdate": 1610040535843, "ddate": null, "tcdate": 1610040535843, "tmdate": 1610474145825, "tddate": null, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "invitation": "ICLR.cc/2021/Conference/Paper528/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Three reviewers agreed to reject and the other reviewer also suggested it is below the threshold."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining", "authorids": ["~Wei_Zhu5"], "authors": ["Wei Zhu"], "keywords": ["pretrained language models", "multi-vocab pretraining", "Chinese BERT"], "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert, which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, $seg\\_tok$, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness.   Experiments show that: (a) compared with char based vocabulary, $seg\\_tok$ does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs' downstream performance, especially it can improve $seg\\_tok$'s performances on sequence labeling tasks. ", "one-sentence_summary": "Redesign Chinese BERT's vocab and propse multi-vocab pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|mvpbert_redesigning_vocabularies_for_chinese_bert_and_multivocab_pretraining", "pdf": "/pdf/b116eb1fc9d82b7721f2ed9cfd8ee801ea015ec8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cx_iRenSB5", "_bibtex": "@misc{\nzhu2021mvpbert,\ntitle={{\\{}MVP{\\}}-{\\{}BERT{\\}}: Redesigning Vocabularies for Chinese {\\{}BERT{\\}} and Multi-Vocab Pretraining},\nauthor={Wei Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=sxZvLS2ZPfH}\n}"}, "tags": [], "invitation": {"reply": {"forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040535829, "tmdate": 1610474145808, "id": "ICLR.cc/2021/Conference/Paper528/-/Decision"}}}, {"id": "QCKeuao6Aw", "original": null, "number": 4, "cdate": 1603810734566, "ddate": null, "tcdate": 1603810734566, "tmdate": 1605926042851, "tddate": null, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "invitation": "ICLR.cc/2021/Conference/Paper528/-/Official_Review", "content": {"title": "Overall experiments are solid and convincing, but the methods are somewhat lack of novelty. ", "review": "    This work attempt to handle the vocabulary problem in Chinese pre-training, which is indeed an unsolved problem (for comparison, byte-pair encoding has been dominant in English pre-training models). Recently, there are some work trying to combine char-based vocab and word-based vocab for Chinese pre-training, such as AMBERT (Zhang et al) and CharBERT (Ma et al). Compared with these works, this paper is somewhat incremental and lacks novelty. In particular, this paper proposed the multi-vocab pretraining (MVP) and designed three versions of MVP: MVP_hier, MVP_obj, and MVP_pair (which is similar to AMBERT). The authors conducted solid experiments for multiple levels of vocabularies and the several MVP variants on eight Chinese benchmark datasets, which induced some useful take-away findings. Overall, the paper focused on the problem of vocabulary in Chinese pre-training, and experimented several MVP methods. Considering the contributions of this paper, it is more suitable as a technical report than a paper to be published.\n\n    Pros:\n\n    1. The overall experiments are solid and convincing. The implementation details including resources and libraries are sufficient, the experiments are repeated for multiple times for reproducibility.\n    2. To handle the problem of Chinese vocabulary, this paper has tested three single-vocab models (char, seg, seg_tok) and several multi-vocab models (MVP). The results and findings demonstrated in this paper should be trusted and also instructive to peers who is interested in the Chinese pre-training.\n\n    Cons:\n\n    1. The methods are somewhat lack of novelty. Few insightful conclusions are revealed either theoretically or experimentally.\n    2. The effect of \\lambda is not a good point to be analyzed. Instead, there are several interesting points that are more worth to be studied such as the interaction/compositional relationships between multiple vocabs.\n    3. The writing should be improved. Too many errors in the current version of paper. For example:\n       1. The form of citation should be corrected, e.g., \"*The pretrained language models (PLMs) including BERT Devlin et al. (2018)*\" should be \"*The pretrained language models (PLMs) including BERT (Devlin et al., 2018)*\"\n       2. In Page 2, the last sentence of the paragraph above Figure 1, \"We will denote this strategy as MVP_pair\", I think the \"MVP_pair\" should be \"MVP_obj\" .\n       3. The second row in page 5.\n       4. seg_tok is not a good name. The authors should take more time to completing this paper.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper528/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper528/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining", "authorids": ["~Wei_Zhu5"], "authors": ["Wei Zhu"], "keywords": ["pretrained language models", "multi-vocab pretraining", "Chinese BERT"], "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert, which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, $seg\\_tok$, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness.   Experiments show that: (a) compared with char based vocabulary, $seg\\_tok$ does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs' downstream performance, especially it can improve $seg\\_tok$'s performances on sequence labeling tasks. ", "one-sentence_summary": "Redesign Chinese BERT's vocab and propse multi-vocab pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|mvpbert_redesigning_vocabularies_for_chinese_bert_and_multivocab_pretraining", "pdf": "/pdf/b116eb1fc9d82b7721f2ed9cfd8ee801ea015ec8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cx_iRenSB5", "_bibtex": "@misc{\nzhu2021mvpbert,\ntitle={{\\{}MVP{\\}}-{\\{}BERT{\\}}: Redesigning Vocabularies for Chinese {\\{}BERT{\\}} and Multi-Vocab Pretraining},\nauthor={Wei Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=sxZvLS2ZPfH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141075, "tmdate": 1606915809999, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper528/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper528/-/Official_Review"}}}, {"id": "f9jTV2ZKPcA", "original": null, "number": 2, "cdate": 1603275478316, "ddate": null, "tcdate": 1603275478316, "tmdate": 1605024667578, "tddate": null, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "invitation": "ICLR.cc/2021/Conference/Paper528/-/Official_Review", "content": {"title": "Too poor novelty and writing", "review": "This paper proposes to use multiple levels of language units, including characters, subwords, and words, for Chinese language modeling. Three versions of multi-vocabulary pretraining methods are also studied. Experiments show that using the optimized seg_tok units could improve the model performance in various downstream tasks, and the MVP strategies boost the seg_tok\u2019s results on sequence labeling tasks.\n\nStrengths:\n1.\tEmpirical studies on building vocabularies with three different granularities of language units for Chinese language model pre-training.\n2.\tEvaluations on three multi-vocab pre-training strategies.\n\nMy most concerns are:\n1.\tThe idea of modeling different levels of language units has been widely studied before. Although they have not been comprehensively evaluated on pre-trained LMs, the findings are quite similar with previous studies, without much new highlights. The claim of the contribution, \u201cthe combination of CWS and subword tokenization is novel.\u201d, is quite weak. \n\nMissing References:\nZhang, X., & Li, H. (2020). AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization. arXiv preprint arXiv:2008.11869.\nZhang et al, (2019). \"Effective subword segmentation for text comprehension.\" IEEE/ACM Transactions on Audio, Speech, and Language Processing 27.11 (2019): 1664-1674.\nLiu, Z., Xu, Y., Winata, G. I., & Fung, P. (2019). Incorporating word and subword units in unsupervised machine translation using language model rescoring. arXiv preprint arXiv:1908.05925.\n\n2. In seg vocab development, the choice of 99.95% is interesting but can you please tell what is the lowest word frequency in this vocab? how many words with a frequency of less than 40 are present? if criteria is changed from 99.95% coverage to the selection of only frequent words (maybe >40), would it had any effect on speed? or performance?\n\n3.\tThis paper is poorly written. Thorough proofreading is required. There are too many typos and grammar errors. e.g.,\nIn the abstract, performances -> performance; remain -> remains; does not only improves -> does not only improve;\nIn the second paragraph of page 2, incorporate -> incorporates; combine -> combinesl\nIn Footnote 5, avalable -> available.\n\n4.\tThere is no comparison on public tasks or datasets.\n\nOther comments:\n1.\tFor the multi-vocabulary pre-training, the vocabulary size, i.e., the size of the seg_toks, would be an important influence factor for the downstream task performance. \n2.\tOnly ALBERT small model is evaluated. It is not quite sure if the method can further enhance the state-of-the-art language models.\n3.\tThe citation format is in chaos, check the usage of \\citep{} and \\citet{}. The citation of ALBERT is missed in the last paragraph of page 2.\n4.\tCheck the second line in page 5. \n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper528/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper528/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining", "authorids": ["~Wei_Zhu5"], "authors": ["Wei Zhu"], "keywords": ["pretrained language models", "multi-vocab pretraining", "Chinese BERT"], "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert, which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, $seg\\_tok$, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness.   Experiments show that: (a) compared with char based vocabulary, $seg\\_tok$ does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs' downstream performance, especially it can improve $seg\\_tok$'s performances on sequence labeling tasks. ", "one-sentence_summary": "Redesign Chinese BERT's vocab and propse multi-vocab pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|mvpbert_redesigning_vocabularies_for_chinese_bert_and_multivocab_pretraining", "pdf": "/pdf/b116eb1fc9d82b7721f2ed9cfd8ee801ea015ec8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cx_iRenSB5", "_bibtex": "@misc{\nzhu2021mvpbert,\ntitle={{\\{}MVP{\\}}-{\\{}BERT{\\}}: Redesigning Vocabularies for Chinese {\\{}BERT{\\}} and Multi-Vocab Pretraining},\nauthor={Wei Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=sxZvLS2ZPfH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141075, "tmdate": 1606915809999, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper528/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper528/-/Official_Review"}}}, {"id": "4bvZLS7MDM", "original": null, "number": 3, "cdate": 1603774925528, "ddate": null, "tcdate": 1603774925528, "tmdate": 1605024667507, "tddate": null, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "invitation": "ICLR.cc/2021/Conference/Paper528/-/Official_Review", "content": {"title": "Interesting ideas and consistent but marginal improvements over baselines. Miss some related works on this topic.", "review": "The paper focuses on pretrain techniques for Chinese vocabulary. The paper proposes several interesting and novel ways to improve the pretrained model in Chinese. These ideas include new ways of combining character, segment and word level tokens and new MLM tasks with different granularity of Chinese vocab. For example MVP_hier explicitly combines the chars into words, which doesn't give the best results. MPV_obj outperforms MPV_hier by teaching the transformer to learn how to aggregate meaning of chars with an additional loss with a coarse-grained vocab. The results show that MVP pretrain can improve the performance on both sentence level tasks and sequence labeling tasks. Improvements are consistent over all testing tasks although the improvements appear to be marginal, especially in comparison to the reported standard deviation. For example, MVP_obj outperforms the best baseline fin by 0.52 while the std is 0.58 for MVP_obj and 0.61 for the best baseline. The best results are given by MVP_pair ensemble methods. Usually, ensemble methods can boost the performance of many ML tasks.\n\nIt is a bit surprising that the seg_tok is worse than char on sequence labeling tasks. It will be useful to give some intuition or examples to illustrate this. The author mentioned that the CWS attributes very little to the performance loss. It will be helpful if more details are given. The whole technique proposed by the paper also heavily relies on the performance of CWS.\n\nBecause the paper proposes a novel technique to work with Chinese characters, one more thing that the paper lacks is to mention and compare with recent works on Chinese language model pretrain and techniques to deal with Chinese vocab. The baseline method is vanilla MLM which was developed for English. \n\nOne editing issue: page 5 the second line.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper528/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper528/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining", "authorids": ["~Wei_Zhu5"], "authors": ["Wei Zhu"], "keywords": ["pretrained language models", "multi-vocab pretraining", "Chinese BERT"], "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert, which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, $seg\\_tok$, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness.   Experiments show that: (a) compared with char based vocabulary, $seg\\_tok$ does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs' downstream performance, especially it can improve $seg\\_tok$'s performances on sequence labeling tasks. ", "one-sentence_summary": "Redesign Chinese BERT's vocab and propse multi-vocab pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|mvpbert_redesigning_vocabularies_for_chinese_bert_and_multivocab_pretraining", "pdf": "/pdf/b116eb1fc9d82b7721f2ed9cfd8ee801ea015ec8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cx_iRenSB5", "_bibtex": "@misc{\nzhu2021mvpbert,\ntitle={{\\{}MVP{\\}}-{\\{}BERT{\\}}: Redesigning Vocabularies for Chinese {\\{}BERT{\\}} and Multi-Vocab Pretraining},\nauthor={Wei Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=sxZvLS2ZPfH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141075, "tmdate": 1606915809999, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper528/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper528/-/Official_Review"}}}, {"id": "gaNvv0yVkv9", "original": null, "number": 1, "cdate": 1603078128007, "ddate": null, "tcdate": 1603078128007, "tmdate": 1605024667374, "tddate": null, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "invitation": "ICLR.cc/2021/Conference/Paper528/-/Official_Review", "content": {"title": "Not well organised, comparison is not enough", "review": "This paper compared three different Chinese language pretraining methods. The paper is not well organized. Although the paper mentioned the char and word are both important in the representation of the Chinese language, it is not clear why the author(s) used the current three pretraining methods.  More intuitive explanation of the design of the pretraining structure should be added. The most intuitive way of combining the pretrained word and character information is to pretrain them separately and concatenate them together, the proposed models should compare with this most intuitive method and also explain why the proposed models are better than this simply pretraining concatenation method. \n\nThe experiments were not persuasive. Although it is not necessary to beat all the state-of-the-art models, the comparison with other models should be given. For example, the MSRA NER in this paper is only 82% which is largely behind the SOTA models (>93%).  Given the poor baseline performance, it is hard to persuade the readers the conclusions in this paper are still hold in the most recent/advanced models. \n\nPage 2, citation error \"Albert ?\"\nPage 5, format error, the sentence is over the page boundary.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper528/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper528/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MVP-BERT: Redesigning Vocabularies for Chinese BERT and Multi-Vocab Pretraining", "authorids": ["~Wei_Zhu5"], "authors": ["Wei Zhu"], "keywords": ["pretrained language models", "multi-vocab pretraining", "Chinese BERT"], "abstract": "Despite the development of pre-trained language models (PLMs) significantly raise the performances of various Chinese natural language processing (NLP) tasks, the vocabulary for these Chinese PLMs remain to be the one provided by Google Chinese Bert, which is based on Chinese characters. Second, the masked language model pre-training is based on a single vocabulary, which limits its downstream task performances. In this work, we first propose a novel method, $seg\\_tok$, to form the vocabulary of Chinese BERT, with the help of Chinese word segmentation (CWS) and subword tokenization. Then we propose three versions of multi-vocabulary pretraining (MVP) to improve the models expressiveness.   Experiments show that: (a) compared with char based vocabulary, $seg\\_tok$ does not only improves the performances of Chinese PLMs on sentence level tasks, it can also improve efficiency; (b) MVP improves PLMs' downstream performance, especially it can improve $seg\\_tok$'s performances on sequence labeling tasks. ", "one-sentence_summary": "Redesign Chinese BERT's vocab and propse multi-vocab pretraining", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|mvpbert_redesigning_vocabularies_for_chinese_bert_and_multivocab_pretraining", "pdf": "/pdf/b116eb1fc9d82b7721f2ed9cfd8ee801ea015ec8.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=cx_iRenSB5", "_bibtex": "@misc{\nzhu2021mvpbert,\ntitle={{\\{}MVP{\\}}-{\\{}BERT{\\}}: Redesigning Vocabularies for Chinese {\\{}BERT{\\}} and Multi-Vocab Pretraining},\nauthor={Wei Zhu},\nyear={2021},\nurl={https://openreview.net/forum?id=sxZvLS2ZPfH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "sxZvLS2ZPfH", "replyto": "sxZvLS2ZPfH", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper528/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141075, "tmdate": 1606915809999, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper528/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper528/-/Official_Review"}}}], "count": 6}