{"notes": [{"id": "Jd8ZTyseU7I", "original": null, "number": 6, "cdate": 1591855492726, "ddate": null, "tcdate": 1591855492726, "tmdate": 1591855492726, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment", "content": {"title": "Updated version in KDD 2020", "comment": "Thank all comments! An updated version in KDD 2020 is at https://arxiv.org/abs/1906.02909"}, "signatures": ["ICLR.cc/2020/Conference/Paper284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1x0CnEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper284/Authors|ICLR.cc/2020/Conference/Paper284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173679, "tmdate": 1576860530960, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment"}}}, {"id": "S1x0CnEtvB", "original": "SkxQYvfSDH", "number": 284, "cdate": 1569438934108, "ddate": null, "tcdate": 1569438934108, "tmdate": 1577168290844, "tddate": null, "forum": "S1x0CnEtvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "b6DIvscGx", "original": null, "number": 1, "cdate": 1576798692232, "ddate": null, "tcdate": 1576798692232, "tmdate": 1576800943102, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes a neural architecture search method based on greedily adding layers with random initializations. The reviewers all recommend rejection due to various concerns about the significance of the contribution, novelty, and experimental design. They checked the author response and maintained their ratings.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729937, "tmdate": 1576800282628, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper284/-/Decision"}}}, {"id": "SkeAVHaRYS", "original": null, "number": 2, "cdate": 1571898677764, "ddate": null, "tcdate": 1571898677764, "tmdate": 1574476809594, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper's contribution is a method for automatically growing the depth of a neural network during training. It compares several heuristics that may be used to successfully achieve this goal and identifies a set of choices that work well together on multiple datasets.\n\nThe paper focuses on CNNs that conform to a popular design pattern where the network is organized into a series of sub-networks, each consisting of a series of sub-modules (sometimes called blocks) operating at the same resolution. To be precise, the proposed method aims to learn the length of each series of sub-modules. A main contribution of the paper is the demonstration that it is not necessary to train a network until convergence before adding new sub-modules as proposed in past work. Instead, it is better to grow the network after training for a short while.\n\nMy current decision for this paper is a weak rejection due to the points below. However, I am open to revising my opinion if these points are addressed satisfactorily.\n\n- The growing strategy identified in the paper as a superior alternative seems to be already known and used, at least in the speech recognition community. Seide et al. (2011) called it Discriminative Pre-training, and showed that it outperforms greedy layer-wise pretraining and DBN pre-training. Zeyer et al. (2017) reported that a similar method also enables the training of very deep LSTM networks which is otherwise notoriously hard. In general, the existence of prior work with the same ideas does not preclude acceptance, but the existence of this work needs to be clearly stated early on and the additional value of the current study sufficiently clarified.\n\n- I find it strange that the final networks found by the proposed method usually have the same/similar number of sub-modules per sub-network (Tables 4,5,6) on multiple datasets. The only exceptions appear to be Basic4ResNet/CIFAR100 in Table 6 and about 50% of ImageNet results in Table 7. This regularity suggests that either A) the proposed algorithm prefers to set same number of sub-modules per sub-network due to its design, or B) datasets except ImageNet have an inherent shared property that produces this result. Since option A suggests a bias in the algorithm, this peculiarity of the results needs to be investigated or explained further.\n\n- Figure 3 constitutes the main evidence that Autogrow finds approximately optimal depths as compared to manual searching, but it is not clear how the plot for baselines is obtained. For any given parameter budget, there are multiple baseline networks possible since the sub-networks can have different number of sub-modules (see previous point). This does not appear to be accounted for in Figure 3. Further, when dealing with CNNs, it would be more useful to have computation budget on the x-axis instead of the parameter budget. This would better account for the difference between increasing depth in an earlier sub-network vs. a later one.\n\n- The reported results appear to be for single trials throughout the paper. This does not seem sufficient especially for results in Tables 2 and 3 where many differences are rather small, and so drawing conclusions from these tables would be unscientific.\n\nReferences:\n\nSeide, Frank, et al. \"Feature engineering in context-dependent deep neural networks for conversational speech transcription.\" 2011 IEEE Workshop on Automatic Speech Recognition & Understanding. IEEE, 2011. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/FeatureEngineeringInCD-DNN-ASRU2011-pub.pdf\n\nZeyer, Albert, et al. \"A comprehensive study of deep bidirectional LSTM RNNs for acoustic modeling in speech recognition.\" 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017. https://arxiv.org/abs/1606.06871\n\nUpdate after rebuttal\n-----------------------------\nI'm sympathetic to the unfortunate situation that the authors are in, since the underlying growing strategy has already been covered by prior work. As I mentioned earlier, a sufficient rewrite of the paper can clearly state what has been done already so as not to take credit from the earlier authors. A revised version of the paper has not been uploaded; I suggest that the authors do so for the future. \n\nI agree that the focus of this paper is learning the 'optimal' depth by using the growing strategy. But I am not convinced that the technique indeed finds optimal depths based on the regularity of the sub-network depths mentioned in my review. The rebuttal suggests reasons for the obtained regularity, but does not prove that these regular structures obtained are indeed optimal and not an artifact of the algorithm itself. The baselines are also using the same regular architectures, which distorts the overall picture because it is possible that a non-regular architecture provides a better trade-off.\n\nWhile my rating doesn't change, I do think that the work is in an interesting direction. My final suggestions for the future are:\n- Investigate where non-regular architectures (unequal sub-network depths) are in the trade-off between accuracy, flops and parameters.\n- Investigate whether the proposed algorithm can be modified to easily find non-regular architectures if they can yield equally good performance as regular ones at similar or lower cost.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper284/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper284/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575754251409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper284/Reviewers"], "noninvitees": [], "tcdate": 1570237754365, "tmdate": 1575754251425, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper284/-/Official_Review"}}}, {"id": "HJxnGdUnjS", "original": null, "number": 3, "cdate": 1573836819921, "ddate": null, "tcdate": 1573836819921, "tmdate": 1573836819921, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "r1emxj7GKH", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for reviewing.\n\nReplies to each item:\n*Suggestion* 1:\n* We will discuss those aspects in a revision. This paper is not a \"result\" paper to show a new STOA accuracy. We propose a framework that can discover an optimal depth.  \"The final accuracy is limited by the sub-module design, not by our AutoGrow.\" Our goal in this paper is that, given a sub-module architecture, how can we find a near-optimal depth. We have briefly discussed it, but will dive deeper.\n\n*Suggestion* 2:\n* In this paper, the major difference of applying ZeroInit is that we first train a shallower net to some extent (not fully converged) and then keep adding some new layers, which are initialized by ZeroInit; while previous literature just uses ZeroInit as parameter initialization and then train it without changing the neural architecture. The ZeroInit plays different roles:\n** in previous literature, an intrinsic shallower net is not trained (not converged) at the beginning and, after just few iterations, the ZeroInit layers turn to non-zeros (random values) quickly, which is why those work well with ZeroInit. We argue that the previous work is more like a random initialization after training for a few iterations.\n** in our literature, the shallower net is trained to some extent; using ZeroInit to initialize new layers will stick to local minimum. This is proved in many experiments in this paper.\nIn summary, the settings in this paper and in previous ones are different, we didn't argue that ZeroInit will be worse than random initialization in the settings that the reviewer mentioned.\nWe will add this discussion in a revision.\n\n* \"(Many datasets)\"\n** We showed in Figure 3 that AutoGrow can stop at a reasonable depth, and the depth found in ImageNet is also reasonable. We indeed observe a very deep net for small datasets (such as MNIST), and we are integrating pruning into the growing to tackle this issue. We hope this paper is the first step to achieve an ideal goal for all settings.\n** we had limited time to duplicate and run results using NASNET cell. The work is on-going.\n\n\n* \"(Scale to ImageNet)\"\nWe will. One method that *directly* searches in imagenet is [1], which achieved \"75.2 \u00b1 0.4%\" top-1. This value is lower than our \"77.99%\". The fact questions if we really need to search over all the combinations or searching depth will be sufficient.\n\n* Mirror \nAgree. We wish our empirical results can inspire \"theoretical understanding\" in the future. Growing architecture is challenging and new, and we wish our work can inspire others.\n\n[1] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. \"Understanding and simplifying one-shot architecture search.\" In International Conference on Machine Learning, pp. 549-558. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1x0CnEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper284/Authors|ICLR.cc/2020/Conference/Paper284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173679, "tmdate": 1576860530960, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment"}}}, {"id": "SkgtGIhjjS", "original": null, "number": 2, "cdate": 1573795345165, "ddate": null, "tcdate": 1573795345165, "tmdate": 1573795345165, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "SkeAVHaRYS", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment", "content": {"title": "Reply", "comment": "Thanks for reviewing.\n\nResponses to each item:\n- Thanks for pointing us to more related works. We will include them in this paper ASAP. The similar research in speech recognition community proves that our approach can be generic to other domains, and our research also indicates that their approaches can potentially generalize to computer vision domain. We quite appreciate similar discoveries of \"(1) stopping very early by going through the data only once and (2) using large learning rate\" in that paper. The major difference in this paper is that we are using AutoGrow to find an optimal depth, instead of enabling training deeper networks, which were hard to train but are easily now because of many recent advances such as weight initialization, batch normalization, shortcut paths, just to name a few.\n\n- We believe that the \"regularity\" could be because of both. We believe that this is because of the nature of a periodic growing with a short interval and because of the small size of dataset. AutoGrow will only stop when the whole network saturates in that case. While in ImageNet, the dataset is larger, therefore, the \"regularity\" is infrequent.\n\n- For baselines, we set the numbers of sub-modules (under each resolution) the same (such as K), then we sweep K from 1 to a large number. We list the baselines for Basic3ResNet below and we will include all others in a revision. We also include the computation in \"Flops\" below as requested. We used \"Params\" as we think the number of layers is more related to the number of parameters, but we will include both in a revision.\n-----------------------------------------------------------------------------\nNet                                                 | Flops            | Params       | Accuracy\nBasic3ResNet-[3, 3, 3]           41214592       272474            92.96\nBasic3ResNet-[5, 5, 5]          69755520        466906            93.44\nBasic3ResNet-[7, 7, 7]          98296448        661338            93.68\nBasic3ResNet-[9, 9, 9]          126837376      855770            93.88\nBasic3ResNet-[24, 24, 24]    340894336      2314010          94.26\nBasic3ResNet-[42, 42, 42]    597762688      4063898          94.3\nBasic3ResNet-[72, 72, 72]    1025876608    6980378          94.17\n-----------------------------------------------------------------------------\n\n- The conclusion holds for each run. We treat both ZeroInit and AdamInit as network morphism and, in table 2, we concluded that ZeroInit and AdamInit were similar and sub-optimal.\nWe run the the first setting in Table 2 for three runs, and the accuracy is 92.73%, 92.86%, 92.51%.\nFor table 3, we run one more \"CIFAR100 constant GauInit\", we get 70.83%, and one more \"CIFAR100 staircase ZeroInit\" and get 70.11%.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1x0CnEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper284/Authors|ICLR.cc/2020/Conference/Paper284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173679, "tmdate": 1576860530960, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment"}}}, {"id": "BkeMtZsijr", "original": null, "number": 1, "cdate": 1573790073725, "ddate": null, "tcdate": 1573790073725, "tmdate": 1573790073725, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "HkgRdsvQqr", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment", "content": {"title": "We should prefer simple and worked methods, instead of complex and hard-to-use methods", "comment": "Thanks for the review.\n\nWe actually started from more complex and \"nicer\" methods, such as network morphism.\nHowever, our experiments showed that those more complex methods worked worse than our simple solution (covered in this paper).\nTherefore, one of our contribution is that complex growing is NOT necessary, a simple periodic  growing can even does better.\n\nThen the question is: should we appreciate more complex methods which have worse results or appreciate simple but more effective methods?\n\nOthers:\n* \"implies the problem may not be intrinsically hard, or even useful.\": growing architecture is harder than pruning architecture [1]. Pruning starts from a predefined large space and just need to find a subspace within it, while growing performs in a reversed order, during which the search space is totally open. A simple pruning method [1] inspires lots of following work and we wish our method can inspire others from a new perspective. We also believe, in the future, that combining pruning and growing will be important.\nWe need more details from the reviewer regarding why automatically growing neural architecture will not be useful. \n* \"detailed narrative\" is exactly how we showed that complex methods are not necessary and can even be worse.\n* \"overfitting\": all results are cross validated by a standard machine learning procedure. It is not a question specifically for our paper. It is a question for the whole community that if using validation dataset of ImageNet can result in overfitting. \n\n[1] Han, Song, Jeff Pool, John Tran, and William Dally. \"Learning both weights and connections for efficient neural network.\" In Advances in neural information processing systems, pp. 1135-1143. 2015."}, "signatures": ["ICLR.cc/2020/Conference/Paper284/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "S1x0CnEtvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper284/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper284/Authors|ICLR.cc/2020/Conference/Paper284/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504173679, "tmdate": 1576860530960, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper284/Authors", "ICLR.cc/2020/Conference/Paper284/Reviewers", "ICLR.cc/2020/Conference/Paper284/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper284/-/Official_Comment"}}}, {"id": "r1emxj7GKH", "original": null, "number": 1, "cdate": 1571072746921, "ddate": null, "tcdate": 1571072746921, "tmdate": 1572972615169, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Contributions:\n\tThis paper best fits in the literature that explores growing network depth.  The main framework here is to interleave training a shallower network and adding new layers.  This paper (their final algorithm) differs from existing methods in that they: 1) initialize the new layers using standard initialization as opposed to the commonly used zero-init in this literature, 2) grows at a fixed interval , and this interval is short (to avoid the shallower nets being  overly-trained)., 3) uses a large and constant learning rate during the growing phase.  \nEmpirically, they show competitive results on standard image benchmarks.  \nMore interestingly (to me), they provide interesting insights to this paradigm of \u2018growing networks\u2019.  \n\nComments/Questions:\nSection 2 of the paper describes the proposed method is good details.  \n\nSection 3 of the paper describes the experiments.  Since for now I see the contribution of this paper is mostly empirical, I will give my detailed feedback here.\n3.1 (Suboptimum of Network Morphism (NM) )\nTable 2 shows NM is worse than training from scratch, and this isn\u2019t fixed by AdamInit.\nTable 3 shows c-AutoGrow (in between p-AutoGrow and NM) still does worse than from scratch, pinpoint the problem to converged subnetworks.\n3.2  (p-AutoGrow)\nTable 3 shows +Constant LR helps, then +RandomInit helps. \nTable 4, 5 shows +Periodic gets the best performance.  \n*Suggestion* The found net is Table 4,5  are significantly deeper than those in Table 2,3, also there are no \\Delta.  Also, although within this write-up those are the highest numbers, in the broader literature of NAS this doesn\u2019t seem to be that good.  From a quick search, many methods in the Table 1 of [1] seems to give >96% accuracy on CIFAR10, some even close to 98%.  It might be good to at least discuss why this method is limited from achieving that.\nI do like the finding that ZeroInit is unnecessary, as reported in the rest of this subsection.  However, it is unsatisfying to me that many past works (as cited by the authors) required this ZeroInit without ever trying RandomInit.  \n*Suggestion* I would love to see a more thorough discussion on why GauInit is better than ZeroInit, not just more numbers.  For example, even just text description on why past works found ZeroInit useful, and countering some of those claims would be interesting.  A more controlled experiment rather than training 2 networks by swapping this would be interesting.  ZeroInit is used in more context than just NM.  For example, good flow models like Glow also uses such initialization, for likely a different reason, but I wonder if findings here have any implication for ZeroInit more generally.\n\n3.3 (Many datasets)\nTable 6 is a strong result.  One odd thing is how deep the found-net has to be for MNIST.  This actually suggest to me that AutoGrow does not have the ability to stop early when it can.  And in the discussion, the authors argue that by using a better sub-module like in NAS they can do better.  This raises the question why the authors did not choose to use it.  I would believe it if the proposed method has obvious reasons that it can transfer to different architecture, but for now I cannot jump to the conclusion that, say, p-AutoGrow with GauInit will necessarily work when using a different sub-module.  Perhaps, the reason past NM works didn\u2019t use a GauInit was also due to the fact that past sub-modules didn\u2019t work with GauInit.  \n\n3.4 (Scale to ImageNet) It\u2019d be good to add reference results from other papers.  \n\nMinor details:\n\n\nThere are some good contents in this work, but for it to be a strong *empirical* contribution, perhaps it would be more useful to include experiments on other data modality where things are not so well tuned, and show state-of-the-art results.  For it to be a strong *analysis* paper, it should expanded, at least addressing some of the *suggestions* mentioned above. \nUnrelated to my evaluation of this work, reading this makes me think we should (and can) develop theoretical understanding to this paradigm of growing networks.\n\n\n\nReferences:\n[1] https://arxiv.org/pdf/1905.13360.pdf\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper284/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper284/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575754251409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper284/Reviewers"], "noninvitees": [], "tcdate": 1570237754365, "tmdate": 1575754251425, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper284/-/Official_Review"}}}, {"id": "HkgRdsvQqr", "original": null, "number": 3, "cdate": 1572203382138, "ddate": null, "tcdate": 1572203382138, "tmdate": 1572972615080, "tddate": null, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "invitation": "ICLR.cc/2020/Conference/Paper284/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper presents a meta-learning algorithm to automatically detemine the depth of neural network through a policy to add depth if this bring improvement on accuracy.\n\nI have conserved opinion based on the technique being used here is extremely simple, basically is an implementation of naive greedy algorithm in such a scenario, which implies the problem may not be intrinsically hard, or even useful. The paper consists of detailed narrative about how these procedure are conducted, but still, it is really hard for me to find the true merit to appreciate, and why this brings a nontrivial and usefull contribution. The tables, visualization figures also didnot imply too much about whether this is more than overfitting on previous works with hand-chosen depth. "}, "signatures": ["ICLR.cc/2020/Conference/Paper284/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper284/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "AutoGrow: Automatic Layer Growing in Deep Convolutional Networks", "authors": ["Wei Wen", "Feng Yan", "Hai Li"], "authorids": ["wei.wen@duke.edu", "fyan@unr.edu", "hai.li@duke.edu"], "keywords": ["Growing", "depth", "neural networks", "automation"], "TL;DR": "A method that automatically grows layers in neural networks to discover optimal depth.", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "pdf": "/pdf/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "code": "https://drive.google.com/file/d/1C_kdg7Ffb4EE1WKpRO7W2t9f83WueuvY/view?usp=sharing", "paperhash": "wen|autogrow_automatic_layer_growing_in_deep_convolutional_networks", "original_pdf": "/attachment/912c2c5db6a8c5cec93ffc61dc5b01e714aefcd0.pdf", "_bibtex": "@misc{\nwen2020autogrow,\ntitle={AutoGrow: Automatic Layer Growing in Deep Convolutional Networks},\nauthor={Wei Wen and Feng Yan and Hai Li},\nyear={2020},\nurl={https://openreview.net/forum?id=S1x0CnEtvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "S1x0CnEtvB", "replyto": "S1x0CnEtvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper284/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575754251409, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper284/Reviewers"], "noninvitees": [], "tcdate": 1570237754365, "tmdate": 1575754251425, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper284/-/Official_Review"}}}], "count": 9}