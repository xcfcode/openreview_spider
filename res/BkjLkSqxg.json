{"notes": [{"tddate": null, "tmdate": 1486418352417, "tcdate": 1486418352417, "number": 24, "id": "BJd3-_L_e", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "H1PUuOXPg", "signatures": ["~Erich_K_Elsen1"], "readers": ["everyone"], "writers": ["~Erich_K_Elsen1"], "content": {"title": "No good implementations of CTC?", "comment": "Without wading into the larger discussion here, I'm just curious what you find lacking about the current implementations of CTC?\n\nGoogle has one as part of Tensorflow.  Baidu has one that is written in C and CUDA C, is very fast and has torch and tensorflow bindings."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396448059, "tcdate": 1486396448059, "number": 1, "id": "By_7nML_e", "invitation": "ICLR.cc/2017/conference/-/paper230/acceptance", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Let me start by saying that your area chair does not read Twitter, Reddit/ML, etc. The metareview below is, therefore, based purely on the manuscript and the reviews and rebuttal on OpenReview.\n \n The goal of the ICLR review process is to establish a constructive discussion between the authors of a paper on one side and reviewers and the broader machine-learning community on the other side. The goal of this discussion is to help the authors leverage the community for improving their manuscript.\n \n Whilst one may argue that some of the initial reviews could have provided a more detailed motivation for their rating, there is no evidence that the reviewers were influenced (or even aware of) discussions about this paper on social or other media --- in fact, none of the reviews refers to claims made in those media. Suggestions by the authors that the reviewers are biased by (social) media are, therefore, unfounded: there can be many valid reasons for the differences in opinion between reviewers and authors on the novelty, originality, or importance of this work. The authors are free to debate the opinion of the reviewers, but referring to the reviews as \"absolute nonsense\", \"unreasonable\", \"condescending\", and \"disrespectful\" is not helping the constructive scientific discussion that ICLR envisions and, frankly, is very offensive to reviewers who voluntarily spend their time in order to improve the quality of scientific research in our field.\n \n Two area chairs have read the paper. They independently reached the conclusion that (1) the reviewers raise valid concerns with respect to the novelty and importance of this work and (2) that the paper is, indeed, borderline for ICLR. The paper is an application paper, in which the authors propose the first\u00caend-to-end sentence level lip reading using deep learning. Positive aspects of the paper include:\n \n - A comprehensive and organized review about previous work.\n - Clear description of the model and experimental methods.\n - Careful reporting of the results, with attention to detail.\n - Proposed method appears to perform better than the prior state-of-the-art, and generalizes across speakers.\n \n However, the paper has several prominent negative aspects as well:\n \n - The GRID corpus that is used for experimentation has very substantial (known) limitations. In particular, it is constructed in a way that leads to a very limited (non-natural) set of sentences.\u00ca(For every word, there is an average of just 8.5 possible options the model has to choose from.)\n - The paper overstates some of its claims. In particular, the claim that the model is \"outperforming experienced human lipreaders\" is questionable: it is not unlikely that model achieves its performance by exploiting unrealistic statistical biases in the corpus that humans cannot / do not exploit. Similarly, the claims about the \"sentence-level\" nature of the model are not substantiated: it remains unclear what aspects of the model make this a sentence-level model, nor is there much empirical evidence that the sentence-level treatment of video data is helping much (the NoLM baseline is almost as good as LipNet, despite the strong biases in the GRID corpus).\n - The paper makes several other statements that are not well-founded. As one of the reviewers correctly remarks, the McGurk effect does not show that lipreading plays a crucial role in human communication (it merely shows that vision can influence speech recognition). Similarly, the claim that \"Bi-GRUs are crucial for efficient further aggregation\" is not supported by empirical evidence.\n \n A high-level downside of this paper is that, while studying a relevant application of deep learning, it presents no technical contributions or novel insights that have impact beyond the application studied in the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396448642, "id": "ICLR.cc/2017/conference/-/paper230/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396448642}}}, {"tddate": null, "tmdate": 1485540692650, "tcdate": 1485174863453, "number": 22, "id": "H1PUuOXPg", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "BkotC1gDg", "signatures": ["~Nando_de_Freitas1"], "readers": ["everyone"], "writers": ["~Nando_de_Freitas1"], "content": {"title": "The reviewing history of this paper", "comment": "\nI apologize for being rude. I however strongly disagree with your opinions.\n\nLet me expand so that perhaps our perspective can become a bit clear.\n\nWe released this paper on Friday and by Saturday it was all over reddit. The titles of the articles on reddit suggested we had achieved super-human performance for lip reading in general. Neil pointed this out to me on Twitter, and we immediately started correcting these statements on Sunday. By Monday the story had exploded fueled, ironically, by Neil's comments that culminated on a BBC article alluding to the dangers of lip reading, when ironically too they have been working on lip reading too.\n\nSo here is how the sequential adversarial game played out for us:\n\n(i) First, we were accused of over-claiming. We made it clear that the paper made no general claims of super-human performance. We only claimed to do better than a human's baseline with crucial details stated clearly in the paper on the GRID dataset. This is no different than claiming superhuman performance on say MNIST. The claim was of course specific to that dataset and the paper was very clear about this. There is in addition a good record of us clarifying this in the media.\n\n(ii) We were then accused of using too simple a dataset. We argued that this was the largest public dataset that has been used by many other labs including Juergen Schmidhuber's to benchmark lip reading. It is about 28 hours of video (more than 2,000,000 frames), and even if the grammar is reduced, the sequence to sequence treatment makes this a problem of complexity similar to ImageNet, and not MNIST or CIFAR as some might have thought. This is a large and difficult task, and many previous hand-engineered as well as deep learning approaches had done much worse. Moreover, to further make this point we started working on building new datasets with NVIDIA. We continued having performance in the 90s with the new dataset, which is not finished because building such datasets is hard work. \n \n(iii) We were told our approach is a toy-problem setting and doesn't scale. We argued that our approach is similar to what was used in automatic speech recognition (ASR) and that provided we could collect more data we saw not impediment to scaling.\n\n(iv) Our previous comment relating to ASR then led to the accusation that there is nothing novel here as this is just a standard ASR pipeline. I hope that at this point the reader starts seeing where our frustration begun. We then argued that while there is a similarity to ASR, we are now doing video to speech and that many baselines (advanced in the paper) or other alternative deep learning approaches (including prior work which we cite as well as several papers that appeared after ours) have done worse than LipNet on GRID. That is, one can come up with many deep learning solutions, but one in particular works much better: LipNet. The paper presents many ablations and comparisons to make this point clear.\n\n(v) We were accused of being unfair because of using a sequence (video) to sequence (imperative sentences with restricted grammar) model and comparing it to what everyone was doing recently: classification. We noted that we were solving a much harder problem (the real one) and that even by solving a harder problem we still do better. We have now added experiments showing that our gains for the classification setup are of course even higher.\n\n(vi) We were asked to compare against approaches that appeared after our paper on arxiv. Weeks after releasing our paper a few papers appeared - all of a sudden this became very popular:\n\n- Thanda et al., 2016, \u201cAudio Visual Speech Recognition using Deep Recurrent Neural Networks\u201d, (9 Nov. 2016), present a multi-modal architecture using CTC and RNNs and evaluate on the GRID corpus. The authors train an audio model to obtain frame-level labels, and then train a feed-forward network with a bottleneck to predict those frame-level labels. This bottleneck is concatenated with its \u0394 and \u0394\u0394 features and is used as the input to the RNN. However, this pipeline is not end-to-end from pixels to characters. The performance is lower.\n\n- Chung et al., 2016, \u201cLipreading Sentences in the Wild\u201d, (16 Nov. 2016), present a multi-modal sequence-to-sequence model with attention. The model is pre-trained on a much larger dataset (BBC Sentences), which is yet not publicly available, and claim 3.3 WER on Speaker Independent and 5.8 WER on Speaker Dependent on the GRID corpus. The only way we can fairly compare to the new results is by also pre-training on the BBC dataset. Our performance is close, but without the need for pre-training. We think this is an excellent competitor to our approach. We have requested the data, but have been told to wait till mid-2007.\n\n- Sanabria et al., 2016, \"Robust end-to-end deep audiovisual speech recognition\", (21 Nov. 2016), present a new state-of-the-art in IBM ViaVoice (Neti et al. 2000), using CTC and RNNs. The training dataset has 17111 utterances of 261 speakers for training (about 34.9 hours), and 1893 of 26 speakers utterance (4.6 hours) for testing, and is not publicly available. The authors use SIFT features and PCA to reduce the dimensionality of the visual modality. The video-only performance reported is 65.7% accuracy in predicting 12 viseme categories as targets. \n\n- Ephrat and Peleg 2016, \u201cVID2SPEECH: Speech Reconstruction from Silent Video\u201d, (2 Jan. 2017, ICASSP 2017), use a CNN, which is similar to LipNet without the RNN temporal feature aggregation, for generating an intelligible acoustic speech signal from silent video frames of a speaking person from the GRID corpus. \n\nRight now most of these papers cite our arxiv paper. Two have been accepted. So it seems that the idea of applying deep learning to solve lip reading is considered to be valuable in the speech and vision communities at least.\n\n(vii) We were accused of over claiming regarding sentence level. We hope this has been made more clear in our current paper version. LipNet is to the best of our knowledge the first \"end-to-end sentence level\" lip reading method. There were prior approaches as pointed out by the reviewers, but the results for those hand-engineered approaches were very poor in comparison. As shown by the references above, the end-to-end approach is now feasible. In any case, this is a minor claim of the paper, the leap in state-of-the-art performance with a scalable architecture is the most important claim.\n\n(viii) LipNet, in the end, has been accused of merely being an engineering contribution. We disagree with this as our paper contains many ablations, experimental variations, analyses using saliency etc. However, ultimately, it is true that this paper is the result of a large engineering effort of two PhD students who in the process contributed largely to Torch and public domain speech code over a period longer than a year. Some might thing that engineering doesn't matter, others might see it as one of the most important things behind the deep learning revolution. I am of the latter view. See also http://blog.dennybritz.com/\n\nWhat makes this frustrating is that we answered all the questions the reviewers asked us. We added the extra experiments they requested, thereby addressing their concerns. We extended the ablations. We felt we had answered all the questions, only to be told at the end the paper was being rejected because: \"The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.\" \n\nThe thing is, none of the other deep learning attempts (and there are many) had obtained the same performance as LipNet. So surely, this cannot be a straightforward solution. Moreover, there is significant innovation in terms of baselines, analysis and ablations in this paper.\n\nThe other two reviewers haven't yet answered. We would be thankful if they could look at the extended paper and rebuttal and comment.\n\nWhat makes this very perplexing too is that we have had and incredibly positive reception for this idea by most tech companies who are interested in the technology, this work was chosen to be show-cased at a plenary talk at CES (one of the largest conferences in the world), we have received numerous emails for folks dealing with hearing-impairment requesting that we continue this research toward hearing aids (Incidentally, in case you think this application is not relevant enough: \"Over 5% of the world\u2019s population \u2013 360 million people \u2013 has disabling hearing loss\", http://www.who.int/mediacentre/factsheets/fs300/en/ ). So to be rejected with two straight 4's (what often we attributed to papers that are deeply flawed) is rather disheartening. \n\nOn a positive note, we continue building larger datasets and extending LipNet. We hope we will eventually use it to help folks with hearing problems and create silent interfaces. We also plant to release some of the new datasets to advance LipReading as well as what we think would be the first public good implementation of CTC.\n\nWe think in the future it will also be very interesting comparing our approach with latent variables and analytical integration via dynamic programming to the sequence-to-sequence approach using attention. There are pros and cons to both approaches. We believe, the dynamic programming approach (LipNet) requires less data, but this needs to be validated with experiments. Some argue that CTC is harder to understand than attention. Yet we point out that the idea of having models with discrete latent variables and using exact algorithms to marginalize them (eg forward-backward, junction tree and belief propagation) is well established in classical machine learning. Many recent papers on networks with discrete latent variables end up using reinforce, instead of just doing everything analytically, which is often possible in those papers. So at least, we hope ideas like CTC will become better understood in the literature. We also desperately need good public implementations of this idea.\n  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1485539352341, "tcdate": 1483986742315, "number": 14, "id": "SkCNv8bUx", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["~Nando_de_Freitas1"], "readers": ["everyone"], "writers": ["~Nando_de_Freitas1"], "content": {"title": "Reply to reviewers", "comment": "We strongly request that our paper be reviewed for its contents, contributions, significance, originality, and impact. We also would like to highlight the following three points to reviewers and readers. \n\n(i) Claim of \"sentence-level\" lipreading: This was the main concern of reviewers 1 and 2, and likely why they gave us the surprisingly low scores of 4 (7 being borderline for acceptance). Our claim is that we proposed the first \"end-to-end sentence-level\" lipreading approach, and we have edited the paper to make this precise. The phrase \u201cend-to-end\u201d is very important. The previous few attempts at sentence-level lipreading used heuristic pipelines and obtained poor results. LipNet, on the other hand, is fully end-to-end and achieves state-of-the-art results by a significant margin. on the largest available public dataset  (with over 24 hours of video, making it far from trivial). Given the removal of the phrase sentence-level, the reviews of reviewers 1 and 2 regarding prior work are not longer applicable.\n\n(ii) Is the dataset too simple? GRID despite all its shortcomings, is the largest available public dataset with over 24 hours of video (more than 2,000,000 frames and 64,000 possible sentences), making it more comparable to ImageNet than to say MNIST or CIFAR-10. May deep learning papers have used simplified versions of this dataset (eg restrictions to speaker-dependent recognition, or classification only) and even then obtained worse results. Moreover, we have embarked on more commercial datasets with success as reported in this keynote by NVIDIA's CEO at CES (177,393 attendees). See this video (https://www.youtube.com/watch?v=YTkqA189pzQ) for an example.\nAs we have stated several times, our solution is scalable and the only key challenge is getting more data.\n\n(iii) Claim of state-of-the-art: At the time of submission to ICLR, we had broken the state-of-the-art on GRID by a very large margin. This alone is a significant achievement and we are rather puzzled by the negative reviews. Two weeks later, another team at Oxford slightly outperformed our results using sequence to sequence attention models, but they pre-train on a much larger BBC dataset, so their results are not comparable to ours. We requested the BBC dataset but is not available for legal reasons. Hence we cannot compare performance at this stage. We hope to do so in the future as there are pros and cons to the two approaches that we need to understand better as a research community. Weeks after our paper, several industrial and academic labs put forward solutions to lip-reading that are similar to our approach but they achieved lower performance on GRID, highlighting the importance of the design choices we made in this paper. \n\nSome of those papers have been accepted at prestigious conferences and they cite us. \n\nLipNet has done for lip-reading what deep learning did before for speech and image recognition. The architecture does however require careful thought and engineering as pointed out above. We came up with many ablations that demonstrate this in the paper. \n\nLipNet continues being one of the best solutions for end-to-end sequence lip-reading, with no other method having been able to match its performance (unless pre-training on much larger, unavailable datasets), and it has had large impact already. It will be terribly unfair if this paper gets rejected on the grounds of subjective opinion with a discussion of the quality of the architecture, experiment details, and quality of the results. \n\nWe have addressed all the reviewer questions in detail below and in the current draft of our paper.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1485171641280, "tcdate": 1485171641280, "number": 21, "id": "HJbTow7Dl", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "ByMQxllPx", "signatures": ["~Nando_de_Freitas1"], "readers": ["everyone"], "writers": ["~Nando_de_Freitas1"], "content": {"title": "When social media meets scientific reviewing", "comment": "I certainly don't mean to bully junior people. In fact, there is no way of me knowing that the reviewer is a junior person and I never made that assumption. The only junior people I'm aware of here are my two PhD students who spent their first year of PhD building LipNet, and ended up with this reviewing process.\n\nWhile I agree that as the discussion progressed I became rude, and apologize to the reviewer and everyone else for this, it is also the case that this reviewing process has the danger of wrong statements being associated with one's paper unless one public objects. That is, in ICML and NIPS reviewing for instance, only the authors and area chairs get to see reviews that may be making incorrect statements about the paper. However, in this ICLR public setup, everyone gets to see those statements even if they are incorrect. The authors therefore have the duty to clarify such statements. In the case of LipNet, this has been a long ongoing battle (see my next post).\n\nI did not mean to say that anonymous folks commenting here have been particularly disrespectful. However, often what they contribute is arguments that are completely orthogonal to the paper. I am personally of the view that I prefer to engage with people by name and have a polite conversation about science as we always have had. The reviewers choosing to stay anonymous is however a good thing. The problem with everyone commenting anonymously and about random topics is that soon a discussion that was meant to be about the paper becomes a discussion about social media, reviewing, etc. \n\nI would like this to be an objective scientific discussion. I would be disappointed if this ends up being no more than another site like youtube. This I see as a threat to this reviewing process, and this particular paper already highlights this threat. For example, while Neil is a wonderful colleague and had good intentions, by inappropriately posting a rant about the dangers of media in ML and emphasizing our paper here and in the media, our paper became associated with something it simply wasn't doing - even if he didn't mean it that way. There was a clear backlash, eg google LipNet. Also in this reviewing setup, the reviewers can see the comments and previous reviews before entering their review. This tends to cause biases. We are of the opinion that the review process in this paper was subject to many biases, even if the reviewers had good intentions to simply write a good review as part of their contribution to the community.\n\nAll I can suggest is for reviewers to follow guidelines such as https://nips.cc/Conferences/2013/PaperInformation/ReviewerInstructions when entering the reviews. I often find it a good idea to read this website before I start writing my reviews. I would still like to see one review following these criteria for our paper.\n\nUltimately, even if rejecting a paper, one has to provide guidance to the authors on how to improve the paper or point out serious flaws. We are all in this together, and the progress in the field is what matters.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1484964706148, "tcdate": 1481788287033, "number": 1, "id": "B1DFj6JEg", "invitation": "ICLR.cc/2017/conference/-/paper230/official/review", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["ICLR.cc/2017/conference/paper230/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper230/AnonReviewer2"], "content": {"title": "A nice result, but with limited novelty", "rating": "4: Ok but not good enough - rejection", "review": "UPDATE:  I have read the authors' responses.  I did not read the social media comments about this paper prior to reviewing it.  \n\nI appreciate the authors' updates in response to the reviewer comments.  Overall, however, my review stands.  The authors have taken a task that had not yet been addressed with a straightforward modern deep learning approach, and addressed it with such an approach.  I assume that if we pick up any task that hasn't been worked on for a while, and give it a solid deep learning treatment, we will do well.  I do not see such papers as a contribution to ICLR, unless they also provide new insights, analysis, or surprising results (which, to my mind, this paper does not).  This is a general point and the program chairs may disagree with it, of course.\n\nI have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus.  \n\n************************\n\nORIGINAL REVIEW:\n\nThe authors show that an appropriately engineered LSTM+CNN+CTC network does an excellent job of lipreading on the GRID corpus.  This is a nice result to know about--yet another example of a really nice result that one can get the first time one applies such methods to an old task--and all of the work that went into getting it looks solid (and likely involved some significant engineering effort).  However, this in itself is not sufficiently novel for publication at ICLR.  The paper also needs to be revised to better represent prior work, and ideally remove some of the vague motivational language.  Some specifics on what I think needs to be revised:\n\n- First, the claim of being the first to do sentence-level lipreading.  As mentioned in a pre-review comment, this is not true.  The paper should be revised to discuss the prior work on this task (even though much of it used data that is not public).  Ideally the title should also be changed in light of this.\n\n- The comparison with human lipreaders needs to be qualified a bit.  This task is presumably very unnatural for humans because of the unusual grammar, so perhaps what you are showing is that a machine can better take into account the strong contraints.  This is great, but not a general statement about LipNet vs. humans.\n\n- The paper contains some unnecessary motivational platitudes.  We do not need to invoke Easton and Basala 1982 to motivate modeling context in a linguistic sequence prediction task, and prior work using older sequence models (e.g. HMMs) for lipreading has modeled context as well.  The McGurk effect does not show that lipreading plays a crucial role in human communication.\n\n- It is worth noting that even without the spatial convolution, your Baseline-2D already does extremely well.  So I am not sure about the \"importance of spatiotemporal feature extraction\" as stated in the conclusion.\n\nSome more minor comments, typos, etc.:\n\n- citations for LSTMs, CTC, etc. should be provided the first time they are mentioned.\n- I did not quite follow the justification for upsampling.\n- what is meant by \"lip-rounding vowels\"?  They seem to include almost all English vowels.\n- Did you consider keeping the vowel visemes V1-V4 separate rather than collapsing them into one?  Since you list Neti et al.'s full viseme set, it is worth mentioning why you modified it.\n- \"Given that the speakers are British, the confusion between /aa/ and /ay/...\" -- I am not sure what this has to do with British speakers, as the relationship between these vowels exists in other English dialects as well (e.g. American).\n- The discussion about confusions within bilabial stops and within alveolar stops is a bit mismatched with the actual confusion data in Fig. 3(b,c).  For example, there does not seem to be any confusion between /m/ and /b/ or between /m/ and /p/.\n- \"lipreading actuations\":  I am not sure what \"actuations\" means in this context\n- \"palato-alvealoar\" --> \"palato-alveolar\"\n- \"Articulatorily alveolar\" --> \"Alveolar\"?", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655366, "id": "ICLR.cc/2017/conference/-/paper230/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper230/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper230/AnonReviewer2", "ICLR.cc/2017/conference/paper230/AnonReviewer1", "ICLR.cc/2017/conference/paper230/AnonReviewer3"], "reply": {"forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655366}}}, {"tddate": null, "tmdate": 1484943386382, "tcdate": 1484943386382, "number": 19, "id": "ByMQxllPx", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "SJxtDylPx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Glad we're on the same page", "comment": "Glad that we seem to agree in spirit, although I still think that your disagreement could have been expressed more respectfully. It's important to avoid the possible appearance of a senior figure from a top lab \"bullying\" a (possibly junior) reviewer.\n\nIn response to your recommendations, \n\n(i) Can you point out anonymous comments from non-reviewers in this thread which haven't been respectful? I only see one other, besides mine, and they both seem pretty reasonable. I'd argue anonymity is important for the same reason as it is with reviewers. Frankly, without the assurance of anonymity, more junior people in the field (such as myself) would surely be more hesitant about questioning the work of established researchers like yourself. \n\n(ii) I presume you're referring to Neil Lawrence's comment? He makes it pretty clear that he's \"not making a comment about whether the paper should be accepted or not\", although I appreciate where you're coming from."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1484942978696, "tcdate": 1484942978696, "number": 1, "id": "BkotC1gDg", "invitation": "ICLR.cc/2017/conference/-/paper230/official/comment", "forum": "BkjLkSqxg", "replyto": "SJxtDylPx", "signatures": ["ICLR.cc/2017/conference/paper230/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper230/AnonReviewer2"], "content": {"title": "clarification re workshop track", "comment": "Unfortunately my review seems to have rubbed the author(s) the wrong way, and I do not want to add to the discussion on my or their reasonableness.  But I do want to clarify an apparent misunderstanding:\n\nI'd very much like to see this paper in the workshop track.  The reason I removed that recommendation is that I noticed that the workshop CFP calls for \"late-breaking developments, very novel ideas and position papers\", which does not seem to me to fit this paper.  Personally I prefer a broader view of the workshop track, and in that broader view would see this as an excellent workshop contribution; but that is not up to me.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674026, "id": "ICLR.cc/2017/conference/-/paper230/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper230/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper230/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674026}}}, {"tddate": null, "tmdate": 1484941175782, "tcdate": 1484941175782, "number": 18, "id": "SJxtDylPx", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "S13CiA1wx", "signatures": ["~Nando_de_Freitas1"], "readers": ["everyone"], "writers": ["~Nando_de_Freitas1"], "content": {"title": "The real danger is social media, popularity, and arguments not based on evidence taking over the scientific process of reviewing ", "comment": "\nI sympathize with your comment. I'm not enjoying this at all.\n\nOur first reply addressed any concerns reviewers might have had in the appropriate way.\n\nHowever, when a reviewer fails to provide a factual review based on evidence, concluding with the comment \"I have removed my recommendation that this be accepted as a workshop paper, as I have since noticed that the workshop track this year has a different focus\" despite the fact that the ICLR Workshops website --- http://www.iclr.cc/doku.php?id=iclr2017:workshopcfp --- states clearly that the topics covered may include \"Applications in vision, audio, speech, natural language processing, robotics, neuroscience, or any other field\", then we know something has gone very wrong and that the reviewer is being unreasonably adversarial.\n\nThe reviewer has done its best to reject this paper. We accept the reject decision, but we do not accept the reasons for rejection. One thing is to reject the work, another one is to trash it without proper objective reason as the reviewer has done.\n\nAs a community we should indeed be thinking of how to avoid this. Two recommendations: \n\n(i) Anonymous commenting should not be allowed unless you're a reviewer. The tone is far more respectful when one knows who one is addressing.\n\n(ii) Comments like the first comment below should have been moderated and deleted. They are about bigger problems that are orthogonal to this paper.\n\nOtherwise, there is little difference between reviewing for ICLR and commenting on videos in youtube.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1484938196191, "tcdate": 1484938196191, "number": 17, "id": "S13CiA1wx", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "H1-Ohjkvg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Disappointing", "comment": "Regardless of the relative technical merit of one side versus another, I think that as a community we should strive to avoid ad hominem attacks like this. Characterizing reviews that disagree with you as \"utterly condescending, disrespectful, and unnecessary\", \"unreasonable and purely subjective\", \"absolute nonsense\", something \"that most researchers on deep learning would disagree with\", \"that you don't understand the experiments\", is not conducive to an open and honest exchange of ideas, and will hurt our field in the long term. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1484933754583, "tcdate": 1484926057465, "number": 16, "id": "H1-Ohjkvg", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "B1DFj6JEg", "signatures": ["~Nando_de_Freitas1"], "readers": ["everyone"], "writers": ["~Nando_de_Freitas1"], "content": {"title": "Engineering good deep learning representations that solve real problems of great interest is important for ICLR", "comment": "The update to the review is unreasonable and purely subjective. It is clear that this paper is not only a straightforward application of deep learning to lip-reading. There have been many such papers by Andrew Zisserman, Juergen Schmidhuber, etc. We are not the first to apply deep learning to lip-reading. We were however the first to do it well and get very good results. The success of the method is the result of careful design of how to use video as input (the baselines clearly show that many other representations do worse), and how to map from this to a sequence. \n\nWhile the component modules (such as LSTMs, 3D convolutions, dynamic programming, and other layers) are known in deep learning, it is true most deep learning papers use these same modules. The challenge is how to put them together in an intelligent way to solve problems.\n\nEngineering matters when building representations!\n\nIf we were to apply your standard for acceptance to all papers that use deep learning to solve real problems of interest, then no papers on applications of deep learning would ever be accepted. Your argument is absolute nonsense. \n\nIf you believe papers about applications of deep learning don't belong in ICLR, please speak to your area chair and ask if this is the case. It is not the case to the best of my understanding. Well executed solutions to important problems matter.\n\nI really fail to understand your intentions. We addressed all your concerns in the rebuttal in a positive way. Now, you've come back and offered to reject the paper, and not even recommend it as a workshop, for a completely different subjective reason that most researchers on deep learning would disagree with. \n\nAll the analysis and insights you are asking for are in the experimental section of the paper. Please read the paper carefully and comment on actual content. It is clear from your comment about 2D convolution baselines that you don't understand the experiments. We spent a great deal of work creating many baselines and ablations specially to provide insights on how to design video to text models for lip reading, but more widely applicably. We would love objective feedback on the experiments and results.\n\nFinally, your recommendations about how to write the paper are utterly condescending, disrespectful, and unnecessary.\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1482148969740, "tcdate": 1482148969740, "number": 13, "id": "SJzOhSBNl", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "B1knAWMEg", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "Thank you for your feedback. Our novelty lies both in the domain as well as the architecture. In contrast to the previous state-of-the-art, which reported a \u201c14% performance drop\u201d using spatiotemporal convolutions instead of spatial convolution (Chung et al. 2016), we present a spatiotemporal convolution architecture, coupled with a recurrent neural network, that substantially improves performance, resulting in ~2.3x lower WER.\n\nFurthermore, for a fair comparison in sentence-level predictions, our Baseline-LSTM model replicates the architecture of Wand et al. 2016, which is the previous deep learning state-of-the-art in the GRID corpus. LipNet achieves ~5x lower WER than Baseline-LSTM.\n\nTo illustrate the effect of the language model, in our latest revision, we have introduced a new baseline (Baseline-NoLM), which exhibits only a minor performance drop. A word-only model was never attempted, as it was not the goal of the present work. We think that sentence-level predictions are far more challenging, interesting, and useful.\n\nGRID is the largest publicly available audio-visual sentence-level dataset. Our results are a major step in end-to-end sentence-level training. Evaluating LipNet and building a larger dataset is one of our current efforts."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1481936550600, "tcdate": 1481936550600, "number": 3, "id": "B1knAWMEg", "invitation": "ICLR.cc/2017/conference/-/paper230/official/review", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["ICLR.cc/2017/conference/paper230/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper230/AnonReviewer3"], "content": {"title": "Impressive result on lip reading, interesting analysis, some flawed comparison", "rating": "6: Marginally above acceptance threshold", "review": "The authors present a well thought out and constructed system for performing lipreading. The primary novelty is the end-to-end nature of the system for lipreading, with the sentence-level prediction also differentiating this with prior work. The described neural network architecture contains convolutional and recurrent layers with a CTC sequence loss at the end, and beam search decoding with an LM is done to obtain best results. Performance is evaluated on the GRID dataset, with some saliency map and confusion matrix analysis provided as well.\n\nOverall, the work seems of high quality and clearly written with detailed explanations. The final results and analysis appear good as well. One gripe is that that the novelty lies in the choice of application domain as opposed to the methods. Lack of word-level comparisons also makes it difficult to determine the importance of using sentence-level information vs. choices in model architecture/decoding, and finally, the GRID dataset itself appears limited with the grammar and use of a n-gram dictionary. Clearly the system is well engineered and final results impress, though it's unclear how much broader insight the results yield.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655366, "id": "ICLR.cc/2017/conference/-/paper230/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper230/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper230/AnonReviewer2", "ICLR.cc/2017/conference/paper230/AnonReviewer1", "ICLR.cc/2017/conference/paper230/AnonReviewer3"], "reply": {"forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655366}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481911632286, "tcdate": 1478278994614, "number": 230, "id": "BkjLkSqxg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkjLkSqxg", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "content": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 28, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481904630686, "tcdate": 1481904630686, "number": 12, "id": "Hyy-z5W4e", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "Sk4BjIWEx", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "Please see our revised document and the responses to the questions below. We believe most of your concerns have been addressed in the latest version of our document.\n\nThanks to the reviewing process, we have learned that the claim of sentence-level lipreading was too broad. We have therefore fixed this. In our defense, the vast literature on lipreading from video only focuses on word-level predictions. Moreover, in works doing both audio and video, video only results take a backstage role and performance tends to be poor.  Given this, it is not surprising that we and many other researchers (we are not alone in this) had arrived at this conclusion.\n\nWe are now making what we think is an accurate but still very important claim: end-to-end sentence level lipreading. We have an extensive list of citations in our paper. We have not found any reference in the literature that does end-to-end sentence-level lipreading, despite a thorough search and given the wide discussion we have had on this paper. Certainly, we have not found any works that match our performance on the popular GRID dataset.\n\nSeveral works have appeared after this work using similar pipelines of RNNs and CTC, but those works have failed to achieve the same performance when tested on GRID. Here, and as in most of deep learning, good engineering matters a lot as you kindly acknowledge.\n\nWe only know of one work can can match the performance of our method (https://arxiv.org/abs/1611.05358), but that work appeared two weeks after we made our paper available. They use a dataset that is not publically available for pretraining, and hence we have no means of comparing to them directly yet."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1481892637963, "tcdate": 1481892637963, "number": 11, "id": "S18QXPbEl", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "B1DFj6JEg", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "1) We have innovated an end-to-end approach to sentence-level lipreading that outperforms the state-of-the-art on GRID. Despite the vast literature on the topic, this had not been done properly before. \n\nWe respectfully, but emphatically, disagree with the subjective opinion that this work only qualifies as a workshop paper. Our line of reasoning is based on the following two facts. First, the many other papers on lip-reading, which presented hand-engineered or insufficiently general models/architectures and far worse results in comparison to ours, were worthy of publication on venues other than workshops. Ours being a simpler, more powerful, and better-performance model should therefore be held to the same standard. Second, much of the progress in speech recognition and computer vision mirrors what we have done here: the introduction of a simple, accessible, scalable, accurate, end-to-end trainable pipeline that yields better results on existing public datasets.\n\n2) LipNet is the first end-to-end model that performs sentence-level sequence prediction. Previous work in the field requires either heavy preprocessing of frames to extract image features, temporal preprocessing of frames to extract video features (e.g., optical flow or movement detection), or other types of handcrafted vision pipelines. However, we agree that to reflect this we should change the title to: LipNet: End-to-End Sentence-Level Lipreading. Please see our revised paper and reply to comments concerning Neti et al 2000 and claims of prior work on sentence level prediction.\n\n3) The grammar of GRID and the setup are indeed unnatural. However, the people doing the test had the grammar in front of them. Our results with people are specific to the GRID dataset and should be treated as a baseline, and no more than this. We chose to create as many baselines as possible because of the scarcity of prior work at the sentence level on GRID, and our desire to be thorough.\n\nUntil we build large datasets for lipreading and real-time apps, it is not wise to make wild claims of superhuman performance. The media recently made such claims about our work, and we spent a lot of time and energy correcting and appropriately qualifying such claims. We certainly make no such claims.  \n\n4) The McGurk effect is frequently used to motivate audio-visual interfaces. Some citations are perhaps unnecessary, but since these are common citations in this research area, we chose to include them.\n\n5) Spatiotemporal convolutions result in 2.3x lower WER, compared to Baseline-2D. This is very significant and not to be dismissed. Again, we chose to be thorough and in the new version add further baselines.\n\n6) We have addressed these concerns in the revised version of our paper.\n\n7) We follow the phoneme-to-viseme categorisation provided by Neti et al. 2000. V1-V4 provided no interesting additional information in terms of plotting.\n\n8) We agree with the reviewer that this exists in other dialects; our analysis is based on this dataset.\n\n9) This is most likely a consequence of the grammar of the dataset. /p/ and /b/ appear in several words, while /m/ appears as a letter.\n\n10) Thank you for the suggestions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1481892524982, "tcdate": 1481892524982, "number": 10, "id": "HkHhfPW4g", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "ByS0-t87l", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Author's Response", "comment": "1) We refer to a language model that assigns binarised probabilities depending on the existence of an n-gram on the training set. Equivalently, this is interpretable as a uniform distribution over n-grams that occur at least once in the training set. This setup is also suggested in the decoding section of Graves and Jaitly 2014.\n\n2) To illustrate the effect of the language model, we have introduced a new baseline (Baseline-NoLM), which exhibits a minor performance drop. A word-only model was never attempted, as it was not the goal of the present work. We think that sentence-level predictions are far more challenging, interesting, and useful.\n\n3) Thank you. This would indeed be a nice thing to visualize.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1481892472299, "tcdate": 1481892472299, "number": 9, "id": "SkxFMP-4l", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "H1OgBDyXe", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "1) The dataset is limited and, since our task is end-to-end, we require more data. Furthermore, Ngiam et al. propose a multi-modal approach, whereas we focus on the single modality of video, which is more demanding. We are aiming for fully silent dictation interfaces. Of course, we could also trivially add audio to our model, but this would be for a different set of applications. Both approaches are worthwhile and lead to different apps. \n\n2) Hilder et al. state that 71.6% is the accuracy of human lip-reading for classifying letters A-F. The results are the mean over 17 participants. However, the word classification accuracy (over 225 monosyllabic words) was 18.42% as stated in Table 4; this is the performance that we cite. Two machine based approaches achieved accuracies of 3.75% and 5%, significantly worse than the human performance.\n\n3) Indeed. We have corrected this in our new paper version. \n\nNew good results on GRID appeared two weeks after this paper, but these are based on using a much larger dataset (BBC) to pre-train the models used in GRID. The only way we can fairly compare to the new results is by also pretraining on the BBC dataset. We have requested this dataset, but the authors and BBC have stated that they cannot make it available to us at present.  Both approaches have pros and cons and at present the results are close. We hope to compare the two approaches systematically once we have access to the BBC dataset, or once we finish new datasets that we are currently constructing.\n\nIn the final version of our paper, we will add an extra paragraph discussing approaches that have appeared after we made our paper public on openreview.\n\n4) Thank you for the references; we have included them in our update.\n\n6) When switching from LSTMs to GRUs, we found that upsampling was not needed. We currently use GRUs and do not upsample.\n\n7) The network was trained for 150 epochs. Our tuning was done on the training set, as most architectures simply either worked or did not work on the training set. The GRID dataset is not big enough to do proper validation. We need bigger datasets to validate our model --- creating this much larger dataset is one of our current efforts.\n\n8) The dropout parameter equalling zero indicates that dropout was disabled. The baseline architecture included two layers so this was indeed an error in the text; thank you for pointing it out. We correct this in our paper update.\n\n9) The means and standard deviations were approximated using a random subset of the training set, aggregating all the pixels channel-wise in the frames for each video.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1481892378103, "tcdate": 1481892378103, "number": 8, "id": "S1z7GvWNl", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "SyiMINJQe", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "After examining the long report of Neti et al, which mostly focuses on both audio and video as inputs, we have realized that indeed their pipeline applies to sentence-level predictions for video only. Thank you for bringing this to our attention. Their pipeline represents the old way of doing speech with many hand-engineered components. As stated, their visual-only results cannot be interpreted as visual-only recognition, as they are used as rescoring of the noisy audio-only lattices. Potamianos et al. (2003), report visual-only results on the same dataset which we have included in our references, as well as the work of Goldschen et al. 1997.\n\nWe believe our end-to-end pipeline, which is similar to modern ASR pipelines, is a better solution. In our revision, we are being more precise with our claim to novelty, namely end-to-end sentence-level prediction. \n\nRelated to this, and following the appearance of our paper,  Thanda et al. 2016 (Audio Visual Speech Recognition using Deep Recurrent Neural Networks) and much later Sanabria et al. 2016 (Robust end-to-end deep audiovisual speech recognition) have proposed similar RNN and CTC architectures. However, the feature extraction in those publications is not fully end-to-end. The first paper does not achieve performance that is as good as LipNet. Interestingly, Sanabria et al present a new state-of-the-art in IBM ViaVoice (Neti et al 2000), using CTC and RNNs. The authors use SIFT features and PCA to reduce the dimensionality of the visual modality. The video-only performance reported is 65.7% accuracy in predicting 12 viseme categories as targets. We have no access to this dataset.\n\n\n\nThank you for pointing this out. We originally intended to refer the reader to the review of Zhou et al. 2014 for details on prior work due to the wealth of such papers which exist. However, this statement appears to have been accidentally removed since. We have fixed this in our revision. \n\nOur citation list is over two pages long, but we are happy to continue adding references to earlier work if reviewers think they improve the presentation and claims."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1481892320933, "tcdate": 1481892320933, "number": 7, "id": "B1FJfPbNx", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "Hy0rG3KWe", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "Thank you for sharing this work with us. We are appropriately citing it in the new version of our paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1481890619621, "tcdate": 1481890619621, "number": 2, "id": "Sk4BjIWEx", "invitation": "ICLR.cc/2017/conference/-/paper230/official/review", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["ICLR.cc/2017/conference/paper230/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper230/AnonReviewer1"], "content": {"title": "Nice application of end to end training on the visual pipeline of traditional AV-ASR systems. Sentence level sequence objectives (MPE/MMI/fMPE) have been applied to this problem in the past", "rating": "4: Ok but not good enough - rejection", "review": "- Proven again that end to end training with deep networks gives large\ngains over traditional hybrid systems with hand crafted features. The results \nare very nice for the small vocabulary grammar task defined by the GRID corpus. The engineering here is clearly very good, will be interesting to see the performance on large vocabulary LM tasks. Comparison to human lip reading performance for conversational speech will be very interesting here.\n\n- Traditional AV-ASR systems which apply weighted audio/visual posterior fusion reduce to pure lip reading when all the weight is on the visual, there are many curves showing performance of this channel in low audio SNR conditions for both grammar and LM tasks.\n\n- Traditional hybrid approaches to AV-ASR are also sentence level sequence trained with fMPE/MPE/MMI etc. objectives (see old references), so we cannot say here that this is the first sentence-level objective for lipreading model (analogous to saying there was no sequence training in hybrid LVCSR ASR systems before CTC). \n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512655366, "id": "ICLR.cc/2017/conference/-/paper230/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper230/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper230/AnonReviewer2", "ICLR.cc/2017/conference/paper230/AnonReviewer1", "ICLR.cc/2017/conference/paper230/AnonReviewer3"], "reply": {"forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512655366}}}, {"tddate": null, "tmdate": 1481179596840, "tcdate": 1481179596835, "number": 2, "id": "ByS0-t87l", "invitation": "ICLR.cc/2017/conference/-/paper230/pre-review/question", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["ICLR.cc/2017/conference/paper230/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper230/AnonReviewer3"], "content": {"title": "Pre-review questions", "question": "1. Could you clarify what is meant by \"binarised\" language model?\n\n2. To see how much context helps, similar to language model vs. no language model results reported below, do the authors have results just training with words instead of sentences?\n\n3. Curious if there's a way to measure the saliency/weight assigned to each time step to see if certain characters associated with those time steps are assigned more/less weight?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481179597361, "id": "ICLR.cc/2017/conference/-/paper230/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper230/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper230/AnonReviewer2", "ICLR.cc/2017/conference/paper230/AnonReviewer3"], "reply": {"forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481179597361}}}, {"tddate": null, "tmdate": 1480714283504, "tcdate": 1480713455568, "number": 6, "id": "H1OgBDyXe", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Comments / Questions", "comment": "\n\nThis is an interesting paper. Here are some comments:\n\n\n1) Testing on 4 subjects only is rather limited given that there are 33 subjects. For example Ngiam et al. used 18 subjects for training and 18 for testing on CUAVE which contains a similar number of subjects. So a similar setup would be 17 subjects for training and 16 for testing. This would make the results much more convincing.\n\n2) The authors state that the performance of human lipreaders is around 20% and cite Hilder et al. In that paper, the human performance is around 70%, please correct this claim. In addition, in that paper machine lip-reading outperforms humans as well so please make this point clear. \n\n3) The state of the art performance on GRID is not 79.6% as mentioned in the text. It is the one mentioned in the other comments. Actually, as of last week there is a new state-of-the-art https://arxiv.org/abs/1611.05358. Technically, that paper was made available after the submission of the current one. The authors may comment on it in the final version.\n\n4) Some recent references on lip reading using deep methods are missing\n H. Ninomiya, N. Kitaoka, S. Tamura, Y. Iribe, and K. Takeda, \u201cIntegration of deep bottleneck features for audio-visual speech recognition,\u201d in Conf. of   the International Speech Communication Association, 2015.\n\nC. Sui, M. Bennamoun, and R. Togneri, \u201cListening with your eyes: Towards a practical visual speech recognition system using deep boltzmann machines,\u201d in IEEE ICCV, 2015, pp. 154\u2013162\n\nS. Petridis and M. Pantic, \u201cDeep complementary bottleneck features for visual speech recognition,\u201d in IEEE ICASSP, 2016, pp. 2304\u20132308\n\nHu, Di, and Xuelong Li. \"Temporal multimodal learning in audiovisual speech recognition.\" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3574-3582. 2016.\n Which achieves the best performance on AVLetters to the best of my knowledge (please update Table 1).\n\n5) The authors claim that 25 tokens per second is too constrained for CTC. Why is that? What's the motivation? The explanation is not clear and an upsampling layer is not common. It would be good that the authors provide results (maybe in the supplementary pages) without the upsampling layer so it is obvious what its impact is. \n\n6) For how many epochs was the network trained? Is there a validation set for parameter optimisation?\n\n7) p.11, A.3, dropout = 0, this must be a typo. Also, the replication of Wand's model might not be accurate. Wand uses a feedforward layer followed by 2 LSTM layers. The authors claim that they use an LSTM with 128 neurons. Is it only layer?\n\n8) p. 11, A.2, how were the means and standard deviations for the 3 channels were computed?\n\n\n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1480701458557, "tcdate": 1480701458553, "number": 1, "id": "SyiMINJQe", "invitation": "ICLR.cc/2017/conference/-/paper230/pre-review/question", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["ICLR.cc/2017/conference/paper230/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper230/AnonReviewer2"], "content": {"title": "prior work on lipreading", "question": "I have not yet read the paper completely, but just a quick note about references to prior work on lipreading.  The paper claims that \"All existing works, however, perform only word classification, not sentence-level sequence\nprediction.\"  This is not true -- prior work on audio-visual speech recognition often also reports video-only (i.e. lipreading) results, and does include sentence prediction.  For example the Neti et al. 2000 paper that you cite does so.  Can you clarify this claim, or remove it?\n\nIn general I am a bit taken aback by the lack of mention of the wealth of prior papers on audio-visual speech recognition which include also lipreading results.  The paper may leave readers with the impression that no such work exists, or that the authors don't know about it.  I hope that you can rectify this in a revision.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481179597361, "id": "ICLR.cc/2017/conference/-/paper230/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper230/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper230/AnonReviewer2", "ICLR.cc/2017/conference/paper230/AnonReviewer3"], "reply": {"forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper230/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481179597361}}}, {"tddate": null, "tmdate": 1479291461640, "tcdate": 1479291461635, "number": 5, "id": "Hy0rG3KWe", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["~Dorothea_Kolossa1"], "readers": ["everyone"], "writers": ["~Dorothea_Kolossa1"], "content": {"title": "state-of-the-art system", "comment": "Dear all,\n\nmany thanks for sharing the paper, which I did find hugely interesting - while the GRiD corpus is small, I do agree that it is currently a good task for taking first steps towards video-based and audio-visual speech recognition, and I believe the work you did here will ultimately be very useful for lipreading (and for lipreading-based speech enhancement) in more general scenarios.\n\nI do have one comment regarding the prior-state-of-the-art system that you cite: As far as I know, our group's Interspeech 2016 holds the record for best lipreading performance on the GRiD corpus, with 86.4% word accuracy (this does include a grammar, but it might still be interesting).\nPlease cite: S. Gergen, S. Zeiler, A. Hussen Abdelaziz, R. Nickel and D. Kolossa: \" Dynamic Stream Weighting for Turbo-Decoding-Based Audiovisual ASR,\" in Proc. Interspeech 2016, San Francisco, Sept. 2016.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1478854424268, "tcdate": 1478853833659, "number": 3, "id": "B1zC4ZXZg", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "S1cC1Ckbl", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "It is precisely because we use a language model, CTC, beam search and other modeling and training techniques that we can do better. (No LM gets CER 3% / WER 9%, and a smaller 3-gram LM gets CER 3% / WER 7%; cf. Table 2 where we show CER 2.4% / WER 6.6% for our best model.) Our goal is to build the best possible end-to-end lipreading system so as to make silent interfaces and novel hearing aids a reality. Crippling our approach to the point of not being speaker-independent or sentence level would completely defeat the point of the paper. \n\n\nInstead of crippling our method, we take the more optimistic and useful route of improving the baselines to produce enhanced baselines in Sections 4.2-4.3. Since we have no prior sequence prediction work to compare against on GRID, we create the 3 baselines by endowing the 2D convolutions and LSTM [Wang et al, 2016] approaches with CTC and language modelling, again with data augmentation for fair comparison. In table 2, we find that LipNet outperforms both of these baselines extended to the speaker-independent sentence-level; the baselines are themselves improvements over the methods that existed before. The task addressed in Table 2 is much harder as it requires speaker-independent sentence-level predictions --- ie what we care about. This table is very important and it sets metrics for future benchmarking on the GRID dataset. \n\n\nMoreover, for reference purposes, members of the Oxford Student\u2019s Disability Community helped us creating a third baseline (details in Section 4.2-4.3). This is in line with our goal, which is to design interfaces and devices for hearing-impaired people. To this end, GRID is an adequate dataset with the right type of video.   \n\n\nThe paper contains numerous references to the field of Automatic Speech Recognition (ASR). We have made it very clear that this architecture shares a lot in common with industrial-strength ASR architectures, in particular including Baidu DeepSpeech's (which we cite) CNNs/RNNs/CTC, among other work e.g. [1,2]. Engineering our architecture is far from trivial, and most certainly the reason behind why it hasn\u2019t been done before. Most industrial-quality deep speech recognition code and data only exists in industry and is not open.\n\n[1] Sainath, Tara N., et al. \"Learning the speech front-end with raw waveform cldnns.\" Proc. Interspeech. 2015. \n[2] Sainath, Tara N., et al. \"Convolutional, long short-term memory, fully connected deep neural networks.\" 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1478853876491, "tcdate": 1478853876484, "number": 4, "id": "HJ2xSZXbg", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "rk90jXplx", "signatures": ["~Yannis_M._Assael1"], "readers": ["everyone"], "writers": ["~Yannis_M._Assael1"], "content": {"title": "Authors' Response", "comment": "We thank you for helping with the GRID corpus and releasing it to the community. While it is small and has a restricted grammar (64000 possible sentences), it is the only public dataset currently available that contains sequences drawn from a large space, rather than a restricted set of sentences, such as the datasets that use a fixed small subset of TIMIT (e.g. VidTIMIT).\n\n\nA description of the GRID dataset (Cooke et al., 2006), other datasets, and the rationale for using GRID appears in Section 2 of the paper. We anticipated the importance of specifying clearly the dataset challenges faced in the lipreading field and hence created a section early in the paper to address this issue. We strongly recommend readers consult this section, but in short, the GRID dataset has audio and video recordings of 34 speakers (male and female) who produced 1000 sentences each, for a total of 28 hours of audio and video, and a subset of 34000 different sentences. \n\n\nWhile the grammar is restricted, there is sufficient structure to enable us to train a sequence model with CTC and significantly outperform published results. We exposed people in the baseline studies to the grammar, see Section 4.2. This helped improve their performance with respect to other reported figures on lipreading by human experts, and naturally we recognize the many limitations imposed by the grid dataset and lack of context in this regard. \n\n\nImportantly, given that our model is based on scalable components and that it has CTC, language modeling and beam search,  it will likely do well on datasets with more complex grammar, as research in ASR has already shown. Consequently, we are confident that if the equivalent of ImageNet materializes for lipreading, this model will do well. Of course how well must be investigated, once the effort of building a massive public dataset is completed. \n\n\nWe agree that accurately represented science is important in both popular media and social media, and it is unfortunate that the claims made by members of the general public did not accurately represent the claims of our paper.\n\n\nHowever, a discussion of social media and its dangers is beyond the scope of this paper, and we kindly request that reviewers respect the scientific nature of this website. There are many fantastic papers submitted to ICLR 2017, and it is wonderful for us to have the ability to exchange technical ideas via this medium. So again, we appeal to tolerance and a respectful scientific discourse."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1478643666051, "tcdate": 1478643666041, "number": 2, "id": "S1cC1Ckbl", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["~Mohamed_Yousef_Bassyouni1"], "readers": ["everyone"], "writers": ["~Mohamed_Yousef_Bassyouni1"], "content": {"title": "Some comments and suggestions", "comment": "- It is not fair to use a character 5-gram language model and compare to [1] which doesn't use any language modeling, the authors should also report their results without using any language modeling (IMHO using a language model with a limited vocabulary corpus like GRID which has only 51 words and 64000 possible sentence makes the results misleading, which, I think, is the reason why no language modeling was used in [1])\n\n- It is not fair to augment the training data to 15x, then compare to [1] which doesn't use any data augmentation\n\n- It should be mentioned and cited that a CNN+RNN+CTC architecture is not novel and has been widely used in literature for sequence recognition tasks (e.g. [2],[3],[4])\n\n- I encourage the authors to also try a ConvLSTM [5], which have recently shown very promising performance in a number of video-related tasks\n\n---------------------------------------------------\n[1] M. Wand, J. Koutnik, and J. Schmidhuber. Lipreading with long short-term memory.\n[2] B.  Shi,  X.  Bai,  and  C.  Yao. An  end-to-end  trainable  neural  network for  image-based  sequence  recognition  and  its  application  to  scene  text recognition.\n[3]  Z.  Xie,  Z.  Sun,  L.  Jin,  Z.  Feng,  and  S.  Zhang. Fully  convolutional recurrent network for handwritten chinese text recognition.\n[4] Li, H., Shen, C.: Reading car license plates using deep convolutional neural networks and lstms\n[5] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional LSTM network: A machine learning approach for precipitation nowcasting."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}, {"tddate": null, "tmdate": 1478470610409, "tcdate": 1478470610400, "number": 1, "id": "rk90jXplx", "invitation": "ICLR.cc/2017/conference/-/paper230/public/comment", "forum": "BkjLkSqxg", "replyto": "BkjLkSqxg", "signatures": ["~Neil_D_Lawrence1"], "readers": ["everyone"], "writers": ["~Neil_D_Lawrence1"], "content": {"title": "Not so much reacting to the paper but to the 'twitter-storm' it generated. ", "comment": "This corpus is a small data set created 10 years ago by colleagues and friends (Martin Cooke, Jon Barker, Stuart Cunningham and Xu Shao) at the Department of Computer Science. I recall that Martin gave me a bottle of Spanish wine for my trouble.\nAs far as I remember the corpus, it was designed to remove higher order language structure. That structure that (I believe) is used by humans to cue on when reading lips.\n\nThe corpus has a limited vocabulary and a single syntax grammar. So while it's promising to perform well on this data, it's not really ground breaking, particularly if you are interested in sentence models: the corpus sentence structure is super simple.\nSo while the model may be able to read my lips better than a human, it can only do so when I say a meaningless list of words from a highly constrained vocabulary in a specific order. That may be an advance, but it's not one worthy of disturbing me on a Sunday (serves me right for reading Twitter on a Sunday).\n\nI'm not making a comment about whether the paper should be accepted or not, but merely reacting to the large number of claims for the paper we are seeing on social media. The particular result for this data set may well be state of the art.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "LipNet: End-to-End Sentence-level Lipreading", "abstract": "Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).", "pdf": "/pdf/34c7079f4e5e130a4293c7e9501cc1e8b04690b1.pdf", "TL;DR": "LipNet is the first end-to-end sentence-level lipreading model to simultaneously learn spatiotemporal visual features and a sequence model.", "paperhash": "assael|lipnet_endtoend_sentencelevel_lipreading", "keywords": ["Computer vision", "Deep learning"], "conflicts": ["cs.ox.ac.uk", "google.com"], "authors": ["Yannis M. Assael", "Brendan Shillingford", "Shimon Whiteson", "Nando de Freitas"], "authorids": ["yannis.assael@cs.ox.ac.uk", "brendan.shillingford@cs.ox.ac.uk", "shimon.whiteson@cs.ox.ac.uk", "nando.de.freitas@cs.ox.ac.uk"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287674161, "id": "ICLR.cc/2017/conference/-/paper230/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkjLkSqxg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper230/reviewers", "ICLR.cc/2017/conference/paper230/areachairs"], "cdate": 1485287674161}}}], "count": 29}