{"notes": [{"id": "rkxQ-nA9FX", "original": "H1eaMjpqYQ", "number": 1153, "cdate": 1538087930522, "ddate": null, "tcdate": 1538087930522, "tmdate": 1556292939855, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkgQZfs0J4", "original": null, "number": 1, "cdate": 1544626683494, "ddate": null, "tcdate": 1544626683494, "tmdate": 1545354501331, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Meta_Review", "content": {"metareview": "This paper conducted theoretical analysis of the effect of batch normalisation to auto rate-tuning. It provides an explanation for the empirical success of BN. The assumptions for the analysis is also closer to the common practice of batch normalization compared to a related work of Wu et al. 2018.\n\nOne of the concerns raised by the reviewer is that the analysis does not immediately apply to practical uses of BN, but the authors already discussed how to fill the gap with a slight change of the activation function. Another concern is about the lack of empirical evaluation of the theory, and the authors provide additional experiments in the revision. R1 also points out a few weaknesses in the theoretical analysis, which I think would help improve the paper further if the authors could clarify and provide discussion in their revision.\n\nOverall, it is a good paper that will help improve our theoretical understanding about the power tool of batch normalization.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Good theoretical contribution to understanding batch normalization."}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1153/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352945008, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352945008}}}, {"id": "S1gZOpsFnQ", "original": null, "number": 1, "cdate": 1541156201401, "ddate": null, "tcdate": 1541156201401, "tmdate": 1544867559427, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Review", "content": {"title": "A  theoretical result about asymptotic convergence with normalization but weakly related to the practical success of BN", "review": "* Description\n\nThe work is motivated by the empirical performance of Batch Normalization and in particular the observed better robustness of the choice of the learning rate.  Authors analyze theoretically the asymptotic convergence rate for objectives involving normalization, not necessarily BN, and show that for scale-invariant groups of parameters (appearing as a result of normalization) the initial learning rate may be set arbitrary while still asymptotic convergence is guaranteed with the same rate as the best known in the general case. Offline gradient descent and stochastic gradient descent cases are considered.\n\n* Strengths\n\nThe work addresses better theoretical understanding of successful heuristics in deep learning, namely batch normalization and other normalizations. The technical results obtained are non-trivial and detailed proofs are presented. Also I did not verify the proofs the paper appears technically correct and technically clear. The result may be interpreted in the following form: if one chooses to use BN or other normalization, the paper gives a recommendation that only the learning rate of scale-variant parameters need to be set, which may have some practical advantages. Perhaps more important than the rate of convergence, is the guarantee that the method will not diverge (and will not get stuck in a non-local minimum). \n\n* Criticism\nThis paper presents non-trivial theoretical results that are worth to be published but as I argue below its has a weak relevance to practice and the applicability of the obtained results is unclear.\n-- Concerns regarding the clarity of presentation and interpretation of the results.\n \nThe properties of BN used as motivation for the study, are observed non-asymptotically with constant or empirically decreased learning rate schedules for a limited number of iterations. In contrast, the studied learning rates are asymptotic and there is a big discrepancy. SGD is observed to be significantly faster than batch gradient when far from convergence (experimental evidence), and this is with or without normalization. In practice, the training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability. There is in fact no experimental evidence that the practical advantages of BN are relevant to the results proven. It makes a nice story that the theoretical properties justify the observations, but they may be as well completely unrelated. \n\nAs seen from the formal construction, the theoretical results apply equally well to all normalization methods. It occludes the clarity that BN is emphasized amongst them. \n\nConsidering theoretically, what advantages truly follow from the paper for optimizing a given function? Let\u2019s consider the following cases.\n1. For optimizing a general smooth function with all parameters forming a single scale-invariant vector. In this case, the paper proves that no careful selection of the learning rate is necessary. This result is beyond machine learning and unfortunately I cannot evaluate its merit. Is it known / not known in optimization?\n\n2. The case of data-independent normalization (such as weight normalization).\nWithout normalization, we have to tune learning rate to achieve the optimal convergence. With normalization we still have to tune the learning rate (as scale-variant parameters remain or are reintroduced with each invariance to preserve the degrees of freedom), then we have to wait for the phase two of Lemma 3.2 so that the learning rate of scale-invariant parameters adapts, and from then on the optimal convergence rate can be guaranteed.\n\n3. The case of Batch Normalization. Note that there is no direct correspondence between the loss of BN-normalized network (2) and the loss of the original network because of dependence of the normalization on the batches. In other words, there is no setting of parameters of the original network that would make its forward pass equivalent to that of BN network (2) for all batches. The theory tells the same as in case 2 above but with an additional price of optimizing a different function.\n\nThese points remain me puzzled regarding either practical or theoretical application of the result. It would be great if authors could elaborate. \n\n\n-- Difference from Wu et al. 2018\n\nThis works is cited as a source of inspiration in several places in the paper. As the submission is a theoretical result with no immediate applicability, it would be very helpful if the authors could detail the technical improvements over this related work. Note, ICLR policy says that arxiv preprints earlier than one month before submission are considered a prior art. Could the authors elaborate more on possible practical/theoretical applications?\n \n\n* Side Notes (not affecting the review recommendation)\n\nI believe that the claim that \u201cBN reduces covariate shift\u201d (actively discussed in the intro) was an imprecise statement in the original work. Instead, BN should be able to quickly adapt to the covariate shift when it occurs. It achieves this by using the parameterization in which the mean and variance statistics of neurons (the quantities whose change is called the covariate shift) depend on variables that are local to the layer (gamma, beta in (1)) rather than on the cumulative effect of all of the preceding layers.\n\n* Revision\nI took into account the discussion and the newly added experiments and increased the score. The experiments verify the proven effect and make the paper more substantial. Some additional comments about experiments follow.\nTraining loss plots would be more clear in the log scale.\nComparison to \"SGD BN removed\" is not fair because the initialization is different (application of BN re-initializes weight scales and biases). The same initialization can be achieved by performing one training pass with BN with 0 learning rate and then removing it, see e.g. Gitman, I. and Ginsburg, B. (2017). Comparison of batch normalization and weight normalization algorithms for the large-scale image classification.\nThe use of Glorot uniform initializer is somewhat subtle. Since BN is used, Glorot initialization has no effect for a forward pass. However, it affects the gradient norm. Is there a rationale in this setting or it is just a more tricky method to fix the weight norm to some constant, e.g. ||w||=1?\n\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Review", "cdate": 1542234293309, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335884380, "tmdate": 1552335884380, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByeehnIq0Q", "original": null, "number": 8, "cdate": 1543298216079, "ddate": null, "tcdate": 1543298216079, "tmdate": 1543298216079, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "SygQ2bBxR7", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "content": {"title": "Thanks again for your thoughtful review! Experiments are added.", "comment": "Thanks again for your thoughtful review.\n\nTheory ---almost by definition---may not lead to immediate practical applications. Sometimes the goal is better understanding. That has proved difficult for BN, as described in the introduction. \n\nPlease also note that we are not proposing some new algorithm which could achieve the same test error as existing methods with less tuning, but are trying to understand why BN helps optimization in the training process. \n\nWe've now uploaded a new revision with additional experiments, which exhibits the advantage of auto rate-tuning led by BN in training."}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606731, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1153/Authors|ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606731}}}, {"id": "HkxDvh89A7", "original": null, "number": 7, "cdate": 1543298142724, "ddate": null, "tcdate": 1543298142724, "tmdate": 1543298142724, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "H1xdKxiDp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "content": {"title": "Thanks for your appreciation! Experiments are added.", "comment": "Thanks for your valuable review! We've added an experiment section in the new revision, showing how BN helps convergence in the training process."}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606731, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1153/Authors|ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606731}}}, {"id": "SJgUKj8cC7", "original": null, "number": 6, "cdate": 1543297918195, "ddate": null, "tcdate": 1543297918195, "tmdate": 1543297918195, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "content": {"title": "Experiment results are added in the new revision", "comment": "We thank the reviewers for their valuable comments. We uploaded a new revision with additional experiments showing the advantage of auto rate-tuning behavior of BN in training.\n\nTwo settings were studied: \n1. Training VGG with BN on cifar10, using standard SGD (without momentum, learning rate decay, weight decay).  \n2.  Training VGG with BN on cifar10, using Projected SGD:  at each iteration, the algorithm first takes a gradient update and then projects each scale-invariant parameter to the sphere with radius equal to its 2-norm before this iteration, i.e., rescales each scale-invariant parameter such that they maintain their norms during training. \n\nIn both settings, learning rate for scale variant parameter is 0.1 but rates for scale invariant parameters vary from 0.01 to 100, a very large range. The plots show that in setting 1 the training loss of SGD lways  gets very small, while in setting 2, the training loss of PSGD remains large for lr > 1. \n\nThe only difference between SGD and PSGD is that the implicit rate-tuning behavior on scale invariant parameters is blocked because of the fixed norm of scale-invariant parameters. So we can conclude that the auto rate-tuning phenomenon does happen here and it helps convergence in training when learning rate is large.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606731, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1153/Authors|ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606731}}}, {"id": "rJgNr6BHRQ", "original": null, "number": 5, "cdate": 1542966588475, "ddate": null, "tcdate": 1542966588475, "tmdate": 1542966588475, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "HkgOWxN0T7", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "content": {"title": "Got it", "comment": "Thanks."}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1153/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606731, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1153/Authors|ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606731}}}, {"id": "SygQ2bBxR7", "original": null, "number": 4, "cdate": 1542635947055, "ddate": null, "tcdate": 1542635947055, "tmdate": 1542635947055, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "B1eSf1V0aX", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "content": {"title": "Some Clarification", "comment": "(i)-(ii)\nMy point was that BN has been never experimentally studied in the asymptotic regime where the results of the paper apply. The shown auto-tuning rate is an interesting property, but there is no evidence that it is relevant to the experimental successes of BN that are mentioned.\n\n(iii) Thanks for clarification. The paper of Wu et al. 2018 claims in particular: \" The recently proposed batch normalization ... is robust to the choice of Lipschitz constant of the gradient in loss function, allowing one to set a large learning rate without worry\". I see now that this work does not make this claim formal and according to the authors' explanation above making it formal takes all the derivations of the submission.\n\n(iv) The theoretical advantage of the shown auto-rate tuning is not completely clear. It is not excluded that introducing normalization while easing the learning rate tuning for scale-invariant parameters is making it harder for scale-variant ones. There is a learning rate to tune in the end, no matter how many parameters are scale-invariant.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1153/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606731, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1153/Authors|ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606731}}}, {"id": "HkgOWxN0T7", "original": null, "number": 3, "cdate": 1542500351692, "ddate": null, "tcdate": 1542500351692, "tmdate": 1542500351692, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "rylog8i62m", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "content": {"title": "Lemma 2.4 is correct and the issue of G_t is fixed", "comment": "Thanks for your positive feedback. \n\n(1). Lemma 2.4, Point 1: The gradient in your example is indeed perpendicular to w which can be seen as follows.\n\nw\u2019 * \\nabla L(w) = w\u2019 * (2/w\u2019*w)(Aw - L(w)*w) =  (2/w\u2019*w)(w\u2019Aw - L(w)*(w\u2019*w)) =  (2/w\u2019*w)(w\u2019Aw - w\u2019Aw) = 0.\n\nIn case of one variable vector, our proof is to take the derivative of c on both sides of F(w) = F(cw), which is the definition of scale-invariance. Then the left-hand side becomes 0 and the right-hand side becomes w\u2019 * \\nabla F(cw)  by chain rule. Taking c = 1, we can conclude that w\u2019 * \\nabla F(w)  = 0.\n\n(2). Theorem 2.5: Sorry G_t should be G_t^{(i)}. We will correct this typo in the next revision of this paper.\n\nFor t = 0, G_t^{(i)} are all initialized to some value. The recursion formula for G_t^{(i)} is shown in equation (9).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606731, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1153/Authors|ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606731}}}, {"id": "B1eSf1V0aX", "original": null, "number": 2, "cdate": 1542500108735, "ddate": null, "tcdate": 1542500108735, "tmdate": 1542500108735, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "S1gZOpsFnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "content": {"title": "Thanks for your careful review.", "comment": "Thanks for your careful review! As mentioned in the intro, we are trying to give some principled insight into benefits of BN, which has proved tricky. Also, it is noted in the paper that BN probably has many desirable properties, of which auto-rate tuning is just one.\n\n(i) Speed of SGD vs GD: \nNote that \u201ctime\u201d here refers to number of iterations, not epochs.  We are not aware of results establishing SGD is faster in this measure. (As noted on p2,  we are working within the standard paradigm of convergence rates in optimization. The only new part is the automatic rate tuning  behavior shown for most parameters when BN is used.) \n\n(ii) \u201cusually training is stopped much before convergence, in the hope of finding solutions close to minimum with high probability.\u201d  \nWe\u2019re assuming training proceeds until gradient is small (stationary point). We are not aware of any prior analysis of speed of convergence that deviates from this assumption. Perhaps the reviewer is thinking of early stopping in context of better generalization?  \n\n(iii) \u201cclarify difference from Wu et al. (2018)\u201d\nWu et al. 2018 introduces a *new* algorithm inspired by weight normalization (WN) and studies its convergence rate to stationary point. This algorithm can be seen as an explicit way to tune the learning rate (thus it is conceptually analogous Adagrad). They don't have any results about WN or BN itself. Their analysis could be adapted to GD on one-neuron network with WN or BN without scale-variant parameters (gamma and beta). Even this adaptation is not immediate because the goal of this work is to find a stationary point on the unit sphere rather than R^d.  Finally, they prove no results for SGD, whereas our paper does. \n\n\n(iv) \u201csingle learning rate doesn\u2019t apply for all parameters\u201d  \nCorrect. The algorithm can use a single learning rate for scale-invariant parameters but needs a tuned rate for the scale-variant ones. In feedforward nets, the number of scale-variant parameters scales as the number of nodes and the number of scale-invariant parameters scales as the number of edges (up to weight sharing).  Thus the vast majority of parameters are scale-invariant.\n\n\n(v) \u201cRelation between original loss and loss using BN.\u201d \nOur results hold for the loss of batch-normalized network (\u201cBN-loss\u201d)  which is different from the loss of the original network (\u201cBN-less loss\u201d). Probably the reshaping of loss function due to BN is very important but currently hard to analyse theoretically because we lack a good mathematical understanding of the loss landscape (even BN-less). "}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621606731, "tddate": null, "super": null, "final": null, "reply": {"forum": "rkxQ-nA9FX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1153/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1153/Authors|ICLR.cc/2019/Conference/Paper1153/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers", "ICLR.cc/2019/Conference/Paper1153/Authors", "ICLR.cc/2019/Conference/Paper1153/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621606731}}}, {"id": "H1xdKxiDp7", "original": null, "number": 3, "cdate": 1542070399985, "ddate": null, "tcdate": 1542070399985, "tmdate": 1542070399985, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Review", "content": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "review": "* Strengths:\n- The paper gives theoretical insight into why Batch Normalization is useful in making neural network training more robust and is therefore an important contribution to the literature.\n- While the actual arguments are somewhat technical as is expected from such a paper, the motivation and general strategy is very easy to follow and insightful.\n\n* Weaknesses:\n- The bounds do not immediately apply in the batch normalization setting as used by neural network practitioners, however there are practical ways to link the two settings as pointed out in section 2.4\n- As the authors point out, the idea of using a batch-normalization like strategy to set an adaptive learning rate has already been explored in the WNGrad paper. However it is valuable to have a similar analysis closer to the batch normalization setting used by most practitioners.\n- Currently there is no experimental evaluation of the claims, which would be valuable given that the setting doesn't immediately apply in the normal batch normalization setting. I would like to see evidence that the main benefit from batch normalization indeed comes from picking a good adaptive learning rate.\n\nOverall I recommend publishing the paper as it is a well-written and insightful discussion of batch normalization. Be aware that I read the paper and wrote this review on short notice, so I didn't have time to go through all the arguments in detail.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Review", "cdate": 1542234293309, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335884380, "tmdate": 1552335884380, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rylog8i62m", "original": null, "number": 2, "cdate": 1541416434721, "ddate": null, "tcdate": 1541416434721, "tmdate": 1541533377368, "tddate": null, "forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "invitation": "ICLR.cc/2019/Conference/-/Paper1153/Official_Review", "content": {"title": "A good paper", "review": "The paper is well written and easy to follow. The topic is apt.\n\nI don\u2019t have any comments except the following ones.\n\nLemma 2.4, Point 1: The proof is confusing. Consider the one variable vector case. Assuming that there is only one variable w, then \\nabla L(w) is not perpendicular to w in general. The Rayleigh quotient example L(w)  = w\u2019*A*w/ (w\u2019*w) for a symmetric matrix A, then \\nabla L(w) = (2/w\u2019*w)(Aw - L(w)*w), which is not perpendicular to w. \nEven if we constrain ||w ||_2 = 1, then also  \\nabla L(w)  is not perpendicular to w.\nAm I missing something?\n\nWhat is G_t in Theorem 2.5. It should be defined in the theorem itself. There is another symbol G_g which is a constant.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1153/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Theoretical Analysis of Auto Rate-Tuning by Batch Normalization", "abstract": "Batch Normalization (BN) has become a cornerstone of deep learning across diverse architectures, appearing to help optimization as well as generalization. While the idea makes intuitive sense, theoretical analysis of its effectiveness has been lacking. Here theoretical support is provided for one of its conjectured properties, namely, the ability to allow gradient descent to succeed with less tuning of learning rates. It is shown that even if we fix the learning rate of scale-invariant parameters (e.g., weights of each layer with BN) to a constant (say, 0.3), gradient descent still approaches a stationary point (i.e., a solution where gradient is zero) in the rate of T^{\u22121/2} in T iterations, asymptotically matching the best bound for gradient descent with well-tuned learning rates. A similar result with convergence rate T^{\u22121/4} is also shown for stochastic gradient descent.", "keywords": ["batch normalization", "scale invariance", "learning rate", "stationary point"], "authorids": ["arora@cs.princeton.edu", "zhiyuanli@cs.princeton.edu", "vfleaking@gmail.com"], "authors": ["Sanjeev Arora", "Zhiyuan Li", "Kaifeng Lyu"], "TL;DR": "We give a theoretical analysis of the ability of batch normalization to automatically tune learning rates, in the context of finding stationary points for a deep learning objective.", "pdf": "/pdf/58fded88783fb98aff715ad1be636726c2bcaac8.pdf", "paperhash": "arora|theoretical_analysis_of_auto_ratetuning_by_batch_normalization", "_bibtex": "@inproceedings{\narora2018theoretical,\ntitle={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},\nauthor={Sanjeev Arora and Zhiyuan Li and Kaifeng Lyu},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rkxQ-nA9FX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1153/Official_Review", "cdate": 1542234293309, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rkxQ-nA9FX", "replyto": "rkxQ-nA9FX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1153/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335884380, "tmdate": 1552335884380, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1153/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}