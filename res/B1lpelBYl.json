{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028638407, "tcdate": 1490028638407, "number": 1, "id": "SJIPOKpjl", "invitation": "ICLR.cc/2017/workshop/-/paper157/acceptance", "forum": "B1lpelBYl", "replyto": "B1lpelBYl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating SGD for Distributed Deep-Learning Using an Approximted Hessian Matrix", "abstract": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime. By leveraging the differences in gradients and parameters of multiple Workers, we are able to efficiently implement a distributed approximation of the Newton-Raphson method. We also present preliminary results which underline advantages and challenges of second-order methods for large stochastic optimization problems. In particular, our work suggests that novel strategies for combining gradients will provide further information on the loss surface.", "pdf": "/pdf/7368e97ae01333bacc8cd95dbe9c998494e70703.pdf", "TL;DR": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime.", "paperhash": "arnold|accelerating_sgd_for_distributed_deeplearning_using_an_approximted_hessian_matrix", "keywords": ["Deep learning", "Optimization"], "conflicts": ["usc.edu", "math.usc.edu"], "authors": ["Sebastien Arnold", "Chunming Wang"], "authorids": ["arnolds@usc.edu", "cwang@math.usc.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028638921, "id": "ICLR.cc/2017/workshop/-/paper157/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1lpelBYl", "replyto": "B1lpelBYl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028638921}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489584390747, "tcdate": 1487368376230, "number": 157, "id": "B1lpelBYl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "B1lpelBYl", "signatures": ["~Sebastien_Arnold1"], "readers": ["everyone"], "content": {"title": "Accelerating SGD for Distributed Deep-Learning Using an Approximted Hessian Matrix", "abstract": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime. By leveraging the differences in gradients and parameters of multiple Workers, we are able to efficiently implement a distributed approximation of the Newton-Raphson method. We also present preliminary results which underline advantages and challenges of second-order methods for large stochastic optimization problems. In particular, our work suggests that novel strategies for combining gradients will provide further information on the loss surface.", "pdf": "/pdf/7368e97ae01333bacc8cd95dbe9c998494e70703.pdf", "TL;DR": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime.", "paperhash": "arnold|accelerating_sgd_for_distributed_deeplearning_using_an_approximted_hessian_matrix", "keywords": ["Deep learning", "Optimization"], "conflicts": ["usc.edu", "math.usc.edu"], "authors": ["Sebastien Arnold", "Chunming Wang"], "authorids": ["arnolds@usc.edu", "cwang@math.usc.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}, {"tddate": null, "tmdate": 1489269985807, "tcdate": 1489269985807, "number": 2, "id": "SJqyBefjx", "invitation": "ICLR.cc/2017/workshop/-/paper157/official/review", "forum": "B1lpelBYl", "replyto": "B1lpelBYl", "signatures": ["ICLR.cc/2017/workshop/paper157/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper157/AnonReviewer2"], "content": {"title": "Interesting approach", "rating": "7: Good paper, accept", "review": "Compared to the other reviewer I found the approach interesting. While I'm not so keen on exact time complexity, algorithmically the approach seems scalable. I agree that the experimental section is a bit disappointing, and that there might be real concerns on how this particular approximation of the curvature works in practice. But given that it is a workshop submission, I find the proposal very simple and elegant, and I wager that with a bit of care and dedication it could work surprisingly well in practice. \n\nMy score however rests heavily on the fact that this is just a workshop submission, I think a lot of work still needs to be done to convert this work in a proper paper. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating SGD for Distributed Deep-Learning Using an Approximted Hessian Matrix", "abstract": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime. By leveraging the differences in gradients and parameters of multiple Workers, we are able to efficiently implement a distributed approximation of the Newton-Raphson method. We also present preliminary results which underline advantages and challenges of second-order methods for large stochastic optimization problems. In particular, our work suggests that novel strategies for combining gradients will provide further information on the loss surface.", "pdf": "/pdf/7368e97ae01333bacc8cd95dbe9c998494e70703.pdf", "TL;DR": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime.", "paperhash": "arnold|accelerating_sgd_for_distributed_deeplearning_using_an_approximted_hessian_matrix", "keywords": ["Deep learning", "Optimization"], "conflicts": ["usc.edu", "math.usc.edu"], "authors": ["Sebastien Arnold", "Chunming Wang"], "authorids": ["arnolds@usc.edu", "cwang@math.usc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489269986535, "id": "ICLR.cc/2017/workshop/-/paper157/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper157/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper157/AnonReviewer1", "ICLR.cc/2017/workshop/paper157/AnonReviewer2"], "reply": {"forum": "B1lpelBYl", "replyto": "B1lpelBYl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper157/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper157/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489269986535}}}, {"tddate": null, "tmdate": 1489150355927, "tcdate": 1489150355927, "number": 1, "id": "ryn5ZXlse", "invitation": "ICLR.cc/2017/workshop/-/paper157/official/review", "forum": "B1lpelBYl", "replyto": "B1lpelBYl", "signatures": ["ICLR.cc/2017/workshop/paper157/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper157/AnonReviewer1"], "content": {"title": "Time complexity, baselines, hyperparameter selection", "rating": "3: Clear rejection", "review": "I am not quite sure about the time complexity of O(m^3 + n). \n\" The algorithm does require computation of the eigenvalues and eigenvectors of the m \u00d7 m matrix G^H \u00d7 G\". Should not G^H \\times G first be computed? Given than G is in R^{m x n}, I would expect m x n somewhere in the complexity formula. To compute g as the average of gradients you would need m x n, right?\n\nThe experimental results are disappointing:\na) SGD as the only baseline and no comparison with second-order methods and their approximates/alternatives\nb) little networks of 16k parameters which raises the question of scalability  \nc) weird hyperparameter selection \"we keep most of our hyper-parameters constant, including learning rates (0.0003 and 0.01)\" given that \"several experiments diverged when using too large a learning rate, whereas this was beneficial to the convergence rate of SGD\" suggesting that the selection of the learning rate was in favor of the proposed approach.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Accelerating SGD for Distributed Deep-Learning Using an Approximted Hessian Matrix", "abstract": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime. By leveraging the differences in gradients and parameters of multiple Workers, we are able to efficiently implement a distributed approximation of the Newton-Raphson method. We also present preliminary results which underline advantages and challenges of second-order methods for large stochastic optimization problems. In particular, our work suggests that novel strategies for combining gradients will provide further information on the loss surface.", "pdf": "/pdf/7368e97ae01333bacc8cd95dbe9c998494e70703.pdf", "TL;DR": "We introduce a novel method to compute a rank $m$ approximation of the inverse of the Hessian matrix, in the distributed regime.", "paperhash": "arnold|accelerating_sgd_for_distributed_deeplearning_using_an_approximted_hessian_matrix", "keywords": ["Deep learning", "Optimization"], "conflicts": ["usc.edu", "math.usc.edu"], "authors": ["Sebastien Arnold", "Chunming Wang"], "authorids": ["arnolds@usc.edu", "cwang@math.usc.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489269986535, "id": "ICLR.cc/2017/workshop/-/paper157/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper157/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper157/AnonReviewer1", "ICLR.cc/2017/workshop/paper157/AnonReviewer2"], "reply": {"forum": "B1lpelBYl", "replyto": "B1lpelBYl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper157/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper157/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489269986535}}}], "count": 4}