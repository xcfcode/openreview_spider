{"notes": [{"id": "r1x3unVKPS", "original": "SJg0nHlF8r", "number": 56, "cdate": 1569438835972, "ddate": null, "tcdate": 1569438835972, "tmdate": 1577168280015, "tddate": null, "forum": "r1x3unVKPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6l2YiXfJP", "original": null, "number": 1, "cdate": 1576798686157, "ddate": null, "tcdate": 1576798686157, "tmdate": 1576800948810, "tddate": null, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "invitation": "ICLR.cc/2020/Conference/Paper56/-/Decision", "content": {"decision": "Reject", "comment": "The submission proposes a method for adversarial imitation learning that combines two previous approaches - GAIL and RED - by simply multiplying their reward functions. The claim is that this adaptation allows for better learning - both handling reward bias and improving training stability. \n\nThe reviewers were divided in their assessment of the paper, criticizing the empirical results and the claims made by the authors. In particular, the primary claims of handling reward bias and reducing variance seem to be not well justified, including results which show that training stability only substantially improves when SAIL-b, which uses reward clipping, is used. \n\nAlthough the paper is promising, the recommendation is for a reject at this time. The authors are encouraged to clarify their claims and supporting experiments and to validate their method on more challenging domains.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795721243, "tmdate": 1576800272227, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper56/-/Decision"}}}, {"id": "S1ex1UxPKS", "original": null, "number": 1, "cdate": 1571386840090, "ddate": null, "tcdate": 1571386840090, "tmdate": 1574321230392, "tddate": null, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "invitation": "ICLR.cc/2020/Conference/Paper56/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "**Summary of the paper: \nThe paper proposes an IL method named support-guided adversarial IL (SAIL), which is based on generative adversarial IL (GAIL) (Ho and Ermon, 2016) and random expert distillation (RED) (Wang et al., 2019). The key idea of SAIL is to construct a reward function by multiplying reward functions learned by GAIL and RED. This multiplication yields two benefits; 1) it handles the issue of biased reward in GAIL, since state-action pairs outside the expert\u2019s support are assigned low reward values. 2) SAIL\u2019s reward is more reliable than RED\u2019s reward for state-action pairs inside the expert\u2019s support. The authors show that SAIL is at least as fast as than GAIL in terms of the sample complexity. Experiments on continuous control benchmarks show that SAIL is overall more stable than GAIL. \n\n**Rating: \nThe paper proposes a simple but effective combination of existing methods. The proposed method is well motivated and performs well on benchmarks. Still, the paper has some issues regarding justification, clarity, and evaluation, which should be addressed (see below). I vote for weak acceptance. \n\n**Major comments/questions: \n- No guarantee of the optimality of the learned policy.\nCan it be guaranteed that SAIL learns the expert policy? (assuming the expert policy is realizable). Propositions 1 and 2 show the convergence of the support estimation, but these results are not related to the optimality of a policy learned with the reward function. This is an important point for justifying SAIL, since SAIL does not perform distribution matching to learn the expert policy, and it also does not perform IRL to learn the reward function. Therefore, SAIL lacks the optimality guarantee from both distribution matching and IRL perspectives. Please address and clarify this point. \n\n- Clarity in the theoretical analysis.\nIn the theoretical analysis, the paper assumes a rate of GAIL for support estimation. This is quite confusing, since GAIL performs distribution matching and does not estimate the support. Also, given that r_gail = -log D(s,a), the reward\u2019s upper-bound (R_gail) is infinity and the bound in Eq. (9) is not informative. \n\n- The reward r_red is constant at the optimal.\nEq. (2) and Eq. (3) imply that, for state-action pairs from the expert\u2019s state-action distribution, r_red is constant at the optimal. Specifically, the optimal solution of Eq. (2) is \\hat{\\theta} = \\theta, which yields to a constant value of r_red(s,a) in Eq. (3). In this scenario, SAIL is equivalent to GAIL for the expert state-action distribution. This means that Eq. (2) should not be optimized until optimal, and some early stopping criteria are required. Does this scenario (constant value of r_red) occur in the experiments?\n\n- IRL baseline methods.\nThe paper should compare SAIL to methods which aim to handle the bias in reward function, e.g., DAC (Kostrikov et al. 2019). While DAC requires the time limit, this time limit is known in the benchmark tasks. Also, IRL methods such as AIRL (Fu et al., 2018) should be compared, since IRL methods are better than GAIL at handling bias in reward function (Kostrikov et al. 2019). \n\n**Minor comments/questions: \n- Typos: \"offline RL algorithms\" should be \"off-policy RL algorithms\". Line 5 of Algorithm 1 should perform gradient ascent instead of gradient descent. An expectation over state-action distribution of expert is missing from Eq. (2). \n\n- What are the bold numbers in table 1 and 2 indicating? Why does the Hopper task have two bold numbers, but the other tasks have only one?\n\n--After author response--\nI have read the author response and other reviews. I thank the authors for including additional experiments. However, the authors' arguments regarding optimality do not fully address my comments (see below). I will keep the vote of weak acceptance.  \n\nThe authors argue that \"In this asymptotic case, SAIL is equivalent to performing distribution matching via GAIL with the additional *constraint* that candidate distributions need to have the same support of the expert distribution\". However, the support of the expert distribution may coincide with the entire state-action space, which makes the additional constraint uninformative in the asymptotic case. Specifically, the expert distribution coincides with the state-action space when the expert policy has an infinite support (e.g., the expert policy is Gaussian). Assuming the asymptotic case, the support estimation in RED will give an indicator function over an entire state-action space, and the support constraint in SAIL is always satisfied. In other words, SAIL is exactly equivalent to GAIL in this case. For these reasons, the authors' arguments regarding optimality do not fully address my comments. I think additional assumptions are required, e.g., the expert policy needs to have a finite support or be deterministic.  \n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper56/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper56/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575503664170, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper56/Reviewers"], "noninvitees": [], "tcdate": 1570237757780, "tmdate": 1575503664182, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper56/-/Official_Review"}}}, {"id": "rJxtJOspFr", "original": null, "number": 2, "cdate": 1571825632817, "ddate": null, "tcdate": 1571825632817, "tmdate": 1574036049437, "tddate": null, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "invitation": "ICLR.cc/2020/Conference/Paper56/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "The paper proposes an imitation learning algorithm that combines support estimation with adversarial training. The key idea is simple: multiply the reward from Random Expert Distillation (RED) with the reward from Generative Adversarial Imitation Learning (GAIL). The new reward combines the best of both methods. Like the GAIL reward, the new reward encourages exploration and can be estimated from a small number of demonstrations. Like the RED reward, the new reward avoids survival bias and is more stable than the adversarial reward.\n\nI have a concern regarding the Lunar Lander experiment. Were the demonstrations generated in the modified environment? If they were generated in the original environment (with early termination), this may have unintentionally created a state distribution mismatch between the demonstration environment and training environment that unfairly hurts the GAIL baseline's performance. If the demonstrations were instead generated in the modified environment (without early termination) where the agent is actually trained, the demonstrations would contain many self-loop transitions at the goal state, and GAIL would likely not exhibit survival bias.\n\nI am also a bit concerned about the MuJoCo results. The stochasticity of the demonstrations and the evaluation trajectories may have a significant effect on the standard deviation of rewards. Was a stochastic policy or a deterministic policy used to generate the demonstrations? Were the evaluation trajectories generated by rolling out the stochastic imitation policy, or by rolling out a deterministic version of the imitation agent? Also, could the authors provide the mean and standard deviation of rewards in the demonstrations in Tables 1-2 and Figure 4? It would be nice to establish a rough upper bound on the performance of the imitation methods.\n\nUpdate: \nAfter reading the author response, I have increased my score from 3 to 6.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper56/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper56/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575503664170, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper56/Reviewers"], "noninvitees": [], "tcdate": 1570237757780, "tmdate": 1575503664182, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper56/-/Official_Review"}}}, {"id": "SkepzwVwor", "original": null, "number": 2, "cdate": 1573500692857, "ddate": null, "tcdate": 1573500692857, "tmdate": 1573501227539, "tddate": null, "forum": "r1x3unVKPS", "replyto": "rJxtJOspFr", "invitation": "ICLR.cc/2020/Conference/Paper56/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the constructive feedback.\n\n- Clarification on LunarLander experiments\nWe first note that the expert demonstrations for LunarLander are generated by a human expert, rather than learned via RL. In the paper, we use the demonstration trajectories generated in the default environment.\n\nTo assess the potential impact of distribution mismatch, we have \u201cpadded\u201d each expert demonstration with loop transitions at the goal state with \u201cno op\u201d action, and re-run the experiment. This is exactly the behavior adopted by the expert after a successful landing (the expert stops giving commands to the lunar lander once it lands). Interestingly, in this setting the training process of the discriminator for the AIL method becomes highly unstable, as the expert demonstration is now dominated by (s, a) = (goal_state, no_op). As the discriminator is trained stochastically via mini-batches, random sampling could lead to a batch of real data with only (goal_state, no_op) present, which destabilizes training.\n\n- Clarifications on Mujoco results\nOur experiment setup follows OpenAI\u2019s reference implementation: a deterministic policy is used for each task to generate the demonstrations to ensure consistent performance. For reproducibility, The evaluations were generated by rolling out the learned policies deterministically. The expert performance is updated in Table 3 from the appendix, and included below.\n\nHopper 3777.8\u00b13.8 | Reacher -3.7\u00b11.4 | HalfCheetah 4159.8\u00b193.1 | Walker2d 5505.8\u00b181.4 | Ant 4821.0\u00b1107.4 | Humanoid 10413.1\u00b147.\n\nTherefore, the results from our paper suggest that the low variance policies can be attributed to our proposed method.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper56/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x3unVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference/Paper56/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper56/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper56/Reviewers", "ICLR.cc/2020/Conference/Paper56/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper56/Authors|ICLR.cc/2020/Conference/Paper56/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177035, "tmdate": 1576860535454, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference/Paper56/Reviewers", "ICLR.cc/2020/Conference/Paper56/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper56/-/Official_Comment"}}}, {"id": "BketIuNPiH", "original": null, "number": 3, "cdate": 1573501009104, "ddate": null, "tcdate": 1573501009104, "tmdate": 1573501009104, "tddate": null, "forum": "r1x3unVKPS", "replyto": "S1ex1UxPKS", "invitation": "ICLR.cc/2020/Conference/Paper56/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We thank the reviewer for the constructive feedback.\n\n- Clarification on method justification & theoretical analysis\nAs the reviewer pointed out, distribution matching does not necessarily imply support estimation (it depends on the metric used for the distribution matching). However, following the intuition in motivating this work, restricting the distribution matching problem to the support of the target distribution (the expert) might help the overall learning.  In this sense, the results in Prop. 1 and 2 show that, even if GAIL were to perform a very slow support estimation (or no support estimation at all), the distribution matching process would be constrained onto the expert\u2019s support, thanks to RED. \n\nIn the asymptotic setting, RED converges to an indicator function on the support of the expert policy: r_red(s, a) =1 if (s, a) belongs to the support of the expert policy, and 0 otherwise.\n\nTherefore, In this asymptotic case, SAIL is equivalent to performing distribution matching via GAIL with the additional *constraint* that candidate distributions need to have the same support of the expert distribution. Since we are restricting candidate estimators to the support of the target distribution, distribution matching methods (e.g. GAIL) can still be adopted.\n\n- Clarification on why optimizing Eq. 2 doesn\u2019t recover the \\theta and thus render the red reward a constant\nThe technique of random network distillation was first proposed in (Burda et al. 2018), which states that empirically, standard training would converge to a local minimum other than the randomly initialized \\theta. We have similar observations in our experiment, and thus no special treatment (e.g. early stopping) is used.\n\n- Clarification on additional comparison\nWe have included a comparison with the absorbing state (AS) technique (Kostrikov et al. 2019) in Table 5 from Appendix. We also run SAIL combined with AS, since the two methods are not mutually exclusive. \n\nThe results are:\nDefault env\nGAIL 258.30\u00b128.98 | GAIL+AS  271.46\u00b111.90 | SAIL 262.97\u00b118.11 | SAIL+AS 270.33\u00b115.86\nModified env (renamed as \u201cGoal-terminal\u201d env in the appendix)\nGAIL -7.16\u00b131.64 | GAIL+AS 110.22\u00b1119.25 | SAIL 252.07\u00b167.22 | SAIL+AS 258.30\u00b120.75\n\nAS improves GAIL significantly in both environments. In particular, in the default environment, GAIL+AS has performance comparable to (or slightly better than) SAIL. \n\nHowever, we also observe that: 1) SAIL+AS either outperforms (or is comparable to) GAIL+AS, showing that the proposed approach is generally more favorable than GAIL. Also, SAIL+AS has much smaller variance than standard SAIL. 2) In the modified environment, GAIL+AS is unable  to reach the expert\u2019s performance and suffers from a high variance, even when it improves significantly upon GAIL. On the contrary, SAIL and SAIL+AS are on-par with the expert. \n\nTo better study the effect of SAIL, we further modified LunarLander to contain no terminal states at all. We refer to this setting as NoTerm. Here each episode ends only after a fixed time limit (1000 steps). In this environment, the absorbing state (AS) method is not applicable. We obtain the following returns (updated Table 1 in the paper): \n\nGAIL 169.73\u00b180.84 | SAIL 256.83\u00b120.99\n\nshowing that SAIL is significantly more robust than GAIL also in this setting. \n\nIn summary, the results on all variants of LunarLander suggests that AS and SAIL both mitigate the implicit reward bias.\n\n- On minor points\nWe thank the reviewer for pointing out the typos, they have been fixed. \nThe bold font highlights the best performance of each row for ease of reading. Hopper had 2 because we considered them comparable. We have updated the paper to remove the additional highlighting. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper56/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x3unVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference/Paper56/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper56/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper56/Reviewers", "ICLR.cc/2020/Conference/Paper56/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper56/Authors|ICLR.cc/2020/Conference/Paper56/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177035, "tmdate": 1576860535454, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference/Paper56/Reviewers", "ICLR.cc/2020/Conference/Paper56/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper56/-/Official_Comment"}}}, {"id": "rJeof8EPir", "original": null, "number": 1, "cdate": 1573500434813, "ddate": null, "tcdate": 1573500434813, "tmdate": 1573500434813, "tddate": null, "forum": "r1x3unVKPS", "replyto": "HJlhjasptB", "invitation": "ICLR.cc/2020/Conference/Paper56/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the constructive feedback. We clarify the concerns below.\n\n- On addressing reward bias\nWe agree that the MDP raised by the reviewer can\u2019t be fully handled by SAIL unless a smaller discount factor is used. We have updated the paper to discuss this limitation.\n\nWe have included a comparison with the absorbing state (AS) technique (Kostrikov et al. 2019) in the Table 5 from Appendix. We also run SAIL combined with AS, since the two methods are not mutually exclusive. \n\nThe results are:\nDefault env\nGAIL 258.30\u00b128.98 | GAIL+AS  271.46\u00b111.90 | SAIL 262.97\u00b118.11 | SAIL+AS 270.33\u00b115.86\nModified env (renamed as \u201cGoal-terminal\u201d env in the appendix)\nGAIL -7.16\u00b131.64 | GAIL+AS 110.22\u00b1119.25 | SAIL 252.07\u00b167.22 | SAIL+AS 258.30\u00b120.75\n\nAs the reviewer suggested, AS improves GAIL significantly in both environments. In particular, in the default environment, GAIL+AS has performance comparable to (or slightly better than) SAIL. \n\nHowever, we also observe that: 1) SAIL+AS either outperforms (or is comparable to) GAIL+AS, showing that the proposed approach is generally more favorable than GAIL. Also, SAIL+AS has much smaller variance than standard SAIL. 2) In the modified environment, GAIL+AS is unable  to reach the expert\u2019s performance and suffers from a high variance, even when it improves significantly upon GAIL. On the contrary, SAIL and SAIL+AS are on-par with the expert. \n\n- On generalization of SAIL\nTo better study the effect of SAIL, we further modified LunarLander to contain no terminal states at all. We refer to this setting as NoTerm. Here each episode ends only after a fixed time limit (1000 steps). In this environment, the absorbing state (AS) method is not applicable. We obtain the following returns (updated Table 1 in the paper): \n\nGAIL 169.73\u00b180.84 | SAIL 256.83\u00b120.99\n\nshowing that SAIL is significantly more robust than GAIL also in this setting. \n\n- On GAIL not converging to the same policy in the default and modified environments:\nWe observe that in all variants of LunarLander, the policies learned by GAIL, GAIL+AS, SAIL and SAIL+AS  keep oscillating between the partially hovering (i.e. hopping) behavior and the landing behavior during training. The oscillation is due to the stochastic optimization and the finite number of expert demonstrations. GAIL is less consistent in the modified environment than in the default one, which leads to lower performance reported.\n\nIn summary, the results on all variants of LunarLander suggests that AS and SAIL both mitigate the implicit reward bias.\n\n- On Fig 3\nWe agree that RL optimizes for Q values rather than immediate reward. Fig 3 is only intended as a visualization to support our intuition that the shaping effect form support guidance favors the expert actions more, ultimately contributing to more robust algorithm.\n\n- Clarification on Naming\n We thank the reviewer for the potential confusion over denoting the methods. We have renamed the bounded variants as GAIL-b and SAIL-b respectively, to avoid confusion.\n\n- Clarification on Mujoco results\n\nFig 4 and Table 2 report different aspects of the experiment.\n\nFor each task and algorithm, we train 5 policies, each initialized by a different random seed. Fig 4 shows the mean performance and standard deviation across the 5 policies. The standard deviation thus shows the sensitivity of different algorithms with respect to the seeds. In contrast, Table 2 reports the performance of the best policy among the 5 for each task and algorithm. The standard deviation in Table 2 shows the robustness/stability of policies with respect to the environment.\n\nTo address the specific concerns raised, the large standard deviation for SAIL-log on HalfCheetah was due to 1 out 5 policies converging to a sub-optimal one with performance near  1.7k) Similarly for GAIL, 2 out of 5 policies converged to sub-optimal ones (average performance near 2k), and thus lowering the average value in Fig 4. The best policy from the 5 policies  was reported in Table 2, which achieved over 10k.\n\n-On large variance for SAIL-log, and why SAIL-log may perform worse than GAIL-log\nIn Table 2, SAIL-log has larger standard deviations as the product reward in this case is biased towards adversarial discriminator. This is because -Log D is in [0, inf] when r_red is in [0, 1]. In contrast, SAIL receives equal contribution from support guidance and adversarial reward as both components have [0. 1] range. That\u2019s why we proposed and advocate for the use of the bounded reward.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper56/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x3unVKPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference/Paper56/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper56/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper56/Reviewers", "ICLR.cc/2020/Conference/Paper56/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper56/Authors|ICLR.cc/2020/Conference/Paper56/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177035, "tmdate": 1576860535454, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper56/Authors", "ICLR.cc/2020/Conference/Paper56/Reviewers", "ICLR.cc/2020/Conference/Paper56/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper56/-/Official_Comment"}}}, {"id": "HJlhjasptB", "original": null, "number": 3, "cdate": 1571827107664, "ddate": null, "tcdate": 1571827107664, "tmdate": 1572972644253, "tddate": null, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "invitation": "ICLR.cc/2020/Conference/Paper56/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes an approach for improving adversarial imitation learning, by combining it with support-estimation-based imitation learning. In particular, the paper explores a combination of GAIL (Ho and Ermon, 2016) and RED (Wang et. al., 2019), where the reward for the policy-gradient is a product of the rewards obtained from them separately. The motivation is that, while AIL methods are sample-efficient (in terms of expert data) and implicitly promote useful exploration, they could be unreliable outside the support of the expert policy. Therefore, augmenting them by constraining the imitator to the support of the expert policy (with a method such as RED) could result in an overall better imitation learning algorithm. \n\nWhile the motivation and intuition are clear to me, I have reservations about the claims made in the abstract and the experimental sections:\n\n1.\tSAIL is an effective method for solving the reward bias in AIL.\nThe reward in SAIL is \u201calways\u201d non-negative (product of 2 non-negative terms), making the method a very ad-hoc way of getting around the reward bias problem, especially when compared to other methods such as those which estimate the value function of the absorbing state (Kostrikov et. al. 2019). Consider a simple chain MDP with 3 states A, B and a terminal state T. The actions are left/right from each state. Let the expert trajectory be A->B->T. Also, for SAIL, consider perfect support estimation with an optimal RED-network. When at B, the agent can terminate with a right-action and collect some reward. But taking left and collecting 0 reward (due to perfect support estimation) makes it land in A, from where it can now achieve a positive reward for the A->B transition, and repeat the process. Hence, one could always create MDPs where the Q value of B->A is higher than B->T.\n\n\tThe Lunar-Lander environment (with certain parameters) in Section 4.1 appears to present a scenario where SAIL get arounds the reward bias, but this doesn\u2019t remove my doubts over the generalization of this approach. Also, in Table 1, why does GAIL not hover above the landing spot even in the default case? If the reward bias is strong there, with sufficient exploration, the agent should converge to the same policy as in the modified case.\n\nFigure 3 is concerning for the same reason as above. It shows the immediate reward at the goal state, and points that SAIL has large reward for no-op action. The issue is that RL optimizes for actions that have the maximum Q value, not the action with the maximum immediate reward.\n\n\n2.\tI would recommend that the authors refer to the original GAIL algorithm as \u201cGAIL\u201d in the experiments section, and their practical stabilization trick as \u201cGAIL-bounded\u201d (or something to that effect). Referring to original algorithm as GAIL-log, and the modification as GAIL could be misleading to readers. \n\n\n3.\tThe authors claim that SAIL has better training stability, leading to more robust policies. If this is due to the algorithmic contribution of combining AIL and Support-Estimation-IL, then GAIL-log and SAIL-log in Table 2. should show this in the standard deviation numbers. This doesn\u2019t appear to be the case. Also, Figure 4 (Half-Cheetah) has unusually large variance for SAIL-log.\n\n\n4.\tFigure 4 and Table 2 numbers are very different. Take Humanoid for instance. From Figure 4, it seems that SAIL is way better than GAIL. But if you look at Table 2, they both achieve mean-score in excess of 10k. What\u2019s the difference between Table 2. and final performance in Figure 4?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper56/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper56/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Support-guided Adversarial Imitation Learning", "authors": ["Ruohan Wang", "Carlo Ciliberto", "Pierluigi Amadori", "Yiannis Demiris"], "authorids": ["r.wang16@ic.ac.uk", "c.ciliberto@imperial.ac.uk", "p.amadori@imperial.ac.uk", "y.demiris@imperial.ac.uk"], "keywords": ["Adversarial Imitation Learning", "Reinforcement Learning", "Learning from Demonstrations"], "TL;DR": "We unify support estimation with the family of Adversarial Imitation Learning algorithms into Support-guided Adversarial Imitation Learning, a more robust and stable imitation learning framework.", "abstract": "We propose Support-guided Adversarial Imitation Learning (SAIL), a generic imitation learning framework that unifies support estimation of the expert policy with the family of Adversarial Imitation Learning (AIL) algorithms. SAIL addresses two important challenges of AIL, including the implicit reward bias and potential training instability. We also show that SAIL is at least as efficient as standard AIL. In an extensive evaluation, we demonstrate that the proposed method effectively handles the reward bias and achieves better performance and training stability than other baseline methods on a wide range of benchmark control tasks.", "pdf": "/pdf/6f1db09c63d920774bf31e141a24bdfeea911cb3.pdf", "paperhash": "wang|supportguided_adversarial_imitation_learning", "original_pdf": "/attachment/0836a7536d0c9a86cacaeb1eda5a8f93abc5cc84.pdf", "_bibtex": "@misc{\nwang2020supportguided,\ntitle={Support-guided Adversarial Imitation Learning},\nauthor={Ruohan Wang and Carlo Ciliberto and Pierluigi Amadori and Yiannis Demiris},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x3unVKPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1x3unVKPS", "replyto": "r1x3unVKPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper56/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575503664170, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper56/Reviewers"], "noninvitees": [], "tcdate": 1570237757780, "tmdate": 1575503664182, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper56/-/Official_Review"}}}], "count": 8}