{"notes": [{"id": "mOO-LfEVZK", "original": "HGhxwYm1T7T", "number": 757, "cdate": 1601308088563, "ddate": null, "tcdate": 1601308088563, "tmdate": 1614985626152, "tddate": null, "forum": "mOO-LfEVZK", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering", "authorids": ["~Ting-An_Yen1", "~Chun-Shien_Lu1", "~Pau-Choo_Chung1"], "authors": ["Ting-An Yen", "Chun-Shien Lu", "Pau-Choo Chung"], "keywords": ["Adversarial Attacks", "Adversarial Defense", "Robustness", "Convolutional Neural Network", "Feature Compactness"], "abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods ($\\textit{e.g.}$, adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, $\\textit{i.e.}$, the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.", "one-sentence_summary": "A training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase their robustness and exhibit significantly higher performance than state-of-the-art robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yen|manifoldaware_training_increase_adversarial_robustness_with_feature_clustering", "supplementary_material": "/attachment/8018c61195338d0a60315b924778764c47f8e087.zip", "pdf": "/pdf/44ee379184c638ec0454ed489d2447255c07af3c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=A2J32u_Fzg", "_bibtex": "@misc{\nyen2021manifoldaware,\ntitle={Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering},\nauthor={Ting-An Yen and Chun-Shien Lu and Pau-Choo Chung},\nyear={2021},\nurl={https://openreview.net/forum?id=mOO-LfEVZK}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "u_DJdrV1-4p", "original": null, "number": 1, "cdate": 1610040533823, "ddate": null, "tcdate": 1610040533823, "tmdate": 1610474143683, "tddate": null, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "invitation": "ICLR.cc/2021/Conference/Paper757/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "Two reviewers expressed clear concerns about the paper but the authors did not provide any response. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering", "authorids": ["~Ting-An_Yen1", "~Chun-Shien_Lu1", "~Pau-Choo_Chung1"], "authors": ["Ting-An Yen", "Chun-Shien Lu", "Pau-Choo Chung"], "keywords": ["Adversarial Attacks", "Adversarial Defense", "Robustness", "Convolutional Neural Network", "Feature Compactness"], "abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods ($\\textit{e.g.}$, adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, $\\textit{i.e.}$, the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.", "one-sentence_summary": "A training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase their robustness and exhibit significantly higher performance than state-of-the-art robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yen|manifoldaware_training_increase_adversarial_robustness_with_feature_clustering", "supplementary_material": "/attachment/8018c61195338d0a60315b924778764c47f8e087.zip", "pdf": "/pdf/44ee379184c638ec0454ed489d2447255c07af3c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=A2J32u_Fzg", "_bibtex": "@misc{\nyen2021manifoldaware,\ntitle={Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering},\nauthor={Ting-An Yen and Chun-Shien Lu and Pau-Choo Chung},\nyear={2021},\nurl={https://openreview.net/forum?id=mOO-LfEVZK}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040533809, "tmdate": 1610474143666, "id": "ICLR.cc/2021/Conference/Paper757/-/Decision"}}}, {"id": "oxM2g9iZ9IY", "original": null, "number": 2, "cdate": 1604030379220, "ddate": null, "tcdate": 1604030379220, "tmdate": 1605024613444, "tddate": null, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "invitation": "ICLR.cc/2021/Conference/Paper757/-/Official_Review", "content": {"title": "No theory, experiments only", "review": "Results: To defend against adversarial attacks, this work experimentally analyzes the feature distribution of traditionally- trained CNNs for gaining more knowledge about adversarial examples. Two properties, i.e., the non-clustering property and confusing-distance property, of the feature distribution are identified by means of t-SNE visualization and clustering analysis (showing the limitations regarding representativeness) in Figure 1. The authors introduce a loss function which separates out cluster centers of CNN output features, setting them as far as possible - so that model accuracy is preserved while strengthening robustness. They test on two datasets: CIFAR10, MNIST, and show improvements in \"robustness\" of the model. \n\nStrong points: The experiments presented are promising in terms of increasing robustness of the learned models. \n\nWeak points: Experiments are only conducted on two datasets, it's unclear how generalization these results are. Further, there is no theoretical development regarding manifolds in the feature space.  \n\nMinor typing errors: \n\"indication of the clean accuracy\" \n\"using PGD optimizationm,\"\n\"an input images x\"\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper757/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper757/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering", "authorids": ["~Ting-An_Yen1", "~Chun-Shien_Lu1", "~Pau-Choo_Chung1"], "authors": ["Ting-An Yen", "Chun-Shien Lu", "Pau-Choo Chung"], "keywords": ["Adversarial Attacks", "Adversarial Defense", "Robustness", "Convolutional Neural Network", "Feature Compactness"], "abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods ($\\textit{e.g.}$, adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, $\\textit{i.e.}$, the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.", "one-sentence_summary": "A training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase their robustness and exhibit significantly higher performance than state-of-the-art robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yen|manifoldaware_training_increase_adversarial_robustness_with_feature_clustering", "supplementary_material": "/attachment/8018c61195338d0a60315b924778764c47f8e087.zip", "pdf": "/pdf/44ee379184c638ec0454ed489d2447255c07af3c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=A2J32u_Fzg", "_bibtex": "@misc{\nyen2021manifoldaware,\ntitle={Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering},\nauthor={Ting-An Yen and Chun-Shien Lu and Pau-Choo Chung},\nyear={2021},\nurl={https://openreview.net/forum?id=mOO-LfEVZK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135750, "tmdate": 1606915809500, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper757/-/Official_Review"}}}, {"id": "DiDrtOmSKyF", "original": null, "number": 1, "cdate": 1603931456573, "ddate": null, "tcdate": 1603931456573, "tmdate": 1605024613368, "tddate": null, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "invitation": "ICLR.cc/2021/Conference/Paper757/-/Official_Review", "content": {"title": "This paper proposes a manifold aware training strategy to learn compact features and improve the robustness of CNNs.", "review": "This paper proposes to leverage the manifold aware training to learn compact representation. The authors proposes to enforce the learned representation along with generated vectors for different clusters, which implicitly enlarge the margin of the prediction.\nHowever the technical contribution as using the three-term loss to improve robustness is limited. In particular, it's is unclear what the equation (10) and (11) try to prove without a concrete theorem or lemma statement. \n\nFrom the empirical performance, it looks promising from table 2 but it's also quite clear that the TRADES loss BIBO dominates the performance, and without adding this loss, the proposed MAT training cannot achieve high robustness. This is as expected and also render the proposed method less effective.  \nIn addition, TRADES is evaluated on ImageNet and it would be good for the work to evaluate on ImageNet to demonstrate the generalization ability and scalability. It would also be good to explain why without the BIBO loss, the robustness against adaptive attack of MAT is almost 0 which again shows the weakness of the main proposed method.\n\nIt would also be necessary to provide analysis for the properties of the learned representation. For instance, if it is compact features, whether its rank is indeed lower, and whether the entropy of the learned features is indeed low in order to claim the consistent and compact feature representation. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper757/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper757/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering", "authorids": ["~Ting-An_Yen1", "~Chun-Shien_Lu1", "~Pau-Choo_Chung1"], "authors": ["Ting-An Yen", "Chun-Shien Lu", "Pau-Choo Chung"], "keywords": ["Adversarial Attacks", "Adversarial Defense", "Robustness", "Convolutional Neural Network", "Feature Compactness"], "abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods ($\\textit{e.g.}$, adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, $\\textit{i.e.}$, the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.", "one-sentence_summary": "A training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase their robustness and exhibit significantly higher performance than state-of-the-art robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yen|manifoldaware_training_increase_adversarial_robustness_with_feature_clustering", "supplementary_material": "/attachment/8018c61195338d0a60315b924778764c47f8e087.zip", "pdf": "/pdf/44ee379184c638ec0454ed489d2447255c07af3c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=A2J32u_Fzg", "_bibtex": "@misc{\nyen2021manifoldaware,\ntitle={Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering},\nauthor={Ting-An Yen and Chun-Shien Lu and Pau-Choo Chung},\nyear={2021},\nurl={https://openreview.net/forum?id=mOO-LfEVZK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135750, "tmdate": 1606915809500, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper757/-/Official_Review"}}}, {"id": "tD4cYRakmsO", "original": null, "number": 3, "cdate": 1604342901166, "ddate": null, "tcdate": 1604342901166, "tmdate": 1605024613298, "tddate": null, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "invitation": "ICLR.cc/2021/Conference/Paper757/-/Official_Review", "content": {"title": "Recommendation to Accept", "review": "Summary:\nThis paper tackles the problem of training models that are robust to adversarial inputs. The authors starts by observing that previous models generate embeddings that can both (i) place same-class embeddings in different clusters and (ii) different-class embeddings in close proximity. They then introduce new loss functions that penalize these behaviors and design a training procedure (MAT) around these new losses. Finally, they show favorable performance of MAT compared to state-of-the-art techniques for addressing adversarial robustness.\n\nReasons for score:\nOverall, I vote for accepting. Training adversarially robust models is an important problem, and the paper\u2019s experimental validation that the features of prior methods (TRADES) exhibit (i) non-clustering and (ii) confusing distance motivates the approach they take. The loss functions are explicitly designed to combat these issues, and the experimental results clearly show the favorability of the MAT procedure. In addition, the ablation study of the various components of the loss functions also adds some insight into the results. The paper is also very well written.\n\nCons:\nIt would be of interest to have some theoretical justification for the approach. Regarding the loss functions, it seems that BIBO should be a consequence of penalizing FTC loss and SO loss, and should not be explicitly needed (this is also somewhat consistent with the results of Table 2). Finally, some of the experimental results can be explored further. For example, in the ablation study, some of the experiments perform better without one of the loss functions, and it may help to explain such behavior. \n\nClarity / Typos:\nThe paper is very well written. A couple of minor points:\nFeature compactness - Maybe explain this phrase better in the introduction (explained well in Section 3 introduction)\nEqn 1: Maybe write J(f(x), y) and J(f(x), f(x\u2019)) instead\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper757/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper757/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering", "authorids": ["~Ting-An_Yen1", "~Chun-Shien_Lu1", "~Pau-Choo_Chung1"], "authors": ["Ting-An Yen", "Chun-Shien Lu", "Pau-Choo Chung"], "keywords": ["Adversarial Attacks", "Adversarial Defense", "Robustness", "Convolutional Neural Network", "Feature Compactness"], "abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods ($\\textit{e.g.}$, adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, $\\textit{i.e.}$, the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.", "one-sentence_summary": "A training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase their robustness and exhibit significantly higher performance than state-of-the-art robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yen|manifoldaware_training_increase_adversarial_robustness_with_feature_clustering", "supplementary_material": "/attachment/8018c61195338d0a60315b924778764c47f8e087.zip", "pdf": "/pdf/44ee379184c638ec0454ed489d2447255c07af3c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=A2J32u_Fzg", "_bibtex": "@misc{\nyen2021manifoldaware,\ntitle={Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering},\nauthor={Ting-An Yen and Chun-Shien Lu and Pau-Choo Chung},\nyear={2021},\nurl={https://openreview.net/forum?id=mOO-LfEVZK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135750, "tmdate": 1606915809500, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper757/-/Official_Review"}}}, {"id": "zzQzZ9CO3wY", "original": null, "number": 4, "cdate": 1604504456525, "ddate": null, "tcdate": 1604504456525, "tmdate": 1605024613225, "tddate": null, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "invitation": "ICLR.cc/2021/Conference/Paper757/-/Official_Review", "content": {"title": "The defense evaluation is not correct", "review": "This work proposes a defense that combines prior work on learning features that are compact for samples from the same but dispersed for samples from different classes (MMD by Pang et al.) with (a) a method to find better class centers, (b) a gradient-norm regularization and (c) an adversarial training regularization.\n\nUnfortunately, the reported results on the robustness of the defense are clearly wrong. For one, the core part of this defense by Pang et al. was broken by [1] which is not mentioned here. More importantly, the adversarial attacks employed here are not suited for finding minimal adversarial perturbations against the proposed defense. This can be seen most clearly in Figure 2 (or Table 10 in the appendix): If we allow a perturbation with L-infinity norm of 0.5 on MNIST, then we can always find an adversarial perturbation simply by setting the whole image to a flat gray value of 0.5. In turn, any effective adversarial attack should drive network performance down to at least random baseline performance (10%) for epsilon = 0.5. Instead, the paper reports > 99% accuracy for this value under a PGD attack, which means that PGD is totally ineffective against the given defense and a very different adaptive attack would be needed to accurately measure its robustness. Similarly, in Table 3 the attack success of targeted attacks is often higher than for untargeted attacks, again a clear sign for ineffective attacks. The work also uses an adaptive attack which works better for some versions of MAT but performs similar to PGD in other cases. Hence, the adaptive attack employed here are not good enought.\n\nThe reason why the proposed attacks fail against the defense are probably simple: for one, the attacks optimise a different classificatioon loss then what is actually used by the model. Second, both auxiliary losses may give rise to gradient masking, the most common issue for gradient-based attacks to fail against a defense. I highly suggest the authors study [1] to get familiar with how to engineer strong adaptive attacks.\n\n[1] On Adaptive Attacks to Adversarial Example Defenses, Florian Tramer, Nicholas Carlini, Wieland Brendel, Aleksander Madry, NeurIPS 2020, https://arxiv.org/abs/2002.08347", "rating": "1: Trivial or wrong", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper757/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper757/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering", "authorids": ["~Ting-An_Yen1", "~Chun-Shien_Lu1", "~Pau-Choo_Chung1"], "authors": ["Ting-An Yen", "Chun-Shien Lu", "Pau-Choo Chung"], "keywords": ["Adversarial Attacks", "Adversarial Defense", "Robustness", "Convolutional Neural Network", "Feature Compactness"], "abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods ($\\textit{e.g.}$, adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, $\\textit{i.e.}$, the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.", "one-sentence_summary": "A training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase their robustness and exhibit significantly higher performance than state-of-the-art robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yen|manifoldaware_training_increase_adversarial_robustness_with_feature_clustering", "supplementary_material": "/attachment/8018c61195338d0a60315b924778764c47f8e087.zip", "pdf": "/pdf/44ee379184c638ec0454ed489d2447255c07af3c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=A2J32u_Fzg", "_bibtex": "@misc{\nyen2021manifoldaware,\ntitle={Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering},\nauthor={Ting-An Yen and Chun-Shien Lu and Pau-Choo Chung},\nyear={2021},\nurl={https://openreview.net/forum?id=mOO-LfEVZK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135750, "tmdate": 1606915809500, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper757/-/Official_Review"}}}, {"id": "VdxOoQuC6FI", "original": null, "number": 5, "cdate": 1604704109953, "ddate": null, "tcdate": 1604704109953, "tmdate": 1605024613140, "tddate": null, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "invitation": "ICLR.cc/2021/Conference/Paper757/-/Official_Review", "content": {"title": "AnonReviewer4 Review", "review": "# Summary\n\nThe authors propose a novel training process called Manifold-Aware\nTraining (MAT) to increase the robustness of the CNN against adversarial\nexamples. The authors compare MAT against the state-of-the-art in\ndefenses against adversarial evasion attacks (i.e., TRADES and MCC) and\nshow their approach outperforms it.\n\n# Strengths\n\n+  Interesting intuition of performing training \"in the\" manifold\n+  Interesting intuition to support SO and BIBO losses\n\n# Weaknesses\n\n-  Lack of comparison with a similar approach\n-  Lack of conclusive remarks / actionable points\n\n# Comments\n\nI praise the authors intuition of exploring the possibility of training\na classifier by exploiting knowledge of the manifold - its immediate\nimplication is that of focusing on lower dimensions of compact features\nthat would be more robust to manipulation (and thus adversarial\nattacks). I also particularly appreciate the threat model and the fact\nthe approach is evaluated in a white-box setting, according to Carlini\net al. (2019). While the authors' intuition is interesting, I wonder how\neasy this is to achieve in practice. In general, we have no knowledge of\nthe underlying manifold and I thus wonder what guarantees this approach\nwould provide. The results seem to show no clear loss-dependent trend\nand I thus wonder whether we can easily draw conclusive remarks. (For\ninstance, should we use SO and BIBO always? From a theoretical\nperspective, it seems so, but experiments seem to show otherwise.)\n\nFigure 1 is interesting as it shows that the representative features of\nsame-class samples are not always similar to one another. Wasn't this\nalready explored in Szegedy et al.? Perhaps not visually, but the fact\nthat objects close in the input space get eventually separated in the\nlatent space across the layers of the CNN is quite known. Also, a\nsimilar approach to the authors' proposal seems to be explored by Crecchi\net al. [1]. It would be interesting to compare and position MAT against\nthis.\n\n## Additional Comments\n\nIn Section 3.2, the authors propose two auxiliary loss functions to\nfurther improve the robustness of MAT. I wonder whether the BIBO loss\nwould just suffice for the purpose, instead of relying on the\nsecond-order loss too. I appreciate the explanation in Section 3.2.3 but\nit would be interesting to understand how one should expect to tune\nalpha and beta accordingly. \n\nResults on CIFAR10 seem less stable than compared to those on MNIST. In\nparticular, there is no trend that shows that relying on SO and BIBO on\na clean dataset provides better results than with a plain FTC loss:\n94%->85%->95%->83%; why the 95%? Is that expected? Similar reasoning\ncan actually be applied to MNIST too when one looks at PGD:\n61%->99%->82->99; why 82%? Is this expected? In contrast, TRADES seem to\nshow an expected trend (even when BIBO loss is considered).\n\nThe authors rely on the library 'foolbox' - my impression was that\ncleverhans [2] represented the state-of-the-art when it comes to\nexperimenting with adversarial ML attacks. What advantages does foolbox\nhave compared to cleverhans?\n\nAlthough off-topic for this work, it would be interesting to understand\nwhether MAT would be beneficial in defending against adversarial attacks\nthat consider realizable attacks (in the problem space). Figure 2 shows\nthe stability of MAT robustness for increasing values of perturbations.\nAdversarial attacks in the problem-space might need to consider\nadditional constraints while being non-necessarily constrained in a\nlp-norm [3].\n\n[1] Crecchi et al. Detecting Adversarial Examples through Nonlinear\nDimensionality Reduction. ESANN 2019\n(https://pralab.diee.unica.it/sites/default/files/crecchi19-esann.pdf)\n\n[2] http://www.cleverhans.io/\n\n[3] https://s2lab.kcl.ac.uk/projects/intriguing/ (IEEE S&P 2020)\n\n### Minor Typos\n\n\"optimizationm\" -> \"optimization\"", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper757/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper757/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering", "authorids": ["~Ting-An_Yen1", "~Chun-Shien_Lu1", "~Pau-Choo_Chung1"], "authors": ["Ting-An Yen", "Chun-Shien Lu", "Pau-Choo Chung"], "keywords": ["Adversarial Attacks", "Adversarial Defense", "Robustness", "Convolutional Neural Network", "Feature Compactness"], "abstract": "The problem of defending against adversarial attacks has attracted increasing attention in recent years. While various types of defense methods ($\\textit{e.g.}$, adversarial training, detection and rejection, and recovery) were proven empirically to bring robustness to the network, their weakness was shown by later works. Inspired by the observation from the distribution properties of the features extracted by the CNNs in the feature space and their link to robustness, this work designs a novel training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase robustness.  The effectiveness of the proposed method is evaluated via comparisons with existing defense mechanisms, $\\textit{i.e.}$, the TRADES algorithm, which has been recognized as a representative state-of-the-art technology, and the MMC method, which also aims to learn compact features. Further verification is also conducted using the attack adaptive to our method. Experimental results show that MAT-trained CNNs exhibit significantly higher performance than state-of-the-art robustness.", "one-sentence_summary": "A training process called Manifold-Aware Training (MAT), which forces CNNs to learn compact features to increase their robustness and exhibit significantly higher performance than state-of-the-art robustness.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yen|manifoldaware_training_increase_adversarial_robustness_with_feature_clustering", "supplementary_material": "/attachment/8018c61195338d0a60315b924778764c47f8e087.zip", "pdf": "/pdf/44ee379184c638ec0454ed489d2447255c07af3c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=A2J32u_Fzg", "_bibtex": "@misc{\nyen2021manifoldaware,\ntitle={Manifold-aware Training: Increase Adversarial Robustness with Feature Clustering},\nauthor={Ting-An Yen and Chun-Shien Lu and Pau-Choo Chung},\nyear={2021},\nurl={https://openreview.net/forum?id=mOO-LfEVZK}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mOO-LfEVZK", "replyto": "mOO-LfEVZK", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper757/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135750, "tmdate": 1606915809500, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper757/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper757/-/Official_Review"}}}], "count": 7}