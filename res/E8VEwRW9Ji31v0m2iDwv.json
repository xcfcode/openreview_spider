{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458120525600, "tcdate": 1458120525600, "id": "OM0QpBZzDip57ZJjtNAw", "invitation": "ICLR.cc/2016/workshop/-/paper/177/review/12", "forum": "E8VEwRW9Ji31v0m2iDwv", "replyto": "E8VEwRW9Ji31v0m2iDwv", "signatures": ["ICLR.cc/2016/workshop/paper/177/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/177/reviewer/12"], "content": {"title": "Not enough content or results to merit publication", "rating": "3: Clear rejection", "review": "The submission presents an experimental setup for analyzing the successful gradient-based optimization and performance of networks with large numbers of parameters. They propose to train a convolutional network on MNIST and analyze the gradient descent paths through weight space. The trajectories are compared and evaluated using PCA. \n\nThis is very similar to the approach taken by Goodfellow et al, and it is difficult to see any new contributions of this submission. The results are mostly well-known at this point, although there is certainly room for further research in this area. The demonstration of divergence during training because of shuffled inputs is interesting but not surprising. There are no new visualizations or qualitative results, and the quantitative results are limited to 2 numbers (the variance explained by the top 2 and top 10 principal components) which are meaningless without more extensive comparison and analysis.\n\nThe submission could be a white paper to justify some further research, but it does not have enough substance or novelty to be in the ICLR workshop.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Stuck in a What? Adventures in Weight Space", "abstract": "Deep learning researchers commonly suggest \nthat converged models are stuck in local minima.\nMore recently, some researchers observed \nthat under reasonable assumptions, \nthe vast majority of critical points are saddle points, not true minima.\nBoth descriptions suggest that weights converge around  a point in weight space, \nbe it a local optima or merely a critical point.\nHowever, it's possible that neither interpretation is accurate.\nAs neural networks are typically over-complete,\nit's easy to show the existence of  vast  continuous regions through weight space with equal loss.\nIn this paper, we build on recent work empirically characterizing the error surfaces of neural networks.\nWe analyze training paths through weight space,\npresenting evidence that apparent convergence of loss\ndoes not correspond to weights arriving at critical points, \nbut instead to large movements through flat regions of weight space.\nWhile it's trivial to show that neural network error surfaces are globally non-convex, \nwe show that error surfaces are also locally non-convex, \neven after breaking symmetry with a random initialization and also after partial training.", "pdf": "/pdf/E8VEwRW9Ji31v0m2iDwv.pdf", "paperhash": "lipton|stuck_in_a_what_adventures_in_weight_space", "authors": ["Zachary C. Lipton"], "authorids": ["zlipton@cs.ucsd.edu"], "conflicts": ["ucsd.edu", "microsoft.com", "amazon.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580177913, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580177913, "id": "ICLR.cc/2016/workshop/-/paper/177/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "E8VEwRW9Ji31v0m2iDwv", "replyto": "E8VEwRW9Ji31v0m2iDwv", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/177/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457550206901, "tcdate": 1457550206901, "id": "1Wv06PjM0cMnPB1oinX1", "invitation": "ICLR.cc/2016/workshop/-/paper/177/review/11", "forum": "E8VEwRW9Ji31v0m2iDwv", "replyto": "E8VEwRW9Ji31v0m2iDwv", "signatures": ["ICLR.cc/2016/workshop/paper/177/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/177/reviewer/11"], "content": {"title": "Mostly already known", "rating": "3: Clear rejection", "review": "\"...descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it's possible that neither interpretation is accurate.\"\nIndeed those descriptions are not accurate, but this is already well-known and described in the Deep Learning textbook:\nhttp://www.deeplearningbook.org/version-2016-02-17/contents/optimization.html\nFig 8.1 caption says \"Gradient descent often deos not arrive at a critical point of any kind.\" and uses the norm of the gradient to show it.\n\n\"Weights do not converge to critical points, instead traveling large (euclidean) distances\nthrough flat basins in weight space.\"\nThis is already shown in the 3D visualizations in the Goodfellow et al paper.\n\n\"While a straight line in weight-space from initialization to solution may correspond to\nmonotonically decreasing loss, the path actually taken by gradient descent seems far from\nstraight.\"\nThis is already shown in the Goodfellow et al paper, both via the 3-D visualizations and via direct measurement of the distance from the path.\n\n\"Even once symmetry is broken, neural network error surfaces are neither convex nor quasiconvex\nbut continue to diverge towards many different low error basins. Starting from the\nsame initialization, but then feeding each network examples in shuffled order is sufficient\nto diverge each network along a different path. This suggests that the error surface is not\nonly globally non-convex, but also locally non-convex even for a partially trained net.\"\nThis is partially new, but there is already a lot of existing work on the effect of the order of presentation of examples.\nThe references in Sec 8.7.6 discuss some of the existing findings:\nhttp://www.deeplearningbook.org/version-2016-02-17/contents/optimization.html\n\n\nThe paper is too misleading and takes too much credit for previously known ideas for me to endorse as it is.\nThe following ideas from the paper are new findings and I encourage you to develop them further for a future\nsubmission:\n\"A small number of principal components explains most of the variance along a training\ntrajectory.\"\n\"All pairs of solutions after a fixed number of epochs appear to be roughly the same euclidean\ndistance from the origin and from each other. This is true even with identical\ninitializations, and pretraining before cloning\"\nFrom the experiments in this paper, it's hard to tell whether this effect is real, or if it is an artifact of the simplicity of the MNIST dataset.\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Stuck in a What? Adventures in Weight Space", "abstract": "Deep learning researchers commonly suggest \nthat converged models are stuck in local minima.\nMore recently, some researchers observed \nthat under reasonable assumptions, \nthe vast majority of critical points are saddle points, not true minima.\nBoth descriptions suggest that weights converge around  a point in weight space, \nbe it a local optima or merely a critical point.\nHowever, it's possible that neither interpretation is accurate.\nAs neural networks are typically over-complete,\nit's easy to show the existence of  vast  continuous regions through weight space with equal loss.\nIn this paper, we build on recent work empirically characterizing the error surfaces of neural networks.\nWe analyze training paths through weight space,\npresenting evidence that apparent convergence of loss\ndoes not correspond to weights arriving at critical points, \nbut instead to large movements through flat regions of weight space.\nWhile it's trivial to show that neural network error surfaces are globally non-convex, \nwe show that error surfaces are also locally non-convex, \neven after breaking symmetry with a random initialization and also after partial training.", "pdf": "/pdf/E8VEwRW9Ji31v0m2iDwv.pdf", "paperhash": "lipton|stuck_in_a_what_adventures_in_weight_space", "authors": ["Zachary C. Lipton"], "authorids": ["zlipton@cs.ucsd.edu"], "conflicts": ["ucsd.edu", "microsoft.com", "amazon.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580178793, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580178793, "id": "ICLR.cc/2016/workshop/-/paper/177/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "E8VEwRW9Ji31v0m2iDwv", "replyto": "E8VEwRW9Ji31v0m2iDwv", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/177/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457543322169, "tcdate": 1457543322169, "id": "81DG6LXXvu6O2Pl0UVM2", "invitation": "ICLR.cc/2016/workshop/-/paper/177/review/10", "forum": "E8VEwRW9Ji31v0m2iDwv", "replyto": "E8VEwRW9Ji31v0m2iDwv", "signatures": ["ICLR.cc/2016/workshop/paper/177/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/177/reviewer/10"], "content": {"title": "Presents preliminary experiments with few clear results as yet", "rating": "3: Clear rejection", "review": "\nSummary:\n\nThis paper presents several qualitative experiments aimed at understanding the path taken by SGD through weight space.\n\nMajor comments:\n\n\u201cFlat region of weight space\u201d: At several points it is claimed that models don\u2019t arrive at a critical point but instead a flat region of weight space. Yet a truly flat region is, of course, a critical point or manifold of critical points; and nearly flat regions of weight space are common next to saddle points (see, eg, Saxe et al. ICLR2014). Hence if solutions do enter a flat region, this is not evidence for or against the local minima or saddle point hypotheses.\n\nThe observation that different networks can be led to very different solutions by reordering input examples is not surprising given the analyses in eg, Baldi & Hornik, 1989 or Saxe et al., 2014. The main point is that the many symmetries in a deep network lead to a manifold of global minima. This is an infinite set of critical points which all attain equal error. Hence, due to noise (such as reordering input samples), solutions can wander along this manifold\u2014all global minima are equally good\u2014and there is no pressure to settle on one particular optimal solution over another. As a simple example, take a deep linear network with just one neuron per layer, y = a*b*x, where a and b are scalar weights, and with just one input example {y=1, x=1}. The manifold of global minima is the hyperbola on which a*b=1. The euclidean distances between different solutions can thus clearly be arbitrarily large (one solution is a=1/10, b = 10; another is a=10, b=1/10), and as large as the euclidean distance to the origin.\n\nSome results in this paper are already known. For instance, Saxe et al. ICLR 2014 showed that gradient descent learning trajectories are nonlinear. Indeed, Goodfellow et al. ICLR2014 also found that gradient descent does not take the straight line path from initialization to eventual solution.\n\nAdditional experimental details are necessary to evaluate the paper. What loss function is optimized? What exact initialization is used? These are centrally important to evaluating the results here. In particular, a small random initialization will perform differently from a large-norm initialization.\n\nThere are some conceptual confusions. At several points it is claimed that \u201cafter symmetry is broken\u201d the error surface is still nonconvex. Finding a particular initial condition for which symmetry is broken does not make a minimization problem convex. The non convexity of a minimization problem can influence gradient dynamics even away from critical points, for example, it can induce long nearly flat plateaus.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Stuck in a What? Adventures in Weight Space", "abstract": "Deep learning researchers commonly suggest \nthat converged models are stuck in local minima.\nMore recently, some researchers observed \nthat under reasonable assumptions, \nthe vast majority of critical points are saddle points, not true minima.\nBoth descriptions suggest that weights converge around  a point in weight space, \nbe it a local optima or merely a critical point.\nHowever, it's possible that neither interpretation is accurate.\nAs neural networks are typically over-complete,\nit's easy to show the existence of  vast  continuous regions through weight space with equal loss.\nIn this paper, we build on recent work empirically characterizing the error surfaces of neural networks.\nWe analyze training paths through weight space,\npresenting evidence that apparent convergence of loss\ndoes not correspond to weights arriving at critical points, \nbut instead to large movements through flat regions of weight space.\nWhile it's trivial to show that neural network error surfaces are globally non-convex, \nwe show that error surfaces are also locally non-convex, \neven after breaking symmetry with a random initialization and also after partial training.", "pdf": "/pdf/E8VEwRW9Ji31v0m2iDwv.pdf", "paperhash": "lipton|stuck_in_a_what_adventures_in_weight_space", "authors": ["Zachary C. Lipton"], "authorids": ["zlipton@cs.ucsd.edu"], "conflicts": ["ucsd.edu", "microsoft.com", "amazon.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456580178894, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456580178894, "id": "ICLR.cc/2016/workshop/-/paper/177/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "E8VEwRW9Ji31v0m2iDwv", "replyto": "E8VEwRW9Ji31v0m2iDwv", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/177/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455831812684, "tcdate": 1455831812684, "id": "E8VEwRW9Ji31v0m2iDwv", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "E8VEwRW9Ji31v0m2iDwv", "signatures": ["~Zachary_Chase_Lipton1"], "readers": ["everyone"], "writers": ["~Zachary_Chase_Lipton1"], "content": {"CMT_id": "", "title": "Stuck in a What? Adventures in Weight Space", "abstract": "Deep learning researchers commonly suggest \nthat converged models are stuck in local minima.\nMore recently, some researchers observed \nthat under reasonable assumptions, \nthe vast majority of critical points are saddle points, not true minima.\nBoth descriptions suggest that weights converge around  a point in weight space, \nbe it a local optima or merely a critical point.\nHowever, it's possible that neither interpretation is accurate.\nAs neural networks are typically over-complete,\nit's easy to show the existence of  vast  continuous regions through weight space with equal loss.\nIn this paper, we build on recent work empirically characterizing the error surfaces of neural networks.\nWe analyze training paths through weight space,\npresenting evidence that apparent convergence of loss\ndoes not correspond to weights arriving at critical points, \nbut instead to large movements through flat regions of weight space.\nWhile it's trivial to show that neural network error surfaces are globally non-convex, \nwe show that error surfaces are also locally non-convex, \neven after breaking symmetry with a random initialization and also after partial training.", "pdf": "/pdf/E8VEwRW9Ji31v0m2iDwv.pdf", "paperhash": "lipton|stuck_in_a_what_adventures_in_weight_space", "authors": ["Zachary C. Lipton"], "authorids": ["zlipton@cs.ucsd.edu"], "conflicts": ["ucsd.edu", "microsoft.com", "amazon.com"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 4}