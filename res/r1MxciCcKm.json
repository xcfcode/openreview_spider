{"notes": [{"id": "r1MxciCcKm", "original": "SygfH-Q5YQ", "number": 505, "cdate": 1538087816215, "ddate": null, "tcdate": 1538087816215, "tmdate": 1545355394987, "tddate": null, "forum": "r1MxciCcKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJgdd4YlxV", "original": null, "number": 1, "cdate": 1544750192510, "ddate": null, "tcdate": 1544750192510, "tmdate": 1545354516441, "tddate": null, "forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Meta_Review", "content": {"metareview": "I enjoyed reading the paper myself and I appreciate the unifying framework connecting RAML and SPG. While I do not put a lot of weight on the experiments, I agree with the reviewers that the experimental results are not very strong, and I am not convinced that the theoretical contribution meets the bar at ICLR.\n\nIn the interpolation algorithm, there seems to be an additional annealing parameter and two tuning parameters. It is important to describe how the parameters are tuned. Given the additional hyper-parameters, one may consider giving all of the algorithms the same budget of hyper-parameter tuning. I also agree with reviewers that the policy gradient baseline seems to underperform typical results. One possible way to strengthen the experiments is to try to replicate the results of SPG or RAML and discuss the behavior of each algorithm as a function of hyper-parameters.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Borderline paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper505/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353192997, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper505/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper505/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper505/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353192997}}}, {"id": "SJglOdb2CX", "original": null, "number": 7, "cdate": 1543407719853, "ddate": null, "tcdate": 1543407719853, "tmdate": 1543407719853, "tddate": null, "forum": "r1MxciCcKm", "replyto": "BJx-HalfAX", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "content": {"title": "Update after rebuttal", "comment": "I appreciate the new experimental results.\n\nWhile I do understand that coming up with a readily comparable experimental setup is difficult, I still think more effort should be put into this. The evaluation protocol is not that variable from my own experience and the relevant choices either appear in the published papers or can be obtained through private communication.\n\nI agree that AR3 that more comprehensive experiments would be very beneficial to the paper, so my assessment remains the same."}, "signatures": ["ICLR.cc/2019/Conference/Paper505/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper505/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617025, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1MxciCcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper505/Authors|ICLR.cc/2019/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617025}}}, {"id": "B1xuDmP93Q", "original": null, "number": 3, "cdate": 1541202784492, "ddate": null, "tcdate": 1541202784492, "tmdate": 1543272763532, "tddate": null, "forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Review", "content": {"title": "AR3 Review: Connecting the Dots Between MLE and RL for Sequence Generation", "review": "The authors propose a more unified view of disparate methods for training sequence models. Specifically, a multi-term objective L(q,theta) consisting of:\n1) The standard reward maximization objective of policy gradient, E_{p_\\theta}[R], \n2) A weighted (weight alpha) reverse KL divergence of the parametric policy and a non-parameteric policy q, \n3) A weighted (weight beta) entropy term on q, \nIs proposed for sequence training (see equation (1). L can be iteratively optimized by solving for q given p_\\theta, and the \\theta given q (see eq. 2).\n\nThis framework mathematically generalizes softmax-policy gradient (SPG, alpha=1, beta=0), and reward-augmented maximum likelihood (alpha=0, beta=temperature), and also standard entropy regularized policy gradient (alpha=0), among other algorithms.\n\nThe paper is well written, and the approach sensible. However, combining SPG and RAML by introducing their respective regularization terms is a rather straightforward exercise, and so seems quite incremental.\n\nOther major concerns are:\n1) the true utility of the model, and \n2) the integrity of the experiments. \n\nWrt: \n1), While RAML was a significant contribution at the time, it is now well established that RAML generally doesn't perform well at all in practice due to exposure bias (not conditioning on it's own previous predictions during training). Moreover SPG, as the authors point out, was supposed to address the need for ML pre-training, but required much engineering to work. The fact is that REINFORCE-based policy gradient methods are still more effective than these methods, provided they have a good baseline to reduce varince. Which brings me to point \n2) Was MIXER run with a learned baseline and judiciously optimized? Table 1 suggests that MIXER can outpeform ML by only 0.1 Bleu points, and outpeformed by RAML? Something is wrong with your implementation then. Moreover, there are techniques like self-critical sequence training (SCST), which far outpeform MIXER, and we haven't even discussed Actor-Critic baselines...\n\nIn summary, the contribution over RAML and SPG in combining them is quite incremental, and the practical importance of combining them is questionable, as is the integrity of the presented experiments, given how poorly MIXER is reported perform, and the omission of stronger baselines like SCST and AC methods. Also, a paper on essentially the same approach was submitted and rejected from ICLR 2018(https://openreview.net/pdf?id=H1Nyf7W0Z), although this paper is better written, and puts the method more fully in context with existing work, I think that several of the concerns with that paper apply here as well. \n\nLook forward to the authors' feedback, and additional/corrected results - will certainly update my score if these concerns are addressed. In particular, if this generalization can significantly outpeform existing methods it generalizes with non-degenerate settings, this would overcome the more incremental contribution of combining SPG and RAML.\n\nCurrent Ratings:\n\nEvaluation      2/5: Results are not consistent with previous results (e.g. MIXER results). Stronger baselines such as SCST and AC are not considered.\nClarity         5/5: Clear paper, well written.\nSignificance    3/5: RAML and SPG have not been established as important methods in practice, so combining them is less interesting.\nOriginality     2/5: RAML and SPG are fairly straightforward to combine for experts interested in these methods.\n\nRating          4/10 Okay but not good enough, reject.    \nConfidence      5/5\n\nPros: \n- Generalizes RAML and SPG (and also standard entropy-regularized policy gradient).\n- Well written paper, clean generalization.\nCons:\n- RAML and SPG have not been established as important methods in practice.\n- generalization of RAML and SPG is straightforward,  incremental.\n- Existing baselines in the paper (i.e. MIXER) do not perform as expected (i.e barely better than ML, worse than RAML)\n- Stronger REINFORCE-based algorthms like SCST, as well as Actor-critic algorithms, have not been compared.\n\nUpdate after author responses:\n--------------------------------------------\n\nAuthors, thank you for your feedback.\n\nWhile it is true that generalizing RAML and SPG into a common framework is not trivial, the presented framework simply augments the dual form of SPG (i.e. REPS [16] in the SPG paper) with a RAML term. Furthermore, the MLE interpretation discussed is contained within the RAML paper, and the reductions to RAML and SPG are straightforward by design, and so do not really provide much new insight. Considering this, I feel that the importance of the paper largely rests on investigating and establishing the utility of the approach experimentally.\n\nWrt the experiments, I appreciate that the authors took the time to investigate the poor performance of MIXER. However, the unusally poor performance of MIXER remains unexplained, and falls short even of scheduled sampling (SS), which suggests a lingering major issue. REINFORCE techniques rely on 1) strong baselines, verified by the authors, 2) larger batch sizes to reduce variance, and 3) pre-training to reduce variance and facilitate efficient exploration. If the MLE is undertrained or overtrained (the latter the more likely issue given the plots), then MIXER will perform poorly. Actually, it is now standard practice to pre-train with MLE+SS before RL training, and this is really the (also dynamically weighted objective) baseline that should be compared against. The current REINFORCE results (MIXER or otherwise) really need to be updated (or at the least removed, as they are not captured by the framework, but the comparison to PG methods is important!).\n\nMore generally, I feel that the experiments are not yet comprehensive enough. While the authors have shown that they can outperform SPG and RAML with a scheduled objective, it is not currently clear how sensitive/robust the results are to the term weight scheduling, or even what most appropriate general weights/scheduling approach actually is.\n\nOverall I feel that the paper is still in need of substantial maturation before publication, although I have revised my score slightly upward.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Review", "cdate": 1542234446013, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335739500, "tmdate": 1552335739500, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BJx-HalfAX", "original": null, "number": 5, "cdate": 1542749497205, "ddate": null, "tcdate": 1542749497205, "tmdate": 1542749497205, "tddate": null, "forum": "r1MxciCcKm", "replyto": "HJl84_xU3m", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "content": {"title": "Author Response", "comment": "Thanks for the encouraging feedback and helpful comments. Below we clarify for some of the concerns raised by the reviewer, and address all other issues.\n\n\n** Overfitting in Figure.3. Results of more dropout\n\nIt\u2019s true that the test-set performance of MLE and RAML decreases slightly as training proceeds. First, we\u2019d like to note that the reported test-set performance of each algorithm (Table.1) is picked using the validation set, which is the standard practice. Second, as analyzed in the paper (e.g., Figure.2), MLE and RAML are easier to overfit due to the limited exploration space.\n\nThanks for the great suggestion of using more dropout. We added the results of dropout=0.3 in Appendix D (Table.3 and Figure.5). We can see that, compared to using dropout=0.2, the performance of each algorithm does increase to some extent. However, the improvement of our algorithm is comparable to our original results. In particular, with dropout 0.3, our algorithm improves over MLE and RAML by 1.5 and 0.49 BLEU points, respectively, comparable to the improvements with dropout 0.2 which are 1.42 and 0.64, respectively. The improvement is significant in the task.\n \n\n** Performance of the competing methods\n\nWe note that the experimental settings and model configurations can vary a lot across the different work, resulting in different performance. Also, not all configuration details are listed in the papers, making it hard to reproduce and compare. For example, \n- In (Bahdanau et al., 2016), they used GRU cells and beam=10. \n- In (Leblond et al., 2018), they used GRU cells and a ConvNet encoder. \nIn contrast, we used LSTM cells and a bi-directional RNN encoder. Also, the evaluation protocol can also vary (e.g., whether smoothing of the BLEU metric is used; whether the UNK token is disallowed in generation, etc). These settings are all common but not directly comparable. \n\nWe are ready to release all experimental code.\n\n\n** Originality of the example new algorithm\n\nWe agree that there is existing work attempting to combine MLE and RL by annealing or similar techniques (e.g., [Ranzato et al., 2015], which is discussed in Appendix A). The proposed example algorithm differs in that it is motivated naturally from the unified perspective, resulting in a different annealing strategy compared to any existing approaches. In particular, we anneal both the reward function and hyperparameters, and have a novel and highly-intuitive understanding of the algorithm as interpolating between existing different algorithms in the hyperparameter space.\n\nThe unified perspective can easily inspire more algorithms. For example, 1) Instead of annealing the weights, we can optimize them to find a single best point that balances well between exploration and training difficulty; 2) Having revealed MLE as using a delta-function reward and Data Noising as using locally-relaxed variants, we can leverage imitation learning algorithms to adapt the reward function, which is equivalent to adaptive data noising. We are happy to explore these exciting topics in the future.\n\n\n** Bibliography and other details\n\nThanks for the suggestions. We\u2019ve fixed all bibliography items. We\u2019ve also fixed the typos and polished the language.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617025, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1MxciCcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper505/Authors|ICLR.cc/2019/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617025}}}, {"id": "r1eI-peMAX", "original": null, "number": 4, "cdate": 1542749437928, "ddate": null, "tcdate": 1542749437928, "tmdate": 1542749437928, "tddate": null, "forum": "r1MxciCcKm", "replyto": "HJeXL_49hQ", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "content": {"title": "Author Response ", "comment": "Thanks for the supportive comments and recognition of our theoretic contributions. \n\nThe major contributions of this work are the unified formulation of a variety of sequence generation learning algorithms, and the new, principled understanding of their connections in terms of effective exploration space and the well-known \u201cexposure bias\u201d problem.\n\nAs a direct application of the unified formulation, the interpolation algorithm has improved over previous methods with a clear margin. For example, in Table.1, our algorithm outperforms MLE by as much as 1.42 BLEU points, which is a large margin in the task. Our algorithm also achieves significant improvement over the 2nd best-performing algorithm RAML by 0.64 BLEU point, which is *comparable or even larger* than other work in the domain. For example, the algorithm in [Dai et al., ACL 2018] improve over RAML by 0.56 (see Table.1 of [Dai et al., ACL 2018]), which is comparable to ours.\n\n[Dai et al., ACL 2018] From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617025, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1MxciCcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper505/Authors|ICLR.cc/2019/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617025}}}, {"id": "HJxXs3xzRm", "original": null, "number": 3, "cdate": 1542749339470, "ddate": null, "tcdate": 1542749339470, "tmdate": 1542749365652, "tddate": null, "forum": "r1MxciCcKm", "replyto": "rye_whgMCX", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "content": {"title": "Clarification and response (2/2)", "comment": "\n** RAML doesn\u2019t perform well due to exposure bias. Utility of RAML and SPG\n\n- First, as explained above, our major contribution is not to propose an algorithm as a simple combination of RAML and SPG, but instead a unified perspective and the resulting new insights of a variety of algorithms including not only RAML and SPG, but more importantly MLE (and Data Noising). The interpolation algorithm is a direct application of the new perspective.\n\n- As has analyzed in the paper (section 3.3 and Figure 2), RAML has partially addressed the exposure bias by exploring the sequence space proportionally to the reward distribution. This is one of the new understandings enabled by our unified perspective. \n\n- RAML has been attracting enormous research interest. For example, [Wang et al., EMNLP 2018],  [Elbayad et al., ACL 2018], and [Dai et al., ACL 2018] are among the most recent work that studied and extended RAML from different perspectives.\n\n[Wang et al., EMNLP 2018] SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation\n[Elbayad et al., ACL 2018] Token-level and sequence-level loss smoothing for RNN language models\n[Dai et al., ACL 2018] From Credit Assignment to Entropy Regularization: Two New Algorithms for Neural Sequence Prediction\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617025, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1MxciCcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper505/Authors|ICLR.cc/2019/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617025}}}, {"id": "rye_whgMCX", "original": null, "number": 2, "cdate": 1542749280281, "ddate": null, "tcdate": 1542749280281, "tmdate": 1542749280281, "tddate": null, "forum": "r1MxciCcKm", "replyto": "B1xuDmP93Q", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "content": {"title": "Clarification and response (1/2)", "comment": "Thanks for the reviews. Below we resolve misunderstandings and concerns raised by the reviewer.\n\n** \u201ccombining SPG and RAML \u2026 seems quite incremental\u201d\n\nWe\u2019d like to clarify that the main contribution of this paper is *not* simply combining SPG and RAML. Instead, this paper aims to establish a unified mathematical formulation of various sequence generation learning algorithms spanning MLE, Data Noising, RAML, and SPG, revealing the formal connections of these algorithms in a principled way. In particular, we show:\n    * MLE is mathematically equivalent to an entropy-regularized policy gradient with a delta-function reward. This provides a new perspective of its well-known \u201cexposure bias\u201d problem. \n    * Data Noising, RAML, and SPG can all be reformulated as special cases of the framework, taking different hyperparameter values and reward functions. They are gradually increasing the effective exploration space to address the exposure bias.\n\nBesides, please note that, to reach the unified perspective, we\u2019ve derived new formulations of all these algorithms (MLE, Data Noising, RAML, SPG) which differ significantly from (yet mathematically equivalent to) their original forms.\n\nThus, we feel it could be unfair to treat this work as simply \u201ccombining SPG and RAML by introducing their respective regularization terms\u201d while ignoring all other major contributions in the paper.\n\n\n** A paper on essentially the same approach: [Alpha-DM] https://openreview.net/pdf?id=H1Nyf7W0Z\n\nOur framework is fundamentally different from the above Alpha-DM approach. Alpha-DM is based on the opposite KL divergences that MLE and RL are optimizing, which has been discussed earlier in RAML [Norouzi et al., 2016].\n\nIn contrast, we have reformulated MLE, Data Noising, RAML, SPG in a completely new way based on entropy-regularized policy optimization. The resulting unified framework enables new principled understanding of the effective exploration space and the \u201cexposure bias\u201d of each of the algorithms. These results cannot be derived from Alpha-DM (which does not study exposure bias at all) and other existing work attempting to combining MLE and RL. We will add the discussion.\n\n\n** Performance of MIXER compared to MLE and RAML\n\nWe explain our implementation and provide more results to verify the correctness of our experiments.\n\n- For MIXER, we tried different baselines including learned baseline and greedy decoding baseline, and got similar results. For RAML, we implemented the enhanced version by [Ma et al., 2017] which directly uses BLEU as the reward (rather than the Hamming distance as in the original paper [Norouzi et al., 2016]) and generally outperforms MIXER (see Table.5 of [Ma et al., 2017]).\n\n- To further verify the correctness, we tested on a public implementation of MIXER and got similar results. Specifically, we run the comparison of MIXER and MLE using a public code of [Ranzato et al., 2015] that applies a learned baseline (https://github.com/violet-zct/pytorch_NMT). Using a configuration that close to ours, we got *BLEU=26.42 for MLE* (the same as in our paper which is 26.44) and *BLEU=26.63 for MIXER*. The improvement (0.22) is slightly higher than that of our own implementation, but is still inferior to RAML and our proposed algorithm which improves over MLE by 1.42. \n(Note that correctness of the public implementation was verified by successfully reproducing the original MIXER results with the same configuration in [Ranzato et al., 2015]. We didn\u2019t use the official code of [Ranzato et al., 2015] as it\u2019s not modularized well and is hard to adapt to our configuration.)\n\n- We also implemented the SCST [Rennie et al., 2017] and got BLEU=26.70, improving over MLE by 0.26, which is better than MIXER and inferior to our implementation of RAML and the proposed algorithm. \n\nWe also note that MIXER in the original paper [Ranzato et al., 2015] is in a setting of BLEU<21, far lower than ours (>26). This may explain the marginal improvement of MIXER over MLE in our experiments. We found little previous work has applied MIXER in settings other than the original one, except [Rennie et al., 2017] that studies on image captioning.\n\nIn sum, our empirical results are reasonable and correct. We will add the above new results and release all experimental code.\n\n[Ma et al., 2017] Softmax Q-Distribution Estimation for Structured Prediction. https://arxiv.org/abs/1705.07136\n[Rennie et al., 2017] Self-critical Sequence Training for Image Captioning. https://arxiv.org/abs/1612.00563"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617025, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1MxciCcKm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper505/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper505/Authors|ICLR.cc/2019/Conference/Paper505/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers", "ICLR.cc/2019/Conference/Paper505/Authors", "ICLR.cc/2019/Conference/Paper505/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617025}}}, {"id": "HJeXL_49hQ", "original": null, "number": 2, "cdate": 1541191755285, "ddate": null, "tcdate": 1541191755285, "tmdate": 1541533937525, "tddate": null, "forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Review", "content": {"title": "Interesting theoretical contribution", "review": "The authors provide a common mathematical perspective on several learning algorithms for sequence models. They also introduce a new algorithm that combines several of the existing ones and achieves significant (but small) improvements on a machine translation and a text summarization task.\n\nThe paper is clearly written, giving a good exposition of the unifying formulation.\n\nI believe the paper is quite insightful, and contributes to the community's understanding of the learning algorithms. However, the improvements in their experiments are rather small, and could probably be improved with more experimental work. They do showcase the usefulness of their new formulation, but not very strongly. Thus my recommendation to accept the paper is mostly based on the theoretical content that opens an interesting new perspective.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Review", "cdate": 1542234446013, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335739500, "tmdate": 1552335739500, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJl84_xU3m", "original": null, "number": 1, "cdate": 1540913198491, "ddate": null, "tcdate": 1540913198491, "tmdate": 1541533937065, "tddate": null, "forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "invitation": "ICLR.cc/2019/Conference/-/Paper505/Official_Review", "content": {"title": "Interesting unifying perspective, but lacklustre experiments", "review": "This paper introduces an interesting unifying perspective on several sequence generation training algorithms, exposing both MLE, RAML and SPG as special cases of this unified framework. This enables insightful new interpretations of standard issues in MLE training in terms of exploration for instance.\nBased on this new perspective, a new algorithm is introduced. Its performance is analysed on a machine translation and a text summarisation task.\n\n==> Quality and clarity\nThe paper is overall well-written, although it can be improved upon (see details below). The bibliography for instance does not reference the conference/journals where the articles were published and lists many (>10) published papers as arXiv preprints.\n\nThe ideas are clearly presented, which is crucial in a paper trying to unify different approaches, and the new perspective on exploration is well motivated.\n\n==> Originality and significance\nThe unifying framework is interesting, and helps shed new light on some standard issues in sequence generation.\nOn the other hand, the new algorithm and its analysis seem like a slightly rushed attempt at leveraging the unifying framework. \nThe experiments, in particular, present several issues.\n- For instance, it's clear from Figure 3 that both MLE and RAML are overfitting and would benefit from more dropout (in the literature, 0.3 is commonly used for this type of encoder-decoder architecture). Having access to these experimental results is important, since it would enable the reader to understand whether the benefits of the new approach are subsumed by regularisation or not.\n- Further, the performance of the competing methods seems a bit low. MLE reports 26.44 BLEU, which is a bit surprising considering that: \n   - with beam-search (beam of size 10, not 5, admittedly), Bahdanau et al (2016) get 27.56 BLEU, and this is without dropout.   \n   - with dropout 0.3 (but without beam search), Leblond et al (2018) get 27.4 BLEU.\nMaking a strong case for the benefits of the new algorithm requires more thorough experiments.\n\nOverall, the first half of the paper is interesting and insightful, while the second would benefit from more time. \n\nPros\n- clarity of the ideas that are presented\n- interesting unifying perspective on sequence generation algorithms\n- insightful new interpretations of existing algorithms in terms of exploration\n\nCons\n- the example new algorithm is not very original\n- the associated experiments are incomplete\n\n==> Details\n1. page 2, \"Dayan & Hinton (1997); Levine (2018); Abdolmaleki et al. (2018) study in a probabilistic inference perspective.\" is an incomplete sentence.\n2. at the beginning of section 3.1, policy optimisation is a family of algorithmS\n3. page 7 in the setup of the experiments, \"We use the Adam optimizer for SGD training\" is incorrect since SGD is not a family but a specific algorithm, which is different from Adam.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper505/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Connecting the Dots Between MLE and RL for Sequence Generation", "abstract": "Sequence generation models such as recurrent networks can be trained with a diverse set of learning algorithms. For example, maximum likelihood learning is simple and efficient, yet suffers from the exposure bias problem. Reinforcement learning like policy gradient addresses the problem but can have prohibitively poor exploration efficiency. A variety of other algorithms such as RAML, SPG, and data noising, have also been developed in different perspectives. This paper establishes a formal connection between these algorithms. We present a generalized entropy regularized policy optimization formulation, and show that the apparently divergent algorithms can all be reformulated as special instances of the framework, with the only difference being the configurations of reward function and a couple of hyperparameters. The unified interpretation offers a systematic view of the varying properties of exploration and learning efficiency. Besides, based on the framework, we present a new algorithm that dynamically interpolates among the existing algorithms for improved learning. Experiments on machine translation and text summarization demonstrate the superiority of the proposed algorithm.", "keywords": ["sequence generation", "maximum likelihood learning", "reinforcement learning", "policy optimization", "text generation", "reward augmented maximum likelihood", "exposure bias"], "authorids": ["tanbowen@sjtu.edu.cn", "zhitinghu@gmail.com", "yangtze2301@gmail.com", "rsalakhu@cs.cmu.edu", "epxing@cs.cmu.edu"], "authors": ["Bowen Tan*", "Zhiting Hu*", "Zichao Yang", "Ruslan Salakhutdinov", "Eric P. Xing"], "TL;DR": "A unified perspective of various learning algorithms for sequence generation, such as MLE, RL, RAML, data noising, etc.", "pdf": "/pdf/eabaa641c003da1cc35177245766901ee73e7774.pdf", "paperhash": "tan|connecting_the_dots_between_mle_and_rl_for_sequence_generation", "_bibtex": "@misc{\ntan*2019connecting,\ntitle={Connecting the Dots Between {MLE} and {RL} for Sequence Generation},\nauthor={Bowen Tan* and Zhiting Hu* and Zichao Yang and Ruslan Salakhutdinov and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1MxciCcKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper505/Official_Review", "cdate": 1542234446013, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1MxciCcKm", "replyto": "r1MxciCcKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper505/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335739500, "tmdate": 1552335739500, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper505/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}