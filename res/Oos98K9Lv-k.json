{"notes": [{"id": "Oos98K9Lv-k", "original": "iHrhjrfU9Yu", "number": 1005, "cdate": 1601308113801, "ddate": null, "tcdate": 1601308113801, "tmdate": 1616117952604, "tddate": null, "forum": "Oos98K9Lv-k", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neural Topic Model via Optimal Transport", "authorids": ["~He_Zhao1", "~Dinh_Phung2", "~Viet_Huynh1", "~Trung_Le2", "~Wray_Buntine1"], "authors": ["He Zhao", "Dinh Phung", "Viet Huynh", "Trung Le", "Wray Buntine"], "keywords": ["topic modelling", "optimal transport", "document analysis"], "abstract": "Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|neural_topic_model_via_optimal_transport", "one-sentence_summary": "This paper presents a neural topic model via optimal transport, which can discover more coherent and diverse topics and derive better document representations for both regular and short texts.", "supplementary_material": "", "pdf": "/pdf/7be7e3b207a273ccbe61f42c2358cc4fb090748f.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021neural,\ntitle={Neural Topic Model via Optimal Transport},\nauthor={He Zhao and Dinh Phung and Viet Huynh and Trung Le and Wray Buntine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Oos98K9Lv-k}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "wNojVQwS2y", "original": null, "number": 1, "cdate": 1610040359724, "ddate": null, "tcdate": 1610040359724, "tmdate": 1610473949803, "tddate": null, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "invitation": "ICLR.cc/2021/Conference/Paper1005/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "The reviewers unanimously agreed that this is an interesting paper that belongs at ICLR. The use of optimal transport in neural topic models is novel and the paper is well-written.\n\nA common theme among the reviewers was that they would like to see more intuition and justification. I suggest you bear this in mind while editing the final version of the paper. I also believe that R3 brings up valid points about evaluating perplexity -- I don't think the lack of perplexity results are a reason to reject the paper, but I believe they can be calculated here (see eg the reference R3 provided) and they would give a clearer view of the model's performance.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Topic Model via Optimal Transport", "authorids": ["~He_Zhao1", "~Dinh_Phung2", "~Viet_Huynh1", "~Trung_Le2", "~Wray_Buntine1"], "authors": ["He Zhao", "Dinh Phung", "Viet Huynh", "Trung Le", "Wray Buntine"], "keywords": ["topic modelling", "optimal transport", "document analysis"], "abstract": "Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|neural_topic_model_via_optimal_transport", "one-sentence_summary": "This paper presents a neural topic model via optimal transport, which can discover more coherent and diverse topics and derive better document representations for both regular and short texts.", "supplementary_material": "", "pdf": "/pdf/7be7e3b207a273ccbe61f42c2358cc4fb090748f.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021neural,\ntitle={Neural Topic Model via Optimal Transport},\nauthor={He Zhao and Dinh Phung and Viet Huynh and Trung Le and Wray Buntine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Oos98K9Lv-k}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040359706, "tmdate": 1610473949784, "id": "ICLR.cc/2021/Conference/Paper1005/-/Decision"}}}, {"id": "bdTINidyy5g", "original": null, "number": 1, "cdate": 1603697658069, "ddate": null, "tcdate": 1603697658069, "tmdate": 1606763450105, "tddate": null, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "invitation": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review", "content": {"title": "Impressive empirical results, but discussion of optimal transport lacks depth", "review": "Summary: The paper proposes a neural topic model which log-likelihood is regularized by Sinkhorn distance, instead of following Variational AutoEncoder (VAE) approach. The proposed model is hence cannot be interpreted as a probabilistic generative model. Still, with respect to metrics such as Topic Coherence and Topic Diversity which don't require probabilistic interpretation of topic model, the proposed model performs very well across five standard benchmark datasets for topic modeling.\n\nQuality and Clarity: At the very high level, the use of Optimal Transport distance for topic modeling is sensible. Neither word counts nor topic distributions have clear ordering of indices, and Optimal Transport distance is usually well-suited for such histogram-like features. However, authors do not provide much discussion on why Optimal Transport distance is particularly well-suited for topic modeling. I can see that compared to the expected log-likelihood, the Optimal Transport distance $d_M(\\tilde{x},z)$ is a more optimistic estimate, as the topic distribution for each word could be individually assigned as long as marginal distribution of topics is equal to $z$. Maybe this is related to some sort of variational approximation which disentangles the usual constraint that topic distribution of all words in the same document are exactly the same. Still, I had hard time understanding why this is favorable for topic modeling, or finding better interpretations of Optimal Transport distance on topic modeling. At the least, some qualitative analysis of how the hyperparameter $\\varepsilon$, which controls the tradeoff between expected likelihood and Sinkhorn distance, affects the characteristics of topic models would be nice. Appendix D. shows how evaluation metrics change as $\\varepsilon$ changes, but not much interpretation of results is provided.\n\nIt's good to see how a well-established technique can be applied to topic modeling and show encouraging empirical results, but the lack of justifications on the modeling approach would make it difficult for the further development and adoption of the method, especially because the proposed method is not a probabilistic generative model anymore. I would encourage others to do a deeper qualitative analysis on the effect of Sinkhorn distance as a regularizer.\n\nOriginality and Significance: The establishment of the connection between Optimal Transport and Topic Modeling would potentially lead to active follow-up research, as both are well-established areas of research.\n\n* Pros:\n  * good empirical results\n  * connects two well-established topics of research\n* Cons:\n  * Justifications of the modeling approach is lacking", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1005/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1005/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Topic Model via Optimal Transport", "authorids": ["~He_Zhao1", "~Dinh_Phung2", "~Viet_Huynh1", "~Trung_Le2", "~Wray_Buntine1"], "authors": ["He Zhao", "Dinh Phung", "Viet Huynh", "Trung Le", "Wray Buntine"], "keywords": ["topic modelling", "optimal transport", "document analysis"], "abstract": "Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|neural_topic_model_via_optimal_transport", "one-sentence_summary": "This paper presents a neural topic model via optimal transport, which can discover more coherent and diverse topics and derive better document representations for both regular and short texts.", "supplementary_material": "", "pdf": "/pdf/7be7e3b207a273ccbe61f42c2358cc4fb090748f.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021neural,\ntitle={Neural Topic Model via Optimal Transport},\nauthor={He Zhao and Dinh Phung and Viet Huynh and Trung Le and Wray Buntine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Oos98K9Lv-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1005/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129444, "tmdate": 1606915761123, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1005/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review"}}}, {"id": "hm5yZhRcIRL", "original": null, "number": 4, "cdate": 1604203940606, "ddate": null, "tcdate": 1604203940606, "tmdate": 1605364604456, "tddate": null, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "invitation": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review", "content": {"title": "Generally interesting, but comparison is not persuasive enough", "review": "This paper proposes a new variant of neural topic model leveraging optimal\ntransport in order to incorporate information from pre-trained word vectors.\nSpecifically, the authors replaced the KL-divergence reguralization term with\nan optimal transport between topic distribution and empirical word distribution\nin each text.\nExperimental evaluation yields generally better topic coherence and high\nprecision of K-means clustering on the induced topic distributions.\n\nBasically this paper is interesting, but still leaves some questions.\n\n- First of all, evaluations are based only on high NPMI topics (section 5.2).\nTherefore, it is trivial that the induced topic-word distribution of these\ntopics are good. In my experience, original vanilla LDA yields mostly\ninterpretable topic-word distributions; however, even for the high-NPMI topics,\neach topic in Figure 4 is somewhat noisy, and unshown topics might be worse.\nTherefore, I would like to know the comparison between the proposed method and\nthe original LDA too.\n\n- The paper first says that \"good document representation and coherent/diverse\ntopics\" is difficult. Then why not including perplexity evaluation in the \nexperiments? K-means results are only auxiliary evaluation of the former, thus\nthe reader would like to know whether the proposed model could yield better \nperplexity on documents or not.\n\n- It seems that the choice of dimensionality and the number of topics seems\ntoo low and arbitrary. Why only the 50-dimensional word vectors are used?\nIs there any difference over the competitors when that dimensionality is\nchanged?\nAlso, I could not know why only the results with K=100 is shown in main text.\nAppendix E shows that K=500 consistently yields better results; why are they\nnot included?\n\n- Finally, I cannot understand what kind of optimization w.r.t M is conducted\nin this paper. To make the paper as self-contained as possible, I strongly\nrecommend to show what kind of optimization is actually done.\n\nThe proposed OT regularization seem to work better, but I cannot see why the\nbaseline of word-vector based topic models like (Dieng+ 2020) is inferior.\nIs there any intuition or explanation over these trivial competitors?\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1005/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1005/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Topic Model via Optimal Transport", "authorids": ["~He_Zhao1", "~Dinh_Phung2", "~Viet_Huynh1", "~Trung_Le2", "~Wray_Buntine1"], "authors": ["He Zhao", "Dinh Phung", "Viet Huynh", "Trung Le", "Wray Buntine"], "keywords": ["topic modelling", "optimal transport", "document analysis"], "abstract": "Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|neural_topic_model_via_optimal_transport", "one-sentence_summary": "This paper presents a neural topic model via optimal transport, which can discover more coherent and diverse topics and derive better document representations for both regular and short texts.", "supplementary_material": "", "pdf": "/pdf/7be7e3b207a273ccbe61f42c2358cc4fb090748f.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021neural,\ntitle={Neural Topic Model via Optimal Transport},\nauthor={He Zhao and Dinh Phung and Viet Huynh and Trung Le and Wray Buntine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Oos98K9Lv-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1005/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129444, "tmdate": 1606915761123, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1005/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review"}}}, {"id": "6KWNmcCvExx", "original": null, "number": 2, "cdate": 1603776821523, "ddate": null, "tcdate": 1603776821523, "tmdate": 1605024553837, "tddate": null, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "invitation": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review", "content": {"title": "a topic model with compelling experimental results especially for short text documents", "review": "This paper builds off existing neural topic model work that's based on variational autoencoders and specifically introduces a novel loss function based on optimal transport (more specifically, the Sinkhorn distance). The paper demonstrates impressive experimental results, where the proposed method Neural Sinkhorn Topic Model (NSTM) outperforms (or is competitive with) a number of SOTA baselines, with a dramatic performance gain in the setting where the text documents are very short.\n\nOverall, I enjoyed reading this paper and found the proposed method well-described albeit somewhat ad hoc, although the authors do justify certain aspects of the method (e.g., Lemma 1). The \"weight\" of this paper seems to really be carried by the experimental results though, which I found to be quite compelling.\n\nStrengths:\n- well-explained method that provides a neat usage of optimal transport in topic models\n- Theorem 1/Lemma 1 provide a nice theoretical comparison of the proposed method vs other neural topic models\n- strong experimental performance especially in the setting of short documents, which is very relevant in practice\n\nWeaknesses:\n- the proposed method comes off as a bit ad hoc\n- while the authors do reference Card et al's paper, I think a comparison with Card et al's Scholar framework would be helpful to get a sense of how the l1 regularization in Scholar/SAGE handles short text documents compared to NSTM\n- there are small typos/grammar glitches here and there - please proofread carefully\n\nIn the conclusion, the authors point out correlated topic models and dynamic topic models as variants worth exploring next. Supervised versions of topic models I think also are worth exploring.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1005/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1005/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Topic Model via Optimal Transport", "authorids": ["~He_Zhao1", "~Dinh_Phung2", "~Viet_Huynh1", "~Trung_Le2", "~Wray_Buntine1"], "authors": ["He Zhao", "Dinh Phung", "Viet Huynh", "Trung Le", "Wray Buntine"], "keywords": ["topic modelling", "optimal transport", "document analysis"], "abstract": "Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|neural_topic_model_via_optimal_transport", "one-sentence_summary": "This paper presents a neural topic model via optimal transport, which can discover more coherent and diverse topics and derive better document representations for both regular and short texts.", "supplementary_material": "", "pdf": "/pdf/7be7e3b207a273ccbe61f42c2358cc4fb090748f.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021neural,\ntitle={Neural Topic Model via Optimal Transport},\nauthor={He Zhao and Dinh Phung and Viet Huynh and Trung Le and Wray Buntine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Oos98K9Lv-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1005/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129444, "tmdate": 1606915761123, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1005/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review"}}}, {"id": "R-ToaUWH0Mx", "original": null, "number": 3, "cdate": 1604117160914, "ddate": null, "tcdate": 1604117160914, "tmdate": 1605024553700, "tddate": null, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "invitation": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review", "content": {"title": "Interesting alternative approach to neural topic model based on optimal transport with good demonstrated performance", "review": "The paper proposes a neural topic model derived from the perspective of optimal transport (OT). Topic embeddings are learned as part of the training process and is used to construct the cost matrix of the transport.  The cost function based on the OT distance is further improved by combining with the cross-entropy loss and by using the Sinkhorn distance to replace the OT distance.\n\nThe paper is well written.  The proposed method is first explained from the perspective from the optimal transport and then developed by considering the cross-entropy loss and the Sinkhorn distance.  It also explains how the model incorporates the word embedding and introduce the topic embedding to simply the cost matrix M of the transport.  The proposed method is sound.\n\nThe experiments included recent neural topic models for comparison.  The chosen test data sets include also some with short text. In general, the proposed method has been show to perform better than other methods in terms of topic coherence and topic diversity.  The experiment section also some quality analysis on the topic discovered by the proposed model.  The experimental results are convincing.\n\nThe paper explains quite clearly the relationship between the proposed method and related methods. In particular, the paper seems to have adequately credited the sources of ideas during the development of the proposed model.  The novelty of the proposed method appears to be the use of optimal transport for developing neural topic model and the construction of the cost matrix M based on the cosine similarity between word embedding and topic embedding.  The novelty is sufficient.\n\nAlthough the clarity of the proposed method is good, the rationale for using OT distances for comparing probabilities may deserve more explanation.  The paper gives an intuition for the transport matrix P in section 2.2.  It may be better if it can also give more explanation on the role of M and what the meaning of the distance between two probability vector is.  References may also be given to other works that use OT distances for comparing probabilities.\n\nThe experiment includes recent neural topic models but does not include other recent topic models not based on neural networks. It would be interesting to see how the proposed method performs compared to those models.\n\nI cannot find any indication that source code will be released.  It is suggested to do so for reproducibility and for the use of practitioners.\n\nMinor comment:\n\n- Add the data set name to the caption of Figure 4.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1005/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1005/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Topic Model via Optimal Transport", "authorids": ["~He_Zhao1", "~Dinh_Phung2", "~Viet_Huynh1", "~Trung_Le2", "~Wray_Buntine1"], "authors": ["He Zhao", "Dinh Phung", "Viet Huynh", "Trung Le", "Wray Buntine"], "keywords": ["topic modelling", "optimal transport", "document analysis"], "abstract": "Recently, Neural Topic Models (NTMs) inspired by variational autoencoders have obtained increasingly research interest due to their promising results on text analysis. However, it is usually hard for existing NTMs to achieve good document representation and coherent/diverse topics at the same time. Moreover, they often degrade their performance severely on short documents. The requirement of reparameterisation could also comprise their training quality and model flexibility. To address these shortcomings, we present a new neural topic model via the theory of optimal transport (OT). Specifically, we propose to learn the topic distribution of a document by directly minimising its OT distance to the document's word distributions. Importantly, the cost matrix of the OT distance models the weights between topics and words, which is constructed by the distances between topics and words in an embedding space. Our proposed model can be trained efficiently with a differentiable loss. Extensive experiments show that our framework significantly outperforms the state-of-the-art NTMs on discovering more coherent and diverse topics and deriving better document representations for both regular and short texts.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhao|neural_topic_model_via_optimal_transport", "one-sentence_summary": "This paper presents a neural topic model via optimal transport, which can discover more coherent and diverse topics and derive better document representations for both regular and short texts.", "supplementary_material": "", "pdf": "/pdf/7be7e3b207a273ccbe61f42c2358cc4fb090748f.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzhao2021neural,\ntitle={Neural Topic Model via Optimal Transport},\nauthor={He Zhao and Dinh Phung and Viet Huynh and Trung Le and Wray Buntine},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Oos98K9Lv-k}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Oos98K9Lv-k", "replyto": "Oos98K9Lv-k", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1005/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129444, "tmdate": 1606915761123, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1005/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1005/-/Official_Review"}}}], "count": 6}