{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521582971489, "tcdate": 1520002630703, "number": 1, "cdate": 1520002630703, "id": "rJy8I1vuz", "invitation": "ICLR.cc/2018/Workshop/-/Paper39/Official_Review", "forum": "H1pDH25IM", "replyto": "H1pDH25IM", "signatures": ["ICLR.cc/2018/Workshop/Paper39/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper39/AnonReviewer2"], "content": {"title": "Interesting analysis of low-dimensional trajectories in the internal state space of LSTM recurrent networks that learned to generate handwriting. ", "rating": "8: Top 50% of accepted papers, clear accept", "review": "Interesting analysis of low-dimensional trajectories in the internal state space of LSTM recurrent networks that learned to generate handwriting. \n\nAre the authors really using the LSTM of 1997, or the LSTM variant by Gers et al (2000) with forget gates (now sometimes called gated recurrent units)? It's the 2000 variant that most people are using now through Tensorflow etc. \n\nShould be accepted after minor revisions.\n\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Trajectory Analysis of Recurrent Neural Network In Handwriting Synthesis", "abstract": "Recurrent neural networks (RNNs) are capable of learning to generate highly realistic, online handwritings in a wide variety of styles from a given text sequence. Furthermore, the networks can generate handwritings in the style of a particular writer when the network states are primed with a real sequence of pen movements from the writer. However, how populations of neurons in the RNN collectively achieve such performance still remains poorly understood. To tackle this problem, we investigated learned representations in RNNs by extracting low-dimensional, neural trajectories that summarize the activity of a population of neurons in the network during individual syntheses of handwritings. The neural trajectories show that different writing styles are encoded in different subspaces inside an internal space of the network. Within each subspace, different characters of the same style are represented as different state dynamics. These results demonstrate the effectiveness of analyzing the neural trajectory for intuitive understanding of how the RNNs work.", "pdf": "/pdf/6577fb5535eeeb16ccbba6202a4c7bb1b522d243.pdf", "TL;DR": "We visualized learned internal representations in the recurrent neural networks by using the neural trajectory analysis.", "paperhash": "bcharbonneau|neural_trajectory_analysis_of_recurrent_neural_network_in_handwriting_synthesis", "keywords": ["recurrent neural network", "internal representation", "neural trajectory", "handwriting synthesis"], "authors": ["Kristof B.Charbonneau", "Osamu Shouno"], "authorids": ["kristof.boucher-charbonneau.1@ens.etsmtl.ca", "shouno@jp.honda-ri.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582971297, "id": "ICLR.cc/2018/Workshop/-/Paper39/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper39/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper39/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper39/AnonReviewer1"], "reply": {"forum": "H1pDH25IM", "replyto": "H1pDH25IM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper39/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper39/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582971297}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582829686, "tcdate": 1520607024642, "number": 2, "cdate": 1520607024642, "id": "rJtVyXxFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper39/Official_Review", "forum": "H1pDH25IM", "replyto": "H1pDH25IM", "signatures": ["ICLR.cc/2018/Workshop/Paper39/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper39/AnonReviewer1"], "content": {"title": "Sound approach but not sure about the visualization", "rating": "3: Clear rejection", "review": "The authors of this paper attempt to analyze the state dynamics of deep LSTM networks using the GPFA model. The method is applied to a deep LSTM network that was trained on handwriting synthesis task. As a workshop paper, I think their approach is sound and interesting, and the choice of the task is also nice since abundant amount of meta data is available in IAM-OnDB dataset. \n\nI think the paper could have made more stronger points if it didn't only show how trajectories can look different when the priming conditions and target texts are different but also show what are shared within the same priming conditions and target texts. Also, analyzing which factor is more dominant and why. It is a bit obvious to me that the neural trajectories will look different when the conditions are different and also depending on which layer. Thus, I don't agree (yet) that neural trajectories have added more intuitive understanding on recurrent neural networks from this work.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Trajectory Analysis of Recurrent Neural Network In Handwriting Synthesis", "abstract": "Recurrent neural networks (RNNs) are capable of learning to generate highly realistic, online handwritings in a wide variety of styles from a given text sequence. Furthermore, the networks can generate handwritings in the style of a particular writer when the network states are primed with a real sequence of pen movements from the writer. However, how populations of neurons in the RNN collectively achieve such performance still remains poorly understood. To tackle this problem, we investigated learned representations in RNNs by extracting low-dimensional, neural trajectories that summarize the activity of a population of neurons in the network during individual syntheses of handwritings. The neural trajectories show that different writing styles are encoded in different subspaces inside an internal space of the network. Within each subspace, different characters of the same style are represented as different state dynamics. These results demonstrate the effectiveness of analyzing the neural trajectory for intuitive understanding of how the RNNs work.", "pdf": "/pdf/6577fb5535eeeb16ccbba6202a4c7bb1b522d243.pdf", "TL;DR": "We visualized learned internal representations in the recurrent neural networks by using the neural trajectory analysis.", "paperhash": "bcharbonneau|neural_trajectory_analysis_of_recurrent_neural_network_in_handwriting_synthesis", "keywords": ["recurrent neural network", "internal representation", "neural trajectory", "handwriting synthesis"], "authors": ["Kristof B.Charbonneau", "Osamu Shouno"], "authorids": ["kristof.boucher-charbonneau.1@ens.etsmtl.ca", "shouno@jp.honda-ri.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582971297, "id": "ICLR.cc/2018/Workshop/-/Paper39/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper39/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper39/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper39/AnonReviewer1"], "reply": {"forum": "H1pDH25IM", "replyto": "H1pDH25IM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper39/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper39/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582971297}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573578860, "tcdate": 1521573578860, "number": 152, "cdate": 1521573578520, "id": "H1mCRRCKz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "H1pDH25IM", "replyto": "H1pDH25IM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Trajectory Analysis of Recurrent Neural Network In Handwriting Synthesis", "abstract": "Recurrent neural networks (RNNs) are capable of learning to generate highly realistic, online handwritings in a wide variety of styles from a given text sequence. Furthermore, the networks can generate handwritings in the style of a particular writer when the network states are primed with a real sequence of pen movements from the writer. However, how populations of neurons in the RNN collectively achieve such performance still remains poorly understood. To tackle this problem, we investigated learned representations in RNNs by extracting low-dimensional, neural trajectories that summarize the activity of a population of neurons in the network during individual syntheses of handwritings. The neural trajectories show that different writing styles are encoded in different subspaces inside an internal space of the network. Within each subspace, different characters of the same style are represented as different state dynamics. These results demonstrate the effectiveness of analyzing the neural trajectory for intuitive understanding of how the RNNs work.", "pdf": "/pdf/6577fb5535eeeb16ccbba6202a4c7bb1b522d243.pdf", "TL;DR": "We visualized learned internal representations in the recurrent neural networks by using the neural trajectory analysis.", "paperhash": "bcharbonneau|neural_trajectory_analysis_of_recurrent_neural_network_in_handwriting_synthesis", "keywords": ["recurrent neural network", "internal representation", "neural trajectory", "handwriting synthesis"], "authors": ["Kristof B.Charbonneau", "Osamu Shouno"], "authorids": ["kristof.boucher-charbonneau.1@ens.etsmtl.ca", "shouno@jp.honda-ri.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518155109503, "tcdate": 1518155109503, "number": 39, "cdate": 1518155109503, "id": "H1pDH25IM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "H1pDH25IM", "signatures": ["~Osamu_Shouno1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Neural Trajectory Analysis of Recurrent Neural Network In Handwriting Synthesis", "abstract": "Recurrent neural networks (RNNs) are capable of learning to generate highly realistic, online handwritings in a wide variety of styles from a given text sequence. Furthermore, the networks can generate handwritings in the style of a particular writer when the network states are primed with a real sequence of pen movements from the writer. However, how populations of neurons in the RNN collectively achieve such performance still remains poorly understood. To tackle this problem, we investigated learned representations in RNNs by extracting low-dimensional, neural trajectories that summarize the activity of a population of neurons in the network during individual syntheses of handwritings. The neural trajectories show that different writing styles are encoded in different subspaces inside an internal space of the network. Within each subspace, different characters of the same style are represented as different state dynamics. These results demonstrate the effectiveness of analyzing the neural trajectory for intuitive understanding of how the RNNs work.", "pdf": "/pdf/6577fb5535eeeb16ccbba6202a4c7bb1b522d243.pdf", "TL;DR": "We visualized learned internal representations in the recurrent neural networks by using the neural trajectory analysis.", "paperhash": "bcharbonneau|neural_trajectory_analysis_of_recurrent_neural_network_in_handwriting_synthesis", "keywords": ["recurrent neural network", "internal representation", "neural trajectory", "handwriting synthesis"], "authors": ["Kristof B.Charbonneau", "Osamu Shouno"], "authorids": ["kristof.boucher-charbonneau.1@ens.etsmtl.ca", "shouno@jp.honda-ri.com"]}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 4}