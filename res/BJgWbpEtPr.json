{"notes": [{"id": "BJgWbpEtPr", "original": "HyeLlHLIPS", "number": 365, "cdate": 1569438969218, "ddate": null, "tcdate": 1569438969218, "tmdate": 1577168243399, "tddate": null, "forum": "BJgWbpEtPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "Few-shot models have become a popular topic of research in the past years. They offer the possibility to determine class belongings for unseen examples using just a handful of examples for each class. Such models are trained on a wide range of classes and their respective examples, learning a decision metric in the process. Types of few-shot models include matching networks and prototypical networks. We show a new way of training prototypical few-shot models for just a single\nclass. These models have the ability to predict the likelihood of an unseen query belonging to a group of examples without any given counterexamples. The difficulty here lies in the fact that no relative distance to other classes can be calculated\nvia softmax. We solve this problem by introducing a \u201cnull class\u201d centered around zero, and enforcing centering with batch normalization. Trained on the commonly used Omniglot data set, we obtain a classification accuracy of .98 on the matched\ntest set, and of .8 on unmatched MNIST data. On the more complex MiniImageNet data set, test accuracy is .8. In addition, we propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples\u2019 distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available.", "title": "One-way prototypical networks", "keywords": ["few-shot learning", "one-shot learning", "prototypical networks", "one-class classification", "anomaly detection", "outlier detection", "matching networks"], "pdf": "/pdf/f47a12c97dc0183d5009af39f1056ae446939b42.pdf", "authors": ["Anna Kruspe"], "TL;DR": "We show how to adapt prototypical few-shot networks to one-class problems; the key is a null class for comparison.", "authorids": ["anna.kruspe@dlr.de"], "paperhash": "kruspe|oneway_prototypical_networks", "original_pdf": "/attachment/c55b42d6292f26d47303fbf41a2884a25a0908bb.pdf", "_bibtex": "@misc{\nkruspe2020oneway,\ntitle={One-way prototypical networks},\nauthor={Anna Kruspe},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgWbpEtPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "52iBqqdy3", "original": null, "number": 1, "cdate": 1576798694309, "ddate": null, "tcdate": 1576798694309, "tmdate": 1576800941189, "tddate": null, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "invitation": "ICLR.cc/2020/Conference/Paper365/-/Decision", "content": {"decision": "Reject", "comment": "This paper extends prototypical networks to few shot 1-way classification. The idea is to introduce a null class to compare against with a null prototype. The reviewers found the idea sound and interesting. However, the response was mixed because the reviewers were not convinced of the significance of the improvements. Furthermore, there were questions raised about the motivation that were not sufficiently addressed in the rebuttal. Batch normalization layers will not necessarily lead to zero mean if the trainable offset is not disabled. The authors did not clarify whether they disable this offset. I encourage the authors to resubmit after addressing the issues raised by the reviewers.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Few-shot models have become a popular topic of research in the past years. They offer the possibility to determine class belongings for unseen examples using just a handful of examples for each class. Such models are trained on a wide range of classes and their respective examples, learning a decision metric in the process. Types of few-shot models include matching networks and prototypical networks. We show a new way of training prototypical few-shot models for just a single\nclass. These models have the ability to predict the likelihood of an unseen query belonging to a group of examples without any given counterexamples. The difficulty here lies in the fact that no relative distance to other classes can be calculated\nvia softmax. We solve this problem by introducing a \u201cnull class\u201d centered around zero, and enforcing centering with batch normalization. Trained on the commonly used Omniglot data set, we obtain a classification accuracy of .98 on the matched\ntest set, and of .8 on unmatched MNIST data. On the more complex MiniImageNet data set, test accuracy is .8. In addition, we propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples\u2019 distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available.", "title": "One-way prototypical networks", "keywords": ["few-shot learning", "one-shot learning", "prototypical networks", "one-class classification", "anomaly detection", "outlier detection", "matching networks"], "pdf": "/pdf/f47a12c97dc0183d5009af39f1056ae446939b42.pdf", "authors": ["Anna Kruspe"], "TL;DR": "We show how to adapt prototypical few-shot networks to one-class problems; the key is a null class for comparison.", "authorids": ["anna.kruspe@dlr.de"], "paperhash": "kruspe|oneway_prototypical_networks", "original_pdf": "/attachment/c55b42d6292f26d47303fbf41a2884a25a0908bb.pdf", "_bibtex": "@misc{\nkruspe2020oneway,\ntitle={One-way prototypical networks},\nauthor={Anna Kruspe},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgWbpEtPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724066, "tmdate": 1576800275652, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper365/-/Decision"}}}, {"id": "rJeP0M7TYH", "original": null, "number": 1, "cdate": 1571791567207, "ddate": null, "tcdate": 1571791567207, "tmdate": 1572972604555, "tddate": null, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "invitation": "ICLR.cc/2020/Conference/Paper365/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper looks at the problem of few-shot classification in the regime when only a single class is present. The task at hand is as follows: given a number of support images of a previously unseen class (not present during training) and a single unlabeled image, we need to decide if this image belongs to the class or not. While previous approaches would explicitly construct negative examples to contrast the positive ones with during training, the authors bypass this by using batch norm in the last layer, which, on average, centers embedding feature vectors at 0, defining effectively the embedding for the negative class. In addition, the authors look at modelling the distribution of support image embeddings to improve the performance of their model.\n\nOverview:\nI like the general idea and find the paper easy to follow. Using the centering effect of batch norm to effectively bypass the need to define the universal negative class embedding is an interesting approach. I think it is especially compelling due to the special role of the \\vec{0} point in the embedding space, where all embedding vectors of length 0, regardless of their direction, meet. I also appreciate that the authors used multiple training runs and their averages as their results, rather than the best result.\n\nI have a few points that I believe should be clarified:\n\n-- Point 1 --\nAre you assuming that the trainable offset in the batch norm will be close to \\vec{0}, or are you identically setting it to \\vec{0}? Since your definition of the \u201cgarbage\u201d class relies on this, it might be worth checking what the actual learned parameters in the batch norm end up being, and if they are small, possibly fixing them to 0?\n\n-- Point 2 --\nI didn\u2019t get what distance metric in particular you were using to determine if the unlabeled example X is closer to the centroid of the class c or the origin 0. If I understood it correctly, you effectively look at length(c-X) > length(X-0) as your condition determining if X belongs to c. What metric did you use as your length measure?\n\n-- Point 3 --\nWhen using the Gaussian approximation estimating the distribution of the support images in the embedding space, how does it affect the condition  length(c-X) > length(X-0)? In \u201cGaussian Prototypical Networks for Few-Shot Learning on Omniglot\u201d by Stanislav Fort you cite, they incorporate the covariance of the Gaussian S in the length function, effectively using length(c,X,S). Their Euclidean metric uses the covariance matrix is a metric tensor to weight different axes differently. How exactly do you use yours?\n\n-- Point 4 --\nWhen comparing to the previous algorithms in the single-class (one-way) regime, you use a random image from one (at random) of the C_train - 1 remaining training classes as your negative example / example of the other class. Is that a fair comparison? How many images of those classes do you use? If it were the case that for a problem with the support size of 1 (or a few), sampling from these C-1 other classes might provide very unrepresentative embedding vector for the negative class. Might it be fairer to look at the average embedding vector over more images from the negative classes?\n\n-- Typos --\n= 2 typos on page 4 at the bottom: \u201c*Is* is then taken into account *explictly* in the distance metric.\u201d\n= 1 typo on page 5 at the bottom: \u201cstandard deviation between runs is generally very low (\u00a1.01). Each train\u201d\n\n-- Conclusion --\nOverall, I think the problem and solution proposed are interesting. The authors provide a large enough set of comparisons to existing algorithms, and explain their approach well.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper365/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper365/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Few-shot models have become a popular topic of research in the past years. They offer the possibility to determine class belongings for unseen examples using just a handful of examples for each class. Such models are trained on a wide range of classes and their respective examples, learning a decision metric in the process. Types of few-shot models include matching networks and prototypical networks. We show a new way of training prototypical few-shot models for just a single\nclass. These models have the ability to predict the likelihood of an unseen query belonging to a group of examples without any given counterexamples. The difficulty here lies in the fact that no relative distance to other classes can be calculated\nvia softmax. We solve this problem by introducing a \u201cnull class\u201d centered around zero, and enforcing centering with batch normalization. Trained on the commonly used Omniglot data set, we obtain a classification accuracy of .98 on the matched\ntest set, and of .8 on unmatched MNIST data. On the more complex MiniImageNet data set, test accuracy is .8. In addition, we propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples\u2019 distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available.", "title": "One-way prototypical networks", "keywords": ["few-shot learning", "one-shot learning", "prototypical networks", "one-class classification", "anomaly detection", "outlier detection", "matching networks"], "pdf": "/pdf/f47a12c97dc0183d5009af39f1056ae446939b42.pdf", "authors": ["Anna Kruspe"], "TL;DR": "We show how to adapt prototypical few-shot networks to one-class problems; the key is a null class for comparison.", "authorids": ["anna.kruspe@dlr.de"], "paperhash": "kruspe|oneway_prototypical_networks", "original_pdf": "/attachment/c55b42d6292f26d47303fbf41a2884a25a0908bb.pdf", "_bibtex": "@misc{\nkruspe2020oneway,\ntitle={One-way prototypical networks},\nauthor={Anna Kruspe},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgWbpEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575747731122, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper365/Reviewers"], "noninvitees": [], "tcdate": 1570237753207, "tmdate": 1575747731136, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper365/-/Official_Review"}}}, {"id": "HklxYwDCKr", "original": null, "number": 2, "cdate": 1571874680452, "ddate": null, "tcdate": 1571874680452, "tmdate": 1572972604509, "tddate": null, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "invitation": "ICLR.cc/2020/Conference/Paper365/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Authors consider the 1-way few shot classification task. Argue that modeling it as a 2-way with a random negative sample is not efficient. Propose a novel technique applicable for prototypical networks. Their proposal is to use 0 as the prototype of null class. So the distance to the prototype is compared against the norm of the query embedding. They also propose modeling the distribution and not just the centroid using a multivariate gaussian. But in practice there is no benefit in doing so. Therefore, the main contribution is proposing to compare against norm of the embedding rather than a prototype for random negative samples. The benefit of this proposal decreases by more shots. Probably because the prototype of 20 random images is 0 anyway.\n\nFig. 4b is wrong. The 2-way results match the matching result in Fig. 3b rather than the prototypical result. \n\nConsidering Fig. 4 seems the most gain comes from the reordering rather than one-way proposal. So a natural question is what is the effect of reordering on the 2-way prototypical. It may boost the 2-way one too.\n\nThe contribution of this paper is intuitive and interesting. But given the experiments it seems insignificant. Specially with the missing reordering of the 2-way model results."}, "signatures": ["ICLR.cc/2020/Conference/Paper365/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper365/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Few-shot models have become a popular topic of research in the past years. They offer the possibility to determine class belongings for unseen examples using just a handful of examples for each class. Such models are trained on a wide range of classes and their respective examples, learning a decision metric in the process. Types of few-shot models include matching networks and prototypical networks. We show a new way of training prototypical few-shot models for just a single\nclass. These models have the ability to predict the likelihood of an unseen query belonging to a group of examples without any given counterexamples. The difficulty here lies in the fact that no relative distance to other classes can be calculated\nvia softmax. We solve this problem by introducing a \u201cnull class\u201d centered around zero, and enforcing centering with batch normalization. Trained on the commonly used Omniglot data set, we obtain a classification accuracy of .98 on the matched\ntest set, and of .8 on unmatched MNIST data. On the more complex MiniImageNet data set, test accuracy is .8. In addition, we propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples\u2019 distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available.", "title": "One-way prototypical networks", "keywords": ["few-shot learning", "one-shot learning", "prototypical networks", "one-class classification", "anomaly detection", "outlier detection", "matching networks"], "pdf": "/pdf/f47a12c97dc0183d5009af39f1056ae446939b42.pdf", "authors": ["Anna Kruspe"], "TL;DR": "We show how to adapt prototypical few-shot networks to one-class problems; the key is a null class for comparison.", "authorids": ["anna.kruspe@dlr.de"], "paperhash": "kruspe|oneway_prototypical_networks", "original_pdf": "/attachment/c55b42d6292f26d47303fbf41a2884a25a0908bb.pdf", "_bibtex": "@misc{\nkruspe2020oneway,\ntitle={One-way prototypical networks},\nauthor={Anna Kruspe},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgWbpEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575747731122, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper365/Reviewers"], "noninvitees": [], "tcdate": 1570237753207, "tmdate": 1575747731136, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper365/-/Official_Review"}}}, {"id": "B1xX8Syx5H", "original": null, "number": 3, "cdate": 1571972427343, "ddate": null, "tcdate": 1571972427343, "tmdate": 1572972604464, "tddate": null, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "invitation": "ICLR.cc/2020/Conference/Paper365/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper addresses a method of applying prototypical networks (which are popular for few-shot learning problems) to few-shot one-classification problems where only one group of examples are available without any counter-examples. The main idea of prototypical networks is to learn an embedding function such that in the embedding space a distance metric well reflects the class structure. When such models are applied to one-class problems, a basis for comparison is required. \nThe idea proposed in this paper is to introduce a null class which models the entire space of possible examples and queries are judged, compared to positive class as well as the null class. Another extension was made to model classes by their distribution instead of prototypes only, leading to one-way normal prototypical models.\n\n---Strength---\n- One-class extension of the prototypical network is a new idea, while prototypical networks are already popular for few-shot learning problems.\n- The proposed model can solve few-shot anomaly detection problems.\n\n---Weakness---\n- Regarding the null class, it was claimed that the centroid of the whole latent space can be assumed to be close to the origin in the embedding space, since batch normalizations are included in the model.  The batch normalization learns the scale and shift parameters to minimize the error function, so the center does not have to be close to zero in all dimensions. More clear justification should be added to make the paper more sense.\n- Regarding one-way normal prototypical models, calculating the distributions require sufficient number of examples. Thus, when applied to few-shot anomaly detection, the performance might be be satisfactory.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper365/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper365/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "Few-shot models have become a popular topic of research in the past years. They offer the possibility to determine class belongings for unseen examples using just a handful of examples for each class. Such models are trained on a wide range of classes and their respective examples, learning a decision metric in the process. Types of few-shot models include matching networks and prototypical networks. We show a new way of training prototypical few-shot models for just a single\nclass. These models have the ability to predict the likelihood of an unseen query belonging to a group of examples without any given counterexamples. The difficulty here lies in the fact that no relative distance to other classes can be calculated\nvia softmax. We solve this problem by introducing a \u201cnull class\u201d centered around zero, and enforcing centering with batch normalization. Trained on the commonly used Omniglot data set, we obtain a classification accuracy of .98 on the matched\ntest set, and of .8 on unmatched MNIST data. On the more complex MiniImageNet data set, test accuracy is .8. In addition, we propose a novel Gaussian layer for distance calculation in a prototypical network, which takes the support examples\u2019 distribution rather than just their centroid into account. This extension shows promising results when a higher number of support examples is available.", "title": "One-way prototypical networks", "keywords": ["few-shot learning", "one-shot learning", "prototypical networks", "one-class classification", "anomaly detection", "outlier detection", "matching networks"], "pdf": "/pdf/f47a12c97dc0183d5009af39f1056ae446939b42.pdf", "authors": ["Anna Kruspe"], "TL;DR": "We show how to adapt prototypical few-shot networks to one-class problems; the key is a null class for comparison.", "authorids": ["anna.kruspe@dlr.de"], "paperhash": "kruspe|oneway_prototypical_networks", "original_pdf": "/attachment/c55b42d6292f26d47303fbf41a2884a25a0908bb.pdf", "_bibtex": "@misc{\nkruspe2020oneway,\ntitle={One-way prototypical networks},\nauthor={Anna Kruspe},\nyear={2020},\nurl={https://openreview.net/forum?id=BJgWbpEtPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJgWbpEtPr", "replyto": "BJgWbpEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper365/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575747731122, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper365/Reviewers"], "noninvitees": [], "tcdate": 1570237753207, "tmdate": 1575747731136, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper365/-/Official_Review"}}}], "count": 5}