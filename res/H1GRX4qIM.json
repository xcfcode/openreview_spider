{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573611648, "tcdate": 1521573611648, "number": 289, "cdate": 1521573611304, "id": "SJ4g1J19M", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "H1GRX4qIM", "replyto": "H1GRX4qIM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Rotational Unit of Memory", "abstract": "The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks.  However, RNN still has a limited capacity to manipulate long-term memory.  To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is,  naturally,  a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem.  Moreover,  the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks.   RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task.  RUM\u2019s performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.", "pdf": "/pdf/9515158b7bbe028893f0a19882ca2fc4d36ce629.pdf", "TL;DR": "A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.", "paperhash": "dangovski|rotational_unit_of_memory", "_bibtex": "@misc{\ndangovski2018rotational,\ntitle={Rotational Unit of Memory },\nauthor={Rumen Dangovski and Li Jing and Marin Soljacic},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk4w0A0Tb},\n}", "keywords": ["RNN", "unitary approach", "associative memory", "language modeling"], "authors": ["Rumen Dangovski", "Li Jing", "Marin Soljacic"], "authorids": ["rumenrd@mit.edu", "ljing@mit.edu", "soljacic@mit.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1518730188797, "tcdate": 1518121930420, "number": 36, "cdate": 1518121930420, "id": "H1GRX4qIM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "H1GRX4qIM", "original": "Sk4w0A0Tb", "signatures": ["~Li_Jing1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Rotational Unit of Memory", "abstract": "The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks.  However, RNN still has a limited capacity to manipulate long-term memory.  To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is,  naturally,  a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem.  Moreover,  the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks.   RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task.  RUM\u2019s performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.", "pdf": "/pdf/9515158b7bbe028893f0a19882ca2fc4d36ce629.pdf", "TL;DR": "A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.", "paperhash": "dangovski|rotational_unit_of_memory", "_bibtex": "@misc{\ndangovski2018rotational,\ntitle={Rotational Unit of Memory },\nauthor={Rumen Dangovski and Li Jing and Marin Soljacic},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk4w0A0Tb},\n}", "keywords": ["RNN", "unitary approach", "associative memory", "language modeling"], "authors": ["Rumen Dangovski", "Li Jing", "Marin Soljacic"], "authorids": ["rumenrd@mit.edu", "ljing@mit.edu", "soljacic@mit.edu"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730188797, "tcdate": 1508990556480, "number": 108, "cdate": 1518730188788, "id": "Sk4w0A0Tb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "Sk4w0A0Tb", "original": "SJXwRCAT-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Rotational Unit of Memory ", "abstract": "The concepts of unitary evolution matrices and associative memory have boosted the field of Recurrent Neural Networks (RNN) to state-of-the-art performance in a variety of sequential tasks.  However, RNN still has a limited capacity to manipulate long-term memory.  To bypass this weakness the most successful applications of RNN use external techniques such as attention mechanisms. In this paper we propose a novel RNN model that unifies the state-of-the-art approaches: Rotational Unit of Memory (RUM). The core of RUM is its rotational operation, which is,  naturally,  a unitary matrix, providing architectures with the power to learn long-term dependencies by overcoming the vanishing and exploding gradients problem.  Moreover,  the rotational unit also serves as associative memory. We evaluate our model on synthetic memorization, question answering and language modeling tasks.   RUM learns the Copying Memory task completely and improves the state-of-the-art result in the Recall task.  RUM\u2019s performance in the bAbI Question Answering task is comparable to that of models with attention mechanism. We also improve the state-of-the-art result to 1.189 bits-per-character (BPC) loss in the Character Level Penn Treebank (PTB) task, which is to signify the applications of RUM to real-world sequential data. The universality of our construction, at the core of RNN, establishes RUM as a promising approach to language modeling, speech recognition and machine translation.", "pdf": "/pdf/3952f478bbb37b9de502a927085ab2dc6ad6f898.pdf", "TL;DR": "A novel RNN model which outperforms significantly the current frontier of models in a variety of sequential tasks.", "paperhash": "dangovski|rotational_unit_of_memory", "_bibtex": "@misc{\ndangovski2018rotational,\ntitle={Rotational Unit of Memory },\nauthor={Rumen Dangovski and Li Jing and Marin Soljacic},\nyear={2018},\nurl={https://openreview.net/forum?id=Sk4w0A0Tb},\n}", "keywords": ["RNN", "unitary approach", "associative memory", "language modeling"], "authors": ["Rumen Dangovski", "Li Jing", "Marin Soljacic"], "authorids": ["rumenrd@mit.edu", "ljing@mit.edu", "soljacic@mit.edu"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}], "count": 2}