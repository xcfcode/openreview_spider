{"notes": [{"id": "rylb3eBtwr", "original": "SygDzebFDr", "number": 2527, "cdate": 1569439913381, "ddate": null, "tcdate": 1569439913381, "tmdate": 1583912030439, "tddate": null, "forum": "rylb3eBtwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "GQzgiTL8Y", "original": null, "number": 1, "cdate": 1576798751297, "ddate": null, "tcdate": 1576798751297, "tmdate": 1576800884397, "tddate": null, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Three reviewers have assessed this paper and they have scored it as 6/6/6/6 after rebuttal. Nonetheless, the reviewers have raised a number of criticisms and the authors are encouraged to resolve them for the camera-ready submission.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795711193, "tmdate": 1576800260350, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Decision"}}}, {"id": "HyxIlEaNFH", "original": null, "number": 1, "cdate": 1571242989912, "ddate": null, "tcdate": 1571242989912, "tmdate": 1574656744200, "tddate": null, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper proposes to use the robust subspace recovery layer (RSR) in the autoencoder model for unsupervised anomaly detection.\nThis paper is well written overall. Presentation is clear and it is easy to follow.\nThe proposed approach is a simple combination of existing approaches.\nAlthough its theoretical analysis with respect to the performance of anomaly detection is limited, experiments show that the proposed method is effective and superior to the existing anomaly detection methods.\n\nI have the following comments:\n- Parameter sensitivity should be examined.\n    The proposed method has the number of parameters including \\lambda_1, \\lambda_2, and parameters in neural networks.\n    Since parameter tuning is fundamentally difficult in the unsupervised setting, the sensitivity of the proposed method with respect to changes of such parameters should be examined.\n- Since the efficiency is also an important issue for anomaly detection methods, runtime comparison would be interesting.\n- It would be also interesting whether the proposed method is also effective for non-structured data, where a dataset is given as just a set of (real-valued) feature vectors, with its comparison to the standard anomaly detection methods such as LOF and iForest.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575740666759, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Reviewers"], "noninvitees": [], "tcdate": 1570237721591, "tmdate": 1575740666780, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Review"}}}, {"id": "ryl4G2X15S", "original": null, "number": 3, "cdate": 1571925004365, "ddate": null, "tcdate": 1571925004365, "tmdate": 1574376325033, "tddate": null, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "After reading all the reviews and the comments, I feel more positive about the paper. I appreciate the feedback of the Authors and I have decided to increase the rating.\n\n============================\n\nThe paper proposes using Robust Subspace Recovery in combination with an autoencoder (and possibly GANs) for anomaly detection. The encoder maps input data to the latent space of dimensionality D, which then is linearly projected to a subspace of dimensionality d (d < D). The projection of the latent space then goes to a decoder that reconstructs the input.\nA transformation matrix A is trained jointly with the autoencoder. Two additional terms are added to the loss: one to encourage the subspace of A^TA to approximate the latent space z and the second one to force it to be an orthogonal projector.\n \nThe paper claims to generalize the existing RSR framework to the nonlinear case. However, the linear RSR is applied to the latent space of the autoencoder. In addition to that, all the following discussion and proofs are limited to the linear case. \n\nSince the proposed method is using RSB as it\u2019s core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNET features, Reuters-21578, and 20 Newsgroups). However, there is no such comparison.\n\nSince autoencoders can potentially learn any, arbitrary entangled latent space, it is not clear why outliers should necessarily have such embedding that is outside of the learned subspace. In the case of the original RSR it happens due to the dimensionality reduction by the orthogonal projector. However, autoencoders already perform dimensionality reduction at each layer down to the bottleneck layer.\n\nThe matrix A and the parameters of the AE are trained jointly. So, it can be seen that two processes can occur:\n- The AE in order to minimize the reconstruction error would learn such latent space z, that would fit into the subspace of A^TA, so that projection \\tilde z =Az doesn\u2019t cause data loss.\n- The AE in order to minimize the reconstruction error would learn such A, so that the subspace that z approximates is the best possible.\n\nIt is not clear, which of the two cases would take place. If the first one would dominate, then it is not clear if such method would have any discriminating capabilities.\n \nMy point is mainly that the presented work is not really a generalization of RSR as it claims to be, but rather it is just using RSR on a leaned embedding of the data. \n\nSome citations are missing, as well as it is missing a comparison to some state-of-the art methods such as OCNN \u2018Robust, Deep and Inductive Anomaly Detection\u2019 ECML 2017;   \u2018Adversarially Learned One-Class Classifier for Novelty Detection\u2019 CVPR 2017; DSVDD \u2018Deep one-class classification.\u2019 ICML, 2018; ODIN  \u2018Enhancing The Reliability  Of Out-of-distribution Image Detection  In Neural Networks\u2019 ICLR 2018; \u2018Generative Probabilistic Novelty Detection with Adversarial Autoencoders\u2019 NeurIPS 2018.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575740666759, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Reviewers"], "noninvitees": [], "tcdate": 1570237721591, "tmdate": 1575740666780, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Review"}}}, {"id": "rJe8kNCjsr", "original": null, "number": 6, "cdate": 1573802973773, "ddate": null, "tcdate": 1573802973773, "tmdate": 1573802973773, "tddate": null, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment", "content": {"title": "Response to all reviewers with summary of changes", "comment": "Thanks again for the comments and suggestions. \n\nWe have modified our manuscript in order to address the concerns we responded to on November 10. The changes in the paper are indicated in red. \n\nThe major revisions are as follows:\n\n- We clarified our contribution in Section 2.2. In particular, we distinguished our method from a direct application of RSR to a regular autoencoder (in response to R1 and R3). We also emphasized that our method is designed to handle the unsupervised setting and not the semi-supervised or supervised ones (in response to R2).\n\n- We added the definition of AUC and AP in Appendix E and mentioned it in Section 4.2 (addressing R2).\n\n- We added Section 5.1 in order to provide more intuition for RSRAE. We explained why an RSR layer is needed instead of a direct application of an $l_{2,1}$ within a regular autoencoder. A proposition for linear autoencoders with different loss functions (that include the cases of PCA and RSR) is proved in Appendix D.1. (Section 5.1 is motivated by various claims of R1, which we disagreed with in our earlier response. It tries to further clarify our work and avoid some misunderstandings). \n\n- We added Appendix F, which compares RSRAE with (1) RSR (using both the FMS and SFMS algorithms) and (2) RCAE by Chalapathy, Menon and Chawla (2017).  (This addressed a request by R1).\n\n- We added Appendix G, which includes sensitivity analysis. Section G.1 addresses sensitivity to the intrinsic dimension (addressing R2 and R3). Section G.2 addresses sensitivity to the learning rate (addressing R3) and Section G.3 addresses sensitivity of RSRAE+ to the lambda\u2019s (addressing R3).\n\n- We added runtime comparison in Appendix H (addressing R3).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylb3eBtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2527/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2527/Authors|ICLR.cc/2020/Conference/Paper2527/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140035, "tmdate": 1576860559246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment"}}}, {"id": "r1l1nslHiH", "original": null, "number": 5, "cdate": 1573354406904, "ddate": null, "tcdate": 1573354406904, "tmdate": 1573354406904, "tddate": null, "forum": "rylb3eBtwr", "replyto": "HyxIlEaNFH", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your comments and constructive suggestions. We respond before the deadline, so we may communicate if needed.\n\n\u201c\nThe proposed approach is a simple combination of existing approaches.\n\u201d\nResponse:  While AE and RSR are previous well-known approaches, RSRAE is not a simple combination of both of them. They are not two independent components that are combined together. In fact, we show that a simple combination of a least absolute deviations energy used in RSR and an AE, which we refer to AE-1, does not improve the performance of AE for anomaly detection. The RSR layer is between the encoder and decoder of the AE and it is optimized together with the autoencoder and not separately from it. We are planning to further clarify why this layer should be effective and what makes it unique. \n\n\u201c\n- Parameter sensitivity should be examined.\n    The proposed method has the number of parameters including \\lambda_1, \\lambda_2, and parameters in neural networks.\n    Since parameter tuning is fundamentally difficult in the unsupervised setting, the sensitivity of the proposed method with respect to changes of such parameters should be examined.\n\u201d\nResponse: We promoted the use of alternating minimization, which does not require predetermined $\\lambda$\u2019s. That is, the values of $\\lambda_1$ and $\\lambda_2$ are not relevant for the algorithm we advocated. They are only relevant to RSRAE+, but we do not advocate this algorithm due to the time it takes to test different values of lambda. Nevertheless, following this comment we will test the sensitivity of RSRAE+ to $\\lambda_1$ and $\\lambda_2$. \nWe also agree that it is important to test the sensitivity with respect to the learning rate and the dimension of the subspace, and we will report it in the revised version.\n\n\u201c\n- Since the efficiency is also an important issue for anomaly detection methods, runtime comparison would be interesting.\n\u201d\nResponse: We plan to have a runtime comparison in the revised version.\n\n\u201c\n- It would be also interesting whether the proposed method is also effective for non-structured data, where a dataset is given as just a set of (real-valued) feature vectors, with its comparison to the standard anomaly detection methods such as LOF and iForest.\n\u201d\nResponse: When we tested our method on the deep features of Imagenet and the features of Reuters and 20 Newsgroups, we worked with a set of feature vectors. In those examples, we didn\u2019t treat the features like images or sequences, and the network was simply a fully-connected network instead of a convolutional one. Please let us know if we missed anything."}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylb3eBtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2527/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2527/Authors|ICLR.cc/2020/Conference/Paper2527/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140035, "tmdate": 1576860559246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment"}}}, {"id": "BylbIigSiB", "original": null, "number": 4, "cdate": 1573354313310, "ddate": null, "tcdate": 1573354313310, "tmdate": 1573354313310, "tddate": null, "forum": "rylb3eBtwr", "replyto": "HkxvUjYsKS", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your comments and constructive suggestions. We respond before the deadline, so we may communicate if needed.\n\n\u201c\n* There is a serious problem in the results (Figure 1) as the AP curves show better scores for larger corruption factors. Are the AP-score graphs flipped ? Please explain.\n* The AUC and AP scores need to be defined. \n\u201d\nResponse: The AUC and AP scores are calculated using the corresponding functions in the scikit-learn package. We will include the definitions in the appendix and will refer to scikit-learn. \n\nThe AP scores are correct. They increase with c because the size of the outliers (that is, the size of the \u201cPositive\u201d class) increases with c. To clarify this issue, let\u2019s assume a dataset with 2n points (where n > 1) with a single inlier and (2n-1) outliers and suppose an algorithm for anomaly detection that returns a similarity score (as mentioned in line 16 of Algorithm 1) of 0.5 for all inliers, 0.9 for (n-1) of the outliers and 0.1 for the other n outliers. Then AUC =  $n/(2n-1)$ while AP = $1 - (n-1)/(4n^2-2n)$. AUC decreases with n and AP increases with n.  \n \n\n* The results should include the case where the training data is not contaminated with outliers (c=0). This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario.\n\u201d\nResponse: Our method was designed for the unsupervised setting. Indeed, RSRAE tries to extract the main structure from data in the presence of outliers without any information on the inliers. The main issue of the semi-supervised setting case is to learn the best model for the training data and use it for identifying outliers. If we use our method in a semi-supervised setting, then we only use its AE component without taking advantage of the RSR layer. This is not special to our method and similar results can be obtained by other autoencoders. Also, generative models may be advantageous for this problem. \n\n\u201c\n* It would be interesting to see the effect of varying the subspace dimension. The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one? This is a key parameter as it defines the structure of the projection subspace. Should this parameter be systematically tuned for each dataset?\n\u201d\nResponse: We plan to add some experiments addressing the subspace dimension. We chose 10 because we were interested in a single parameter that may work for a wide range of real datasets, and we generally noticed some stability to changes of this dimension. However, we agree that we need to report the experiments that demonstrate this stability."}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylb3eBtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2527/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2527/Authors|ICLR.cc/2020/Conference/Paper2527/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140035, "tmdate": 1576860559246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment"}}}, {"id": "rJxzJoxBsH", "original": null, "number": 3, "cdate": 1573354201836, "ddate": null, "tcdate": 1573354201836, "tmdate": 1573354201836, "tddate": null, "forum": "rylb3eBtwr", "replyto": "ryl4G2X15S", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment", "content": {"title": "Response to Review #1 (part 3) ", "comment": "\u201cThe matrix A and the parameters of the AE are trained jointly. So, it can be seen that two processes can occur:\n- The AE in order to minimize the reconstruction error would learn such latent space z, that would fit into the subspace of A^TA, so that projection \\tilde z =Az doesn\u2019t cause data loss.\n- The AE in order to minimize the reconstruction error would learn such A, so that the subspace that z approximates is the best possible.\nIt is not clear, which of the two cases would take place. If the first one would dominate, then it is not clear if such method would have any discriminating capabilities.\n\u201d\nResponse: Our minimization does not wait until the convergence of the first term in eq. (5) of our paper before backpropagating the second and third terms in this equation. Instead, we alternatively backpropagate the three terms (please review the detailed pseudocode that we provided in the appendix). Therefore, it is unlikely that the algorithm only focuses on learning the best z that fits A. Nevertheless, even if the latent space for z is already low-rank before applying A, the method should still have discriminating capabilities because the latent space should correspond to the inliers. But this should not be the case in general, since otherwise, AE-1 would perform well (as opposed to the demonstrated results in Section 4.3 and Appendix F.2).\nThe description above of the reviewer corresponds to that in Section 3.2 of  \u201cRobust, Deep and Inductive Anomaly Detection\u201d by R. Chalapathy, A. K. Menon and S. Chawla, but our paper is completely different than this paper.  \n\n\u201cMy point is mainly that the presented work is not really a generalization of RSR as it claims to be, but rather it is just using RSR on a leaned embedding of the data. \u201d\nResponse: Again, we did not advocate for establishing a generalization of RSR. We proposed a method that effectively addresses unsupervised anomaly detection by incorporating ideas of RSR within an autoencoder. Also, it is not true to claim that it uses RSR on the learned embedding of the data, as we explained above. The RSR loss plays an essential role in the learning process. \n\n\u201cSome citations are missing, as well as it is missing a comparison to some state-of-the art methods such as OCNN \u2018Robust, Deep and Inductive Anomaly Detection\u2019 ECML 2017;   \u2018Adversarially Learned One-Class Classifier for Novelty Detection\u2019 CVPR 2017;  \u2018Deep one-class classification.\u2019 ICML, 2018; ODIN  \u2018Enhancing The Reliability  Of Out-of-distribution Image Detection  In Neural Networks\u2019 ICLR 2018; \u2018Generative Probabilistic Novelty Detection with Adversarial Autoencoders\u2019 NeurIPS 2018.\u201d\nResponse:  RCAE in \u201cRobust, Deep and Inductive Anomaly Detection\u201d (ECML, 2017) is indeed another unsupervised method for anomaly detection. However, RCAE is related to RPCA, where one assumes entry-wise corruption of the data matrix. On the other hand, we discuss here the case where whole data points are outliers (that is, rows of the data matrix are corrupted), instead of few entries of a matrix are corrupted. Therefore, we don\u2019t expect that RCAE will work well for our model of outliers. We can make few comparisons with RCAE in the appendix or supplemental material.\n  \nAll the other methods you mentioned above are state-of-the-art for the semi-supervised setting (novelty detection), but here we emphasize a completely unsupervised setting (anomaly detection). Note that some of those methods (the ICLR 2018 and CVPR 2017 papers you mentioned) claim to be unsupervised anomaly detection / outlier detection but they are actually performing semi-supervised tasks, where one trains a model for the inliers. We already included GT from NIPS 2018 and DAGMM from ICLR 2018 and we do not find a reason to add more baseline methods, which are not unsupervised. \n\nAs for the claim that \u201csome citations are missing\u201d, we will be happy to cite whatever relevant references we missed. Please let us know specific relevant papers that we failed to cite. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylb3eBtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2527/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2527/Authors|ICLR.cc/2020/Conference/Paper2527/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140035, "tmdate": 1576860559246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment"}}}, {"id": "rylGn9lSir", "original": null, "number": 2, "cdate": 1573354154432, "ddate": null, "tcdate": 1573354154432, "tmdate": 1573354154432, "tddate": null, "forum": "rylb3eBtwr", "replyto": "ryl4G2X15S", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment", "content": {"title": "Response to Review #1 (part 2)", "comment": "\u201cIn addition to that, all the following discussion and proofs are limited to the linear case.\u201d\nResponse: Our discussion and experiments are for the nonlinear case. The fact that the transformation A is linear does not make the whole method linear. Let us also clarify that this is not a theoretical paper (even though it has some theory). It proposes a method and numerically tests it. We never claimed to provide theoretical justification for the proposed method. It does not seem feasible to theoretically justify this nonlinear procedure. It is also unfair to ask for theoretical justification as the effectiveness of the nonlinear dimension reduction by autoencoders is not well justified even though autoencoders have been around since the 80s and are now well recognized as effective practical tools for nonlinear dimension reduction.\nThe proof in Appendix C does is not \u201climited to the linear case\u201d, but it addresses the fact that the second term in (4) may be dropped in case one can find the minimizer in (10). We would like to emphasize the motivation of formulating and proving Proposition 5.1, which deals with a linear setting. Since the linear case of PCA motivates the notion of an autoencoder, we find it interesting to observe the case of a linear generator in WGAN and carefully understand its output, in particular, notice its relationship with the robust energy that we use. While we find the proofs of these propositions interesting (especially of the second proposition), they are not meant to address the performance of the main algorithm that we focus on and we believe that we made it clear in our writing.\n\n\u201cSince the proposed method is using RSB as its core part, and claims to be a non-linear extension of it, it would be crucial to have a comparison with RSB, at least on those experimental setups, where high-level features are used (Tiny Imagenet with ResNet features, Reuters-21578, and 20 Newsgroups).\u201d\nResponse: Again, we emphasize that our method is not a direct extension of RSR to the nonlinear case and we do not claim so. Also, the RSR component is integrated in a very special way within the autoencoder and is not a separate component. \n\nMore importantly, one cannot expect a linear model for the normal points to do well in practice and we also mentioned it in the paper. In particular, when considering a dataset of features, \u201chigh-level\u201d features are not expected to lie in a common subspace. We can exemplify the poor performance of direct RSR on a particular dataset in the appendix, even though we believe it should be clear. We also want to clarify that what we call RSR is different than what is commonly referred to as RPCA, which the reviewer alludes to later.\n\n \u201cSince autoencoders can potentially learn any, arbitrary entangled latent space, it is not clear why outliers should necessarily have such embedding that is outside of the learned subspace. In the case of the original RSR it happens due to the dimensionality reduction by the orthogonal projector. However, autoencoders already perform dimensionality reduction at each layer down to the bottleneck layer.\u201d\nResponse: We don\u2019t agree that \u201cautoencoders can potentially learn any, arbitrary entangled latent space\u201d.  The number of neurons in the autoencoder (and in particular, the dimension of the latent code) imposes a serious constraint.  Actually, you mentioned that there is a bottleneck layer in a regular autoencoder. Due to this bottleneck layer, it is impossible to learn an autoencoder that produces zero error for all the data points. We try to focus only on the inliers and have a latent subspace for the inliers within the ambient space of the encoder. If outliers are mapped to this subspace, as you may worry, then these outliers will have huge reconstruction errors, which will easily help distinguish them and thus RSRAE should perform well in this case. That is, the answer to your point \u201cit is not clear why outliers should necessarily have such embedding that is outside of the learned subspace\u201d,  is that there is no reason for the outliers to be outside the learned inliers\u2019 subspace and it will be actually easier to distinguish them if they also happen to lie in this subspace. However, we wish to carefully learn such a subspace, while putting special emphasis on the inliers, instead of mapping all points to a low-dimensional subspace without recognizing properties of the inliers.  The loss function of RSRAE does not care much about the outliers, but takes careful care of the inliers. It tries to carefully learn their structure and use it to distinguish between inliers and outliers. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylb3eBtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2527/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2527/Authors|ICLR.cc/2020/Conference/Paper2527/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140035, "tmdate": 1576860559246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment"}}}, {"id": "S1gfPcgSiB", "original": null, "number": 1, "cdate": 1573354074008, "ddate": null, "tcdate": 1573354074008, "tmdate": 1573354074008, "tddate": null, "forum": "rylb3eBtwr", "replyto": "ryl4G2X15S", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment", "content": {"title": "Response to Review #1 (part 1)", "comment": "Thank you for your comments. We respond before the deadline, so we may communicate if needed.\n\n\u201cThe paper claims to generalize the existing RSR framework to the nonlinear case.\u201d \nResponse: Our paper does not claim to generalize the RSR framework to a nonlinear one, but it combines ideas from RSR within an autoencoder in order to make the autoencoder more robust to outliers.  The mere use of a \u201crobust metric\u201d (such as the least absolute deviations) for an autoencoder is not sufficiently robust to anomalies and this is why we find an idea like ours natural.  We believe that the above wrong interpretation of our paper is due to the following two sentences in Section 5: \u201cGoodfellow et al. (2016) exemplified how PCA can be structured as a linear autoencoder. Similarly, RSR can be directly used to form an outliers-robust linear autoencoder and our current work generalizes this basic idea to a nonlinear setting\u201d. The first claim here is that similarly to having an autoencoder for PCA (with linear encoder and decoder and least squares minimization), one can have an autoencoder for RSR (with linear encoder and decoder and absolute deviations minimization). Note that the difference between a PCA autoencoder and an RSR autoencoder is obtained by changing the minimization from least squares to absolute deviations (where it is also natural to introduce a simple normalization in the implementation part). On the other hand, changing the metric of a general autoencoder, which corresponds to \u201cnonlinear PCA\u201d, from least squares to absolute deviations does not make it robust (especially as ideas of simple normalizations for adversarial outliers do not work out in this case). We demonstrated this in our experiments (see performance of AE-1 vs. RSRAE in Section 4.3 and Appendix F2). We find that the RSR layer is a very natural idea to make an autoencoder robust. By writing \u201cgeneralizes this basic idea to a nonlinear setting\u201d, we meant extending the autoencoder to be robust to outliers, in analogy to making a PCA autoencoder robust by changing it into an RSR (linear) autoencoder, but we did not mean using the same method as in the latter one and formally generalizing RSR to the nonlinear case. \n\nThe purpose of these two sentences was only to motivate the rigorous theory for WGAN. We considered them as part of a minor comment, which the expert can easily figure out. There was no claim about generalizing RSR to the nonlinear case in the introduction or the conclusion.  Due to the confusion, we will extend and rewrite this part with a careful explanation how our method is expected to make an autoencoder more robust to outliers, and why the mere change of a metric is not sufficient to make the method robust. Anyway, we don\u2019t find these new details necessary for understanding the paper, but they may avoid a similar confusion.\n\n\u201cHowever, the linear RSR is applied to the latent space of the autoencoder.\u201d\n Response: The linear layer is in the middle of an autoencoder and an RSR loss is part of the total loss function. It is not correct to think of two separate entities: an autoencoder and a linear RSR applied to the latent space of the autoencoder. That is, we do not try to first get the latent code form AE and then apply RSR to the latent code. Note that a general autoencoder tries to parametrize the structure of the data with a latent code within a low-dimensional linear space. Here we try to parametrize only the structure of the inliers, where the low-dimensional subspace lies within the ambient space of the encoder, we then map the output of the encoder onto a low-dimensional space corresponding to the latter subspace. This mapping is natural to the inliers, but not to the outliers, which are expected to have high reconstruction error."}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rylb3eBtwr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2527/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2527/Authors|ICLR.cc/2020/Conference/Paper2527/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504140035, "tmdate": 1576860559246, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Authors", "ICLR.cc/2020/Conference/Paper2527/Reviewers", "ICLR.cc/2020/Conference/Paper2527/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Comment"}}}, {"id": "HkxvUjYsKS", "original": null, "number": 2, "cdate": 1571687246616, "ddate": null, "tcdate": 1571687246616, "tmdate": 1572972326928, "tddate": null, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "invitation": "ICLR.cc/2020/Conference/Paper2527/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper adapts the concept of Robust Subspace Recovery (RSR) as a layer in an auto-encoder model for anomaly detection. A loss function is proposed that combines reconstruction error and a regularizer that enforces robustness against outliers. The reconstruction error expresses the accuracy of the nonlinear dimensionality reduction imposed by the autoencoder. The regularizer is the sum of absolute deviations from the latent subspace that represents a linear structure robust against outliers. An alternative procedure is applied where the loss terms are applied iteratively during training. Once trained, the reconstruction error is used directly for anomaly detection with a threshold. The AUC is used as a performance measure. The method is compared against 6 other methods (LOF, OCSVM, IF, DESBM, GT, DAGMM). The setting is fully unsupervised, meaning that the training data contains various amounts of anomalies, and the results are parametrized with the amount of corruption. The results show that the proposed approach outperforms the other methods in most cases, especially for larger amounts of corruption. An ablation study compares the approach with auto-encoder-only and a non-alternating gradient descent (fixed factors for each part of the loss function) and shows that the alternating method outperfroms all by a wide margin.\n\n\nPROS:\n\n* A novel approach to fully unsupervised anomaly detection that beats the state of the art.\n\n* The RSR layer is a simple fully connected layer and the loss function is simple to calculate, making the approach computationally efficient.\n\n* A pseudo-code algorithm is provided in the appendix, which should help reproducibility. \n\n* The paper is well written and the math is clearly laid out.\n\n* The result benchmarks are sufficiently exhaustive in both the methods that are compared and the datasets used.\n\n* The ablation study is informative and shows the effect of the regularization term of the loss function as well as the effect of alternating the gradient descent with the separate losses.\n\n\n\nCONS:\n\n* There is a serious problem in the results (Figure 1) as the AP curves show better scores for larger corruption factors. Are the AP-score graphs flipped ? Please explain.\n\n* The AUC and AP scores need to be defined. \n\n* The results should include the case where the training data is not contaminated with outliers (c=0). This would correspond to the semi-supervised scenario and it would be very interesting to see how the method compares to DAGMM and GT which are build for that scenario.\n\n* It would be interesting to see the effect of varying the subspace dimension. The authors chose 10 for all experiments, why is this number chosen, what would be the effect of choosing a smaller one ? This is a key parameter as it defines the structure of the projection subspace. Should this parameter be systematically tuned for each dataset ?\n\n\nOverall this is a good paper proposing a novel approach to fully unsupervised anomaly detection with state-of-the art results.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2527/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2527/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["laixx313@umn.edu", "dzou@umn.edu", "lerman@umn.edu"], "title": "Robust Subspace Recovery Layer for Unsupervised Anomaly Detection", "authors": ["Chieh-Hsin Lai", "Dongmian Zou", "Gilad Lerman"], "pdf": "/pdf/4016b3ae0e7a6bfe6bea03eb07e7ba31b98508b6.pdf", "TL;DR": "This work proposes an autoencoder with a novel robust subspace recovery layer for unsupervised anomaly detection and demonstrates state-of-the-art results on various datasets.", "abstract": "We propose a neural network for unsupervised anomaly detection with a novel robust subspace recovery layer (RSR layer). This layer seeks to extract the underlying subspace from a latent representation of the given data and removes outliers that lie away from this subspace. It is used within an autoencoder. The encoder maps the data into a latent space, from which the RSR layer extracts the subspace. The decoder then smoothly maps back the underlying subspace to a ``manifold\" close to the original inliers. Inliers and outliers are distinguished according to the distances between the original and mapped positions (small for inliers and large for outliers). Extensive numerical experiments with both image and document datasets demonstrate state-of-the-art precision and recall. ", "keywords": ["robust subspace recovery", "unsupervised anomaly detection", "outliers", "latent space", "autoencoder"], "paperhash": "lai|robust_subspace_recovery_layer_for_unsupervised_anomaly_detection", "_bibtex": "@inproceedings{\nLai2020Robust,\ntitle={Robust Subspace Recovery Layer for Unsupervised Anomaly Detection},\nauthor={Chieh-Hsin Lai and Dongmian Zou and Gilad Lerman},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rylb3eBtwr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/1583e0e66f338f0a1931fde8fa1c06b6b2aa9279.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rylb3eBtwr", "replyto": "rylb3eBtwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2527/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575740666759, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2527/Reviewers"], "noninvitees": [], "tcdate": 1570237721591, "tmdate": 1575740666780, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2527/-/Official_Review"}}}], "count": 11}