{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124431914, "tcdate": 1518459395790, "number": 189, "cdate": 1518459395790, "id": "ryn-581vM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "ryn-581vM", "signatures": ["~Xianxu_Hou1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training", "abstract": "We present a new method for improving the performances of variational autoencoder (VAE). In addition to enforcing the deep feature consistency principle thus ensuring the VAE output and its corresponding input images to have similar deep features, we also implement an adversarial generative training mechanism to force the VAE to output realistic and natural images. We present experimental results to show that the VAE trained with our new method outperform state of the art in generating face images with much clearer and more natural noses, eyes, teeth, hair textures as well as reasonable backgrounds. We also show that the VAE trained with the new method can extract more effective features that outperform state of the art in facial attribute recognition.  ", "paperhash": "hou|improving_variational_autoencoder_with_deep_feature_consistent_and_generative_adversarial_training", "_bibtex": "@misc{\n  hou2018improving,\n  title={Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training},\n  author={Xianxu Hou and Guoping Qiu},\n  year={2018},\n  url={https://openreview.net/forum?id=ryn-581vM}\n}", "authorids": ["xianxu.hou@nottingham.edu.cn", "guoping.qiu@nottingham.ac.uk"], "authors": ["Xianxu Hou", "Guoping Qiu"], "keywords": ["Variational autoencoder", "VAE", "generative adversarial network", "GAN", "Facial Attribute Recognition"], "pdf": "/pdf/18defd2dcda6841a64d90bde5d3a808fdea70e72.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582811665, "tcdate": 1520617679580, "number": 1, "cdate": 1520617679580, "id": "S1OR_BlYf", "invitation": "ICLR.cc/2018/Workshop/-/Paper189/Official_Review", "forum": "ryn-581vM", "replyto": "ryn-581vM", "signatures": ["ICLR.cc/2018/Workshop/Paper189/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper189/AnonReviewer3"], "content": {"title": "Review", "rating": "3: Clear rejection", "review": "The paper proposes to improve the visual quality of VAE samples by pairing the model with two additional components: 1) a pre-trained classifier used as a feature extractor to align the reconstructed images with their associated input in a more abstract, \u201cperceptual\u201d feature space, and 2) a discriminator network used to provide an additional adversarial loss signal.\n\nThe first issue I have with the proposed approach is that it feels very ad-hoc: several architectural components are stitched together with loose justification. The idea of augmenting VAE training with auxiliary losses is itself not very new: aside from the cited work of Ridgeway et al. (2015) and Hou et al. (2017), work done by Larsen et al. (2016), Lamb et al. (2016), and Dosovitskiy and Brox (2016)  explore very related ideas. It is unclear to me what the proposed approach brings to the table.\n\nThe second (and more concerning) issue I have is the apparent lack of awareness of the line of work the authors\u2019 proposed approach inscribes itself in. Even a cursory read of the cited Hou et al. (2017) paper points towards (uncited) prior work which this paper borrows heavily from. To my knowledge, the idea of augmenting the VAE reconstruction loss with auxiliary reconstruction losses derived from a pre-trained classifier was first studied in Dosovitskiy and Brox (2016) and Lamb et al. (2016). Likewise, the idea of augmenting the VAE loss with an adversarial loss was first studied in Larsen et al. (2016) and Dosovitskiy and Brox (2016). Finally, the facial attribute manipulation experiment features a procedure (exemplified by a \u201csmiling vector\u201d) that appears to be lifted directly from White (2016) without attribution.\n\nFor these reasons, I don\u2019t think the paper should be accepted.\n\nReferences:\n\nLarsen, A. B. L., S\u00f8nderby, S. K., Larochelle, H., & Winther, O. (2016). Autoencoding beyond pixels using a learned similarity metric. In Proceedings of the International Conference on Machine Learning.\n\nLamb, A., Dumoulin, V., & Courville, A. (2016). Discriminative regularization for generative models. arXiv preprint arXiv:1602.03220.\n\nDosovitskiy, A., & Brox, T. (2016). Generating images with perceptual similarity metrics based on deep networks. In Advances in Neural Information Processing Systems.\n\nWhite, T. (2016). Sampling generative networks: Notes on a few effective techniques. arXiv preprint arXiv:1609.04468.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training", "abstract": "We present a new method for improving the performances of variational autoencoder (VAE). In addition to enforcing the deep feature consistency principle thus ensuring the VAE output and its corresponding input images to have similar deep features, we also implement an adversarial generative training mechanism to force the VAE to output realistic and natural images. We present experimental results to show that the VAE trained with our new method outperform state of the art in generating face images with much clearer and more natural noses, eyes, teeth, hair textures as well as reasonable backgrounds. We also show that the VAE trained with the new method can extract more effective features that outperform state of the art in facial attribute recognition.  ", "paperhash": "hou|improving_variational_autoencoder_with_deep_feature_consistent_and_generative_adversarial_training", "_bibtex": "@misc{\n  hou2018improving,\n  title={Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training},\n  author={Xianxu Hou and Guoping Qiu},\n  year={2018},\n  url={https://openreview.net/forum?id=ryn-581vM}\n}", "authorids": ["xianxu.hou@nottingham.edu.cn", "guoping.qiu@nottingham.ac.uk"], "authors": ["Xianxu Hou", "Guoping Qiu"], "keywords": ["Variational autoencoder", "VAE", "generative adversarial network", "GAN", "Facial Attribute Recognition"], "pdf": "/pdf/18defd2dcda6841a64d90bde5d3a808fdea70e72.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582811435, "id": "ICLR.cc/2018/Workshop/-/Paper189/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper189/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper189/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper189/AnonReviewer1"], "reply": {"forum": "ryn-581vM", "replyto": "ryn-581vM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper189/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582811435}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582715304, "tcdate": 1520687031222, "number": 2, "cdate": 1520687031222, "id": "HykTPUZtz", "invitation": "ICLR.cc/2018/Workshop/-/Paper189/Official_Review", "forum": "ryn-581vM", "replyto": "ryn-581vM", "signatures": ["ICLR.cc/2018/Workshop/Paper189/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper189/AnonReviewer1"], "content": {"title": "Quantitative results are ok, qualitative results are unsatisfying", "rating": "7: Good paper, accept", "review": "This approach uses a GAN-based approach to sample natural faces with a precise attribute exhibited therein. \nThe idea seems nice, and the quantitative results are convincing. In practice, the generated images are able to trigger an attribute predictor which capture the synthetic modification to the original image as a legit facial attribute.\nI'm happy with these results. The bad comes with the qualitative results, where the generated faces in some cases are weird (bald, black hair) while in some other are more impressive (smiling, male-female overall)\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training", "abstract": "We present a new method for improving the performances of variational autoencoder (VAE). In addition to enforcing the deep feature consistency principle thus ensuring the VAE output and its corresponding input images to have similar deep features, we also implement an adversarial generative training mechanism to force the VAE to output realistic and natural images. We present experimental results to show that the VAE trained with our new method outperform state of the art in generating face images with much clearer and more natural noses, eyes, teeth, hair textures as well as reasonable backgrounds. We also show that the VAE trained with the new method can extract more effective features that outperform state of the art in facial attribute recognition.  ", "paperhash": "hou|improving_variational_autoencoder_with_deep_feature_consistent_and_generative_adversarial_training", "_bibtex": "@misc{\n  hou2018improving,\n  title={Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training},\n  author={Xianxu Hou and Guoping Qiu},\n  year={2018},\n  url={https://openreview.net/forum?id=ryn-581vM}\n}", "authorids": ["xianxu.hou@nottingham.edu.cn", "guoping.qiu@nottingham.ac.uk"], "authors": ["Xianxu Hou", "Guoping Qiu"], "keywords": ["Variational autoencoder", "VAE", "generative adversarial network", "GAN", "Facial Attribute Recognition"], "pdf": "/pdf/18defd2dcda6841a64d90bde5d3a808fdea70e72.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582811435, "id": "ICLR.cc/2018/Workshop/-/Paper189/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper189/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper189/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper189/AnonReviewer1"], "reply": {"forum": "ryn-581vM", "replyto": "ryn-581vM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper189/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper189/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582811435}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573586065, "tcdate": 1521573586065, "number": 184, "cdate": 1521573585723, "id": "HJ5CAACYG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "ryn-581vM", "replyto": "ryn-581vM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training", "abstract": "We present a new method for improving the performances of variational autoencoder (VAE). In addition to enforcing the deep feature consistency principle thus ensuring the VAE output and its corresponding input images to have similar deep features, we also implement an adversarial generative training mechanism to force the VAE to output realistic and natural images. We present experimental results to show that the VAE trained with our new method outperform state of the art in generating face images with much clearer and more natural noses, eyes, teeth, hair textures as well as reasonable backgrounds. We also show that the VAE trained with the new method can extract more effective features that outperform state of the art in facial attribute recognition.  ", "paperhash": "hou|improving_variational_autoencoder_with_deep_feature_consistent_and_generative_adversarial_training", "_bibtex": "@misc{\n  hou2018improving,\n  title={Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training},\n  author={Xianxu Hou and Guoping Qiu},\n  year={2018},\n  url={https://openreview.net/forum?id=ryn-581vM}\n}", "authorids": ["xianxu.hou@nottingham.edu.cn", "guoping.qiu@nottingham.ac.uk"], "authors": ["Xianxu Hou", "Guoping Qiu"], "keywords": ["Variational autoencoder", "VAE", "generative adversarial network", "GAN", "Facial Attribute Recognition"], "pdf": "/pdf/18defd2dcda6841a64d90bde5d3a808fdea70e72.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 4}