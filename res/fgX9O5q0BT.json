{"notes": [{"id": "fgX9O5q0BT", "original": "Y3Ow3GS8iY", "number": 1560, "cdate": 1601308173039, "ddate": null, "tcdate": 1601308173039, "tmdate": 1614985704458, "tddate": null, "forum": "fgX9O5q0BT", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2A8xLMkbDxi", "original": null, "number": 1, "cdate": 1610040439447, "ddate": null, "tcdate": 1610040439447, "tmdate": 1610474040315, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper studies the role of \u201cnoise injection\u201d in GANs with tools from Riemannian geometry, and derives a new noise injection approach that aims to learn a fuzzy coordinate system to model non-Euclidean geometry. The new noise injection approach is shown to improve over StyleGANv2 noise injection on lower-resolution 128x128 FFHQ, LSUN, and 32x32 CIFAR-10 images. \n\n\nSome reviewers found the experimental results a \u201cconsiderable improvement on DCGAN and StyleGANv2\u201d (R3), \u201cextensive and convincing\u201d (R2), while others had concerns around the experimental setup using lower resolution images (R1, R4).  While reviewers were mostly positive about the experimental wins of the paper, there was confusion (R3) and several concerns (R4) around the theory and the relationship between the theory and the practical noise injection algorithm. I additionally had several concerns around the presentation and relation to prior work on generative models. Thus in the current state I cannot recommend this paper for acceptance. Below I highlight concerns that should be addressed in future revisions.\n\n\n1. My biggest concern is the tremendous gap between the theoretical claims and the practical implementation. When training a GAN with the new form of noise injection, does it learn the skeleton and fuzzy equivalence relationships you claim? This paper is missing any kind of toy experimenting showing that training a GAN with fuzzy reparameterization discovers these relationships or coordinates. Such an experiment would greatly strengthen the paper and help to answer the question of why this new method works (i.e. it\u2019s not just more parameters, a slightly better architecture, or better hyperprameters as mentioned by R3 and R4). There\u2019s also no discussion of what happens theoretically when you have multiple layers of fuzzy reparameterization, and the claims that StyleGAN2\u2019s noise injection limits to Euclidean geometry is false in this case (and thus StyleGAN2\u2019s noise injection can also overcome the \u201cadversarial dimension trap\u201d).\n \n2. Theoretical setting: As mentioned by R4, there is much prior work on the difficulties in fitting a lower-dimensional model manifold to a higher-dimensional data manifold (e.g. WGAN). Theorem 1 highlights the impossibility of exactly fitting the data manifold with (smooth) neural networks, but the resulting solutions of increasing the dimensionality of the latent space is well-known and commonly used (e.g. StyleGAN). This paper also doesn\u2019t discuss the alternative of *approximately* fitting the data manifold with a lower-dimensional structure, which is what is often studied in practice. \n\n\n3. Clarity: The term \u201cnoise injection\u201d is overloaded in the literature, and the current presentation of the paper does not sufficiently describe the method. There\u2019s also no discussion of \u201cinstance noise\u201d that is another solution to this problem that adds noise to inputs of the discriminator to yield finite f-divergences (Sonderby et al., 2016, Roth et al., 2017). The work on instance noise is very related to the approach here, but only adds noise to the output of the generator, not at all levels. \nThere's also no discussion of how adding noise is just expanding the generative model with additional latent variables, a standard approach that is often discussed in the context of hierarchical generative models. The authors mention the relation to reparameterization trick in VAEs, but argue it is doing something fundamentally different. However, modern VAE architectures (IAF-VAE, Very Deep VAE), use a very similar form of modulation at multiple levels in the hierarchy. \n\n\n4. Experiments: There are no error bars in experimental results, and many results are presented in a new experimental setting defined by the authors (lower resolution than prior work even if using prior code). Rerunning experiments in more standard settings on full resolution images would greatly improve the confidence that the new noise injection strategy is effective.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040439433, "tmdate": 1610474040298, "id": "ICLR.cc/2021/Conference/Paper1560/-/Decision"}}}, {"id": "eaRSziRFwd0", "original": null, "number": 4, "cdate": 1603948816899, "ddate": null, "tcdate": 1603948816899, "tmdate": 1606800055208, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review", "content": {"title": "Nice Results", "review": "To summarize, this paper proposed a new noise injection method that is easy to implement and is able to replace the original noise injection method in StyleGAN 2. The approach is supported by detailed theoretical analysis and impactful performance improvement on GAN training and inversion. The results show that they are able to achieve a considerable improvement on DCGAN and StyleGAN2.\n\nMeanwhile, this reviewer did not fully understand the theoretical part and also has some questions regarding the implementation and results.\n\nBased on my understanding, the fuzzy reparameterization technique realizes something that StyleGAN2 cannot achieve, and resolves some fundamental limitations of StyleGAN2. However, the improvement is not contiguous in Table 1, where we see the vanilla StyleGAN2 still outperforms the proposed architecture. What could be the reason?\n\nFR seems to bring more parameters (Eq 6, 7, 8, 9). How many more compared to the additive noise implementation? Could the number of parameters be the reason that the proposed method performs better? Since FR can be seen as a generalization of StyleGAN2 noise injection, we would naturally expect that the proposed method should perform better than StyleGAN2. However, this is not always the case in Table 1. I guess more ablation studies can also be done on $\\sigma$, such as interpolating between StyleGAN2 implementation and FR implementation, or a linear layer with the same number of additional parameters but has no constraint as in Eq. 6, 7, 8, 9.\n\nFor Figure 8, do we have the reconstruction visualization? Is the inversion done in the z space, w space or the w+ space?  I am curious to see how better this method performs in terms of inverting real images in the wild. I also believe the inversion in z space allows me to appreciate more about the inversion improvement.\n\nOverall, I vote to accept this paper due to its good performance improvement over prior standard noise injection implementation. Meanwhile, I hope the theoretical analysis can be made easier to understand for a researcher that lacks the related background.\n\n[Update after reading authors' comments]\nBased on the authors' and other reviewers' comments,  I keep the score unchanged.", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115957, "tmdate": 1606915783608, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1560/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review"}}}, {"id": "55QT618mp1M", "original": null, "number": 2, "cdate": 1603878340776, "ddate": null, "tcdate": 1603878340776, "tmdate": 1606763591493, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review", "content": {"title": "Could contain useful bits but not ready for publication (but see updated part)", "review": "The paper analyzes the theoretical properties of noise injection in StyleGAN-like networks, and proposes an extension to in particular to StyleGAN2 that results in somewhat improved metric scores. Unfortunately the paper is rather confusingly written and hard to follow. To highlight what I mean, I will try to paraphrase my understanding of the paper in the following.\n\nThe theoretical treatment begins by framing the problem around optimal transport, but later seems to mostly drop this viewpoint. While the cited OT/GAN work presents interesting and relevant viewpoints about the difficulties in GAN training, I am not sure if it is particularly more relevant here than any number of other theoretical works. It may be noted that StyleGAN and DCGAN are not even formulated as to minimize a Wasserstein divergence.\n\nThe paper then coins a term \"adversarial dimension trap\", which I am not exactly sure why this terminology was chosen. The gist of the observation seems to be mostly well known, i.e. the generator can only cover a zero-measure region of the data space whereas the data is more spread out. That said, I am not thoroughly familiar with previous theoretical work on GANs and the particular formulation here may be novel. The paper then introduces a fairly general form of stochastic noise injection into the network layers and calls this fuzzy reparametrization. Here some connections to are drawn to \"fuzzy equivalence relations\" which (apparently?) are an existing concept, however as far as I see there is no citation to discuss these and little insight is given about why this is relevant.\n\nThen, the key theory is developed. If I understand correctly, the key idea here is that the zero-measure manifold is \"puffed up\" with random distributions centered around points on it, with spread that depends on the point. This makes sense as a principle but overall I am confused about whether something fundamental was discrovered or proved, or whether this is just an introduction to the reasoning behind the practical algorithms.\n\nThis then leads to a proposal of a practical algorithm. If I understand correctly, it basically generalizes the StyleGAN noise injection in such a way that not only the mean, but also the stdev of each feature is predicted by the network. This plausibly allows for more flexibility. Unfortunately the description here is again rather confusing. Apparently formulas 6-8 are not really equations, but rather some kind of imperative pseudocode with variable assignments. It would be better to spell this out as an algorithm listing. As for the content of these formulas, I am not sure if I understand what the operations or the reasoning behind them is. Why the pixsum here? Apparently it produces a single value per feature map? After this there seems that these numbers are transformed by some global matrix(?) A and bias b, however I'm not sure what the convex combination with a matrix(?) I means here given that the first half of the formula is a vector (?). Then the result seems to be normalized again (why?) And finally the means are transformed by this standard deviation? There may well be good reasons to use these steps, but they are not explained so it ends up looking like an arbitrary heuristic. Here it would be important to make a strong connection to the insights derived from theory.\n\nThe presentation is further made confusing by the language. I understand that the authors may not be native speakers, but the readability is much below the usual standard of ICLR papers and the paper would benefit from improving this.\n\nAs for the results, it does appear that there is some improvement in some of the metrics, and the proposed method may in principle be useful. It is not hard to believe that adding some extra flexibility to the noise injection might improve the results, at least in a limited number of scenarios. In this sense the paper may be on to something. \n\nWhat is the meaning of using PageRank to reduce the number of LSUN-Cat images? How is PageRank related to choosing images and what's the difference between that and just taking the first 100k pictures in the set? And for that matter, I am not sure if we learn anything from randomly limiting the set to 100k images, when we don't know how it worked for the full set. For the inversion experiments, the table in the appendix does show improvement and this may well be the case. In figure 9, though, it's hard to see much of a difference between any of the methods, perhaps in part because the images shown are very low resolution and do not correspond to anywhere near the SG2 output image size -- any differences in details are completely hidden by this.\n\nThe architecture figures 6-7 are unnecessarily low detail. They contain a black box \"FR\" node precisely at the place where you'd want to know more. Perhaps this node could be expanded into its own architecture diagram as well, given that there is no shortage of space in the appendix.\n\nIn summary the paper might contain useful bits -- this is somewhat hard to judge -- but whether or not that is the case, it is not in an acceptable condition without some significant rewriting, and I would recommend rejection at this time.\n\n_UPDATE AFTER REBUTTAL_\n\nThe authors have improved the paper somewhat by expanding and clarifying the discussion on some key parts. While I think there is still much room for improvement in the paper, the general consensus seems to be towards acceptance. I will not oppose if that is the decision, and have increased my score accordingly. However I remain very borderline and I am not sure if I am fully convinced by all the claims.\n\nOne specific issue: I think the authors should make it more clear in the paper that the experiments are done in 128 pixel resolution, in light of R1's questions. It is important that the reader be aware of this, as the noise inputs arguably become much more important in high resolutions where there is more stochastic detail. I personally did not realize this when writing my review, and now wonder how the results would be at e.g. 256 or 512 resolution. If possible I would suggest the authors still run such experiments. This also probably explains my comment above on the lack of apparent visual differences in inversion results.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115957, "tmdate": 1606915783608, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1560/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review"}}}, {"id": "6XS5lmJ26bP", "original": null, "number": 10, "cdate": 1606126832065, "ddate": null, "tcdate": 1606126832065, "tmdate": 1606126832065, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment", "content": {"title": "To all", "comment": "We just updated the new version of our submission again. We further polished our writing\u00a0in this version to make our paper more readable. We thank all the reviewers to help improve our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fgX9O5q0BT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1560/Authors|ICLR.cc/2021/Conference/Paper1560/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858351, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment"}}}, {"id": "1Z2wPrE81s1", "original": null, "number": 9, "cdate": 1606065594113, "ddate": null, "tcdate": 1606065594113, "tmdate": 1606065594113, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment", "content": {"title": "To all", "comment": "To all:\n\nWe thank all the reviewers for their helpful comments.\u00a0We have uploaded a revised version of our paper according to the suggestions of the reviewers. The revision mainly includes the following four aspects:\n\n\n1. We rewrite the proposed FR method in section 4.3 with more detailed descriptions and motivations.\n\n\n2. We add an ablation study in the appendix to support the motivations of FR implementation in section 4.3, which demonstrates the effectiveness of those procedures.\n\n\n3. We add related citations to the PageRank algorithm we used in LSUN-Cat dataset in the appendix.\n\n\n4. We add more explanation to the meaning and influence of Theorem 1 (adversarial dimension trap) to avoid possible misunderstandings.\n\n\nWe welcome any further questions or suggestions about our paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fgX9O5q0BT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1560/Authors|ICLR.cc/2021/Conference/Paper1560/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858351, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment"}}}, {"id": "n5m_rwCJyOl", "original": null, "number": 6, "cdate": 1605714969856, "ddate": null, "tcdate": 1605714969856, "tmdate": 1605751489859, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "55QT618mp1M", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment", "content": {"title": "To Reviewer #4", "comment": "To Reviewer #4:\n\nConcern1: Connection with the cited OT/GAN works.\n\nThe idea of adversarial dimension trap is in fact a generalization of the discontinuity of optimal transportation maps. The cited OT/GAN works majorly concern the cases with a Wasserstein distance. The Jenson-Shannon divergence and other loss functions are widely applied in the GAN community, which motivates us to generalize it to broader occasions without specification of a certain loss function. In Appendix A.1, we actually demonstrate the adversarial dimension trap for three different loss functions, the Wasserstein distance, Jenson-Shannon divergence, and KL divergence. Our theory is not limited to any certain form of loss functions. We are obligated to cite and emphasize the cited OT/GAN works, because they may be the first to raise this concern of discontinuous generator from a rather solid Riemannian geometry perspective.\n\nConcern2: Term of ''Adversarial dimension trap\u2019\u2019\n\nThe reviewer may misunderstand the key idea of Theorem 1. It does not mean that the generator can only learn a zero-measure set. It means that the discriminator will severely punish the intention of the generator to only learn a low-dimensional zero-measure set of the data manifold, which results in an unsmooth generator (the first case in Theorem 1), or a totally disabled discriminator and ill-conditioned generator (the second case in Theorem 1).  As it is a consequence of the adversarial game between generator and discriminator, we call it ''adversarial dimension trap''. To the best of our knowledge, this issue has not yet got widely concerned.\n\nConcern3: Citations to the fuzzy equivalence class.\n\nThere are citations to the works of fuzzy topology in lines 3-4 of page 5. Please check it.\n\nConcern4: Whether something fundamental was discovered or proved.\n\nTheorems 2&3 PROVE that a Riemannian manifold can be well approximated by the form of noise injection method, and clearly state the meanings of different parts of noise injection. The noise injection should be small in order to obtain a globally first-order approximation. The variance matrix of injected noise stands for the local geodesic chart of the manifold. These are non-trivial results as noise injection itself is not included in the input of generator, and we do not control it when sampling images. It is very important to make sure that noise injection will not damage the good learning potential of the generator. Theorem 4 further proves that the noise injected network will admit a Lipschitz continuity locally. This confirms that the proposed method can address the adversarial dimension trap.\n\nConcern 5. Eq. 6-8.\n\nThe major motivation behind eq. 6-8 is inspired by the deviation map in Fig. 1. The variance (sigma) of proposed noise injection method should model the local variance in the data manifold, which in its semantic meaning, is the detailed part of synthesis images, such as hair, parts of background, and silhouettes. The standard deviation map in Fig. 1 visualizes the deviation of sum of channels in the feature maps of StyleGAN blocks, which exactly corresponds to the semantic meaning of sigma. \u201cA\u201d is a learnable matrix which element-wisely controls the weight of contributions from each pixel of the feature map. \u201cb\u201d  is a learnable bias matrix to further adjust the bias of sigma. \u201cr\u201d is a scalar which controls the volume of injected noise as suggested in Theorems 2&3&4 to maintain locally Lipschitz property. Alpha is also a learnable scalar to explicitly regularize the structure of noise injection distribution and make sure that sigma is not ill-conditioned by adding an identity matrix to it., The matrices A and b in fact serve as a spatial attention enhancer to adjust the semantic attention in sigma. The proposed noise injection method is applied in each block of the StyleGAN2 network, thus resulting in a hierarchical adjustment to the detail information in the final outputs.\n\n\nConcern 6. Limited number of scenarios.\n\nThe FFHQ, LSUN datasets are benchmarks for style-based GAN models. We follow the state-of-the-art papers to conduct the experiment.\n\nConcern 7. PageRank in LSUN Cat.\n\nThe full LSUN-Cat dataset contains over 1 million images. The full-scale training of StyleGAN2 on it for ONE time will require 8 16G V100 GPUs for more than one week. We cannot afford such time-consuming with our limited server facilities. Thus, we have to select a small subset of it. Considering that the data variance in LSUN-Cat is considerable, we use PageRank to select a most compact subset to use, which can guarantee the success of GAN models for such data. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fgX9O5q0BT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1560/Authors|ICLR.cc/2021/Conference/Paper1560/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858351, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment"}}}, {"id": "OjOBQq4OijV", "original": null, "number": 3, "cdate": 1605713370643, "ddate": null, "tcdate": 1605713370643, "tmdate": 1605715105030, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "eaRSziRFwd0", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment", "content": {"title": "To Reviewer #3", "comment": "To Reviewer #3:\n\nConcern1: Why vanilla StyleGAN2 outperforms the proposed architecture in FID score?\n\nVisual quality of StyleGAN2 has already been very good. Generated images are so close to real ones that FID distance is not that meaningful for measuring synthesis quality. Instead, semantic consistency turns to be a dominant factor. This phenomenon is clearly revealed and analyzed by the authors of StyleGAN2 in Figures 4&13&14 and section 3 of their paper. PPL is capable of capturing such semantic consistency and the smoothness of the network, which is essential to network generalization. Therefore, PPL is more sensible for visual quality of high-fidelity images. Our proposed method promotes PPL by a large margin. In the FFHQ case, our method, even without the help of PPL regularizer, can obtain better PPL score than the StyleGAN2. \n\nIn the DCGAN cases, the synthesis quality is low, such that the FID still dominates the measurement of image quality. We then find that our method promotes both PPL and FID by large margins.\n\nThe reviewer may also consult Fig. 3 in our paper. The hair quality of synthesis images of vanilla StyleGAN2 is significantly lower than those enhanced models after it in Fig. 3. While it could still maintain the lowest FID score, we do not think this really justifies the synthesis quality.\n\nConcern2: How many more parameters compared to the additive noise?\n\nThe StyleGAN2 generator (with additive noise injection) has 24525213 parameters, while our proposed method has 24612552 parameters. Our method yields 87339 more parameters than StyleGAN2, which is about 0.3% of the total volume of StyleGAN2. Such minor increasement cannot be the main reason for those significant improvements in PPL scores and other metrics.\n\nConcern3: Ablation study of sigma.\n\nWe in fact conduct many experiments to verify the detailed form of FR. As the priority of this work is a theoretical analysis to the general GAN models, and a theoretical framework for the new design of noise injection, we did not include that part in the submitted paper. We will add those contents in the revised version in the coming days. \n\nConcern4: Motivation of the implementation of FR\n\nThe major motivation behind eq. 6-8 is inspired by the deviation map in Fig. 1. The variance (sigma) of proposed noise injection method should model the local variance in the data manifold, which in its semantic meaning, is the detailed part of synthesis images, such as hair, parts of background, and silhouettes. The standard deviation map in Fig. 1 visualizes the deviation of sum of channels in the feature maps of StyleGAN blocks, which exactly corresponds to the semantic meaning of sigma. \u201cA\u201d is a learnable matrix which element-wisely controls the weight of contributions from each pixel of the feature map. \u201cb\u201d  is a learnable bias matrix to further adjust the bias of sigma. \u201cr\u201d is a scalar which controls the volume of injected noise as suggested in Theorems 2&3&4 to maintain locally Lipschitz property. Alpha is also a learnable scalar to explicitly regularize the structure of noise injection distribution and make sure that sigma is not ill-conditioned by adding an identity matrix to it., The matrices A and b in fact serve as a spatial attention enhancer to adjust the semantic attention in sigma. The proposed noise injection method is applied in each block of the StyleGAN2 network, thus resulting in a hierarchical adjustment to the detail information in the final outputs.\n\n\nConcern5: Where is inversion done?\n\nWhile this paper is not for the principle of GAN inversion, we simply use the Image2StyleGAN method to perform the inversion in the w+ space. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fgX9O5q0BT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1560/Authors|ICLR.cc/2021/Conference/Paper1560/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858351, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment"}}}, {"id": "gt3JqezO5k", "original": null, "number": 5, "cdate": 1605713781985, "ddate": null, "tcdate": 1605713781985, "tmdate": 1605713966164, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "GRSM9ubBojE", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment", "content": {"title": "To Reviewer #1", "comment": "To Reviewer #1:\nWe use the exact implementation in the StyleGAN2 official repository. We use the same experiment settings for all models. The reviewer may note that in this paper all experiments are conducted on 128 resolution images, thus the FID scores are different from those in the StyleGAN2 paper that uses 1024 resolution to compute FID. \n\nThe resolution issue further influences the PPL score. The network depth is a primary factor in the PPL score. Recall its definition in the StyleGAN paper. the PPL score is some kind of \u2018gradient norm\u2019 with respect to the VGG network. The chain rule then implies that it will increase fast as network gets deeper. The StyleGAN2 for 1024 resolution images contains 12 more convolution layers than our 128 setting. As each convolution layer is further normalized by weight demodulation, they all maintain non-negligible gradient norms. The reviewer may also note that in Tab. 1 of StyleGAN2, the application of large networks brings a significant growth to the PPL score of StyleGAN2 on FFHQ (while the proposed method improves PPL though it introduces more parameters). These issues together result in a much larger base score of PPL. For LSUN church, the StyleGAN2 paper uses 256 resolution images, which is   closer to our 128 setting, thus the PPL score is also closer to ours.\n\nA similar difference of PPL is shown in the NIPS2019 workshop paper \u201cConditional Image Sampling by Deep Automodulators\u201d, Ari Heljakka et. al. They measure the PPL score of StyleGAN on 256 resolution CelebA-HQ dataset in Tab. 1. The PPL score is 50.08, which also admits large margin from the score on 1024 resolution FFHQ. \n\nFor recall and precision on 128 resolution FFHQ images, the plain StyleGAN2 has 0.6737 Precision score and 0.3772 Recall score; StyleGAN2 has 0.6831 Precision score and 0.3242 Recall score. The proposed FR method has 0.6670 Precison score and 0.3544 Recall score.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fgX9O5q0BT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1560/Authors|ICLR.cc/2021/Conference/Paper1560/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858351, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment"}}}, {"id": "tTv4ceREJc", "original": null, "number": 4, "cdate": 1605713432130, "ddate": null, "tcdate": 1605713432130, "tmdate": 1605713731281, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "gKHxvgSzYzI", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment", "content": {"title": "To Reviewer #2", "comment": "To Reviewer #2:\nThe major motivation behind eq. 6-8 is inspired by the deviation map in Fig. 1. The variance (sigma) of proposed noise injection method should model the local variance in the data manifold, which in its semantic meaning, is the detailed part of synthesis images, such as hair, parts of background, and silhouettes. The standard deviation map in Fig. 1 visualizes the deviation of sum of channels in the feature maps of StyleGAN blocks, which exactly corresponds to the semantic meaning of sigma. \u201cA\u201d is a learnable matrix which element-wisely controls the weight of contributions from each pixel of the feature map. \u201cb\u201d  is a learnable bias matrix to further adjust the bias of sigma. \u201cr\u201d is a scalar which controls the volume of injected noise as suggested in Theorems 2&3&4 to maintain locally Lipschitz property. Alpha is also a learnable scalar to explicitly regularize the structure of noise injection distribution and make sure that sigma is not ill-conditioned by adding an identity matrix to it., The matrices A and b in fact serve as a spatial attention enhancer to adjust the semantic attention in sigma. The proposed noise injection method is applied in each block of the StyleGAN2 network, thus resulting in a hierarchical adjustment to the detail information in the final outputs.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fgX9O5q0BT", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1560/Authors|ICLR.cc/2021/Conference/Paper1560/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858351, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Comment"}}}, {"id": "GRSM9ubBojE", "original": null, "number": 1, "cdate": 1603161384820, "ddate": null, "tcdate": 1603161384820, "tmdate": 1605024414938, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review", "content": {"title": "This work explains the properties and proposes a novel form of noise injection in GANs to reduce the adversarial dimension trap problem. Some issues in experiments need to be addressed.", "review": "Pros:\n\nThis work introduces the problem of adversarial dimension trap, which leads to punishment on the smoothness and invertibility of GANs. \n\nThis work proposes to learn fuzzy equivalence relation of the features and uses reparameterization trick to model the high-dimensional feature manifolds.\n\nA novel form of noise injection is proposed to overcome the adversarial dimension trap. Prior noise injection methods can be explained as a special case with certain hyper-parameters. This method is universal for the families of GANs, including WGAN, DCGAN, etc.\n\nExperiments on three datasets are conducted. The results of both image synthesis and GAN inversion are desirable with plausible texture details.\n\n\nCons:\n\nMy major concern is about the experiments. In Table 1, it seems that the reported FID and PPL differ from the scores reported by StyleGAN2 [1]. For the FFHQ dataset, [1] report that the FID is 3.31 (config E), but the FID of the baseline in this paper is 7.14 (the same setting). Such a huge discrepancy is wired. Recall that [1] improve the FID from 4.40 (StyleGAN v1) to 2.84, but the baseline, which should be the same, performs even worse than StyleGAN v1. The authors need to explain these contradictions.\n \nThe differences between the PPL scores reported in this paper and [1] are even more significant. I notice it is 13.05 (the best in this paper) versus 122.5 (the best in [1]). Why is the PPL about ten times better (even without the proposed method)?\n\nThe authors need to provide more details to explain how they calculated the FIDs and PPL. It seems that the authors calculate these scores in a non-standard manner. Besides, I suggest the author evaluate the Precision and Recall [2] on FFHQ. I wonder whether these metrics will be consistent.\n\n[1] Karras, Tero, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. \"Analyzing and improving the image quality of stylegan.\" CVPR, pp. 8110-8119. 2020.\n\n[2] Kynk\u00e4\u00e4nniemi, Tuomas, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. \"Improved precision and recall metric for assessing generative models.\" NeurIPS, pp. 3927-3936. 2019.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115957, "tmdate": 1606915783608, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1560/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review"}}}, {"id": "gKHxvgSzYzI", "original": null, "number": 3, "cdate": 1603896660739, "ddate": null, "tcdate": 1603896660739, "tmdate": 1605024414814, "tddate": null, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "invitation": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review", "content": {"title": "Good contribution to the GAN research field", "review": "In this paper the authors highlight two major drawbacks of GANs. 1) The optimal Generator is discontinuous and 2) the 'adversarial dimension trap' caused by the relatively lower dimension of the latent space compared to the real-world data which makes the generator not Lipschitz and/or the generator fails to capture the real-world data distribution and is not invertible. \nBoth issues could lead to an unsmooth generator.\n\nSecondly, the authors provide a form of generalization of noise injection in GANs called fuzzy reparameterization, which leads to a solution by letting the generator map onto an arbitrarily low dimensional skeleton of the feature spaces and filling up the remaining space with random noise. Therefore, the difference to the latent space dimensionality is minimized addressing issue 2). The solution consists of two stages, first a map from feature space onto the skeleton set is learned followed by noise injection adapted to the local geometry of the orginal feature manifold. \n\nExperiments:\n\nExperiments were done on FFHQ faces, LSUN objects, and CIFAR-10 datasets with models DCGAN, StyleGAN2, and bald StyleGAN2 which is StyleGAN2 without noise injection and path length regularizer. On DCGAN the proposed fuzzy reparameterization (FR) outperforms DCGAN with and without additive noise on FFHQ, CIFAR10, LSUN-Church measured with the Path Perceptual Length (PPL) and the FID. Also StyleGAN2 with FR outperforms StyleGAN2 with additive noise and bald StyleGAN2 on FFHQ and LSUN objects. To test numerical stability, condition numbers of 50000 Input, Pertubation pairs were computed for StyleGAN2 models with, without additive noise and with FR and path length regularization. StyleGAN2 + FR outperforms both on the mean and top-1000 mean condition number indicating that FR improves numerical stability. StyleGAN2 + FR also outperforms on the image inversion experiments.\n\nPros: This paper provides a theoretical framework for noise injection for GANs which is novel and interesting for the GAN community. The experimental results are extensive and convincing and support the theoretical analysis.\n\nCons: In section 4.3 the algorithm eq. 6-8 is not very clear to me. E.g. what are the parameters A,b, alpha and r and how are they motivated? PixSum is over the feature maps? A more detailed description with comments would be helpful for the reader. I could not find the FR implementation in the supplementary file, it looks like it contains only the original StyleGAN(2), DCGAN and DCGAN with additive noise models.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1560/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1560/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On Noise Injection in Generative Adversarial Networks", "authorids": ["~Ruili_Feng1", "~Deli_Zhao1", "~Zheng-Jun_Zha2"], "authors": ["Ruili Feng", "Deli Zhao", "Zheng-Jun Zha"], "keywords": ["Generative Adversarial Networks", "StyleGAN", "learning theory"], "abstract": "Noise injection is an effective way of circumventing overfitting  and enhancing generalization in machine learning, the rationale of which has been validated in deep learning as well.  Recently, noise injection exhibits surprising performance when\n  generating high-fidelity images in Generative Adversarial Networks (GANs). Despite its successful applications in GANs, the mechanism of its validity is still unclear. In this paper, we propose a geometric framework to theoretically analyze the role of noise injection in GANs. Based on Riemannian geometry, we successfully model the noise injection framework as fuzzy equivalence on geodesic normal coordinates. Guided by our theories, we find that existing methods are incomplete and a new strategy for noise injection is devised. Experiments on image generation and GAN inversion demonstrate the superiority of our method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "feng|on_noise_injection_in_generative_adversarial_networks", "supplementary_material": "/attachment/30f68e24828c7d5b9f9c13a5dc1f184f1b853038.zip", "pdf": "/pdf/d5565350cd8acb41ae5717ceeb6d81c047a668b6.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tdhZdgqtyX", "_bibtex": "@misc{\nfeng2021on,\ntitle={On Noise Injection in Generative Adversarial Networks},\nauthor={Ruili Feng and Deli Zhao and Zheng-Jun Zha},\nyear={2021},\nurl={https://openreview.net/forum?id=fgX9O5q0BT}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgX9O5q0BT", "replyto": "fgX9O5q0BT", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1560/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115957, "tmdate": 1606915783608, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1560/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1560/-/Official_Review"}}}], "count": 12}