{"notes": [{"id": "HJfxbhR9KQ", "original": "HkxDoqa5YQ", "number": 1140, "cdate": 1538087928331, "ddate": null, "tcdate": 1538087928331, "tmdate": 1545355383246, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HkeDl48bxE", "original": null, "number": 1, "cdate": 1544803310629, "ddate": null, "tcdate": 1544803310629, "tmdate": 1545354526762, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Meta_Review", "content": {"metareview": "The paper proposes an interesting idea for more effective imitation learning.  The idea is to include short actions sequences as labels (in addition to the basic actions) in imitation learning.  Results on a few Atari games demonstrate the potential of this approach.\n\nReviewers generally like the idea, think it is simple, and are encouraged by its empirical support.  That said, the work still appears somewhat preliminary in the current stage: (1) some reviewer is still in doubt about the chosen baseline; (2) empirical evidence is all in the similar set of Atari games --- how broadly is this approach applicable?", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Nice work with potential, but contributions need to be strengthened"}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1140/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352950789, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1140/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1140/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352950789}}}, {"id": "ryeDrw8qkN", "original": null, "number": 7, "cdate": 1544345407502, "ddate": null, "tcdate": 1544345407502, "tmdate": 1544346351103, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "B1g14LSq07", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "content": {"title": "Rating remains the same", "comment": "Dear authors,\n\nThank you for your clarifications and an additional comparison with a baseline using a random subset of action pairs.\n\nThe main idea of this paper is interesting. However, my major concern is still in the experimental part, as it remains unclear to me how we should use the method. Many of the hyperparameters are empirically selected and lack a systematic evaluation, e.g. the number and the length of the \"meta-actions\".\n\nYour previous response has shown the performance on action-triplets. It is surprising to me that longer \"meta-actions\" leads to worse performance, which is somewhat against the main vein of the paper: the training difficulty is not clearly described. It would be better to show the correlation between the number of available demonstrations and the length/number of \"meta-actions\" we should adopt.\n\nI'm still confused about the selection of the baseline. Again, InfoGAIL is proposed to imitate multi-modal expert demonstrations. The tasks used in the paper do not seem to be in this particular setting. GAIL [1] might be a more suitable baseline.\n\nAs a result, my rating will stay the same for now, but I encourage the authors to keep on improving the paper.\n\n[1] Jonathan Ho, Stefano Ermon. \"Generative Adversarial Imitation Learning\". In NIPS 2016."}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/AnonReviewer4"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1140/AnonReviewer4", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618370, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJfxbhR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1140/Authors|ICLR.cc/2019/Conference/Paper1140/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618370}}}, {"id": "HygO5LHq0Q", "original": null, "number": 5, "cdate": 1543292559828, "ddate": null, "tcdate": 1543292559828, "tmdate": 1543292559828, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "ByxMU9pvjm", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "content": {"title": "Thank you very much for the positive comments and suggestion about another baseline. Clarifications are given below.", "comment": "Please check the updated figures in our paper that include the comparison of a random subset of action pairs vs the most frequent action pairs (the line in magenta). The new plots strengthen our proposal that the most-frequent action pairs have useful information.\n\nQ1. Ideally, an expert should be consistent with the action pair distribution over a set of few episodes. In our analysis, we found that the frequent action pairs after 12 hrs, 13 hrs, 14 hrs and 15 hrs of training the expert network are consistent. Hence, it is evident that after training the expert network for reasonable time, the top action pairs saturate. We have made our choice more concrete by training all expert networks for 15 hrs.\n\nQ2. As mentioned in the paper, imitation learning algorithms presume that the expert information is available beforehand. We just substitute human data with a pre-trained network. Collecting traces of human data is a fast and viable but it is highly dependent on the task/game. In our case, assuming access to expert action sequences, calculating the frequency distribution and obtaining top action sequences is a trivial task with few seconds of time. \n\nQ3. Please check the new plots for random subset of action-pairs. As for adding all possible action-pairs, the action space grows exponentially, and the network must classify lot more classes with the same information. With games like FishingDerby and Asteroids (18 and 14 actions), it become too hard for network to classify hundreds of classes with same information.\n\nQ4. Action triplets are inconsistent and statistically insignificant with limited demonstration: Our focus was on using very limited (small) demonstration. The number of episodes that we use is quite small (25 episodes each with actions ranging from 700 to 7000) as we wanted very limited demonstration. We observe that with such limited demonstration, only action-pairs are reliable. The frequent action triplets after 12 hrs, 13 hrs, 14 hrs and 15 hrs of training expert network are different each time. Furthermore, for the game FishingDerby with 18 basic actions, the top 18 action pairs account for 33.85% of all the action-pairs in the 25 expert episodes. The top 18 action triplets account to just 7.36% of the all triplets in the same 25 episodes. Even for other games, we have similar discrepancy for action pairs vs triplets (DemonAttack-27.21% vs 8.87%, Asteroids-21.67% vs 6.37%, Atlantis 31.32% vs 9.61%, SpaceInvaders-28.82% vs 10.24%, BeamRider-14.78% vs 2.81%, TimePilot-15.41% vs 2.05%, Qbert-67% vs 51%). We still experimented with 3-step actions and noticed that for Atlantis, action triplets outperform action-pairs which Is great. But for other games, action-triplets perform worse than action-pairs. \n\nQ5. Thank you for the suggestion. We\u2019ll investigate search algorithms in the future to identify informative action-sequences. One class of models that we mentioned in the paper is \u2018Options Framework\u2019. The main drawback of Options Framework is that we need human designed options. Our work is a generic way of identifying options.\n\nThank you for spotting typos. We have fixed them in the latest revision."}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618370, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJfxbhR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1140/Authors|ICLR.cc/2019/Conference/Paper1140/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618370}}}, {"id": "SJljw8S90X", "original": null, "number": 4, "cdate": 1543292514886, "ddate": null, "tcdate": 1543292514886, "tmdate": 1543292514886, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "Hkgw1asmhX", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "content": {"title": "Thank you for the review. Clarifications are given below. ", "comment": "Q1. We would like to stress that our setting, also clearly mentioned in the paper at several places, is standard imitation learning setting, where access to expert information is given input to the algorithm. We do not need any GA3C training. It is a proxy to generate very few expert action sequences.  For the other imitation learning baselines, the same pretrained GA3C training is used as a proxy for expert. Hence, it is a fair comparison.\n\nQ2.  The memory advantage of our approach is quite straight forward. Out of all imitation baseline, only our method does not need to store state information at all. We only need few action sequences for ~25 episodes (each with a few 1000 integers) which takes trivially low memory. On the other hand, to store any reasonable (say 10000) state-action pairs of an expert in an environment, we will need at least 4032MB memory. Please note that each state is an image is originally 210*160*3 dimensional.\n\nQ3. As we understand, you\u2019re concerned about difference in variance when we plot episode-wise and time-wise.  Please note that we ran all the 5 runs of each game for 15 hrs. But the number of episodes in each run is different. For Atlantis game, the number of episodes range between 9114 to 10366. In our episode-wise plots, we only show the mean and variance of first 9114 episodes for each run. Hence, even though the time-wise and episode-wise plots are generated from the same output, the variance is higher for episode-wise plot. This is more glaring on Atlantis game as our idea gets much higher score than the baselines.\n\nWe believe we have answered all your questions. If you have any questions on reproducibility, we\u2019ve our code ready for release once the review period is over. Since our idea is simple and very effective, it needs more visibility so that more investigation can be made on this idea. Simplicity is the very reason why we can beat GA3C by significant margin. If the idea is not computationally simple, most likely it won\u2019t beat GA3C (a highly optimized implementation on GPUs) on running time.  We hope you will change your opinion about the overall score."}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618370, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJfxbhR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1140/Authors|ICLR.cc/2019/Conference/Paper1140/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618370}}}, {"id": "B1g14LSq07", "original": null, "number": 3, "cdate": 1543292455284, "ddate": null, "tcdate": 1543292455284, "tmdate": 1543292455284, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "BJgTCSrEp7", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "content": {"title": "Thank you for the review. Clarifications are provided below.", "comment": "Q1. Action triplets are inconsistent and statistically insignificant with limited demonstration: Our focus was on using very limited (small) demonstration. The number of episodes that we use is quite small (25 episodes each with actions ranging from 700 to 7000) as we wanted very limited demonstration. We observe that with such limited demonstration, only action-pairs are reliable. An expert should be consistent with the frequent action pairs/triplets over a set of episodes. In our analysis, we found that the frequent action pairs are consistent after 12 hrs, 13 hrs, 14 hrs and 15 hrs of training the expert network. The action pairs at different time instants in training were just permutations of each other. The same was not true for action triplets. Furthermore, for the game FishingDerby with 18 basic actions, the top 18 action pairs account for 33.85% of all the action-pairs in the 25 expert episodes. The top 18 action triplets account to just 7.36% of the all triplets in the same 25 episodes. Even for other games, we have similar discrepancy for action pairs vs triplets (DemonAttack-27.21% vs 8.87%, Asteroids-21.67% vs 6.37%, Atlantis 31.32% vs 9.61%, SpaceInvaders-28.82% vs 10.24%, BeamRider-14.78% vs 2.81%, TimePilot-15.41% vs 2.05%, Qbert-67% vs 51%). We still experimented with 3-step actions and noticed that for Atlantis, action triplets outperform action-pairs which Is great. But for other games, action-triplets perform worse than action-pairs.\n\nQ2. Thank you for the suggestion. It is an interesting exercise to interpret frequent action pairs.\n\nQ3. The memory advantage of our approach is quite straight forward. Out of all imitation baseline, only our method does not need to store state information at all. Even when we are resizing images to 84*84*4, we need several thousands of those images to get noticeable advantage when compared to having no information at all.\n\nQ4. InfoGAIL in one of the most recent techniques in Imitation Learning. Hence, we wanted to compare against InfoGAIL and ensure that we are not missing any subtleties. For our Dagger implementation, we used the simple parameter free version of beta=Indicator(i=1), i.e., 1 for the first episode and then 0 from the second episode.\n\nQ5. Thanks for the great suggestion! We were intending to explore this direction in future on continuous action spaces by binning continuous values to discrete.\n\nThank you for spotting typos, we have corrected them in the current version. Since our idea is simple and very effective, it needs more visibility so that more investigation can be made on this idea. Simplicity is the very reason why we can beat GA3C by significant margin. If the idea is not computationally simple, most likely it won\u2019t beat GA3C (a highly optimized implementation on GPUs) on running time.  We hope you will change your opinion about the overall score."}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618370, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJfxbhR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1140/Authors|ICLR.cc/2019/Conference/Paper1140/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618370}}}, {"id": "BkxqR35J07", "original": null, "number": 1, "cdate": 1542593745654, "ddate": null, "tcdate": 1542593745654, "tmdate": 1542593745654, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "content": {"title": "Additional experiments and revision", "comment": "We have added the plots for another baseline which is to choose a random subset of action pairs and append to the original action space. Please check the new plots in magenta in Figure 2. These plots strengthen our hypothesis that the frequent action pairs have useful information that random action pairs do not."}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618370, "tddate": null, "super": null, "final": null, "reply": {"forum": "HJfxbhR9KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1140/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1140/Authors|ICLR.cc/2019/Conference/Paper1140/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers", "ICLR.cc/2019/Conference/Paper1140/Authors", "ICLR.cc/2019/Conference/Paper1140/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618370}}}, {"id": "BJgTCSrEp7", "original": null, "number": 3, "cdate": 1541850581442, "ddate": null, "tcdate": 1541850581442, "tmdate": 1541850581442, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Review", "content": {"title": "need more in-depth analysis", "review": "[Summary]\n\nThis paper presents an interesting idea that to append the agent's action space with the expert's most frequent action pairs, by which the agent can perform better exploration as to achieve the same performance in a shorter time. The authors show performance gain by comparing their method with two baselines - Dagger and InfoGAIL.\n\n\n[Stengths]\n\nThe proposed method is simple yet effective, and I really like the analogy to mini-moves in sports as per the motivation section.\n\n\n[Concerns]\n\n- How to choose the number and length of the action sequences?\nThe authors empirically add the same number of expert's action sequences as the basic ones and select the length k as 2. However, no ablation studies are performed to demonstrate the sensitivity of the selected hyperparameters. Although the authors claim that \"we limit the size of meta-actions k to 2 because large action spaces may lead to poor convergence\", a more systematic evaluation is needed. How will the performance change if we add more and longer action sequences? When will the performance reach a plateau? How does it vary between different environments?\n\n- Analysis of the selected action sequences.\nIt might be better to add more analysis of the selected action sequences. What are the most frequent action pairs? How does it differ from game to game? What if the action pairs are selected in a random fashion?\n\n- Justification of the motivation\nThe major motivation of the method is to release the burden of memory overheads. However, no quantitative evaluations are provided as to justify the claim. Considering that the input images are resized to 84x84, storing them should not be particularly expensive.\n\n- The choice of baseline.\nInfoGAIL (Li et al., 2017) is proposed to identify the latent structures in the expert's demonstration, hence it is not clear to me how it suits the tasks in the paper. The paper also lacks details describing how they implemented the baselines, e.g. beta in Dagger and the length of the latent vector in InfoGAIL.\n\n- The authors only show experiments in Atari games, where the action space is discrete. It would be interesting to see if the idea can generalize to continuous action space. Is it possible to cluster the expert action sequences and form some basis for the agent to select?\n\n- Typos\n{LRR, RLR/RRL} --> {LRR, RLR, RRL}\nsclability --> scalability\nwe don't need train a ... --> we don't need to train a ...\nAtmost --> At most\n\n\n[Recommendation]\n\nThe idea presents in the paper is simple yet seemingly effective. However, the paper lacks a proper evaluation of the proposed method, and I don't think this paper is ready with the current set of experiments. I will decide my final rating based on the authors' response to the above concerns.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Review", "cdate": 1542234297007, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335881652, "tmdate": 1552335881652, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkgw1asmhX", "original": null, "number": 2, "cdate": 1540762846718, "ddate": null, "tcdate": 1540762846718, "tmdate": 1541533388253, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Review", "content": {"title": "Border line paper", "review": "The paper proposes an idea of using the most frequent expert action sequence to assist the novice, which, as claimed, has lower memory overhead than other imitation learning methodologies. The authors present comparison of their proposed method with state-of-the-art and show its superior performance. However I do have the following few questions. \n\n1. The proposed method requires a long time of GA3C training. How is that a fair comparison in Figure 2, where proposed method already has a lead over GA3C? It could be argued that it's not using all of the training outcome, but have the authors considered other form of experts and see how that works?\n\n2. The authors claimed one of the advantages of their method is reducing the memory overhead. Some supporting experiments will be more convincing.\n\n3. In Figure 3, atlantis panel, the score shows huge variance, which is not seen in Figure 2. Are they generated from the same runs? Could the authors give some explanation on the phenomenon in Figure 3?\n\nOverall, I think the paper has an interesting idea. But the above unresolved questions raises some challenge on its credibility and reproducibility. ", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Review", "cdate": 1542234297007, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335881652, "tmdate": 1552335881652, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByxMU9pvjm", "original": null, "number": 1, "cdate": 1539983945935, "ddate": null, "tcdate": 1539983945935, "tmdate": 1541533388049, "tddate": null, "forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1140/Official_Review", "content": {"title": "A well written paper with a simple, yet powerfull, idea that needs further analysis", "review": "The paper describes an imitation reinforcement learning approach where\nthe primitive actions of the agent are augmented with the most common\nsequences of actions perform by experts. It is experimentally shown\nhow this simple change has clear improvements in the performance of\nthe system in Atari games. In practice, the authors double the number\nof primitive actions with the most frequent double actions perform by\nexperts. \n\nA positive aspect of this paper comes from the simplicity of the\nidea. There are however several issues that should be taken into\naccount:\n- It is not clear how to determine when the distribution of action\n  pair saturates. This is relevant for the use of the proposed approach.\n- The total training time should consider both the initial time to\n  obtain the extra pairs of frequent actions plus the subsequent\n  training time used by the system. Either obtained from a learning\n  system (15 hours) or by collecting traces of human experts (< 1\n  hour?). \n- It would be interesting to see the performance of the system with\n  all the possible pairs of primitive actions and with a random subset\n  of these pairs, to show the benefits of choosing the most frequent\n  pairs used by the expert.\n- This analysis could be easily extended to triplets and so on, as\n  long as they are the most frequently used by experts.\n- The inclusion of macro-actions has been extensively studied in\n  search algorithms. In general, the utility of those macros depends on\n  the effectiveness of the heuristic function. Perhaps the authors\n  could revise some of the literature.\n- Choosing the most frequent pairs in all the game may not be a\n  suitable strategy. Some sequences of actions may be more frequent\n  (important) at certain stage of the game (e.g., at the beginning/end\n  of the game) and the most frequent sequences over all the game may\n  introduce additional noise in those cases.\n\nThe paper is well written and easy to follow, there are however, some\nsmall typos:\n- expert(whose => expert (whose\n% there are several places where there is no space between a word and\n% its following right parenthesis \n- don't need train => don't need to train\n- experiments4. => experiments.\n- Atmost => At most\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1140/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences", "abstract": "Imitation Learning is the task of mimicking the behavior of an expert player in a Reinforcement Learning(RL) Environment to enhance the training of a fresh agent (called novice) beginning from scratch. Most of the Reinforcement Learning environments are stochastic in nature, i.e., the state sequences that an agent may encounter usually follow a Markov Decision Process (MDP). This makes the task of mimicking difficult as it is very unlikely that a new agent may encounter same or similar state sequences as an expert. Prior research in Imitation Learning proposes various ways to learn a mapping between the states encountered and the respective actions taken by the expert while mostly being agnostic to the order in which these were performed. Most of these methods need considerable number of states-action pairs to achieve good results. We propose a simple alternative to Imitation Learning by appending the novice\u2019s action space with the frequent short action sequences that the expert has taken. This simple modification, surprisingly improves the exploration and significantly outperforms alternative approaches like Dataset Aggregation. We experiment with several popular Atari games and show significant and consistent growth in the score that the new agents achieve using just a few expert action sequences.", "keywords": ["Reinforcement Learning", "Imitation Learning", "Atari", "A3C", "GA3C"], "authorids": ["tharun.medini@rice.edu", "anshumali@rice.edu"], "authors": ["Tharun Medini", "Anshumali Shrivastava"], "TL;DR": "Appending most frequent action pairs from an expert player to a novice RL agent's action space improves the scores by huge margin.", "pdf": "/pdf/888c9ff135a94729e2061a5d2fbfc1d025acab8f.pdf", "paperhash": "medini|mimicking_actions_is_a_good_strategy_for_beginners_fast_reinforcement_learning_with_expert_action_sequences", "_bibtex": "@misc{\nmedini2019mimicking,\ntitle={Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences},\nauthor={Tharun Medini and Anshumali Shrivastava},\nyear={2019},\nurl={https://openreview.net/forum?id=HJfxbhR9KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1140/Official_Review", "cdate": 1542234297007, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HJfxbhR9KQ", "replyto": "HJfxbhR9KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1140/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335881652, "tmdate": 1552335881652, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1140/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}