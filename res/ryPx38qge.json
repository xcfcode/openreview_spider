{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396510395, "tcdate": 1486396510395, "number": 1, "id": "SyID3G8Og", "invitation": "ICLR.cc/2017/conference/-/paper325/acceptance", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The program committee appreciates the authors' response to concerns raised in the reviews. Reviewers are generally excited about the combination of predefined representations with CNN architectures, allowing the model to generalize better in the low data regime. This was an extremely borderline paper, and the PCs have determined that the paper would have needed to be further revised and should be rejected."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396512403, "id": "ICLR.cc/2017/conference/-/paper325/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryPx38qge", "replyto": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396512403}}}, {"tddate": null, "tmdate": 1484928109760, "tcdate": 1484928109760, "number": 2, "id": "rJIdNhyDx", "invitation": "ICLR.cc/2017/conference/-/paper325/official/comment", "forum": "ryPx38qge", "replyto": "Syqy0lYHe", "signatures": ["ICLR.cc/2017/conference/paper325/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper325/AnonReviewer1"], "content": {"title": "Rating update", "comment": "Thank you for your response, additional results and Appendix B, very interesting. \nI have updated my rating."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620290, "id": "ICLR.cc/2017/conference/-/paper325/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper325/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper325/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620290}}}, {"tddate": null, "tmdate": 1484927951235, "tcdate": 1482086424264, "number": 3, "id": "S1x7uU44x", "invitation": "ICLR.cc/2017/conference/-/paper325/official/review", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["ICLR.cc/2017/conference/paper325/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper325/AnonReviewer1"], "content": {"title": "Interesting work with promising first results.", "rating": "7: Good paper, accept", "review": "In this paper, the authors explicitly design geometrical structure into a CNN by combining it with a Scattering network. This aids stability and limited-data performance. The paper is well written, the contribution of combining Scattering and CNNs is novel and the results seem promising. I feel that such work was a missing piece in the Scattering literature to make it useful for practical applications.\n\nI wish the authors would have investigated the effect of the stable bottom layers with respect to adversarial examples. This can be done in a relatively straightforward way with software like cleverhans [1] or deep fool [2]. It would be very interesting if the first layer's stability in the hybrid architectures increases robustness significantly, as this would tell us that these fooling images are related to low-level geometry. Finding that this is not the case, would be very interesting as well.\n\nFurther, the proposed architecture is not evaluated on real limited data problems. This would further strengthen the improved generalization claim. However, I admit that the Cifar-100 / Cifar-10 difference already seems like a promising indicator in this regard.\n\nIf one of the two points above will be addressed in an additional experiment, I would be happy to raise my score from 6 to 7.\n\nSummary: \n\n+ An interesting approach is presented that might be useful for real-world limited data scenarios.\n+ Limited data results look promising.\n- Adversarial examples are not investigated in the experimental section.\n- No realistic small-data problem is addressed.\n\nMinor:\n- The authors should add a SOTA ResNet to Table 3, as NiN is indeed out of fashion these days.\n- Some typos: tacke, developping, learni.\n\n[1] https://arxiv.org/abs/1610.00768v3\n[2] https://arxiv.org/abs/1511.04599\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623050, "id": "ICLR.cc/2017/conference/-/paper325/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper325/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper325/AnonReviewer2", "ICLR.cc/2017/conference/paper325/AnonReviewer3", "ICLR.cc/2017/conference/paper325/AnonReviewer1"], "reply": {"forum": "ryPx38qge", "replyto": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623050}}}, {"tddate": null, "tmdate": 1483441309420, "tcdate": 1483441309420, "number": 9, "id": "SJSsEWtHx", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "Comments on the revision of the paper", "comment": "Dear reviewers,\n\nHere are two elements that I have added thanks to your constructive and helpful suggestions:\n\n- I have added a note in the Appendix B that quantifies precisely the additive perturbations of a Deep network. It proves the unstability of the hybrid deepnetwork is always smaller than the unstability of the cascaded deepnetwork and discusses the equality case. Besides, no straightforward softwares were available in Lua. Since I am convinced that without an appropriate constraints during the optimization the deep network will not perform a contraction, I decided not to investigate experimentally this question.\nHowever a scattering transform can build invariance to deformations for instance, which is not raised (to my knowledge) by the works that try to fool deepnetworks.\n\n- I have added as well state-of-the-art results on the STL10 dataset that leads to  77.4% accuracy. I would like to highlight that no specific fine tuning of the architecture was performed, and that using the scattering layers is quite straightforward.\n\nI will put in few weeks the code online on github, since it needs to be cleaned. For the potential readers of this review that might be interested in this work, we will release in few weeks a software that we scaled for Imagenet, which will be available on my website.\n\nThank you again very much for your remarks and comments that were helpful to improve this paper.\n\nBest regards,\nEO\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1483440832116, "tcdate": 1478286319251, "number": 325, "id": "ryPx38qge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryPx38qge", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "content": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 17, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483440480707, "tcdate": 1483440480707, "number": 8, "id": "HJYv-ZFHl", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "r1QNxgfVg", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "Answer to reviewer 3", "comment": "Dear reviewer,\n\nI thank you for your constructive review that was very helpful for the (hopefully) final submission of this work.\n\nThank you again for your comments and several issues you highlight. I would like to answer to each of them.\n\nThe paper does not claim that the hybrid network is strictly superior in term of generalization on the full dataset, and I do not believe this would be possible, with such a wide litterature of incremental improvements obtained via a huge process of hyper parameters fine tuning. (Or at least until the mathematical fundations are fully understood). \n\nInstead, we fix a simple architecture that was not fine tuned, since this is simply a preliminary work. The author has indeed positively investigates the question of cascading a resnet, but this is not the purpose of this paper. However, the comparison with a resnet in the limited sample situation is very interesting. It shows that the resnet architecture is really robust to limited sample situation on CIFAR datasets, but it is beaten by the hybrid architecture when really few samples are available, e.g. 2000 samples. Adapting the architecture of those deep networks to the limited sample situation (e.g. reducing the number of parameters w.r.t. the number of samples) will add another process of selecting hyper parameters that is desirable to avoid. In order to convince you that scattering networks are an enjoyable initialization, I have added a state-of-the-art result on STL10 that leads to 77.4% accuracy. STL10 is a challenging dataset with large images, 10 classes and 500 samples per class.\n\nI believe the notion of stability is not specific to the scattering litterature, yet it has mathematical foundations. Concretely talking, let us take the example of a navigation system of a car which is controlled by a deep network: for the safety of its passengers, such system should never be fooled by small additive perturbations. I believe this is a central topic to build robust systems and that it is extremely connected to understanding deep networks.\nI agree that the additive stability of the whole network is unlikely to happen, and this is proven in the Appendix B of this paper. It shows the instabilities of the hybrid network are bounded by the instabilities of the learned network. The resulting instabilities come from the cascaded and learned deep network. Such instabilities could be avoided if they were correctly handled during the optimization process, yet more work in this direction has to be adressed, and I believe a scattering transform is the first step to this kind of work, because this property is explicitly implemented at no cost. However, as developed in  the section 2.1.2, the stability to other sources (such as deformation) of variability is explicitly obtained. Unfortunately, I do not know a software that permits to quantify these and this is not the purpose of this paper\n\nFinally, I do agree that faster computation time are one of the natural outcome of this paper. While being not explicitely adressed here, in our future work, this will be a central question not only for mobile device issues but as well for processing bigger dataset such as imagenet. \n\nI thank you again for your comments, review and time that were very helpful to improve this paper.\n\nBest regards,\nEO\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1483439586435, "tcdate": 1483439586435, "number": 7, "id": "Syqy0lYHe", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "S1x7uU44x", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "Answer to reviewer 1", "comment": "Dear reviewer,\n\nI thank you for your positive and constructive review.\n\nI have added the results of the Wide Resnet in the Table 3, as well as the VGG, in the limited sample situation setting. They interestingly show  that a resnet is  extremly robust to these situations. I should add, but I will not put it in the final paper since this is not the purpose of this work, that cascading a resnet on top of a scattering network does not reduce (when the appropriate J parameter is selected) the resulting accuracy on the whole dataset. \n\nI have added a SOTA result on STL10, which leads to 77.4% accuracy. This dataset is a challenging dataset with few labeled samples available.  I agree this was a missing result, and I thank you for this suggestion. However, I believe CIFAR datasets are also interesting since one can observe the evolution of the accuracy with respect to the amount of available labeled data. Interestingly, a scattering network that does not lead to SOTA beats the resnet when 2000 samples are available.\n\nI thank you for your observation about the stability w.r.t. additive perturbations. Unfortunately, I did not use the softwares you mention since the implementations are unfortunately not done in Lua, however I believe this will not be a limitation. Indeed, in the Appendix B of this paper, I did theoritically quantify  the instabilities. One observes that the scattering is unlikely to reduce the additive instabilities that are due to the learned and cascaded deep network ; recent works seem to indicate those instabilities are always present( https://arxiv.org/pdf/1610.08401.pdf ). However a scattering transform can help to build invariance to other source of instabilities such as deformations.\n\nI thank you again very much for your helpful comments, questions, review and your time.\n\nBest regards,\nEO\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1483438836753, "tcdate": 1483438836753, "number": 6, "id": "rk6gsxtre", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "S1hvP1fVx", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "Answer to reviewer 2", "comment": "Dear reviewer,\n\nI thank you for your positive review.\n\nI complete my global answer specifically to your review, for which I thank you again. Each deepnet seems to be fooled by small valued vectors (https://arxiv.org/pdf/1610.08401.pdf ), and if no additional constraints are added during the optimization process, there is no reason that the hybrid network becomes more robust. In the Appendix B, I have added a note that shows that the amplitude of the instabilities of the hybrid network is likely to be of the order of magnitude of the instabilities of the cascaded network. Bounds are derived. However, a scattering network is stable to other source of transformation such as deformation, as developed in the paper, which means a scattering network is potentially a good initialization.\n\nThank you very much for your remarks, comments that were helpful to resubmit this paper and thank you again for your time.\n\nBest regards,\nEO\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1481928746872, "tcdate": 1481928746872, "number": 2, "id": "r1QNxgfVg", "invitation": "ICLR.cc/2017/conference/-/paper325/official/review", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["ICLR.cc/2017/conference/paper325/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper325/AnonReviewer3"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "The paper investigates a hybrid network consisting of a scattering network followed by a convolutional network. By using scattering layers, the number of parameters is reduced, and the first layers are guaranteed to be stable to deformations. Experiments show that the hybrid network achieves reasonable performance, and outperforms the network-in-network architecture in the small-data regime.\n\nI have often heard researchers ask why it is necessary to re-learn low level features of convolutional networks every time they are trained. In theory, using fixed features could save parameters and training time. As far as I am aware, this paper is the first to investigate this question. In my view, the results show that using scattering features in the bottom layers does not work as well as learned CNN features. This is not completely obvious a priori, and so the results are interesting, but I disagree with the framing that the hybrid network is superior in terms of generalization.\n\nFor the low-data regime, the hybrid network sometimes gives better accuracy than NiN, but this is quite an old architecture and its capacity has not been tuned to the dataset size. For the full dataset, the hybrid network is clearly outperformed by fully learned models. If I understood correctly, the authors have not simply compared identical architectures with and without scattering as the first layers, which further complicates drawing a conclusion.\n\nThe authors claim the hybrid network has the theoretical advantage of stability. However, only the first layers of a hybrid network will be stable, while the learned ones can still create instability. Furthermore, if potentially unstable deep networks outperform stable scattering nets and partially-stable hybrid nets, we have to question the importance of stability as defined in the theory of scattering networks. \n\nIn conclusion, I think the paper investigates a relevant question, but I am not convinced that the hybrid network really generalizes better than standard deep nets. Faster computation (at test time) could be useful e.g. in low power and mobile devices, but this aspect is not really fleshed out in the paper.\n\nMinor comments:\n-section 3.1.2: \u201clearni\u201d\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623050, "id": "ICLR.cc/2017/conference/-/paper325/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper325/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper325/AnonReviewer2", "ICLR.cc/2017/conference/paper325/AnonReviewer3", "ICLR.cc/2017/conference/paper325/AnonReviewer1"], "reply": {"forum": "ryPx38qge", "replyto": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623050}}}, {"tddate": null, "tmdate": 1481926499698, "tcdate": 1481926499698, "number": 1, "id": "S1hvP1fVx", "invitation": "ICLR.cc/2017/conference/-/paper325/official/review", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["ICLR.cc/2017/conference/paper325/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper325/AnonReviewer2"], "content": {"title": "review", "rating": "7: Good paper, accept", "review": "Thanks a lot for your detailed response and clarifications.\n\nThe paper proposes to use a scattering transform as the lower layers of a deep network. This fixed representation enjoys good geometric properties (local invariance to deformations) and can be thought as a form of regularization or prior. The top layers of the network are trained to perform a given supervised task. This is the final model can be thought as plugging a standard deep convolutional network on top of the scattering transform. Evaluation on CIFAR 10 and 100 shows that the proposed approach achieves performance competitive with high performing baselines.\n\nI find the paper very interesting. The idea of cascading these representations seems very natural thing to try. To the best of my knowledge this is the first work that combines predefined and generic representations with modern CNN architectures achieving competitive performance to high performing approaches. While the state of the art (Resnets and variants) achieves significantly higher performances, I believe that this work strongly delivers it's point.\n\nThe paper convincingly shows that lower level invariances can be obtained from analytic representations (scattering transform), simplifying the training process (using less parameters) and allowing for faster evaluation. The of the hybrid approach become crucial in the low data regime. \n\nThe author argues that with the scattering initialisation instabilities cannot occur in the first layers contrary as the operator is non-expansive. This naturally suggests that the model is more robust to adversarial examples. It would be extremely interesting to present an empirical evaluation of this task. What's the practical impact? Can this hybrid network be fooled with adversarial? If this is the case, it would render the use of scattering initialization very attractive.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512623050, "id": "ICLR.cc/2017/conference/-/paper325/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper325/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper325/AnonReviewer2", "ICLR.cc/2017/conference/paper325/AnonReviewer3", "ICLR.cc/2017/conference/paper325/AnonReviewer1"], "reply": {"forum": "ryPx38qge", "replyto": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512623050}}}, {"tddate": null, "tmdate": 1480889622203, "tcdate": 1480889622194, "number": 5, "id": "HyAGBMfXe", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "S1k_FvyXg", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "Structure", "comment": "Thank you for your comment.\n\nA CNN does not perform an aggregation, but it performs a linear operation that is covariant with the action of translation, and so do a Scattering Network. I agree that the number of dimension grows, however, the spatial size is also down-sampled at each layer. And the wavelet filter-bank cascade can be formulated as a CNN implementation. All those elements implies that the Scattering Network is a constrained CNN.\n\nThe first layers are not fully deterministic, in the sens that the parameters of a Scattering Network are cross-validated, which is a supervised learning algorithm. I totally agree it would be definitely possible to learn  mother wavelets, so that they adapt to the specific biases of a dataset, and this would be an interesting work. However, I do believe the most complicated part to understand deep networks is to define the directions of variabilities, which could correspond to groups of symmetry. Scattering Transform bears structures that might help to tackle this difficulty, by structuring the internal layers of a deep network. And this work seems to indicate it might also act as a form of regularization by incorporating knowledge of geometric variabilities.\n\nThank you again for your comment and let me know if I could clarify some points.\n\nBest,\nEO"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1480887781860, "tcdate": 1480887781850, "number": 4, "id": "H1AJRZzmx", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "HyFMMoyQl", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "Limited samples situation", "comment": "Dear reviewer,\n\nThank you for your comment!\n\nThank you for your suggestion. I agree there is a link with Spatial Transformers. A Spatial Transformer module builds an explicit transformation of the image which is estimated, in order to help to minimize the loss function. My understanding of the mechanism is that this technique implictely builds the orbits of predefined group of variabilities, and then selects a specific element along this orbit, which helps for the classification task. In some sens, it is an invariant, yet not a representation. However, in the case of a scattering network, we build a representation invariant locally along the orbit of variability, while being discriminative, thanks to wavelets that permits to recover the loss frequencies. And this is very different. Besides, the Spatial Transformer modules seem to be applied on datasets such as rotated & distorted MNIST, SVHN or GRSRB, and to remove variabilities that correspond to global elastic distorsion, global affine transformation. However, those variabilities might not be present in the CIFAR datasets. (I do not know any papers applying this technique on CIFAR10, yet I would be glad if you had some references) Besides, it requires the learning of more parameters than a scattering transform, it does not build linear invariants to variabilities and it does not correspond to an interpretable representation of signals. Yet, I agree a baseline based on this technique would be interesting to study.\n\nI answer to your questions below.\n\n\"In Section 3.1.3, when dealing with small datasets, comparisons against a NiN are presented. Is this network of the same size as the one used with the full dataset? \"\n\nYes. And the same network was used for the Hybrid Scattering CNN. It is important to keep the architectures identical for a fair comparison. I believe it is possible to find a CNN architecture that would lead to a better accuracy than the hybrid network, by reducing for instance the number of parameters. However, it would have required more engineering effort and the obtained network would not probably get a good accuracy when using the full dataset. Besides, I believe that incorporating the knowledge of roto-translation acts as a regularization whose network can benifits in a limited sample situation. This situation occurs for instance in medical imaging.\n\n\"There is a typo at the end of this paragraph, the difference with the semi-supervised GAN is larger for 1000 samples. This comparison is a bit unfair with the scattering-CNN. Maybe a good baseline here would be to pre-train a standard network on a different dataset and just fine tune the top layers \"\nYou are mentionning the sentence: \"For 1000 samples, there is a difference of 4.3% but only a difference of 2:9%with 8000samples.\" The absolute percentage corresponds to the difference between the supervised NiN and Hybrid network, I will clarify this.\nIn the case of the semi supervised GAN, observe that we are not in a limited sample situation. Indeed, only 1 000 labels are known, but the remaning 49 000 samples are all used to optimize the architecture. I will clarify this. In the case of the NiN, VGG (that does not converge) or Hybrid network, we only use 1000 samples for training, the architecture being identical to the case of 50 000 samples. The  goal is not to compare the network with a pretrained architecture, that might incorporate refined invariants since during the optimization it might have accessed to more samples, but it is to observe the degradation of the numerical accuracies in limited samples situations.\n\nThank you again for your questions and comments, and let met know if I should clarify some points.\n\nBest,\nEO"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1480729104764, "tcdate": 1480729104759, "number": 3, "id": "HyFMMoyQl", "invitation": "ICLR.cc/2017/conference/-/paper325/pre-review/question", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["ICLR.cc/2017/conference/paper325/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper325/AnonReviewer2"], "content": {"title": "Baseline", "question": "\nThe paper is very well written and it is nice to read. The idea makes a lot of sense.\nIn this first stage, a couple of comments and a question. \n\nI think that it would be interesting to also compare to spatial transformer networks. As the paper shares the same goals with a very different approach. \n\nIn Section 3.1.3, when dealing with small datasets, comparisons against a NiN are presented. Is this network of the same size as the one used with the full dataset? \n\nThere is a typo at the end of this paragraph, the difference with the semi-supervised GAN is larger for 1000 samples. This comparison is a bit unfair with the scattering-CNN. Maybe a good baseline here would be to pre-train a standard network on a different dataset and just fine tune the top layers "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959339236, "id": "ICLR.cc/2017/conference/-/paper325/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper325/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper325/AnonReviewer3", "ICLR.cc/2017/conference/paper325/AnonReviewer1", "ICLR.cc/2017/conference/paper325/AnonReviewer2"], "reply": {"forum": "ryPx38qge", "replyto": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959339236}}}, {"tddate": null, "tmdate": 1480714598642, "tcdate": 1480714598638, "number": 3, "id": "S1k_FvyXg", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "BJhqpRqfx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "CNN/Scattering comparison", "comment": "As mentioned in another comment, the topologies of the scattering network and CNN are different. In fact, in the scattering network, the number of dimension grows for each layer as you develop the network. However for a CNN there is an aggregation of the last layer filters so that the shape is always a 3D tensor. \nIn addition, filters are not multiscale at a given layer as opposed to the scattering network where there is a complete wavelet filter-bank. \nSo given that, is it really better to use fully deterministic first layers as opposed to for example letting the ANN optimize the used wavelets in a parametric estimation framework for example ?\n\nThank you.\n\nRegards."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1480701973436, "tcdate": 1480701973431, "number": 2, "id": "Hy6GuVk7l", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "BJKLoQJ7e", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "5 layers", "comment": "Dear reviewer,\n\nThank you for your comment!\n\nAlso, thank you for your question. First, I tried several architectures, shallower, and I could not achieve a good accuracy. The objective was to prove one can replace the scattering network by a CNN. However, the fact I could not train networks of depth less than 5 is not a proof that it is not possible, and I will try to give your my intuition on this question.\nIn the setting of this paper, with this implementation, a scattering transform can be interpreted as a complex deep network with depth 3 (there 3 linear operators and 2 non linearity). However, the non-linearity is really different. If a filter at the first layer of a real deepnetwork was corresponding to a real wavelet, a ReLU might create undesirable singularity (when the signal oscillates and gets negative) that might lead to artifact. To avoid this kind of artifacts, I think it is necessary to increase the depth so that the next layer can handle this instability (at the potential cost to create new ones..). Furthermore, the support of the filters of the CNN is chosen equal to 3. It means one can obtain filters (by cascading them and removing ReLUs) with support 3, 5, 7, ... etc. Yet, a Morlet wavelet has no compact support: it makes the approximation by those filters more complex and requires specific design. Consequently, one must cascade more than two convolutional layers in this case. And a non-linearity is added because usually, in deep learning litterature, people claim it helps.\n\nBest,\nEO"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1480698705168, "tcdate": 1480698705164, "number": 2, "id": "BJKLoQJ7e", "invitation": "ICLR.cc/2017/conference/-/paper325/pre-review/question", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["ICLR.cc/2017/conference/paper325/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper325/AnonReviewer1"], "content": {"title": "3.2.2 Replacement", "question": "Thank you for an interesting read!\n\nIn 3.2.2 you replace the Scatter network with a learned CNN, is there a particular reason you consider a 5 layer CNN here, why is it not the same architecture as the fixed one but with learned weights instead?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959339236, "id": "ICLR.cc/2017/conference/-/paper325/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper325/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper325/AnonReviewer3", "ICLR.cc/2017/conference/paper325/AnonReviewer1", "ICLR.cc/2017/conference/paper325/AnonReviewer2"], "reply": {"forum": "ryPx38qge", "replyto": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959339236}}}, {"tddate": null, "tmdate": 1480416660114, "tcdate": 1480416660108, "number": 1, "id": "BJhqpRqfx", "invitation": "ICLR.cc/2017/conference/-/paper325/public/comment", "forum": "ryPx38qge", "replyto": "rkb5Y59fl", "signatures": ["~Edouard_Oyallon1"], "readers": ["everyone"], "writers": ["~Edouard_Oyallon1"], "content": {"title": "Answer to anonymous reviewer 3", "comment": "Dear reviewer,\n\nThank you for your questions, I answer below to them.\n\n\"Comment: The paper claims that \"A scattering network belongs to the class of convolutional networks whose filters are predefined as wavelets\". Is this really true? The scattering network involves convolutions and non-linearities, but unlike a CNN it does not combine information from different channels in one convolution (each channel is convolved separately). It seems like this is a big difference in architecture, aside from the (non)-learnability of the filters.\"\n\nThank you for pointing out that this sentence is not clear. Indeed, there is also a final spatial averaging after each cascade of wavelets, as described in Section 2.1.1. It could be also added that it is a complex valued network. You are correct, each channel is convolved separately. This is a special case of CNNs. Indeed, as you said, a CNN can be defined as the cascade of a linear operator covariant with translation (e.g. \"different channels in one convolution\") and a point-wise non-linearity. As a cascade of wavelet transforms and modulus non-linearity, the linear operators of a scattering network are covariant with translation. In the Subsection 2.1 where the scattering transform is presented, it is written \"$W_2=W_1$\" which is a slight abuse of nations that you might have noticed.  Another paper ( http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oyallon_Deep_Roto-Translation_Scattering_2015_CVPR_paper.pdf ) refines the comparison between a CNN and a scattering network, by showing that actually a wavelet transform can be interpreted as a deep cascade of conjugate mirror filters and downsampling (e.g. $h$ and $g_\\theta$ with the notations of this paper) whose weights are shared. Those filters are analogs to the convolutionnal kernels of a CNN. I will add this reference here in order to clarify this sentence. \nFurthermore, the special variants of the translation scattering transforms such as scattering transform on the roto-translation or separable roto-translation group can recombine the channels across angles by applying a wavelet transform along angles. Those are more sophisticated architectures that were not used here.\n\n\n\"On a related note, if I remember correctly the wavelet transform as implemented in a computer produces feature maps of different spatial sizes. A CNN expects a uniform size for the input feature maps. How is this dealt with? Do you only use the smallest resolution maps (i.e. the \"top layer\" of the scattering network)?\"\n\nAs explained in the Subsection 2.1.1 and the end of the Section 2.1, a final averaging is applied at the output of the cascade of the wavelet transform. It will fix the final sampling of the output tensor accordingly to the spatial averaging, e.g. $J$. A scattering networks of order 2 outputs the 0 order ($S_0$), first order ($S_1$) and second order ($S_2$) scattering coefficients, that have the same sampling since they use the same spatial averaging. Because each scattering coefficient has the same spatial resolution, the output can be concatenated into a tensor, which is fed to a CNN. Consequently, there are 3 dimensions: two indexes for the spatial variable, and the third corresponds to the channels.\n\nThank you again for your questions and let me know if I should clarify some specific points.\n\nBest,\nEO\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287620424, "id": "ICLR.cc/2017/conference/-/paper325/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryPx38qge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper325/reviewers", "ICLR.cc/2017/conference/paper325/areachairs"], "cdate": 1485287620424}}}, {"tddate": null, "tmdate": 1480399240924, "tcdate": 1480399240920, "number": 1, "id": "rkb5Y59fl", "invitation": "ICLR.cc/2017/conference/-/paper325/pre-review/question", "forum": "ryPx38qge", "replyto": "ryPx38qge", "signatures": ["ICLR.cc/2017/conference/paper325/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper325/AnonReviewer3"], "content": {"title": "Scattering as a CNN, and downsampling", "question": "The paper claims that \"A scattering network belongs to the class of convolutional networks whose filters are predefined as wavelets\". Is this really true? The scattering network involves convolutions and non-linearities, but unlike a CNN it does not combine information from different channels in one convolution (each channel is convolved separately). It seems like this is a big difference in architecture, aside from the (non)-learnability of the filters.\n\nOn a related note, if I remember correctly the wavelet transform as implemented in a computer produces feature maps of different spatial sizes. A CNN expects a uniform size for the input feature maps. How is this dealt with? Do you only use the smallest resolution maps (i.e. the \"top layer\" of the scattering network)?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A hybrid network: Scattering and Convnet", "abstract": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks, which mean they perform as well as a deep network learned from scratch. We use scattering as a generic and fixed initialization of the first layers of a deep network, and learn the remaining layers in a supervised manner. We numerically demonstrate that deep hybrid scattering networks generalize better on small datasets than supervised deep networks. Scattering networks could help current systems to save computation time, while guaranteeing the stability  to geometric transformations and noise of the first internal layers. We also show that the learned operators explicitly build invariances to geometrical variabilities, such as local rotation and translation, by analyzing the third layer of our architecture.  We demonstrate that it is possible to replace the scattering transform by a standard deep network at the cost of having to learn more parameters and potentially adding instabilities. Finally, we release a new software, ScatWave, using GPUs for fast computations of a scattering network that is integrated in Torch. We evaluate our model on the CIFAR10, CIFAR100 and STL10 datasets.", "pdf": "/pdf/4787c58ce796ae759bfe5fb0a6d5a9603d86aa8e.pdf", "TL;DR": "This paper shows how, by combining prior and supervised representations, one can create architectures that lead to nearly state-of-the-art results on standard benchmarks.", "paperhash": "oyallon|a_hybrid_network_scattering_and_convnet", "conflicts": ["ens.fr"], "authors": ["Edouard Oyallon"], "authorids": ["edouard.oyallon@ens.fr"], "keywords": ["Computer vision", "Unsupervised Learning", "Deep learning"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959339236, "id": "ICLR.cc/2017/conference/-/paper325/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper325/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper325/AnonReviewer3", "ICLR.cc/2017/conference/paper325/AnonReviewer1", "ICLR.cc/2017/conference/paper325/AnonReviewer2"], "reply": {"forum": "ryPx38qge", "replyto": "ryPx38qge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper325/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959339236}}}], "count": 18}