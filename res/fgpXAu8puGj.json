{"notes": [{"id": "fgpXAu8puGj", "original": "4X1_WdNd6n5", "number": 672, "cdate": 1601308079783, "ddate": null, "tcdate": 1601308079783, "tmdate": 1614985625217, "tddate": null, "forum": "fgpXAu8puGj", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "NAHAS: Neural Architecture and Hardware Accelerator Search", "authorids": ["~Yanqi_Zhou1", "~Xuanyi_Dong1", "~Daiyi_Peng1", "ethanzhu@google.com", "~Amir_Yazdanbakhsh1", "~Berkin_Akin1", "~Mingxing_Tan3", "~James_Laudon1"], "authors": ["Yanqi Zhou", "Xuanyi Dong", "Daiyi Peng", "Ethan Zhu", "Amir Yazdanbakhsh", "Berkin Akin", "Mingxing Tan", "James Laudon"], "keywords": ["neural architecture search", "systems", "hardware"], "abstract": "Neural architectures and hardware accelerators have been two driving forces for the rapid progress in deep learning.\nAlthough previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly. In this paper, we study the importance of co-designing neural architectures and hardware accelerators. To this end, we propose NAHAS, an automated hardware design paradigm that jointly searches for the best configuration for both neural architecture and accelerator. In NAHAS, accelerator hardware design is conditioned on the dynamically explored neural networks for the targeted application, instead of fixed architectures, thus providing better performance opportunities. Our experiments with an industry-standard edge accelerator show that NAHAS consistently outperforms previous platform-aware neural architecture search and state-of-the-art EfficientNet on all latency targets by 0.5% - 1% ImageNet top-1 accuracy, while reducing latency by about 20%. Joint optimization reduces the search samples by 2x and reduces the latency constraint violations from 3 violations to 1 violation per 4 searches, compared to independently optimizing the two sub spaces.", "one-sentence_summary": "We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nahas_neural_architecture_and_hardware_accelerator_search", "pdf": "/pdf/4bec214a6a17e41b35876512de1c4b3976c6176b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XAINZ05nBy", "_bibtex": "@misc{\nzhou2021nahas,\ntitle={{\\{}NAHAS{\\}}: Neural Architecture and Hardware Accelerator Search},\nauthor={Yanqi Zhou and Xuanyi Dong and Daiyi Peng and Ethan Zhu and Amir Yazdanbakhsh and Berkin Akin and Mingxing Tan and James Laudon},\nyear={2021},\nurl={https://openreview.net/forum?id=fgpXAu8puGj}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "luJckO2TdF4", "original": null, "number": 1, "cdate": 1610040534580, "ddate": null, "tcdate": 1610040534580, "tmdate": 1610474144483, "tddate": null, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "invitation": "ICLR.cc/2021/Conference/Paper672/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper considers the problem of searching over the joint space of hardware and neural architectures to trade-off accuracy and latency. \n\nReviewers raised some valid questions about the following aspects:\n1. Low technical novelty\n2. Prior work on hardware and neural architecture co-design, and closely related work are not addressed\n3. Lacking details on hardware platform and discussion on physical constraints to determine invalid hardware designs (addressed somewhat, but the response is not satisfactory)\n\nOne additional comment: if we care about latency for a particular hardware platform, it is possible to automatically configure adaptive inference techniques to meet the latency constraints. \n\nOverall, my assessment is that the paper requires more work before it is ready for publication."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NAHAS: Neural Architecture and Hardware Accelerator Search", "authorids": ["~Yanqi_Zhou1", "~Xuanyi_Dong1", "~Daiyi_Peng1", "ethanzhu@google.com", "~Amir_Yazdanbakhsh1", "~Berkin_Akin1", "~Mingxing_Tan3", "~James_Laudon1"], "authors": ["Yanqi Zhou", "Xuanyi Dong", "Daiyi Peng", "Ethan Zhu", "Amir Yazdanbakhsh", "Berkin Akin", "Mingxing Tan", "James Laudon"], "keywords": ["neural architecture search", "systems", "hardware"], "abstract": "Neural architectures and hardware accelerators have been two driving forces for the rapid progress in deep learning.\nAlthough previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly. In this paper, we study the importance of co-designing neural architectures and hardware accelerators. To this end, we propose NAHAS, an automated hardware design paradigm that jointly searches for the best configuration for both neural architecture and accelerator. In NAHAS, accelerator hardware design is conditioned on the dynamically explored neural networks for the targeted application, instead of fixed architectures, thus providing better performance opportunities. Our experiments with an industry-standard edge accelerator show that NAHAS consistently outperforms previous platform-aware neural architecture search and state-of-the-art EfficientNet on all latency targets by 0.5% - 1% ImageNet top-1 accuracy, while reducing latency by about 20%. Joint optimization reduces the search samples by 2x and reduces the latency constraint violations from 3 violations to 1 violation per 4 searches, compared to independently optimizing the two sub spaces.", "one-sentence_summary": "We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nahas_neural_architecture_and_hardware_accelerator_search", "pdf": "/pdf/4bec214a6a17e41b35876512de1c4b3976c6176b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XAINZ05nBy", "_bibtex": "@misc{\nzhou2021nahas,\ntitle={{\\{}NAHAS{\\}}: Neural Architecture and Hardware Accelerator Search},\nauthor={Yanqi Zhou and Xuanyi Dong and Daiyi Peng and Ethan Zhu and Amir Yazdanbakhsh and Berkin Akin and Mingxing Tan and James Laudon},\nyear={2021},\nurl={https://openreview.net/forum?id=fgpXAu8puGj}\n}"}, "tags": [], "invitation": {"reply": {"forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040534567, "tmdate": 1610474144468, "id": "ICLR.cc/2021/Conference/Paper672/-/Decision"}}}, {"id": "8fxhY9sKzv", "original": null, "number": 4, "cdate": 1605823692192, "ddate": null, "tcdate": 1605823692192, "tmdate": 1606256309736, "tddate": null, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "invitation": "ICLR.cc/2021/Conference/Paper672/-/Official_Comment", "content": {"title": "Authors' responses to all reviewers", "comment": "We thank the reviewers for their feedback. We would like to address some common questions:\n\n**Q1.** NAHAS novelty compared to related work?\n\n**A1.** To our best knowledge, NAHAS is the first work on co-optimizing neural architectures and hardware accelerators based on industry-standard accelerators and large-scale workloads such as ImageNet. All the hardware performance metrics in our paper were generated using a cycle-accurate simulator, taking a binary code generated by a machine learning compiler. The binary code contains rich information of the program and the target device that enables a more accurate evaluation of the hardware metrics. However, all three related works recommended by Rev#2 are targeting FPGAs with smaller search spaces, where FPGAs are usually used for prototyping ideas. The compilers and target hardware are less optimized compared to an industry-standard accelerator like the NAHAS used. Therefore, our numbers presented in this paper are built on top of much stronger baselines compared to related work. In addition, none of the related work has demonstrated strong empirical results on large, realistic benchmarks like ImageNet, compared to SoTA models, such as EfficientNet. We would consider improvements over a stronger baseline much more significant, compared to related work. \n\n|               Paper              |             Hardware                    |  Large-scale Data | SoTA Accuracy  |\n|:-------------------------------:|:---------------------------------------:|:-----------------------:|:----------------------:|\n| FPGA/DNN Co-Design | FPGA                                       |              No          |   No |     \n| Best of Both Worlds      | FPGA                                       |              No          |   No |\n| NAS Meets Hardware   | FPGA                                       |              No          |   No |\n| **Our NAHAS**             | industry-standard accelerator  |       **Yes**          |  **Yes** |\n\n\n**Q2.** Should the accelerator be optimized over a range of applications?\n\n**A2.** We agree that multi-task optimization would make the hardware design more generalizable to unseen tasks and evolving workloads. However, we would like to emphasize the importance of applications using jointly optimizing a single-task. It is quite common that in a mobile systems-on-chip environment, a set of accelerators (around 20 accelerators) are designed for different workloads. Moreover, for real-time workloads like foreign object detection on autonomous driving vehicles, it is highly desirable to co-optimize the model and the hardware accelerator to meet the stringent latency objective. NAHAS has demonstrated strong empirical results in reducing latency while improving model accuracy and chip area. We can evision that in the future, the reduction of costs in chip design and manufacturing further necessitates the optimization of accelerators over narrow downed domains. This paper is the first step towards that end goal and we will investigate multi-task co-optimization in our future work\n\n**Q3.** Hardware details are not available.\n\n**A3.** We understand the reviewers\u2019 concern that more hardware details might be helpful.. However, we would like to point out that like many related work [1], the hardware accelerator used in NAHAS is a highly parameterized design that all the important tunable parameters are listed in Table 1. We include most of the important details in Section 3.3 and Section 4.1, that is essential for reproducing the results. Our method should not be limited to one particular hardware accelerator, it can be applied to any parameterized hardware design, like the hardware design in related work [1]. The proposed unified search space joining NAS and HAS, efficient oneshot search with a cost model, analysis over joint search and phase search are practical solutions and insights that should generalize to different hardware platforms. Unlike most related work, our target hardware is optimized over a set of highly competitive industry workloads and the performance simulator is validated, which makes our improvement even more significant compared to results generated on less optimized hardware.\n\n[1] Neural-Hardware Architecture Search, NeurIPS 2019 Workshop on Machine Learning for Systems. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper672/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper672/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NAHAS: Neural Architecture and Hardware Accelerator Search", "authorids": ["~Yanqi_Zhou1", "~Xuanyi_Dong1", "~Daiyi_Peng1", "ethanzhu@google.com", "~Amir_Yazdanbakhsh1", "~Berkin_Akin1", "~Mingxing_Tan3", "~James_Laudon1"], "authors": ["Yanqi Zhou", "Xuanyi Dong", "Daiyi Peng", "Ethan Zhu", "Amir Yazdanbakhsh", "Berkin Akin", "Mingxing Tan", "James Laudon"], "keywords": ["neural architecture search", "systems", "hardware"], "abstract": "Neural architectures and hardware accelerators have been two driving forces for the rapid progress in deep learning.\nAlthough previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly. In this paper, we study the importance of co-designing neural architectures and hardware accelerators. To this end, we propose NAHAS, an automated hardware design paradigm that jointly searches for the best configuration for both neural architecture and accelerator. In NAHAS, accelerator hardware design is conditioned on the dynamically explored neural networks for the targeted application, instead of fixed architectures, thus providing better performance opportunities. Our experiments with an industry-standard edge accelerator show that NAHAS consistently outperforms previous platform-aware neural architecture search and state-of-the-art EfficientNet on all latency targets by 0.5% - 1% ImageNet top-1 accuracy, while reducing latency by about 20%. Joint optimization reduces the search samples by 2x and reduces the latency constraint violations from 3 violations to 1 violation per 4 searches, compared to independently optimizing the two sub spaces.", "one-sentence_summary": "We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nahas_neural_architecture_and_hardware_accelerator_search", "pdf": "/pdf/4bec214a6a17e41b35876512de1c4b3976c6176b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XAINZ05nBy", "_bibtex": "@misc{\nzhou2021nahas,\ntitle={{\\{}NAHAS{\\}}: Neural Architecture and Hardware Accelerator Search},\nauthor={Yanqi Zhou and Xuanyi Dong and Daiyi Peng and Ethan Zhu and Amir Yazdanbakhsh and Berkin Akin and Mingxing Tan and James Laudon},\nyear={2021},\nurl={https://openreview.net/forum?id=fgpXAu8puGj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "fgpXAu8puGj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper672/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper672/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper672/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper672/Authors|ICLR.cc/2021/Conference/Paper672/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper672/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper672/-/Official_Comment"}}}, {"id": "_V8B21k1DkO", "original": null, "number": 2, "cdate": 1603909887293, "ddate": null, "tcdate": 1603909887293, "tmdate": 1605024632717, "tddate": null, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "invitation": "ICLR.cc/2021/Conference/Paper672/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "This paper proposes NAHAS for co-designing neural network architecture and hardware architecture. It shows performance improvements on ImageNet compared to previous hardware-aware NAS methods which only optimize the neural network architectures. \n\nPros:\n1. Co-designing neural network architecture and hardware architecture is a promising and important direction. \n2. The proposed method clearly outperforms hardware-aware NAS. \n\t\nCons:\n1. The technical contribution of this paper is limited. The main difference between this paper and previous hardware-aware NAS papers is that this paper has an additional hardware search space beside the neural network architecture search space. The other components, such as the training method, search algorithm, and learning objectives, are borrowed from previous papers with minor modifications. \n\n2. Co-designing neural network architecture and hardware architecture is not new. Similar ideas have been explored in [1]. The authors should discuss the difference between this paper and [1]. \n\nIn summary, co-designing neural network architecture and hardware architecture is not new. The authors do not discuss the difference between this paper and [1]. Besides, I think the technical contribution of the proposed method is limited. Therefore, I recommend rejecting this submission. \n\n[1] Neural-Hardware Architecture Search, NeurIPS 2019 Workshop on Machine Learning for Systems.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper672/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper672/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NAHAS: Neural Architecture and Hardware Accelerator Search", "authorids": ["~Yanqi_Zhou1", "~Xuanyi_Dong1", "~Daiyi_Peng1", "ethanzhu@google.com", "~Amir_Yazdanbakhsh1", "~Berkin_Akin1", "~Mingxing_Tan3", "~James_Laudon1"], "authors": ["Yanqi Zhou", "Xuanyi Dong", "Daiyi Peng", "Ethan Zhu", "Amir Yazdanbakhsh", "Berkin Akin", "Mingxing Tan", "James Laudon"], "keywords": ["neural architecture search", "systems", "hardware"], "abstract": "Neural architectures and hardware accelerators have been two driving forces for the rapid progress in deep learning.\nAlthough previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly. In this paper, we study the importance of co-designing neural architectures and hardware accelerators. To this end, we propose NAHAS, an automated hardware design paradigm that jointly searches for the best configuration for both neural architecture and accelerator. In NAHAS, accelerator hardware design is conditioned on the dynamically explored neural networks for the targeted application, instead of fixed architectures, thus providing better performance opportunities. Our experiments with an industry-standard edge accelerator show that NAHAS consistently outperforms previous platform-aware neural architecture search and state-of-the-art EfficientNet on all latency targets by 0.5% - 1% ImageNet top-1 accuracy, while reducing latency by about 20%. Joint optimization reduces the search samples by 2x and reduces the latency constraint violations from 3 violations to 1 violation per 4 searches, compared to independently optimizing the two sub spaces.", "one-sentence_summary": "We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nahas_neural_architecture_and_hardware_accelerator_search", "pdf": "/pdf/4bec214a6a17e41b35876512de1c4b3976c6176b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XAINZ05nBy", "_bibtex": "@misc{\nzhou2021nahas,\ntitle={{\\{}NAHAS{\\}}: Neural Architecture and Hardware Accelerator Search},\nauthor={Yanqi Zhou and Xuanyi Dong and Daiyi Peng and Ethan Zhu and Amir Yazdanbakhsh and Berkin Akin and Mingxing Tan and James Laudon},\nyear={2021},\nurl={https://openreview.net/forum?id=fgpXAu8puGj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper672/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137789, "tmdate": 1606915809685, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper672/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper672/-/Official_Review"}}}, {"id": "zcIcN0z4HuE", "original": null, "number": 4, "cdate": 1603996713272, "ddate": null, "tcdate": 1603996713272, "tmdate": 1605024632644, "tddate": null, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "invitation": "ICLR.cc/2021/Conference/Paper672/-/Official_Review", "content": {"title": "Good exploration with weak technical contributions.", "review": "##########################################################################\n\nSummary:\n\nThe paper proposes an algorithm to search for hardware designs and neural architectures jointly.\nThe results show that joint optimization improves accuracy and latency, reduces the search samples.\n\n##########################################################################\n\nPros:\n\n- The paper introduces a new dimension, hardware design, to the neural architecture search domain.\n- The results show that joint optimization outperforms phased optimization.\n\n##########################################################################\n\nCons:\n\n- The used techniques are not novel\n- The hypothesis \"joint optimization is better than two-phased optimization\" seems obvious.\n\n##########################################################################\n\nOther comments:\n\n1. Can you describe more of the hardware. Is it a publicly accessable platform? How long does it take to switch between different configurations?\n2. Can you add wall-clock time or used computing resource in table 3?\n3. I also have concerns about the motivation. Is it typical to design an accelerator for just one network? I think an accelerator should be optimized for at least a range of neural networks.\n\ntypo: Section 3.1 \"The objective for NAHAS it to\" -> \"The objective for NAHAS is to\"", "rating": "5: Marginally below acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper672/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper672/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NAHAS: Neural Architecture and Hardware Accelerator Search", "authorids": ["~Yanqi_Zhou1", "~Xuanyi_Dong1", "~Daiyi_Peng1", "ethanzhu@google.com", "~Amir_Yazdanbakhsh1", "~Berkin_Akin1", "~Mingxing_Tan3", "~James_Laudon1"], "authors": ["Yanqi Zhou", "Xuanyi Dong", "Daiyi Peng", "Ethan Zhu", "Amir Yazdanbakhsh", "Berkin Akin", "Mingxing Tan", "James Laudon"], "keywords": ["neural architecture search", "systems", "hardware"], "abstract": "Neural architectures and hardware accelerators have been two driving forces for the rapid progress in deep learning.\nAlthough previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly. In this paper, we study the importance of co-designing neural architectures and hardware accelerators. To this end, we propose NAHAS, an automated hardware design paradigm that jointly searches for the best configuration for both neural architecture and accelerator. In NAHAS, accelerator hardware design is conditioned on the dynamically explored neural networks for the targeted application, instead of fixed architectures, thus providing better performance opportunities. Our experiments with an industry-standard edge accelerator show that NAHAS consistently outperforms previous platform-aware neural architecture search and state-of-the-art EfficientNet on all latency targets by 0.5% - 1% ImageNet top-1 accuracy, while reducing latency by about 20%. Joint optimization reduces the search samples by 2x and reduces the latency constraint violations from 3 violations to 1 violation per 4 searches, compared to independently optimizing the two sub spaces.", "one-sentence_summary": "We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nahas_neural_architecture_and_hardware_accelerator_search", "pdf": "/pdf/4bec214a6a17e41b35876512de1c4b3976c6176b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XAINZ05nBy", "_bibtex": "@misc{\nzhou2021nahas,\ntitle={{\\{}NAHAS{\\}}: Neural Architecture and Hardware Accelerator Search},\nauthor={Yanqi Zhou and Xuanyi Dong and Daiyi Peng and Ethan Zhu and Amir Yazdanbakhsh and Berkin Akin and Mingxing Tan and James Laudon},\nyear={2021},\nurl={https://openreview.net/forum?id=fgpXAu8puGj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper672/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137789, "tmdate": 1606915809685, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper672/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper672/-/Official_Review"}}}, {"id": "Sn5Ow78BSsH", "original": null, "number": 3, "cdate": 1603911098111, "ddate": null, "tcdate": 1603911098111, "tmdate": 1605024632576, "tddate": null, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "invitation": "ICLR.cc/2021/Conference/Paper672/-/Official_Review", "content": {"title": "NAHAS review ", "review": "The authors describe an automated system for co-designing neural architectures and HW accelerators. The system is able to find the best solution under latency and chip-area constraints.  A highly parameterized (commercial) edge accelerator defines the hardware search space.  Results are compared to MnasNet, platform-aware NAS and EfficientNet.\n\nThis is an interesting area and the authors demonstrate clear advantages of their approach. \n\nA claim is made that \"although previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly.\". Does the work outlined in the \"co-design\" paragraph of section 2 not attempt to do this?  Some other examples that could have been discussed include: \n\n* \u201cFPGA/DNN co-design: An efficient design methodology for IoT intelligence on the edge\u201d, Hao et. al. DAC\u201919\n* \u201cBest of Both Worlds: AutoML Codesign of a CNN and its Hardware Accelerator\u201d, Abdelfattah et al, DAC\u201920\n* When Neural Architecture Search Meets Hardware Implementation: from Hardware Awareness to Co-Design\nhttps://ieeexplore.ieee.org/abstract/document/8839421\n\nIt is a little unclear why the papers by Jiang and Yang and the papers above are dismissed? It would be good to understand better how the author's NAS approach improves on these previous works? \n\nLimited information is provided regarding the architecture of the accelerator. What fraction of the HAS search space is unavilable due to restrictions imposed by the compiler? Perhaps these design points are all uninteresting?\n\nIs there anything to learn by a discussion of the good hardware configurations that were found? Are these surprising or unexpected? How do they differ from the baseline configuration?\n\nThe work is interesting but the claimed contributions perhaps need to be clarrified. \n\n\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper672/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper672/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NAHAS: Neural Architecture and Hardware Accelerator Search", "authorids": ["~Yanqi_Zhou1", "~Xuanyi_Dong1", "~Daiyi_Peng1", "ethanzhu@google.com", "~Amir_Yazdanbakhsh1", "~Berkin_Akin1", "~Mingxing_Tan3", "~James_Laudon1"], "authors": ["Yanqi Zhou", "Xuanyi Dong", "Daiyi Peng", "Ethan Zhu", "Amir Yazdanbakhsh", "Berkin Akin", "Mingxing Tan", "James Laudon"], "keywords": ["neural architecture search", "systems", "hardware"], "abstract": "Neural architectures and hardware accelerators have been two driving forces for the rapid progress in deep learning.\nAlthough previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly. In this paper, we study the importance of co-designing neural architectures and hardware accelerators. To this end, we propose NAHAS, an automated hardware design paradigm that jointly searches for the best configuration for both neural architecture and accelerator. In NAHAS, accelerator hardware design is conditioned on the dynamically explored neural networks for the targeted application, instead of fixed architectures, thus providing better performance opportunities. Our experiments with an industry-standard edge accelerator show that NAHAS consistently outperforms previous platform-aware neural architecture search and state-of-the-art EfficientNet on all latency targets by 0.5% - 1% ImageNet top-1 accuracy, while reducing latency by about 20%. Joint optimization reduces the search samples by 2x and reduces the latency constraint violations from 3 violations to 1 violation per 4 searches, compared to independently optimizing the two sub spaces.", "one-sentence_summary": "We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nahas_neural_architecture_and_hardware_accelerator_search", "pdf": "/pdf/4bec214a6a17e41b35876512de1c4b3976c6176b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XAINZ05nBy", "_bibtex": "@misc{\nzhou2021nahas,\ntitle={{\\{}NAHAS{\\}}: Neural Architecture and Hardware Accelerator Search},\nauthor={Yanqi Zhou and Xuanyi Dong and Daiyi Peng and Ethan Zhu and Amir Yazdanbakhsh and Berkin Akin and Mingxing Tan and James Laudon},\nyear={2021},\nurl={https://openreview.net/forum?id=fgpXAu8puGj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper672/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137789, "tmdate": 1606915809685, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper672/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper672/-/Official_Review"}}}, {"id": "qht9SH31hml", "original": null, "number": 1, "cdate": 1603690681242, "ddate": null, "tcdate": 1603690681242, "tmdate": 1605024632504, "tddate": null, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "invitation": "ICLR.cc/2021/Conference/Paper672/-/Official_Review", "content": {"title": "The paper needs to improve its clarity", "review": "##########################################################################\n\nSummary:\n\nThe paper presents NAHAS, which is a combination of Neural Architecture Search (NAS) and Hardware Architecture Search (HAS) \nfor software-hardware co-design. It uses PPO with joint search space: model accuracy and hardware constraints.\n\n##########################################################################\n\nReasons for score: \n\nThe paper claims it demonstrate effectiveness of hardware aware NAS for first time, which is dubious.\n\nThe paper should reference the industry-standard parametrized accelerator used. And reference or briefly present the in-house simulator.\n\nIn the beginning of section 3, it is unclear about the entire workflow. Explain the high-level workflow in Figure 1 caption.\n\nTested only on Imagenet, mobilnet and efficientnet. It didn't demonstrate that the method can be generalized across different the vertical stack as claimed in the introduction.\n\nIn 1. introduction, typo: Intel\u2019s Nervana\n\n##########################################################################\n\nPros: \n\n- Demosntrated improved accuracy and latency upon other work\n\n##########################################################################\n\nCons: \n\n\n- The paper needs to improve its clarity by explaining the figure workflow in section 3.\n\n- It also needs to revise the claim that it demonstrate effectiveness of hardware aware NAS for first time. E.g: NSGA-NETmultiobjective\n\n##########################################################################\n\nQuestions during rebuttal period: \n\n \nPlease address and clarify the cons above \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper672/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper672/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "NAHAS: Neural Architecture and Hardware Accelerator Search", "authorids": ["~Yanqi_Zhou1", "~Xuanyi_Dong1", "~Daiyi_Peng1", "ethanzhu@google.com", "~Amir_Yazdanbakhsh1", "~Berkin_Akin1", "~Mingxing_Tan3", "~James_Laudon1"], "authors": ["Yanqi Zhou", "Xuanyi Dong", "Daiyi Peng", "Ethan Zhu", "Amir Yazdanbakhsh", "Berkin Akin", "Mingxing Tan", "James Laudon"], "keywords": ["neural architecture search", "systems", "hardware"], "abstract": "Neural architectures and hardware accelerators have been two driving forces for the rapid progress in deep learning.\nAlthough previous works have optimized either neural architectures given fixed hardware, or hardware given fixed neural architectures, none has considered optimizing them jointly. In this paper, we study the importance of co-designing neural architectures and hardware accelerators. To this end, we propose NAHAS, an automated hardware design paradigm that jointly searches for the best configuration for both neural architecture and accelerator. In NAHAS, accelerator hardware design is conditioned on the dynamically explored neural networks for the targeted application, instead of fixed architectures, thus providing better performance opportunities. Our experiments with an industry-standard edge accelerator show that NAHAS consistently outperforms previous platform-aware neural architecture search and state-of-the-art EfficientNet on all latency targets by 0.5% - 1% ImageNet top-1 accuracy, while reducing latency by about 20%. Joint optimization reduces the search samples by 2x and reduces the latency constraint violations from 3 violations to 1 violation per 4 searches, compared to independently optimizing the two sub spaces.", "one-sentence_summary": "We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhou|nahas_neural_architecture_and_hardware_accelerator_search", "pdf": "/pdf/4bec214a6a17e41b35876512de1c4b3976c6176b.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=XAINZ05nBy", "_bibtex": "@misc{\nzhou2021nahas,\ntitle={{\\{}NAHAS{\\}}: Neural Architecture and Hardware Accelerator Search},\nauthor={Yanqi Zhou and Xuanyi Dong and Daiyi Peng and Ethan Zhu and Amir Yazdanbakhsh and Berkin Akin and Mingxing Tan and James Laudon},\nyear={2021},\nurl={https://openreview.net/forum?id=fgpXAu8puGj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "fgpXAu8puGj", "replyto": "fgpXAu8puGj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper672/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538137789, "tmdate": 1606915809685, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper672/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper672/-/Official_Review"}}}], "count": 7}