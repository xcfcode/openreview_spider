{"notes": [{"id": "pbUcKxmiM54", "original": "SDd8a4bhExT", "number": 3676, "cdate": 1601308409117, "ddate": null, "tcdate": 1601308409117, "tmdate": 1614985768755, "tddate": null, "forum": "pbUcKxmiM54", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_ZUDqplYWxp", "original": null, "number": 1, "cdate": 1610040363973, "ddate": null, "tcdate": 1610040363973, "tmdate": 1610473954339, "tddate": null, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "invitation": "ICLR.cc/2021/Conference/Paper3676/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper is not suitable for publication at ICLR. The paper contains a useful message, that neural networks are not a silver bullet, and are especially not well suited to deductive problems. However, as several reviewers pointed out, the claims of the paper are undermined by the fact that it ignores a lot of relevant work on using neural networks in the context of logic reasoning. Reviewer 2 provides a particularly useful list of relevant works on the topic. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "tags": [], "invitation": {"reply": {"forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040363959, "tmdate": 1610473954322, "id": "ICLR.cc/2021/Conference/Paper3676/-/Decision"}}}, {"id": "RZ1vM3gQnyO", "original": null, "number": 3, "cdate": 1603866313760, "ddate": null, "tcdate": 1603866313760, "tmdate": 1606626799313, "tddate": null, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "invitation": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review", "content": {"title": "Interesting analysis, but lacking details and unclear motivation", "review": "*Summary:*\nThe paper argues that deductive reasoning is an open problem in current machine learning scenarios where features are learned rather than hand-crafted. To highlight the limitations of current approaches, the paper proposes a benchmark suite of 10 simple tasks (finding the minimum, divisibility test, etc.) that are trivial with some feature engineering, but are shown to be very hard without it. Experiments are performed with random forests, neural networks (MLP?), and recurrent neural networks.\n\n*Strengths:*\n1. Important to highlight limitations of current neural network based methods. The proposed tasks are very simple for humans, but are discrete and deductive, rather than the inductive setups NNs typically work with.\n2. Experiments (although quite limited) show that recent ML approaches exhibit performance close to random.\n\n*Weaknesses:*\n1. While I agree that highlighting the drawbacks of current inductive ML approaches is important and that the proposed tasks are hard to do, I don't necessarily see the problem with small feature engineering. Almost every neural approach that is proposed has some engineering - architecture, hyperparameters, data augmentations, etc. that benefit from knowledge about the task or data. For example, CNNs trained on ImageNet use a lot of knowledge: convolutions better than standard linear layers; random crop of the image during train, 5 crops + flips at test time to further improve spatial understanding; image rotation or intensity variation as data augmentation strategies, etc.\n2. The paper has a lot of space (is only 6 pages), but does very little to explain the models. No details about the RF, NN, or RNN are mentioned. Is the NN an MLP? How many layers? What are the hidden sizes? How is the RNN used? How is the output produced, last time step hidden state? What is hidden size dimensionality? Details like this matter, and performance metrics without them do not say much.\n3. Since the paper takes the stand that feature engineering is key, it would be nice to show improved results with little feature mapping. While it seems that most tasks should be solvable, it is nice to prove that nevertheless. For example, what feature engineering strategy should be used for finding the maximum (when feature engineering already solves the task)? Or would it be enough to represent the real number as a binary sequence for the parity problem?\n\n*Overall rating:*\nWhile the premise is interesting, the work needs to be developed further and presented in much more detail than the current state. In addition, I would like to see some discussion on how some of these deductive reasoning tasks are required as part of an overall intelligent system, rather than just a set of tasks specifically built to break NNs.\n\n*Post-rebuttal*\nAll reviewers agree that this paper is not up to the mark. While the revision does include several additional related works, they are not very well integrated with the rest of the discussion on the paper. For example, how would some of these memory networks perform? How would Neural Turing Machine do? Considering this, I am hesitant to improve my rating for the paper, even if the collection of related works will certainly help in the re-submission.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3676/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3676/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071631, "tmdate": 1606915762343, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3676/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review"}}}, {"id": "Wt7FGgyoXSq", "original": null, "number": 3, "cdate": 1606263260121, "ddate": null, "tcdate": 1606263260121, "tmdate": 1606268266626, "tddate": null, "forum": "pbUcKxmiM54", "replyto": "RZ1vM3gQnyO", "invitation": "ICLR.cc/2021/Conference/Paper3676/-/Official_Comment", "content": {"title": "We have updated the manuscript keeping in mind what the reviewers suggested.", "comment": "Reviewer Comment: I don't necessarily see the problem with small feature engineering. Almost every neural approach that is proposed has some engineering - architecture, hyperparameters, data augmentations, etc. that benefit from knowledge about the task or data.\n\nResponse:  We are trying to show that feature engineering is indeed a deductive reasoning mechanism.\nWe also demonstrate that machine learning formulation is not suitable to directly operate on trivially simple deduction tasks.\nAlthough the feature itself may be easy to write code, it brings in huge cost savings in terms of network size and reliability. \n\nReviewer Comment: The paper has a lot of space (is only 6 pages), but does very little to explain the models.\n\nResponse: Our main intention is to demonstrate via simple \u2018numeric\u2019 data sets for learning deductive reasoning from examples.\nThe existing methodologies on symbolic reasoning data sets are mainly \u2018text\u2019 based where it requires the network to learn parsing in addition to inference. They are also complex, requiring to understand theorem and proof steps.\n\nReviewer Comment: Since the paper takes the stand that feature engineering is key, it would be nice to show improved results with little feature mapping.\n\nResponse: In all the problem statements (except sorting), the output itself is the feature. As this would trivially achieve a 100% accuracy or a 0 error,  we have chosen to skip including these numbers.\n\nReviewer Comment: The work needs to be developed further and presented in much more detail than the current state. In addition, I would like to see some discussion on how some of these deductive reasoning tasks are required as part of an overall intelligent system, rather than just a set of tasks specifically built to break NNs.\n\nResponse: We have added a number of citations for the symbolic reasoning efforts in the field to expand the scope of neural networks to encompass deductive reasoning.\nAlso, the notion of memory, processor and code how it is attempted to be addressed.\n\nReviewer Comment: Interesting analysis, but lacking details and unclear motivation\n\nResponse: Thanks to the reviewer for valuable inputs.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3676/Authors"], "readers": ["ICLR.cc/2021/Conference/Paper3676/Reviewers", "ICLR.cc/2021/Conference/Paper3676/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pbUcKxmiM54", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3676/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3676/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3676/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3676/Authors|ICLR.cc/2021/Conference/Paper3676/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835023, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3676/-/Official_Comment"}}}, {"id": "h-8KlJ641Bq", "original": null, "number": 2, "cdate": 1606263208982, "ddate": null, "tcdate": 1606263208982, "tmdate": 1606268206516, "tddate": null, "forum": "pbUcKxmiM54", "replyto": "PC38LFQ_z71", "invitation": "ICLR.cc/2021/Conference/Paper3676/-/Official_Comment", "content": {"title": "We have updated the manuscript keeping in mind what the reviewers suggested.", "comment": "Reviewer Comment: The paper is hard to follow at places. The main contributions are the algorithms 1-5.\n\u201cThe listings of the algorithms seem quite redundant considering the simple types of datasets one wishes to generate when this would often be achievable using mathematical formula or code \"one-liner\"\n\nResponse: Revised the manuscript to bring in more clarity. The pseudocodes are provided for reproducibility.\n\nReviewer Comment: The algorithms are also missing information about what is returned and the fonts are used inconsistently.\n\nResponse: The issue has been addressed in the revised manuscript.\n\nReviewer Comment: The experimental evaluation gives no details of the trained models.\n\nResponse: The issue has been addressed in the revised manuscript\n\nReviewer Comment: Significance and novelty seem questionable\n\nResponse: Comparison of the proposed data sets against existing symbolic reasoning sets are provided in the introduction section.\n\nReviewer Comment: Simple datasets that are hard to model using neural networks without feature engineering\n\nResponse: Thanks to the reviewer for valuable inputs.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3676/Authors"], "readers": ["ICLR.cc/2021/Conference/Paper3676/Reviewers", "ICLR.cc/2021/Conference/Paper3676/Authors", "everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "pbUcKxmiM54", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3676/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3676/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3676/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3676/Authors|ICLR.cc/2021/Conference/Paper3676/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923835023, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3676/-/Official_Comment"}}}, {"id": "p-N7Z2BZFgK", "original": null, "number": 1, "cdate": 1602620448283, "ddate": null, "tcdate": 1602620448283, "tmdate": 1605023957519, "tddate": null, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "invitation": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review", "content": {"title": "Major gaps in related work", "review": "The paper \"Simple deductive reasoning tests and data sets for exposing limitation of today's deep neural networks\" describes several datasets for deep learning to test deductive reasoning abilities of neural networks. The paper tests several neural network architectures (as well as random forests) on these datasets and concludes that neural networks are generally not able to perform deductive reasoning.\n\nMy main critique is that the authors do not seem to be aware of any of the research that is going on in the area. The opening statement of the paper is as follows: \"Learning for Deductive Reasoning is an open problem not yet explicitly called out in the machine learning world today.\" I'm afraid such a statement is simply not true. Reasoning (including deductive reasoning) is a very active research area in the machine learning communities. See below for a very partial list of works.\n\nSecond, the paper contains many assertions that neural networks are incapable of reasoning. For example: \"The deep neural networks with today\u2019s notion of a neuron are not suitable for deductive reasoning\" The main reason the authors give for this claim is that \"neurons\" perform only simple arithmetic operations. However, consider that computers only perform simple boolean operations and yet can perform the tasks described in this paper.\n\nThese claims are also contradictory to some findings in the literature, which the authors do not seem to be familiar with. Here is a bunch related work that the authors might want to take a look at. (And there are plenty more papers in the area.)\n\nDatasets with similar objectives:\n\n    Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv preprint arXiv:1804.06028, 2018.\n\n    \"ANALYSING MATHEMATICAL REASONING ABILITIES OF NEURAL MODELS\", by Saxton, Grefenstette, Hill, Kohli, ICLR 2019\n\n    \"INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving\" Wu, Jian, Ba, Grosse, https://arxiv.org/abs/2007.02924\n\n\nDatasets and theorem proving with neural networks:\n\n    \"DeepMath - Deep Sequence Models for Premise Selection\" Alemi et al. https://arxiv.org/pdf/1606.04442.pdf\n\n    \"Learning to Prove with Tactics\" Gauthier et al. 2018\n\n    \"HOList: An Environment for Machine Learning of Higher-Order Theorem Proving\", Bansal et al, ICML 2019\n\n    \"Learning to Prove Theorems via Interacting with Proof Assistants\" Yang, Deng, ICML 2019\n\n    \"GamePad: A Learning Environment for Theorem Proving\", Huang, Dhariwal, Song, Sutskever, ICLR 2018\n\n    \"Generative Language Modeling for Automated Theorem Proving\", Polu, Sutskever, arxiv 2020\n\n    \"Can Neural Networks Learn Symbolic Rewriting?\" Piotrowski et al., 2019\n\nNeural network architectures for reasoning, including (Tree)RNNs, GNNs:\n\n    \"Can Neural Networks Understand Logical Entailment?\", Evans et al. 2018 https://arxiv.org/abs/1802.08535\n\n    \"Graph Representations for Higher-Order Logic and Theorem Proving\" Paliwal et al, AAAI 2021\n\n    Also, plenty of pre-deep learning work by Joseph Urban on how to turn logical formulas into features.\n\nRecently, Transformers have been shown to be good at logical reasoning:\n\n    \"Deep Learning for Symbolic Mathematics\", Lample and Charton, ICML 2020.\n\n    \"Transformers Generalize to the Semantics of Logics\", Hahn et al, 2020. https://arxiv.org/abs/2003.04218\n\n    \"Mathematical Reasoning via Self-supervised Skip-tree Training\", Rabe et al, 2020, https://arxiv.org/abs/2006.04757\n\n\nMy third point is that the paper does not specify the experiments precisely. What are the hyperparameters of the neural networks?\n\nFourth, the paper claims to consider \"today's deep neural networks\" but does not consider modern neural architectures, such as GNNs and Transformers. These have been shown much better reasoning abilities than RNNs.\n\nIn summary: The paper addresses an important question and I encourage the authors to continue to follow this path. But this work does not consider the existing literature at all and a does not make significant contributions beyond the state-of-the-art as far as I can see.\n\n\nMinor comments:\n\n\"A majority of the machine learning models are inductive reasoning models\"\n\nI believe by \"inductive reasoning\" the authors here refer to the learning process. I think the learning phase has to be contrasted with the inference phase.\n\n\"However for the sake of convenience and interpretation, a vector is typically represented as a tensor\"\n\nThe notion of tensor is a generalization of vector.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3676/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3676/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071631, "tmdate": 1606915762343, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3676/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review"}}}, {"id": "rNdfLvwfBmR", "original": null, "number": 2, "cdate": 1603814937939, "ddate": null, "tcdate": 1603814937939, "tmdate": 1605023957445, "tddate": null, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "invitation": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review", "content": {"title": "Great work on developing the deductive reasoning test sets but ignored existing state-of-the-art models and efforts", "review": "This paper's contribution is introducing a set of tasks and datasets that require deductive approaches as opposed to common induction-based models. The paper tackles an important and interesting problem that helps to shape the future of the neuro-symbolic research area. My main concern however is, the paper ignores and does not cover the current state-of-the-art techniques and their corresponding datasets and by just introducing some datasets fail to give a correct image of the current efforts in this area. For example, the variation of Neural Turing Machine and Memory Networks has been successfully applied to the sorting problem (which has been proposed as one of the tasks of interest in deductive reasoning in this paper as well) [1], however, the authors have not discussed these class of networks at all. In fact, the authors mention the gap in the current models by talking about the need for models that can store the facts and the intermediate results for being able to conduct deductive reasoning but do not talk about the role and shortcomings of Memory Networks and Neural Turing based models or  Neural\nStacks/Queues. Similarly, there are no arguments in the paper about why Neural Theorem Provers [2] cannot be used to emulate the deductive inference mechanism. \nIn summary, the authors have initiated a good step toward defining the simple deductive reasoning tasks; However, the work has not placed well on the body of current neural and neuro-symbolic techniques, tasks, and datasets and therefore the contribution is not enough for the publication in ICLR.\n\nMinor comments:\n- 3rd sentence of the introduction needs rewriting.\n- Section 2.2: of of ---> of\n- Results: 2^5 0 ---> 2^50\n\n\n1) Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. \"Order matters: Sequence to sequence for sets.\" arXiv preprint arXiv:1511.06391 (2015).\n2) Rockt\u00e4schel, Tim, and Sebastian Riedel. \"Learning knowledge base inference with neural theorem provers.\" Proceedings of the 5th Workshop on Automated Knowledge Base Construction. 2016.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3676/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3676/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071631, "tmdate": 1606915762343, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3676/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review"}}}, {"id": "PC38LFQ_z71", "original": null, "number": 4, "cdate": 1603888317224, "ddate": null, "tcdate": 1603888317224, "tmdate": 1605023957319, "tddate": null, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "invitation": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review", "content": {"title": "Simple datasets that are hard to model using neural networks without feature engineering", "review": "This paper studies the limitations of deep neural networks to model deduction based inferences. This is done by crafting simple datasets and experimentally showing that some (details are not provided) RF, NN and RNN models fail on these.\n\nThe paper is hard to follow at places. The main contribution seems to be Algorithms 1-5, which can be used to generate 10 different dataset \"benchmarks\". The listings of the algorithms seem quite redundant considering the simple types of datasets one wishes to generate, when this would often be achievable using mathematical formula or code \"one-liner\" (the algorithms are also missing information what is returned and the fonts are used inconsistently). The experimental evaluation gives no details of the trained models.\n\nI agree with the authors, that feature engineering is very relevant when it comes to using ML models and in recent years there has been some tendency to consider neural networks as simple plug-in solutions to all scenarios. However, it seems hardly surprising that the crafted benchmarks proposed here are difficult or even impossible to learn for random forest or neural networks. I might be missing something crucial here, but the paper's contribution seems not really warrant publication.\n\nPros:\nRaising awareness that deep learning is not a plug-in solution for every occasion\n\nCons:\nSignificance and novelty seem questionable\n\nQuestions:\nPlease address and clarify the con above \n\nMinor:\n\nThis structure is repeated in several places and is hard to parse, consider clarifying it:\n\" - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisability of two numbers and divisability by 3; (d) representation (2 data sets) - binary representation and parity.\"\n\nRegarding the statement \"However to the best of our knowledge and exploration, today there is no RNN formulation which is meant to learn facts, unification and deductive inferences.\", have the authors checked the recent approaches to use deep learning to learn to solve combinatorial problems (e.g., SAT, CSPs) and using GNNs. This is a currently very active area of research that might be interesting to the authors, see e.g., \nhttps://arxiv.org/abs/1905.13211\nhttps://arxiv.org/abs/1905.12149\nhttps://arxiv.org/abs/1904.01557\nhttps://link.springer.com/chapter/10.1007/978-3-319-98334-9_38\nhttps://openreview.net/forum?id=BJxgz2R9t7\nhttps://openreview.net/forum?id=HJMC_iA5tm\nfor some recent examples.\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3676/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3676/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks", "authorids": ["~Kalidas_Yeturu1", "~Manish_Kumar_Srivastava1"], "authors": ["Kalidas Yeturu", "Manish Kumar Srivastava"], "keywords": ["inductive reasoning", "deductive reasoning", "neural network", "memory", "feature engineering"], "abstract": "Learning for Deductive Reasoning is an open problem in the machine learning world today. \nDeductive reasoning involves storing facts in memory and generation of newer facts over time. \nThe concept of memory, processor and code in deduction systems is fundamentally different from the purpose and formulation of weights in a deep neural network. \nA majority of the machine learning models are inductive reasoning models including state of the art deep neural networks which are effectively tensor interpolation based models.\nA step towards realization of memory is through recurrent neural networks and its variants, however the formal representation is not sufficient enough to capture a complex mapping function between input and output patterns.\nDeep neural networks are positioned to do away with feature engineering which is essentially deductive reasoning methodology.\nThere are existing works in deductive reasoning in neural networks that require learning of syntax, unification and deduction and operate on text data as sequence of tokens.\nHowever the performance of deductive reasoning networks is far from perfection which may be either due to syntax or deduction aspects.\nIn this context, we have proposed a suite of completely numeric data sets which do not require parsing as with text data.\nThe 10 data sets are for - (a) selection (3 data sets) - minimum, maximum and top 2nd element in an array of numbers; (b) matching (3 data sets) - duplicate detection, counting and histogram learning; (c) divisibility tests (2 data sets) - divisibility of two numbers and divisibility by 3; (d) representation (2 data sets) - binary representation and parity. \nThough extremely simple in terms of feature engineering, in all of these tests, simple deep neural networks, random forest and recurrent neural networks have failed with very low accuracies. \nWe propose these as numerical test-bed for testing learning models for deductive reasoning.", "one-sentence_summary": "Ten simple tests and data sets on which today's deep neural networks fail and call for algorithms for learning deductive reasoning", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "yeturu|simple_deductive_reasoning_tests_and_numerical_data_sets_for_exposing_limitation_of_todays_deep_neural_networks", "pdf": "/pdf/4dd67cbfbc1aaa50668623d9ec4cdb864bf1d362.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=SF4YHV6fH_", "_bibtex": "@misc{\nyeturu2021simple,\ntitle={Simple deductive reasoning tests and numerical data sets for exposing limitation of today's deep neural networks},\nauthor={Kalidas Yeturu and Manish Kumar Srivastava},\nyear={2021},\nurl={https://openreview.net/forum?id=pbUcKxmiM54}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "pbUcKxmiM54", "replyto": "pbUcKxmiM54", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3676/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538071631, "tmdate": 1606915762343, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3676/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3676/-/Official_Review"}}}], "count": 8}