{"notes": [{"id": "r1x0lxrFPS", "original": "B1gVLAJtPr", "number": 2118, "cdate": 1569439733989, "ddate": null, "tcdate": 1569439733989, "tmdate": 1583912033204, "tddate": null, "forum": "r1x0lxrFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "fYXG9tn1c6", "original": null, "number": 11, "cdate": 1578632508035, "ddate": null, "tcdate": 1578632508035, "tmdate": 1578632508035, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "HJgBJh3KuS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Updated code link", "comment": "We moved the source code to a github repository. Please find the code at the following link:\nhttps://github.com/Hyungjun-K1m/BinaryDuo"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "bKgiICH2iO", "original": null, "number": 1, "cdate": 1576798741004, "ddate": null, "tcdate": 1576798741004, "tmdate": 1576800895219, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "Three reviewers suggest acceptance. Reviewers were impressed by the thoroughness of the author response. Please take reviewer comments into account in the camera ready. Congratulations!", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719938, "tmdate": 1576800270675, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Decision"}}}, {"id": "HJgd6tglcS", "original": null, "number": 2, "cdate": 1571977664122, "ddate": null, "tcdate": 1571977664122, "tmdate": 1576562166034, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper proposes a new measure of gradient mismatch for training binary networks, and additionally proposes a method for getting better performance out of binary networks by initializing them to behave like a ternary network.\n\nI found the new measure of gradient deviation fairly underdeveloped, and I suspect the method of converting ternary activations into binary activations works for a different reason than that proposed by the authors.\n\nThere were English language issues that somewhat reduced clarity, though the intended meaning was always understandable.\n\nDetailed comments:\n\n\"Binary Neural Network (BNN) has been gaining interest thanks to its computing cost reduction and memory saving.\" --> \"Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings.\" (will stop making English language corrections from here on)\n\n\"Therefore, we argue that the sharp accuracy drop for the binary activation stems from the inefficient training method, not the capacity of the model.\"\nThis could also be due to poor initialization in the binary case. e.g., it might make sense to initialize the binary network with bias=-0.5, so that the nonlinearity has a kink at pre-activation=0, rather than pre-activation=0.5.\n\n\"Unfortunately, it is not possible to measure the amount of gradient mismatch directly because the\ntrue gradient of a quantized activation function is zero almost everywhere. \" It *is* possible to measure the mismatch to the true gradient exactly. One could even train using the true gradient. It's just that the true gradient is useless.\n\nFig 1b -- this is a nice baseline.\n\n\"the steepest descent direction, which is the direction toward the point with the smallest loss at given distance\"\nThis is not the usual definition of steepest descent direction. If you're going to redefine this, should do so mathematically and precisely (for instance, you are going to run into trouble with the word \"distance\", since your coordinate discrete gradient more closely resembles an L\\infty-ball perturbation, rather than an L2-ball perturbation.\n\neq. 3:\nNote that this equation is equivalent to taking the true gradient of a function which has been boxcar-smoothed along each parameter. This may more closely resemble existing measures of deviation than you like.\n\nYou should also consider the relationship to an evolutionary strategies style gradient estimate, which similarly provides an unbiased gradient estimate for a smoothed function, and which allows that estimate to be computed with fewer samples (at the cost of higher error).\n\nSec. 4.2 / Figure 3:\nThe results in this section will be *highly* sensitive to the choice of epsilon. You should discuss this, specify the epsilon used, and experimentally explore the dependence on epsilon.\n\n\"The results indicate that the cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better than previous approaches. \"\nDon't know that I followed this. Gradient mismatch is never formally defined, so it's hard to know what this says about its relationship. Additionally, CDG sounds more like something which is correlated with, rather than an explanation for, performance.\n\n\" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \" --> \" cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better \"\n\n\"we shift the bias of BN layer which comes right before the activation function layer. \"\nDid you try using these bias values without pre-training as a ternary network? I suspect it would work just as well!\n\n\"Please note that BN layers followed by binary activation layer can be merged to the threshold of the binary activation layer, incurring no overhead at inference stage.\"\nDid not understand this.\n\n\"it is expected that the fine-tuning increases the accuracy even further\"\nDoes it improve the accuracy further? Should state this as result, not prediction, and should have an ablation experiment showing this.\n\n\"Table 2 shows the validation accuracy of BNN in various schemes.\"\nWhy not test accuracy?\n\nFigure 6:\nWhat are the filled circles?\nWhat was the sampling grid for the HP search? The images have high spatial frequency structure that I suspect is an artifact of the interpolation function, rather than in the data.\n\n----\n\nUpdate post-rebuttal:\n\nThe authors have addressed the majority of my concerns, through both text changes and significant additional experiments. I am therefore increasing my score. Thank you for your hard work!", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576647262124, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Reviewers"], "noninvitees": [], "tcdate": 1570237727452, "tmdate": 1576647262138, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Review"}}}, {"id": "HyemulP_sB", "original": null, "number": 4, "cdate": 1573576810739, "ddate": null, "tcdate": 1573576810739, "tmdate": 1573629232452, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "rJgT-SZ0tB", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (part 1)", "comment": "Thank you very much for your constructive comments. We could improve the quality of our manuscript thanks to your comments. Here are our responses.\n\n\nQ1: One main concern is that since the computation of the decoupled binary model and the coupled ternary model are the same, why does the decoupled binary model can finally to tuned to perform better than the original ternary model? Is there any intuition or theoretical explanation?\n\nA1: When a ternary model is decoupled, a weight is divided into two identical weights as shown in Figure 4 (right). Before fine-tuning starts, the two weights have the same value, which is half of the original weight value. However, once fine-tuning begins, the two weights are allowed to have different values with backpropagation because they are connected to separate neurons with different threshold values after decoupling. Therefore, it becomes possible to find better minima by letting them independently be updated during the fine-training process.\n\n\nQ2: Yet another concern is that ternary activation basically can be viewed as binary+sparse activations, can it be even more computationally cheaper than the decoupled binary activation?\n\nA2: As reviewer rightfully suggested, a ternary activation can be viewed as a binary+sparse activations and bitwise binary operations can be applied to compute it. However, we think that such an approach is more expensive to compute than the decoupled binary activation. Let us elaborate the detail process to assess the computing overhead.\n\nLet $Y$ be a dot-product result of a ternary input activation vector ($X$) and a binary weight vector ($W$) with length of n each. $X \\in \\{-1,0,1\\}$ has to be encoded to a 2-bit binary number: for example, $\\{10, 00, 11\\}$.\nLet us call the higher bit of the 2-bit input which indicates whether the number is zero or not as $X^{MSB}$, and the other bit as $X^{LSB}$. Computations for ternary activation consist of 3 steps as follows.\n\nStep 1: To utilize bitwise operation for computing ternary activation, we first need to count the number of inputs that are not zero. \n    (1)    $POPCNT(X^{MSB}) = m$\n\nStep 2: Then, we need to mask zero inputs and pack non-zero inputs which will participate in bitwise computation.\n    (2)    $X^{LSB} \\in {0,1}^n \u21d2 X^{\\emptyset,LSB} \\in {0,1}^m, W \u21d2 W^{\\emptyset}$\nTo pair the non-zero value inputs with corresponding weights, index information for the non-zero values needs to be kept. Note that the number of bits to store index information is not negligible compared to the bits required to store the ternary input values. In addition, it takes substantial time to find the matching pair of non-zero input and corresponding weight.\n\nStep 3: Lastly, with XNOR_POPCNT operation of $X^{\\emptyset,LSB}$ and $W^{\\emptyset}$, we can derive the desired output Y as follows. \n    (3)    $Y = 2*{\\text{XNOR_POPCNT}(X^{\\emptyset,LSB}, W^{\\emptyset})}-m$\n\nIn summary, computing ternary activation with bitwise operations requires the extra index information for 0-input value because 0-input values need to be excluded from the bitwise operation. While the overhead of indexing process is manageable in high-bit precision case, it becomes non-negligible when input has 1-bit or 2-bit precision since the amount of extra index information becomes comparable with the amount of original input data. Therefore, we believe that computing ternary activation as binary+sparse activations is computationally more expensive than the decoupled binary activation.\n \nOn the other hand, the cost of conventional computing for ternary activation is actually lower than the cost of computing ternary activation which is regarded as binary+sparse activations.\nIn fact, the cost of conventional computing for ternary activation is computationally same as that of computing decoupled binary activation. In this case, the ternary input $X \\in \\{-1, 0, 1\\}$ may be encoded to a 2-bit binary number $\\{00, 01, 11\\}$, in which the number of `1's indicates the relative order of the numbers. With such encoding, we can derive the desired output Y as follows.\n$Y = 2*{\\text{XNOR_POPCNT}(X^{MSB},W)+\\text{XNOR_POPCNT}(X^{LSB},W)}-n$\n\nIn this case, the cost of computing ternary activation is twice as that of computing binary activation (XNOR_POPCNT). Since decoupling basically splits each ternary activation into two separate binary activations, compute cost of coupled ternary model and that of decoupled binary model are the same. Therefore, we think that cost of computing ternary activation is not cheaper than computing corresponding decoupled binary activation.\nMeanwhile, the accuracy of the decoupled binary model is higher than that of the coupled ternary model after fine-tuning. In conclusion, the decoupled binary model can achieve higher accuracy with the same amount of compute cost as the coupled ternary model.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "HygLemDdiS", "original": null, "number": 5, "cdate": 1573577454495, "ddate": null, "tcdate": 1573577454495, "tmdate": 1573578958711, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "HJgd6tglcS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Response to Reviewer #4 (part 4)", "comment": "\nQ11: \"it is expected that the fine-tuning increases the accuracy even further\" Does it improve the accuracy further? Should state this as result, not prediction, and should have an ablation experiment showing this.\n\nA11: We agree to state this as result rather than prediction. Our experimental results (Figure 6 in Section 6.2) indeed show that the fine-tuning procedure actually increases the accuracy further. Please note that decoupling the ternary model without the fine-tuning does not change computation results of the network. Therefore, the accuracy of the decoupled binary model is same as that of the coupled ternary model when fine-tuning is not applied. For example, in case of VGG-7 on CIFAR-10 dataset, the accuracy of decoupled binary model without fine-tuning is 89.69% which is same as that of the coupled ternary model (shown in Section 6.1). In contrast, during the fine-tuning process, the weight for each of the decoupled binary network is tuned separately. The results for VGG7 on CIFAR-10 dataset shows that 0.75% of accuracy improvement can be achieved by fine-tuning which results in 90.44% accuracy (shown in Section 6.1 again).\n\nThe improvement was also observed on ImageNet dataset. Accuracy results before and after fine-tuning for AlexNet, ResNet-18, and ResNet-18(+sc) are shown below.\n=================================================================\n|\t\t\t\t\t|\tBefore\tfine-tuning\t|\t  After fine-tuning\t|\n|          Network\t\t|    Top-1(%)\t|    Top-5(%)\t|    Top-1(%)\t|    Top-5(%)\t|\n---------------------------------------------------------------------------------------------------------\n|           AlexNet\t\t|        50.7\t|        74.4\t|        52.7\t|        76.0\t|\n|        ResNet-18\t\t|        58.8\t|        81.3\t|        60.4\t|        82.3\t|\n|     ResNet-18(+sc)\t|        59.1\t|        81.3\t|         60.9\t|        82.6\t|\n=================================================================\n\nTo reflect the update, we revised the sentence as follows.\n\u201cexperimental results show that the fine-tuning increases the accuracy even further. Detailed experimental results will be discussed in the next section.\u201d \n\n\n\nQ12: \"Table 2 shows the validation accuracy of BNN in various schemes.\" Why not test accuracy?\n\nA12: Table 2 shows the validation accuracy of BNN in various schemes on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset. ImageNet dataset has 1.28M images for training set and 50K images for validation set. Because the test set is not available to the public, (the test set is privately used for the ImageNet Large Scale Visual Recognition Challenge competition only), all the results of previous work in Table 2 are validation accuracy. For fair comparison with previous results, we also provided validation accuracy for our scheme.\nIf required, it is possible to split the provided training set to train/valid sets and to use the provided validation set as a test set. However, the test accuracy from the experiment cannot be fairly compared with other results. Nevertheless, if reviewer asks for the test accuracy, we are willing to conduct addition training and report the test accuracy. Please let us know.\n\n\n\nQ13: Figure 6: What are the filled circles? What was the sampling grid for the HP search? The images have high spatial frequency structure that I suspect is an artifact of the interpolation function, rather than in the data.\n\nA13: We apologize for lack of detailed description of the figure. We mistakenly moved the information along with other details to the Appendix A while we made the original submission fit to 8-page limit.\nFor the experiment on CIFAR-10 dataset, we tried 13 weight decay values (from 1e-6 to 1e-2) and 10 different initial learning rates (from 1e-4 to 1e-1). Therefore, 130 different data are plotted in each contour plot. Since our goal for hyper-parameter search is to ensure that we are not with completely wrong hyper-parameters, we believe that this amount of sampling grid is large enough for our experiment. The 5 circles represent top 5 test accuracy points and the red circle is for the best result.\n\nWe updated the caption of Figure 6 as follows.\n\u201cFigure 6: Training results of VGG-7 on CIFAR-10 dataset in the order of coupled ternary model, decoupled binary model and decoupled binary model trained from scratch (left). 5 circles represent the top 5 test accuracy points and the red circle is for the best result. The best accuracy result is shown at the top left corner of each contour plot. Test accuracy of various models with different activation precision and training schemes (right).\u201d\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "ByeFTEDOir", "original": null, "number": 6, "cdate": 1573577920858, "ddate": null, "tcdate": 1573577920858, "tmdate": 1573578947529, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "HJgd6tglcS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Response to Reviewer #4 (part 3)", "comment": "\nQ8: \"The results indicate that the cosine similarity between coarse gradient and CDG can explain the relationship between gradient mismatch and performance of model better than previous approaches. \" Don't know that I followed this. Gradient mismatch is never formally defined, so it's hard to know what this says about its relationship. Additionally, CDG sounds more like something which is correlated with, rather than an explanation for, performance.\n\nA8: We agree that CDG is correlated with performance rather than giving an explanation for performance. So we revised the above sentences as follows.\n\"The results indicate that the cosine similarity between coarse gradient and CDG can provide better correlation with the performance of a model than previous approaches do. \"\n \n\n\nQ9: \"we shift the bias of BN layer which comes right before the activation function layer. \" Did you try using these bias values without pre-training as a ternary network? I suspect it would work just as well!\n\nA9: Per reviewer\u2019s suggestion, we conducted additional experiments in which, instead of using ternary pretrained model as initialization, we initialized the decoupled binary model with the shifted bias values used in section 5.1. For fair comparison, we tried the same amount of hyper-parameter search as that for the case shown in Figure 6. The results show that initializing the decoupled binary model with shifted bias still shows lower accuracy than using pre-trained coupled ternary model as initialization while it achieves a small increase in accuracy compared to initializing the model with zero bias values.\nComparison among various training results of different schemes are shown below.\n=====================================================================\nScheme\t\t\t\t\t\t\t\t\t\t\t\t|     Best Accuracy\t|\n----------------------------------------------------------------------------------------------------------------\nBaseline binary\t\t\t\t\t\t\t\t\t\t|\t\t89.07%\t\t|\nCoupled ternary\t\t\t\t\t\t\t\t\t\t|\t\t89.69%\t\t|\nDecoupled binary with ternary initialization\t\t\t\t|\t\t90.44%\t\t|\nDecoupled binary from scratch\t\t\t\t\t\t|\t\t88.93%\t\t|\nDecoupled binary from scratch with shifted bias values\t|\t\t89.21%\t\t|\n=====================================================================\n\n\nQ10: \"Please note that BN layers followed by binary activation layer can be merged to the threshold of the binary activation layer, incurring no overhead at inference stage.\" Did not understand this.\n\nA10:  In BNN inference, computations for Batch-Normalization (BN) and the binary activation can be merged as a function which compares the weighted-sum value with a threshold [R1]. Therefore, modulating the BN bias with different values as in Eq. 5 does not incur additional overhead at inference stage. \nTo avoid confusion, we revised the sentence as follows.\n\n\u201cPlease note that computations for BN  and the binary activation can be merged to a thresholding function. Therefore, calculating BNs with different bias values does not incur additional overhead at inference stage.\u201d\n\nWe described the merging process in detail for your information below.  \n\nLet $X$ be a weighted-sum vector. Applying batch-normalization on $X$ results in $Y$ as in Eq. R1.\n\n$Y = \\gamma*(X-\\mu)/\\sigma+\\beta$            (Eq. R1)\n\nHere, $\\gamma$,$\\beta$,$\\mu$, and $\\sigma$ denote for weight, bias, mean, and standard deviation of the batch-normalization layer after training is finished. Then, the batch-normalization output $Y$ goes through binary activation function producing a binary output $Z$.\n\n$Z = +1 (\\text{if}\\;\\;  Y \\geq 0),  -1 (\\text{else})$\t\t\t  (Eq. R2)\n\nHere we formulate the binary activation function as the sign function for concise expression, but the same development is possible with different binary activation functions without loss of generality. At inference stage, Eq. R1 and Eq. R2 can be merged as Eq. R3  since batch-normalization parameters are fixed.\n\n$Z = +1 (\\text{if} \\;\\;  X \\geq \\mu-\\beta*\\sigma/\\gamma),  -1 (\\text{else})$\t\t(Eq. R3)\n\nIn this way, batch-normalization can be merged to binary activation layer, incurring no overhead at inference stage.\nFurthermore, scaling factor for weights can also be merged to the binary activation function similar to the batch-normalization. \nThis method has already been used in many previous works on BNNs [R1,R2].\n\n[R1] Umuroglu, Yaman, et al. \"Finn: A framework for fast, scalable binarized neural network inference.\" Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays. ACM, 2017.\n[R2] Liu, Zechun, et al. \"Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm.\" Proceedings of the European Conference on Computer Vision (ECCV). 2018.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "BkeuMLwujS", "original": null, "number": 7, "cdate": 1573578256327, "ddate": null, "tcdate": 1573578256327, "tmdate": 1573578928566, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "HJgd6tglcS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Response to Reviewer #4 (part 2)", "comment": "\nQ6: eq. 3: Note that this equation is equivalent to taking the true gradient of a function which has been boxcar-smoothed along each parameter. This may more closely resemble existing measures of deviation than you like. You should also consider the relationship to an evolutionary strategies style gradient estimate, which similarly provides an unbiased gradient estimate for a smoothed function, and which allows that estimate to be computed with fewer samples (at the cost of higher error).\n\nA6: Thank you very much for pointing out this important point. We agree that the CDG is equivalent to the gradient of the smoothed loss function. We think their equivalence can provide better explanation about the theoretical background of CDG. Therefore, we revised section 4.1 to introduce CDG with theoretical background based on the smoothed loss function. Please note that our intention was to provide a proper gradient measure as an alternative to the true gradient for gradient mismatch estimation.\nIn that respect, we think that the basic philosophy of evolutionary strategy-style (ES) gradient estimator is very similar to that of our CDG (Eq. 3) since the ES gradient estimator also provides (estimated) gradient for a smoothed function as the reviewer correctly described. We were not aware of the ES gradient estimate when writing the manuscript, and after a quick survey of the literature for the ES gradient estimate, we believe that ES gradient estimator can be another good candidate for the quantitative assessment of the gradient mismatch.\n\nTo verify, we measured the cosine similarity between the ES gradient estimator and the coarse gradient and observed that the results show a similar trend with the case using CDG (Figure 15 in Appendix H). Most notably, the ES gradient estimator-based assessment also shows that there is a large gap in the cosine similarity between the ternary activation case and the binary activation case with any STEs.\n\nDue to the time limitation in rebuttal period, we cannot thoroughly assess the relative strengths and weaknesses of the CDG estimator-based approach compared to the ES gradient estimator-based approach. However, we plan to continue to study the two approaches as the comparative assessment of the two approaches may open up an opportunity to find more sophisticated and practical gradient mismatch assessment methodology for quantized activation neural network. \n\nIn this study, our main goal for the derivation of CDG was to have a solid justification to apply ternary activation neural network to train binary activation neural network, so the similar results from another approach further strengthens our main motivation for developing the BinaryDuo scheme. \n\nTo reflect the update, we revised the manuscript as follows.\n\n    - First, we changed the title of section 4.1 to \u201cGradient of Smoothed Loss Function\u201d from \u201cCoordinate Discrete Gradient\u201d. We also updated section 4.1 to introduce CDG with theoretical background based on the smoothed loss function.\n    - We replaced the term \u201cCDG\u201d with \u201cgradient of smoothed loss function\u201d in the abstract and conclusion. \n    - We updated Eq. 3 to explain the equivalence of  the gradient of smoothed loss function and CDG. \n    - At the end of section 4.1, we added the following sentence: \u201cNote that the evolutionary strategy-style gradient estimation is another good candidate which can be used for the same purpose as it provides an unbiased gradient estimate for a smoothed function in a similar way (Choromanski et al., 2018). Please refer to Appendix H for more information.\u201d\n    - At the end of section 4.2, we added the following sentence: \u201cWe also measured the cosine similarity between the estimated gradient using evolutionary strategies and the coarse gradient. The results show a similar trend to that of the results using CDG (Figure 15 in Appendix H). Most notably, the evolutionary strategy-based gradient mismatch assessment also shows that there is a large gap in the cosine similarity between the ternary activation case and the binary activation case with any STEs.\u201d\n\nWe thank the reviewer again to help us to generalize and strengthen the gradient estimate flow to provide the strong justification to develop the BinaryDuo scheme.\n\n\n\nQ7: Sec. 4.2 / Figure 3: The results in this section will be *highly* sensitive to the choice of epsilon. You should discuss this, specify the epsilon used, and experimentally explore the dependence on epsilon.\n\nA7: We measured the cosine similarities for various epsilon values (epsilon = 1e-4 to 1e-2) and the results show that overall trend is maintained regardless of the epsilon value although the absolute values change depending on the epsilon value.  Detailed experimental results with various epsilon values are given in Appendix C.1).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "SJg8uvvusr", "original": null, "number": 8, "cdate": 1573578606291, "ddate": null, "tcdate": 1573578606291, "tmdate": 1573578916913, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "HJgd6tglcS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Response to Reviewer #4 (part 1)", "comment": "Thank you very much for your constructive comments. We could improve the quality of our work significantly by responding to your comments.\n\n\nQ1: \"Binary Neural Network (BNN) has been gaining interest thanks to its computing cost reduction and memory saving.\" --> \"Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings.\" (will stop making English language corrections from here on)\n\nA1: Thanks very much for pointing out grammatical errors in our writing. We updated some sentences including the above one  in the revised manuscript. We will do our best to improve the writing in the final manuscript.\n\n\n\nQ2: \"Therefore, we argue that the sharp accuracy drop for the binary activation stems from the inefficient training method, not the capacity of the model.\" This could also be due to poor initialization in the binary case. e.g., it might make sense to initialize the binary network with bias=-0.5, so that the nonlinearity has a kink at pre-activation=0, rather than pre-activation=0.5.  \n\nA2:  Per reviewer\u2019s suggestion, we conducted more experiment on CIFAR-10 dataset by initializing the binary network with bias=0.5 so that the nonlinearity has a kink at pre-activation=0. Please note that bias=0.5 instead of -0.5 needs to be used to make the nonlinearity have a kink at pre-activation=0 for the current activation function because each pre-activation needs to be shifted by 0.5 after the bias value is added. \nWe trained four different networks with different width factors (x1, x1.25, x1.5 and x2) and each network was trained for 4 runs and the mean results are reported below.\n---------------------------------------------------------------------------------------\n|\t\t|\t\t\t\t\twidth factor\t\t\t\t\t|\n|   Bias\t|\tx1\t\t|\tx1.25\t|\tx1.5\t\t|\tx2\t\t| \n---------------------------------------------------------------------------------------\n|     0\t|\t89.07\t|\t89.22\t|\t89.41\t|\t89.60\t|\n|     0.5\t|\t88.96\t|\t89.32\t|\t89.58\t|\t89.69\t|\n---------------------------------------------------------------------------------------\nAs shown in the table above, we observe that initializing with bias=0.5 does not significantly improve the results.\nIn addition, the sharp accuracy drop for the binary activation was also observed in many previous works where symmetric signum function was used for binary activation function (e.g. ABC-Net as shown in Table 1 in our paper) for which nonlinearity has a kink at pre-activation=0. Therefore, we believe that using bias value of 0 is not the reason for sharp accuracy drop. \n\n\n\nQ3: \"Unfortunately, it is not possible to measure the amount of gradient mismatch directly because the true gradient of a quantized activation function is zero almost everywhere. \" It *is* possible to measure the mismatch to the true gradient exactly. One could even train using the true gradient. It's just that the true gradient is useless.\n\nA3: We agree that it is possible to calculate the true gradient. As the reviewer mentioned, the main point should have been that the measured results will not be \u201cuseful\u201d because the value will be zero almost everywhere. We replaced the word \u201cpossible\u201d with \u201cuseful\u201d in the revised draft as follows.\n\u201cUnfortunately, it is not useful to measure the amount of gradient mismatch directly because the true gradient of a quantized activation function is zero almost everywhere.\"\n \n\n\nQ4: Fig 1b -- this is a nice baseline.\n\nA4: Thanks very much for your encouraging comments.\n\n\n\nQ5: \"the steepest descent direction, which is the direction toward the point with the smallest loss at given distance\" This is not the usual definition of steepest descent direction. If you're going to redefine this, should do so mathematically and precisely (for instance, you are going to run into trouble with the word \"distance\", since your coordinate discrete gradient more closely resembles an L\\infty-ball perturbation, rather than an L2-ball perturbation.\n\nA5: To get rid of the confusion over the terminology, we decided not to use the term \u201csteepest descent direction\u201d  and updated the sentence in the revised manuscript as follows.\n\n(original) \u201cSince the true gradient of quantized activation network is zero almost everywhere, we cannot use the true gradient to find the steepest descent direction, which is the direction toward the point with the smallest loss at given distance.\u201d\n\n(revised) \u201cSince the true gradient of quantized activation network is zero almost everywhere, using the value of the true gradient does not provide a useful measure of the gradient mismatch problem.\u201c\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "Skga_kvOiS", "original": null, "number": 3, "cdate": 1573576565202, "ddate": null, "tcdate": 1573576565202, "tmdate": 1573578821212, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "rJgT-SZ0tB", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Response to Reviewer #1 (part 2)", "comment": "\nQ3: One line below eq (2), does STE mean the estimated gradient? How can the difference be calculated based on different things (i.e., activations and gradients)?\n\nA3: After reading reviewer\u2019s question, we noticed that there might be a confusion over the terminology \u2018STE\u2019. Therefore, we updated our manuscript to clarify our intention as follows.\nWe use the term \u2018STE\u2019 to indicate the derivative of the approximation of the binary activation function used at backward pass. For example, derivative of HardTanh function was used as STE in Courbariaux et al. (2016). In Figure 2, g\u2019(x) is for STE.\nWe call the presumed activation function which is used at backward pass as differentiable approximation of binary activation function. For example, HardTanh or SwishSign is one of the differentiable approximations of the binary activation function. In Figure 2 and Eq. 2, g(x) is for the differentiable approximation of the binary activation function.\nTherefore, STE (g\u2019(x)) is the derivative of the differentiable approximation of binary activation function (g(x)).\nThe cumulative difference in Eq. 2 was used to measure the difference between the actual binary activation function and its differentiable approximation. Yellow area in Fig.2 will help understanding Eq.2 graphically.\n\nWe thank the reviewer for pointing this out and helping us to improve our manuscript for better understanding.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "HklEl_vusS", "original": null, "number": 9, "cdate": 1573578731685, "ddate": null, "tcdate": 1573578731685, "tmdate": 1573578731685, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "H1ldnneeiH", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you very much for your constructive comments. We could improve the quality of our work significantly by responding to your comments. Below, we summarize your questions and address them in order.\n\n\nQ1: The motivation is clear. The 1-bit activation networks usually deteriorates the performance greatly.\n\nA1: Thank you very much for the positive comment.\n \n\n\nQ2: The gradient mismatch for discrete variable did bring difficult for optimization. Do you mean 1-bit activation has larger gradient mismatch than other bits, at least in the defined cosine similarity by this paper?\n\nA2: Yes. We think that 1-bit activation has larger gradient mismatch than other bit cases because the cosine similarity between the gradient estimate and the coarse gradient shows a large difference between ternary and 1-bit activation cases while similar cosine similarity values are exhibited for 1-bit activation cases with various STEs (Fig. 3)\n\n\n\nQ3: As to Eq. 3, Appendix C.1 describes the way to choose step size. I understand the logic, but for the detailed method, is it cross-validation with grid search or some other tricks?\n\nA3: We measured the cosine similarities for various epsilon values (epsilon = 1e-4 to 1e-2) and the results show that overall trend is maintained regardless of the epsilon value although the absolute values change depending on the epsilon value.  Detailed experimental results with various epsilon values have been added in Appendix C.1.\n \n\n\nQ4. Is there any relation between the decoupling method in Section 5 and the proposed estimated gradient mismatch in Section 4.2?\n\nA4: The results from gradient mismatch analysis confirm that there is a large accuracy gap between ternary activation neural network and binary activation neural network with any STEs. The results gave us strong justification to apply ternary activation neural network to train binary activation neural network, which is a key idea in the decoupling method used in the proposed BinaryDuo scheme.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}, {"id": "H1ldnneeiH", "original": null, "number": 3, "cdate": 1573026992071, "ddate": null, "tcdate": 1573026992071, "tmdate": 1573026992071, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "To be honest, I only read several papers on the neural network quantization. I am not familiar with this research topic, so I  provide my judgement based on my own limited knowledge rather than thorough comparison with other related works.\n\n1. The motivation is clear. The 1-bit activation networks usually deteriorates the performance greatly.\n2. The gradient mismatch for discrete variable did bring difficult for optimization. Do you mean 1-bit activation has larger gradient mismatch than other bits, at least in the defined cosine similarity by this paper?\n3. As to Eq(3), Appendix C.1 describes the way to choose step size. I understand the logic, but for the detailed method, is it cross-validation with grid search or some other tricks?\n4. Is there any relation between the decoupling method in Section 5 and the proposed estimated gradient mismatch in Section 4.2?", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576647262124, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Reviewers"], "noninvitees": [], "tcdate": 1570237727452, "tmdate": 1576647262138, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Review"}}}, {"id": "rJgT-SZ0tB", "original": null, "number": 1, "cdate": 1571849477187, "ddate": null, "tcdate": 1571849477187, "tmdate": 1572972380793, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies activation quantization in deep networks. The authors first compare the coordinate discrete gradient and those obtained by various kinds of straight-through estimators, and found 1-bit activation networks have much poorer gradient estimation than 2-bit ones. Thus they speculate that this explains the poorer performance of 1-bit activation networks than 2-bit ones. To utilize higher precision of activation, the authors then propose to decouple a ternary activation into two binary ones, and achieve competitive results on typical image classification data sets CIFAR-10 and ImageNet.\n\nThe paper is overall well-written and easy to follow. The decoupling method is simple and straightforward. The experiments are also well conducted. One main concern is that since the computation of the decoupled binary model and the coupled ternary model are the same, why does the decoupled binary model can finally to tuned to perform better than the original ternary model? Is there any intuition or theoretical explanation? Yet another concern is that ternary activation basically can be viewed as binary+sparse activations, can it be even more computationally cheaper than the decoupled binary activation?\n\nQuestion:\n1. One line below eq (2), does STE mean the estimated gradient? How can the difference be calculated based on different things (i.e., activations and gradients)?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576647262124, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Reviewers"], "noninvitees": [], "tcdate": 1570237727452, "tmdate": 1576647262138, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Review"}}}, {"id": "HJgBJh3KuS", "original": null, "number": 2, "cdate": 1570520028834, "ddate": null, "tcdate": 1570520028834, "tmdate": 1570520028834, "tddate": null, "forum": "r1x0lxrFPS", "replyto": "r1x0lxrFPS", "invitation": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment", "content": {"comment": "As the ICLR policy strongly recommends the code submission, we prepared an anonymized link for our code. The code can be accessed in the following link: https://drive.google.com/open?id=1NxZdaSB7gZPMVH35hqp1xaqZ7ilwVAtD", "title": "code submission"}, "signatures": ["ICLR.cc/2020/Conference/Paper2118/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["hyungjun.kim@postech.ac.kr", "kyungsu.kim@postech.ac.kr", "jinseok.kim@postech.ac.kr", "jaejoon@postech.ac.kr"], "title": "BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations", "authors": ["Hyungjun Kim", "Kyungsu Kim", "Jinseok Kim", "Jae-Joon Kim"], "pdf": "/pdf/d2650d092434446c1f01a83f6ccdebefd03bfdd9.pdf", "abstract": "Binary Neural Networks (BNNs) have been garnering interest thanks to their compute cost reduction and memory savings. However, BNNs suffer from performance degradation mainly due to the gradient mismatch caused by binarizing activations. Previous works tried to address the gradient mismatch problem by reducing the discrepancy between activation functions used at forward pass and its differentiable approximation used at backward pass, which is an indirect measure. In this work, we use the gradient of smoothed loss function to better estimate the gradient mismatch in quantized neural network. Analysis using the gradient mismatch estimator indicates that using higher precision for activation is more effective than modifying the differentiable approximation of activation function. Based on the observation, we propose a new training scheme for binary activation networks called BinaryDuo in which two binary activations are coupled into a ternary activation during training. Experimental results show that BinaryDuo outperforms state-of-the-art BNNs on various benchmarks with the same amount of parameters and computing cost.", "keywords": [], "paperhash": "kim|binaryduo_reducing_gradient_mismatch_in_binary_activation_network_by_coupling_binary_activations", "code": "https://github.com/Hyungjun-K1m/BinaryDuo", "_bibtex": "@inproceedings{\nKim2020BinaryDuo:,\ntitle={BinaryDuo: Reducing Gradient Mismatch in Binary Activation Network by Coupling Binary Activations},\nauthor={Hyungjun Kim and Kyungsu Kim and Jinseok Kim and Jae-Joon Kim},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=r1x0lxrFPS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/970d1c6bba9943d568a4fac36e42e522cf180c6a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1x0lxrFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2118/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2118/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2118/Authors|ICLR.cc/2020/Conference/Paper2118/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146065, "tmdate": 1576860544657, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2118/Authors", "ICLR.cc/2020/Conference/Paper2118/Reviewers", "ICLR.cc/2020/Conference/Paper2118/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2118/-/Official_Comment"}}}], "count": 14}