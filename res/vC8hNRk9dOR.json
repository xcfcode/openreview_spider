{"notes": [{"id": "vC8hNRk9dOR", "original": "P9apC5r2og4", "number": 856, "cdate": 1601308098540, "ddate": null, "tcdate": 1601308098540, "tmdate": 1614985734854, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "utCDn-hwUx3", "original": null, "number": 1, "cdate": 1610040403426, "ddate": null, "tcdate": 1610040403426, "tmdate": 1610473999643, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The initial reviews were mixed for this paper. On one hand, some of the reviewers highlighted that the proposed datasets could be useful to researchers. On the other, reviewers found a few important flaws with the current manuscript including missing baselines, issues with the proposed tasks, and possibly inaccurate/imprecise statements.\n\nOur discussion after the author's response focussed on whether the positives aspects of the current paper outweighed some of the perceived weaknesses of the paper. In particular, while some of the initial criticisms from the reviewers were successfully addressed by the authors (including possible imprecisions and to a certain extent motivation), all the reviewers remained convinced that standard continual learning baselines could be adapted to this setting. They also conjectured that these missing baselines might not allow readers to appreciate the strength of the proposed datasets. \n\nIn their response, the authors argued that adapting models would require research. The reviewers are under the impression that it would be useful to test baselines more or less \"as-is\" even if the authors do not think these baselines will be competitive. For example, in the discussion, a reviewer suggested that \"an experience replay baseline could [...] have been implemented\" where the replay buffer includes the hidden states of an LSTM. It might also be useful to study baselines that do not strictly obey the proposed setting, again to get a better understanding of the proposed tasks (including how difficult it is).  Overall, having some of these baselines would be one way to better connect the proposed work to the current continual-learning literature. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040403413, "tmdate": 1610473999627, "id": "ICLR.cc/2021/Conference/Paper856/-/Decision"}}}, {"id": "-a8GcVlBEcN", "original": null, "number": 5, "cdate": 1606230069488, "ddate": null, "tcdate": 1606230069488, "tmdate": 1606230069488, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "1xNlev0BOds", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment", "content": {"title": "authors' response", "comment": "We thank the reviewer for the detailed review. We appreciate the fact that the reviewer recognizes the qualities of the presented datasets, and of the complementary analysis. \nWhile we agree that it would be nice to have comparative results to other online continual learning algorithms, one of the main problems is that as current methods are designed for learning with single independent data instances, like in the case of labelled images. (We now highlight more explicitly this common assumption in the introduction.) Therefore, they assume, on the one hand, that a handful of examples can well characterize the data region that is not to be forgotten. On the other hand, every example is assumed to be independent of its previous context. Both of these assumptions are violated in the online continual language modelling benchmark that we are proposing. Furthemore, EWC assumes that after training on one task, we have found an optimal set of weights that we do not want to forget. In our case, however, the model will be exposed multiple times to the same underlying language/domain. Therefore, it would be completely unsurprising if EWC is not a good method for this setting. Plus, on top of requiring explicit labels it also needs a second pass over each example to compute the Fisher information, which is incompatible with the online setting. We can further think of other ways of trying to adapt existing methods to this setting, but as we argue these adaptations are not at all trivial, and represent research questions on their own right. Here, instead we focus on introducing a novel way of evaluating Online Continual Learning methods, which we hope would spur research on a more naturalistic setting that has been so far completely neglected.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vC8hNRk9dOR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper856/Authors|ICLR.cc/2021/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment"}}}, {"id": "k02ih-qQMit", "original": null, "number": 4, "cdate": 1606229945836, "ddate": null, "tcdate": 1606229945836, "tmdate": 1606229945836, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "MRKIunxT0M", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment", "content": {"title": "authors' response", "comment": "We wholeheartedly thank the reviewer for such a thoughtful review. It was refreshing to read the positive aspects of the paper laid out in a way that legitimately reflect the reviewer's own appreciation of the work. Furthermore, the negative aspects have served us to refine our arguments on the revised submission (mostly, the abstract and introduction), for which we are also thankful. To address these latter remarks:\n\n1. We are not sure of whether this means that the construction of the dataset is incremental, or even the whole notion of tackling the construction of this dataset is incremental. While the former could be true, the fact that Online Continual Learning continues to be focused on settings lacking all the desired features that we discuss shows that it is not incremental with respect to the state of the art, as it argues for a very different approach from the one that is prevalent when evaluating these systems.\n2. True. We do not fully grasp however why this is a reason for rejection.\n3. The issue here is not that the current models fail. The problem is that for most cases they cannot even be applied to a data stream with temporal dependencies. The reason is that the standard setting is one on which each example is self-contained and independent of the previous history. However, for language modelling, capturing temporal dependencies is crucial, but no model that we are aware of even has a notion of context that would allow us to apply it in our setting. Consider applying a model like GEM, which relies on storing memories. How should we decide to chunk the data stream? What shall we use as the hidden vector? All these decisions are non-trivial and research questions on their own. \n\nRegarding the questions, we cannot apply Masson d'Autume model for the same reasons of point 3 above. Namely, that model was conceived for sentence-classification / QA datasets, where each training instance is independent from each other. This should probably also address your second question. \n\nRegarding the additional feedback, we hope the revised version does a better job at establishing the hole that this work is filling. We also take note of your vocabulary suggestion, and while we have replaced \"demarcation\" for the simpler \"delimitation\", we could not find a good alternative for \"desideratum\" that expresses the same concept.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vC8hNRk9dOR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper856/Authors|ICLR.cc/2021/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment"}}}, {"id": "t-H3fWLPVf", "original": null, "number": 3, "cdate": 1606229781270, "ddate": null, "tcdate": 1606229781270, "tmdate": 1606229781270, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "AF__a-kCNc", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment", "content": {"title": "rebuttal", "comment": "\nWe have revised the submission to streamline the core motivations of this work (mainly, abstract and introduction). In what follows, we discuss the reviewer's reasons to reject:\n\n1. It is hard to argue with this statement. What is enough?\n2. The core problem is the scarcity of benchmarks for Online Continual Learning on a domain with temporally-dependent data, such as the linguistic domain. We have clarified this in the abstract and the second paragraph of the introduction.\n3. We have revised the submission to clarify that the metrics are intended as a complement of the standard online performance, and not to substitute alternatives. Their goal is simply to zoom in into the behaviour at the switching points, which we discuss extensively.\n4. The model is fully general to support any kind of module. We did conduct experiments with LSTMs because, as we reported, they worked much better out-of-the-box than Transformers in this setting. We did not conduct experiments on CNNs, but then again, nothing prevents the gating mechanism being applied to these kinds of experts.\n5. We are not sure to understand what the reviewer means here. We experimented with two settings for the $\\lambda$ parameter on the multilingual and multidomain datasets, and for each of these, two levels of aggregation on the number of modules.\n6. Thanks for the references. As it happens with other methods, we cannot directly port these methods to our setting because they all assume that each instance is presented as a self-contained history-independent example $x_t$, whereas learning to represent the history is a fundamental aspect of learning in a linguistic setting. We believe that our contribution will allow the continual learning community to devote some attention to this problem which is obscured by the current most commonly used benchmarks.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vC8hNRk9dOR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper856/Authors|ICLR.cc/2021/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment"}}}, {"id": "kUM-IVrcZpY", "original": null, "number": 2, "cdate": 1606229672713, "ddate": null, "tcdate": 1606229672713, "tmdate": 1606229672713, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "guotFOB741v", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment", "content": {"title": "rebuttal", "comment": "We are sorry to hear the reviewer found the paper hard to follow, but please note that R1 and R2 have exact opposite views on this.\n1. As for what the reviewer considers imprecise statements:\n\n    (1.1) The same reference that we cite in the paper begins Section 2 points out that bilingualism is a widespread phenomenon, which is the norm rather than the exception among children. What we meant here was that even if they rely on any cue, this cue must necessarily be learned. We have further clarified this in the submission.\n\n    (1.2.) Data leakage is a fairly standard term referring to a situation where you train on test data.\n\n    (1.3.) Thanks for pointing this out, we have improved the presentation of this paragraph in the revised submission.\n    \n    (1.4.) We understand that this might sound counterintuitive given the usual framing in the literature, but consider that the best way to adapt quickly to a distribution that you have actually seen in the past is to _remember_ about it, rather than re-learn it. Perhaps that in this light, the stability-plasticity dilemma could look less of a dilemma.\n\n    (1.5.) We know when switches happen as evaluators, but this information is hidden from the models, analogously to how you hide test labels when evaluating on a regular classification setting.\n2. One could try to apply the techniques that the reviewer is proposing for the multilingual setting, but then how would they generalize to the multidomain? It is important not to miss from sight the long-term goal of developing general learners on multiple types of data. Here we are contributing a new benchmark on a linguistic setting, but it is not an end-goal per se. As for the one-expert-per-class suggested baseline, we do have an even stronger baseline, which is given by the independent LSTM. In contrast to what the reviewer is suggesting, we do not claim that the gating model benefits from the multilingual setting, in which it doesn't, but rather that it displays lower levels of forgetting than other alternatives. In contrast, the gating model does benefit from transfer on the multidomain setting. See last paragraph of Section 5.1 for a discussion that addresses this point.\n3. Because it is far from obvious how that should be done in language, and it is a research question that goes beyond the scope of this single paper. While an instance or two of how to draw the number 5 can be a good representation of a class, which sentences can be a good representation for a full language? Moreover, how should you chunk the input into an experience? Note that one of the crucial points that distinguish language from the image domain is that while the latter is naturally chunked into independent examples, language occurs as a continuous stream of correlated data.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "vC8hNRk9dOR", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper856/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper856/Authors|ICLR.cc/2021/Conference/Paper856/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866471, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Comment"}}}, {"id": "1xNlev0BOds", "original": null, "number": 1, "cdate": 1603702992949, "ddate": null, "tcdate": 1603702992949, "tmdate": 1605024590176, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Review", "content": {"title": "Useful dataset for online continual learning, evaluation could be improved with a wider range of baselines", "review": "This paper\u2019s main contributions are (i) to propose two new benchmarks for online continual learning in the context of language modelling and (ii) evaluate the performance of a number of composition-of-experts-based models on the new datasets using a number of metrics. The multilingual benchmark, derived from an existing multilingual news corpus, consists of sequences of characters where the language is periodically switched, and the MultiDomain benchmark consists of sequences of English words where the corpus is periodically switched. The comparative performances of the various baselines on the two datasets, as well as an analysis of the mixture weights in one of the models during training, are used to provide insights into the qualitative differences between the datasets.\n\nOverall, I am inclined to recommend acceptance for this paper on the margin because it makes a good contribution towards evaluating continual learning models in more real world settings, more specifically in the context of online learning. The datasets proposed are well-suited for purpose for reasons outlined below, and the evaluation using various composition-of-experts models is fairly conducted and followed up with an informative analysis. The key downside of the paper is that no standard continual learning baselines are trained on the proposed datasets; I would be inclined to increase my score if results were shown for 1 or 2 algorithms specifically designed for continual learning with neural networks (as discussed in more detail below).\n\nPositives:\n\t\u2022\tThere is a need to start evaluating continual learning in closer-to-real-life settings; in providing datasets that facilitate evaluation of continual learning models in an online setting without task boundaries, this paper makes a positive contribution in this direction.\u2028\n\t\u2022\tThe datasets are simply composed, but seem well suited for evaluating online continual learning because (i) language data is sequential, (ii) by imposing a truncated exponential (and thus memoryless) distribution on the length of subsequences, it is hard for models to cheat in predicting the next task switch, preserving task-agnosticity, and (iii) in both datasets, the subtasks share latent similarities, creating the possibility for forward/backward transfer between them.\u2028\n\t\u2022\tThe analysis of the experiments provides interesting insights into the datasets and differences between the baselines. E.g. Figure 1d effectively shows how the weights of one of the Product of Experts models switch after a task change, indicating a degree of specialisation of the modules, and 1e uses the correlations of the mixtures weights used for different subtasks to highlight the latent similarity between pairs of subtasks.\u2028\n\t\u2022\tThe paper is clearly written and easy to follow.\u2028\n\nMain Concern\n\t\u2022\tLimited set of baselines. While a range of composition-of-experts baselines are used for evaluation, it would have been much better to also include other methods specifically designed for online continual learning, such as those cited in the paper [1, 2] or, though not strictly online, a replay-based method such as CLEAR, which works in the task-agnostic setting. It is claimed in the paper that including state-of-the-art online continual learning methods would have involved \u201cnon-trivial adaptations significantly departing from the original models, which would limit any possible conclusions we could draw\u201d as they are designed for image-based datasets. I don\u2019t fully understand the basis of this claim; perhaps the authors could elaborate - as far as I am aware, for example, [1] is not restricted for use on image-based datasets.\u2028\n\t\u2022\tSince the subtasks do have discrete boundaries, even though these are not passed to the model during training, it would be possible to evaluate methods that use task boundaries for consolidation on the proposed datasets by either providing knowledge of the boundaries (although this breaks the task-agnosticity) or by using methods that can detect task boundaries - e.g. EWC uses the Forget-Me-Not Process [3].\u2028\n\t\u2022\tOverall, not evaluating the datasets with any standard continual learning baselines is an important weakness.\u2028\n\nOther comments\n\t\u2022\tThe proposed method, plastic gates, which performs best amongst the baselines used when combined with product of experts models, seems simple and effective but I am inclined to question how novel it is, since it just amounts to multi-step online gradient descent on the mixture weights.\u2028\n\t\u2022\tThe metrics used for evaluating continual learning, loss after switch and recovery time after switch, which are one of the main selling points of the paper are suitable for the datasets provided, but would not be applicable in a setting where either the task boundaries are not known or there are no hard task boundaries to be identified.\u2028\n\t\u2022\tTypo Section 2 Paragraph 2: \u201cMNNIST\u201d -> \u201cMNIST\u201d", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133419, "tmdate": 1606915773557, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper856/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Review"}}}, {"id": "MRKIunxT0M", "original": null, "number": 2, "cdate": 1603897616242, "ddate": null, "tcdate": 1603897616242, "tmdate": 1605024590113, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Review", "content": {"title": "Official Blind Review R2", "review": "#############################################################################################\n\nSummary:\n\nThis paper introduces a dataset and benchmark for language modeling in the online continual learning framework. The key characteristics of this benchmark are: the data are temporally correlated, there are no task identifiers presented to the model (task-free setting), and the evaluation is performed in an online fashion.\n\nThe benchmark consists of a Multilingual character-level dataset and a Multidomain word-level dataset. The authors introduce several metrics and evaluate using several simple baselines of mixtures-of-experts and products-of-experts. \n\n#############################################################################################\n\nPros:\n\n1.\tThe paper is clear and well-written. \n2.\tThe authors provide sufficient details on data collection and modeling\n3.\tThe relevant work section is extensive\n4.\tThe design choices in constructing the dataset are well thought out and make sense given the objective of the paper. In particular, the dataset along with the proposed evaluation metrics captures the three stated objectives of the benchmark. \n5.\tThe authors are upfront about materials left out of the main text. It\u2019s nice when potential questions are anticipated and answered, for example, \u201cwhy weren\u2019t continual learning SOTA models evaluated?\u201d and \u201cwhy weren\u2019t transformers considered as baselines?\u201d The authors answer these questions candidly. \n\n#############################################################################################\n\nCons\n\n1.\tThe dataset seems incremental over existing work\n2.\tThe introduced evaluation metrics are described intuitively, but are not analyzed empirically or theoretically \n3.\tThe necessity/value of the introduced dataset is not adequately justified in relation to existing challenges in the continual learning setting. A component of this is showing where existing models fail (and why this dataset will help improve them). \n\n#############################################################################################\n\nRecommendation and explanation\n\nI recommend rejection for the previously outlined reasons. \n\n#############################################################################################\n\nI also have some questions that I hope the author can help address:\n\n1. What is the key innovation over existing work such as d\u2019Autume et al. who also study language models in the continual learning, task-free setting?\n2. What failure of current models does this benchmark address? Note that the answer to this question should also be empirically demonstrated. \n\n#############################################################################################\n\nAdditional feedback\n1. This benchmark could very well be a valuable contribution that fills a hole in the existing body of work, but the paper in its current form does not adequately establish this. The rebuttal should better address how this benchmark fits into existing work by comparing it to existing datasets and more relevant baselines.\n2. The paper as a whole is well written, but I question some of the choices in syntax: terms such as \u201cdemarcation\u201d and \u201cdesideratum\u201d are spirited but may be better replaced by plainer alternatives.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133419, "tmdate": 1606915773557, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper856/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Review"}}}, {"id": "guotFOB741v", "original": null, "number": 4, "cdate": 1604090111511, "ddate": null, "tcdate": 1604090111511, "tmdate": 1605024590048, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Review", "content": {"title": "Hard to follow, imprecise statements, no comparison with old continual learning approaches", "review": "Summary: The paper proposes two benchmarks for continual language modeling: one evaluating character-level multilingual drift between languages which share similar characters and second evaluating word-level drift between English corpora of different domains. The setup is online in the sense of evaluation: they evaluate on the new sentences and then train over them (unlike image datasets), and catastrophic forgetting is hence characterised as having higher error than was in the past when there is a switch between the domains/languages. Hence, the loss functions measuring forgetting quantify the height and length of the rise in error. They compare a mixture of expert baselines with gating by different gating methods on this setup.\n\n\nPrimary Concerns:\n\n1. There are few  sentences and terms that are hard to understand and to me they seem imprecise. Examples would be: \n\n    (1.1) Intro: \u201chuman children still manage to acquire multiple languages without being explicitly asked to keep them separated\u201d -- not sure if I buy this as it is known that if children are exposed to situation where there are many languages, they get confused, sometimes many kids find it hard to learn any of them, and it becomes important to give them guiding signal. Do you have any reference to support this hypothesis?\n\n    (1.2) Section 3, second para: \u201cpreventing data leakage\u201d: what do you mean by data leakage?\n\n    (1.3) Section 3, third para: hard to follow, notation isn\u2019t clear. And it seems there is a typo in S_i = \\sum_j T_i.\n\n    (1.4) Section 3, fourth para: \u201cfor a model to be resilient to forgetting, it must adapt quickly\u201d: this statement is not correct because if a model adapts quickly to a new distribution, the parameter change would lead to forgetting and that\u2019s primary the reason why there are regularization based approaches for continual learning enforcing models to be in the vicinity of old parameters. Too much adaptivity does not ensure less forgetting.\n\n    (1.5) Section 3, loss after switch: what do you mean by a switch? How do you know when a switch happens (task label is not given)? In practice the loss curve is not smooth. How do you identify the switch? Fig 1 (a) is too smooth, does not represent the real loss curve. \n\n2. Regarding experiments, is it not possible to design much simpler methods which work for this problem? If it's known there is expected to be a character/word-sequence distribution shift, I believe it's likely they can be detected easily with traditional n-gram models and style distinguishing attributes typically used for author identification [1,2]. Why isn't it possible to use a baseline which consists of experts for one domain/language where the character-sequence decides which expert to use instead of these weaker gating-based methods? Also, English/czech/german/french seem very distinguishable and share little in common in terms of character sequences [3], hence I am doubtful of the finding that combining these models will improve any single language performance.\n\n3. Why is it not possible to apply traditional continual methods like Experience replay to this setting-- you simply store intelligently selected past sentences in memory (when say error shoots up) and replay using them. There are many other continual learning approaches that potentially could be applied here. Any particular reason for not using them?\n\n[1] Koppel et. al., Computational Methods in Authorship Attribution\n[2] Sapkota et. al., Not All Character N-grams Are Created Equal: A Study in Authorship Attribution\n[3] Gerz et. al., On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling (edited) \n", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133419, "tmdate": 1606915773557, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper856/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Review"}}}, {"id": "AF__a-kCNc", "original": null, "number": 3, "cdate": 1603974695362, "ddate": null, "tcdate": 1603974695362, "tmdate": 1605024589989, "tddate": null, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "invitation": "ICLR.cc/2021/Conference/Paper856/-/Official_Review", "content": {"title": "The paper designs a benchmark CALM for evaluating online continual learning. It adds a third dimension, temporally situated evaluation, to the existing evaluation benchmark. In addition, new metrics, Loss after switch and Recovery time after switch, are proposed to study catastrophic forgetting. Finally, evaluate multiple baseline models based on the composition of experts.", "review": "Strengths:\nThis paper proposes a new evaluation framework and gives two available evaluation datasets\nWeakness\uff1a\n- the paper needs a major rewrite to improve fluency and to better state motivation and contribution\n- the empirical validation is weak.\n\nReasons for accept:\nThe advantages of this paper are: \n1)\tthis paper proposed a new evaluation benchmark and dataset to promote the related research of online continual learning; \n2)\tthe proposed plastic gate allows it to distribute different distributions among different experts, which has certain effects from the experimental results. \n\nReasons for reject:\nThe shortcomings of this paper are:\n1.\tThis paper is not enough novel and has not contributed enough to continual learning related research; \n2.\tThe core motivation of this paper is not clear enough. The abstract mentioned that \"it is hard to demarcate task boundaries in actual tasks\", and then said that a new benchmark, new metrics, and gating technique are proposed. Stacked statements like this can hardly capture the main problem to be solved.\n3.\tThe advantages of the new metrics are not clear. Because from the experimental results, PPL and PPL@sw have a strong correlation. Therefore, please explain its advantages in detail (including the advantages of this evaluation framework compared with the evaluation framework of related literature, and verify it)\n4.\tThe baseline uses LSTM and does not use CNN, Transformer, .etc, which shows that its generalization is limited.\n5.\tCan you provide the experimental results when \u03bb is other values, and the combination of the number of modules\uff1f\n6.\tBecause what you are proposing is a continuous language modeling evaluation framework. Is it possible to evaluate some of the latest online continual learning systems? \nFor example\uff1a\n1) Lifelong Machine Learning with Deep Streaming Linear Discriminant Analysis\n2) Learning a Unified Classifier Incrementally via Rebalancing\nOr other Task-Free Continual Learning related work. This will have a good evaluation effect on measuring the versatility of your evaluation framework. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper856/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper856/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Evaluating Online Continual Learning with CALM", "authorids": ["~Germ\u00e1n_Kruszewski1", "~Ionut_Teodor_Sorodoc1", "~Tomas_Mikolov1"], "authors": ["Germ\u00e1n Kruszewski", "Ionut Teodor Sorodoc", "Tomas Mikolov"], "keywords": ["online continual learning", "catastrophic forgetting", "benchmark", "language modelling"], "abstract": "Online Continual Learning (OCL) studies learning over a continuous data stream without observing any single example more than once, a setting that is closer to the experience of humans and systems that must learn \u201con-the-wild\u201d. Yet, commonly available benchmarks are far from these real world conditions, because they explicitly signal different tasks, lack latent similarity structure or assume temporal independence between different examples. Here, we propose a new benchmark for OCL based on language modelling in which input alternates between different languages and domains without any explicit delimitation. Additionally, we propose new metrics to study catastrophic forgetting in this setting and evaluate multiple baseline models based on compositions of experts. Finally, we introduce a simple gating technique that learns the latent similarities between different inputs, improving the performance of a Products of Experts model.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "kruszewski|evaluating_online_continual_learning_with_calm", "one-sentence_summary": "We introduce a benchmark for Online Continual Learning based on language modelling, evaluating multiple baselines and improving one of them.", "supplementary_material": "/attachment/3f6c9a652bc772abdecf12f0ad0f793bb267d027.zip", "pdf": "/pdf/306a24d78b5bb9ff28485312af1080724879c7a5.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5AB5JfhcI", "_bibtex": "@misc{\nkruszewski2021evaluating,\ntitle={Evaluating Online Continual Learning with {\\{}CALM{\\}}},\nauthor={Germ{\\'a}n Kruszewski and Ionut Teodor Sorodoc and Tomas Mikolov},\nyear={2021},\nurl={https://openreview.net/forum?id=vC8hNRk9dOR}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "vC8hNRk9dOR", "replyto": "vC8hNRk9dOR", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper856/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538133419, "tmdate": 1606915773557, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper856/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper856/-/Official_Review"}}}], "count": 10}