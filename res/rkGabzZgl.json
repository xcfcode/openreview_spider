{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487193966146, "tcdate": 1477677498406, "number": 9, "id": "rkGabzZgl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rkGabzZgl", "signatures": ["~Xuezhe_Ma1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396308081, "tcdate": 1486396308081, "number": 1, "id": "By2qozIue", "invitation": "ICLR.cc/2017/conference/-/paper9/acceptance", "forum": "rkGabzZgl", "replyto": "rkGabzZgl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "This paper presents a theoretical underpinning of dropout, and uses this derivation to both characterize its properties and to extend the method. A solid contribution. I am surprised that none of the reviewers mentioned that this work is closely related to the uncited 2015 paper \"Variational Dropout and the Local Reparameterization Trick\" by Diederik P. Kingma, Tim Salimans, Max Welling.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396308634, "id": "ICLR.cc/2017/conference/-/paper9/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rkGabzZgl", "replyto": "rkGabzZgl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396308634}}}, {"tddate": null, "tmdate": 1483238999549, "tcdate": 1483238999549, "number": 5, "id": "H11DC1UHe", "invitation": "ICLR.cc/2017/conference/-/paper9/public/comment", "forum": "rkGabzZgl", "replyto": "rkGabzZgl", "signatures": ["~Xuezhe_Ma1"], "readers": ["everyone"], "writers": ["~Xuezhe_Ma1"], "content": {"title": "Revision of the paper", "comment": "We made the following revisions:\n\n1. We switched the section 6.3 and 6.4 to make the paper more clear.\n\n2. We added the definition of MC dropout on page 8.\n\n3. We fixed all the typos in the three reviewers' comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287763585, "id": "ICLR.cc/2017/conference/-/paper9/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkGabzZgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper9/reviewers", "ICLR.cc/2017/conference/paper9/areachairs"], "cdate": 1485287763585}}}, {"tddate": null, "tmdate": 1482466229879, "tcdate": 1482466229879, "number": 4, "id": "SJA27Q9Ne", "invitation": "ICLR.cc/2017/conference/-/paper9/public/comment", "forum": "rkGabzZgl", "replyto": "S1SwhMcEx", "signatures": ["~Xuezhe_Ma1"], "readers": ["everyone"], "writers": ["~Xuezhe_Ma1"], "content": {"title": "Response to Reviewer 1", "comment": "Thanks so much for your review.\nFor the typos you pointed out, we will revise this paper soon to fix them.\nFor the comment about evaluation on small scale benchmark problems (same comment also mentioned by Review 3), we leave the evaluation of the regularizer on large scale tasks and more complex networks (such as RNNs) to one of future work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287763585, "id": "ICLR.cc/2017/conference/-/paper9/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkGabzZgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper9/reviewers", "ICLR.cc/2017/conference/paper9/areachairs"], "cdate": 1485287763585}}}, {"tddate": null, "tmdate": 1482464348745, "tcdate": 1482464348745, "number": 3, "id": "S1SwhMcEx", "invitation": "ICLR.cc/2017/conference/-/paper9/official/review", "forum": "rkGabzZgl", "replyto": "rkGabzZgl", "signatures": ["ICLR.cc/2017/conference/paper9/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper9/AnonReviewer1"], "content": {"title": "Good paper", "rating": "8: Top 50% of accepted papers, clear accept", "review": "summary\n\nThe paper explains dropout with a latent variable model where the dropout variable (0 or 1 depending on which units should be dropped) is not observed and is accordingly marginalised. Maximum likelihood under this model is not tractable but standard dropout then corresponds to a simple Monte Carlo approximation of ML for this model.\n\nThe paper then introduces a theoretical framework for analysing the discrepancy (called inference gap) between the model at training (model ensemble, or here the latent variable model), and the model at testing (where usually what should be an expectation over the activations over many models becomes the activation of one model with averaged weights).\nThis framework introduces several notions (e.g. expectation linearity) which allow the study of which transition functions (and more generally layers) can have a small inference gap. Theorem 3 gives a bound on the inference gap.\n\nFinally a new regularisation term is introduced to account for minimisation of the inference gap during learning.\n\nExperiments are performed on MNIST, CIFAR-10 and CIFAR-100 and show that the method has the potential to perform better than standard dropout and at the level of Monte Carlo Dropout (the standard method to compute the real dropout outputs consistently with the training assumption of an ensemble, of course quite expensive computationally)\n\n\nThe study gives a very interesting theoretical model for dropout as a latent variable model where standard dropout is then a monte carlo approximation. This is very probably widely applicable to further studies of dropout.\n\nThe framework for the study of the inference gap is interesting although maybe somewhat less widely applicable.\n\nThe proposed model is convincing although 1. it is tested on simple datasets 2. the gains are relatively small and 3. there is an increased computational cost during training because a new hyper-parameter is introduced.\n\np6 line 8 typo: expecatation", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512725267, "id": "ICLR.cc/2017/conference/-/paper9/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper9/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper9/AnonReviewer3", "ICLR.cc/2017/conference/paper9/AnonReviewer2", "ICLR.cc/2017/conference/paper9/AnonReviewer1"], "reply": {"forum": "rkGabzZgl", "replyto": "rkGabzZgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512725267}}}, {"tddate": null, "tmdate": 1482040904543, "tcdate": 1482040904543, "number": 3, "id": "Skl8IsQVe", "invitation": "ICLR.cc/2017/conference/-/paper9/public/comment", "forum": "rkGabzZgl", "replyto": "B1WHbQQNe", "signatures": ["~Xuezhe_Ma1"], "readers": ["everyone"], "writers": ["~Xuezhe_Ma1"], "content": {"title": "Response to Reviewer2", "comment": "Thanks so much for your review.\n\nYes, currently our LVM framework is only applicable to probabilistic models. But our regularizer can be applied to general deep networks.\nIt is an interesting direction for future work to development more general framework which is capable to analyze dropout on general deep networks.\n\nFor the comparison of performance between MC and standard dropout, the MC dropout did achieve better results on CIFAR.\nSo the statement of Page 9 should be only on MNIST. We will revise the paper to make it clearer.\n\nWe really appreciate your comments about typos and issues of writing. We will fix them."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287763585, "id": "ICLR.cc/2017/conference/-/paper9/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkGabzZgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper9/reviewers", "ICLR.cc/2017/conference/paper9/areachairs"], "cdate": 1485287763585}}}, {"tddate": null, "tmdate": 1482034301317, "tcdate": 1482034301317, "number": 2, "id": "rkBthtXEe", "invitation": "ICLR.cc/2017/conference/-/paper9/public/comment", "forum": "rkGabzZgl", "replyto": "S153C5-Ve", "signatures": ["~Xuezhe_Ma1"], "readers": ["everyone"], "writers": ["~Xuezhe_Ma1"], "content": {"title": "Response to reviewer3", "comment": "Thanks so much for your review.\n\nYes, in this paper, we evaluated the proposed regularization method on small scale benchmark problems.\nEvaluating the regularizer on large scale tasks and more complex networks (such as RNNs) is clearly one \nof potential directions for future work.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287763585, "id": "ICLR.cc/2017/conference/-/paper9/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkGabzZgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper9/reviewers", "ICLR.cc/2017/conference/paper9/areachairs"], "cdate": 1485287763585}}}, {"tddate": null, "tmdate": 1482006840923, "tcdate": 1482006840923, "number": 2, "id": "B1WHbQQNe", "invitation": "ICLR.cc/2017/conference/-/paper9/official/review", "forum": "rkGabzZgl", "replyto": "rkGabzZgl", "signatures": ["ICLR.cc/2017/conference/paper9/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper9/AnonReviewer2"], "content": {"title": "", "rating": "7: Good paper, accept", "review": "This paper introduces dropout as a latent variable model (LVM). Leveraging this formulation authors analyze the dropout \u201cinference gap\u201d which they define to be the gap between network output during training (where an instance of dropout is used for every training sample) and test (where expected dropout values are used to scale node outputs).  They introduce the notion of expectation linearity and use this to derive bounds on the inference gap under some (mild) assumptions.  Furthermore, they propose use of per-sample based inference gap as a regularizer, and present analysis of accuracy of models with expectation-linearization constraints as compared to those without.\n\nOne relatively minor issue I see with the LVM view of dropout is that it seems applicable only to probabilistic models whereas dropout is more generally applicable to deep networks.  However I\u2019d expect that the regularizer formulation of dropout would be effective even in non-probabilistic models.\n\nMC dropout on page 8 is not defined, please define.\n\nOn page 9 it is mentioned that with the proposed regularizer the standard dropout networks achieve better results than when Monte Carlo dropout is used.  This seems to be the case only on MNIST dataset and not on CIFAR?\n\nFrom Tables 1 and 2 it also appears that MC dropout achieves best performance across tasks and methods but it is of course an expensive procedure.  Comments on the computational efficiency of various dropout procedures - to go with the accuracy results - would be quite valuable.\n\nCouple of typos:\n- Pg. 2 \u201c \u2026 x is he input \u2026\u201d -> \u201c \u2026 x is the input \u2026\u201d\n- Pg. 5 \u201c \u2026 as defined in (1), is \u2026\u201d -> ref. to (1) is not right at two places in this paragraph\n\nOverall it is a good paper, I think should be accepted and discussed at the conference.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512725267, "id": "ICLR.cc/2017/conference/-/paper9/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper9/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper9/AnonReviewer3", "ICLR.cc/2017/conference/paper9/AnonReviewer2", "ICLR.cc/2017/conference/paper9/AnonReviewer1"], "reply": {"forum": "rkGabzZgl", "replyto": "rkGabzZgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512725267}}}, {"tddate": null, "tmdate": 1481907890036, "tcdate": 1481907890036, "number": 1, "id": "S153C5-Ve", "invitation": "ICLR.cc/2017/conference/-/paper9/official/review", "forum": "rkGabzZgl", "replyto": "rkGabzZgl", "signatures": ["ICLR.cc/2017/conference/paper9/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper9/AnonReviewer3"], "content": {"title": "Good paper", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper puts forward a not entirely new, but also not sufficiently understood interpretation of dropout regularization. The authors derive useful theorems that estimate or put bounds on key quantities that are of interest when analyzing dropout regularized networks from their perspective. They furthermore introduce an explicit regularization term that should have a well understood impact on these key quantities. In the experimental section they convincingly show that the proposed regularization indeed has the expected effect and that their perspective on dropout is therefore useful and meaningful.\n\nTheir proposed regularization also seems to have a positive impact on the models performance but they demonstrate this only on rel. small scale benchmark problems. I therefore don\u2019t belief that this approach will have a large impact on how practitioner train models.  But their general perspective is well aligned with the recently proposed idea of \u201cDropout as a bayesian approximation\u201d and the insights and theorems in this paper might enable future work in that direction.\n ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512725267, "id": "ICLR.cc/2017/conference/-/paper9/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper9/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper9/AnonReviewer3", "ICLR.cc/2017/conference/paper9/AnonReviewer2", "ICLR.cc/2017/conference/paper9/AnonReviewer1"], "reply": {"forum": "rkGabzZgl", "replyto": "rkGabzZgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512725267}}}, {"tddate": null, "tmdate": 1480717259264, "tcdate": 1480717259260, "number": 1, "id": "SyQRmuJml", "invitation": "ICLR.cc/2017/conference/-/paper9/public/comment", "forum": "rkGabzZgl", "replyto": "BkerWD1me", "signatures": ["~Xuezhe_Ma1"], "readers": ["everyone"], "writers": ["~Xuezhe_Ma1"], "content": {"title": "Re:Question", "comment": "- \\eta, which is defined in Eq (21), is the parameter of the last layer (softmax layer) of the network.\n\n- As mentioned in Section 5.2, the second term (lambda term) at the RHS of Eq (15) can be regarded as a regularization term, similar to other commonly used regularization terms like L-2 norm or Lasso. When lambda=0, the loss function is exactly the same as log-likelihood. This regularizer pushes the model towards expectation-linearity, and lambda controls the strength of it. In Eq (22), lambda is reflected by \\delta, with lambda=0 corresponding to \\delta=\\infty.\n\n- Table 1 contains results of both MNIST and CIFAR (first 6 rows are for MNIST, 7th row is for CIFAR-10 and 8th row is for CIFAR-100).\nFig 1 is consistent with Table 1 (NOT Table 2). The architectures in Table 1 and Fig 1 are the same. \nBut Table 2 are for the experiments comparing with Dropout Distillation (Bul\u00f2 et al., 2016). So the architectures used in Table 2 are the same as those in (Bul\u00f2 et al., 2016). We really appreciate your comments for the confusing part of this paper. We will revise this part (maybe it is better to switch Section 6.3 and 6.4 so that Fig 1 would be before Table 2).\n\n- Following previous works, for each data set, we randomly select 10,000 images as the validation set. All the hyper-parameters are tuned on the validation set. When they are fixed, we train the final model with all the training data, including the validation data."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287763585, "id": "ICLR.cc/2017/conference/-/paper9/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rkGabzZgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper9/reviewers", "ICLR.cc/2017/conference/paper9/areachairs"], "cdate": 1485287763585}}}, {"tddate": null, "tmdate": 1480712503782, "tcdate": 1480712503776, "number": 1, "id": "BkerWD1me", "invitation": "ICLR.cc/2017/conference/-/paper9/pre-review/question", "forum": "rkGabzZgl", "replyto": "rkGabzZgl", "signatures": ["ICLR.cc/2017/conference/paper9/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper9/AnonReviewer1"], "content": {"title": "precisions on the details", "question": " - In Theorem 5, what is the variable \\eta ?\n\nFrom Eq 22, it seems that lambda=0 is essentially the same as log-likelihood training in your latent variable network model. This should therefore be nearly equivalent to standard dropout according to the arguments you present.\n\nNow comparing the results of Table 1&2 and  Fig. 1:\n - for lambda=0 (which I assume corresponds to the leftmost points in every graph of Fig. 1) are consistent for Mnist.\n - However for Cifar, the results seem better in Table 2 (11.18, 35.5 for standard and 10.86, 35.10 for EL) than in Fig.1 (12.8, 37.2 for normal and 12.2, 36.3).\n\nYou indicate at the page before that you used the same networks as Table 1 (but table 1 only concerns Mnist). Did you use the same networks as Table 2 for the Cifar graphs in Fig 1 ? If so, how would you account for the small but statistically significant difference ?\n\nAdditionally, how was lambda selected for the results in table 1 and 2 ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Dropout with Expectation-linear Regularization", "abstract": "Dropout, a simple and effective way to train deep neural networks, has led to a number of impressive empirical successes and spawned many recent theoretical investigations. However, the gap between dropout\u2019s training and inference phases, introduced due to tractability considerations, has largely remained under-appreciated. In this work, we first formulate dropout as a tractable approximation of some latent variable model, leading to a clean view of parameter sharing and enabling further theoretical analysis. Then, we introduce (approximate) expectation-linear dropout neural networks, whose inference gap we are able to formally characterize. Algorithmically, we show that our proposed measure of the inference gap can be used to regularize the standard dropout training objective, resulting in an explicit control of the gap. Our method is as simple and efficient as standard dropout. We further prove the upper bounds on the loss in accuracy due to expectation-linearization, describe classes of input distributions that expectation-linearize easily. Experiments on three image classification benchmark datasets demonstrate that reducing the inference gap can indeed improve the performance consistently.", "pdf": "/pdf/436cd523473505454012d97b09f3321aa071a2cc.pdf", "paperhash": "ma|dropout_with_expectationlinear_regularization", "conflicts": ["cs.cmu.edu"], "keywords": ["Theory", "Deep learning", "Supervised Learning"], "authors": ["Xuezhe Ma", "Yingkai Gao", "Zhiting Hu", "Yaoliang Yu", "Yuntian Deng", "Eduard Hovy"], "authorids": ["xuezhem@cs.cmu.edu", "yingkaig@cs.cmu.edu", "zhitinghu@cs.cmu.edu", "yaoliang@cs.cmu.edu", "dengyuntian@gmail.com", "hovy@cmu.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959508500, "id": "ICLR.cc/2017/conference/-/paper9/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper9/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper9/AnonReviewer1"], "reply": {"forum": "rkGabzZgl", "replyto": "rkGabzZgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper9/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959508500}}}], "count": 11}