{"notes": [{"id": "boZj4g3Jocj", "original": "-Wy7mIcHQNk", "number": 3553, "cdate": 1601308394662, "ddate": null, "tcdate": 1601308394662, "tmdate": 1614985735627, "tddate": null, "forum": "boZj4g3Jocj", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning to communicate through imagination with model-based deep multi-agent reinforcement learning", "authorids": ["~Arnu_Pretorius1", "scott.a.cameron@live.co.uk", "dries.epos@gmail.com", "elanvanbiljon@gmail.com", "l.francis@instadeep.com", "f.azeez@instadeep.com", "~Alexandre_Laterre1", "kb@instadeep.com"], "authors": ["Arnu Pretorius", "Scott Cameron", "Andries Petrus Smit", "Elan van Biljon", "Lawrence Francis", "Femi Azeez", "Alexandre Laterre", "Karim Beguir"], "keywords": [], "abstract": "The human imagination is an integral component of our intelligence. Furthermore, the core utility of our imagination is deeply coupled with communication. Language, argued to have been developed through complex interaction within growing collective societies serves as an instruction to the imagination, giving us the ability to share abstract mental representations and perform joint spatiotemporal planning. In this paper, we explore communication through imagination with multi-agent reinforcement learning. Specifically, we develop a model-based approach where agents jointly plan through recurrent communication of their respective predictions of the future. Each agent has access to a learned world model capable of producing model rollouts of future states and predicted rewards, conditioned on the actions sampled from the agent's policy. These rollouts are then encoded into messages and used to learn a communication protocol during training via differentiable message passing. We highlight the benefits of our model-based approach, compared to a set of strong baselines, by developing a set of specialised experiments using novel as well as well-known multi-agent environments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pretorius|learning_to_communicate_through_imagination_with_modelbased_deep_multiagent_reinforcement_learning", "pdf": "/pdf/8e517b8c1c71632aecca36506992a30f69b4031e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-XuhlIUaIY", "_bibtex": "@misc{\npretorius2021learning,\ntitle={Learning to communicate through imagination with model-based deep multi-agent reinforcement learning},\nauthor={Arnu Pretorius and Scott Cameron and Andries Petrus Smit and Elan van Biljon and Lawrence Francis and Femi Azeez and Alexandre Laterre and Karim Beguir},\nyear={2021},\nurl={https://openreview.net/forum?id=boZj4g3Jocj}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "eL40rEFbm8w", "original": null, "number": 1, "cdate": 1610040402620, "ddate": null, "tcdate": 1610040402620, "tmdate": 1610473998748, "tddate": null, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "invitation": "ICLR.cc/2021/Conference/Paper3553/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors present a model-based method for cooperative multi-agent reinforcement learning and propose to use communication of future predictions (as given by a learned world model) as a way to overcome partial observability.\n\nOverall, all reviewers found this work to be of great interest and the combination of planning + communication novel. However, all reviewers pointed that the claims that the papers makes are not fully supported by the experimental framing of the paper pointing to several shortcomings around experimental design in general and better control of appropriate baselines. The authors have since clarified several aspects in their paper and also included a new RL environment. \n\nHowever, as the paper still stands does not fully provide convincing evidence of their proposal, which is however very intriguing. I would like to echo though reviewers' suggestions that the authors work a bit more on the experimental design and I really hope this work will appear at a later venue."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to communicate through imagination with model-based deep multi-agent reinforcement learning", "authorids": ["~Arnu_Pretorius1", "scott.a.cameron@live.co.uk", "dries.epos@gmail.com", "elanvanbiljon@gmail.com", "l.francis@instadeep.com", "f.azeez@instadeep.com", "~Alexandre_Laterre1", "kb@instadeep.com"], "authors": ["Arnu Pretorius", "Scott Cameron", "Andries Petrus Smit", "Elan van Biljon", "Lawrence Francis", "Femi Azeez", "Alexandre Laterre", "Karim Beguir"], "keywords": [], "abstract": "The human imagination is an integral component of our intelligence. Furthermore, the core utility of our imagination is deeply coupled with communication. Language, argued to have been developed through complex interaction within growing collective societies serves as an instruction to the imagination, giving us the ability to share abstract mental representations and perform joint spatiotemporal planning. In this paper, we explore communication through imagination with multi-agent reinforcement learning. Specifically, we develop a model-based approach where agents jointly plan through recurrent communication of their respective predictions of the future. Each agent has access to a learned world model capable of producing model rollouts of future states and predicted rewards, conditioned on the actions sampled from the agent's policy. These rollouts are then encoded into messages and used to learn a communication protocol during training via differentiable message passing. We highlight the benefits of our model-based approach, compared to a set of strong baselines, by developing a set of specialised experiments using novel as well as well-known multi-agent environments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pretorius|learning_to_communicate_through_imagination_with_modelbased_deep_multiagent_reinforcement_learning", "pdf": "/pdf/8e517b8c1c71632aecca36506992a30f69b4031e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-XuhlIUaIY", "_bibtex": "@misc{\npretorius2021learning,\ntitle={Learning to communicate through imagination with model-based deep multi-agent reinforcement learning},\nauthor={Arnu Pretorius and Scott Cameron and Andries Petrus Smit and Elan van Biljon and Lawrence Francis and Femi Azeez and Alexandre Laterre and Karim Beguir},\nyear={2021},\nurl={https://openreview.net/forum?id=boZj4g3Jocj}\n}"}, "tags": [], "invitation": {"reply": {"forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040402606, "tmdate": 1610473998732, "id": "ICLR.cc/2021/Conference/Paper3553/-/Decision"}}}, {"id": "bAzXVsu0aHt", "original": null, "number": 3, "cdate": 1603886836789, "ddate": null, "tcdate": 1603886836789, "tmdate": 1606818022200, "tddate": null, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "invitation": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review", "content": {"title": "The claim is not well-supported.", "review": "Summary:\n\n \nThis paper proposes to combine model-based and multi-agent reinforcement learning. The authors follow the typical recurrent neural world models setting to generate imagined rollouts for decision-time planning. To tackle the non-stationarity of a multi-agent environment, they build end-to-end differentiable communication channels between agents within a pre-defined neighborhood. The communication message is defined as abstract information encoded from the imagined rollout. Agents then make decisions based on the message they received and the output of recurrent neural world models. Empirical studies are performed to show the superiority of proposed methods over SOTA model-free MARL approaches. Results are shown in two simple environments, which are designed to require communication between agents to solve the task.\n\n\n##########################################################################\n\npros: \n\n+ The motivation of doing model-based MARL is very clear and challenging.\n\n+ Overall, the paper is well written.\n\n+ The ablation study on the roles of world models and communication channels is interesting.\n\n##########################################################################\n \ncons: \n\n\n- Although the paper claims as a combination of model-based and multi-agent RL, my major concern is that the proposed model still deals with these two problems separately. In particular, the world model doesn't consider the dynamics of other agents, thus being an independent model only. The paper proposed to tackle the multi-agent part of the problem by building an explicit communication channel, which lacks enough novelty.\n\n- I'm also concerned about the lack of rigorous experimentation to support the paper's claim. \nThe two proposed environments are extremely tailored for algorithms with explicit communication channels and are limited in the number of agents. \n\t- For the digit game, the non-stationarity is not quite clear when there are only two agents. I'd like to see what would happen if the agent number in the digit game increases.\n\t- For the invisible spread, the ablation study shows that the role of world models is not important. I'd like to see the performance of other baseline algorithms that use explicit communication channels, which is not compared and seems to work well as the paper reported. If so, I don't see why this experiment supports the claim of combining model-based and multi-agent RL.\n\n##########################################################################\n\nPost rebuttal\n\nThe author's response does not address my primary concern and I'd like to keep my original score.", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3553/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3553/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to communicate through imagination with model-based deep multi-agent reinforcement learning", "authorids": ["~Arnu_Pretorius1", "scott.a.cameron@live.co.uk", "dries.epos@gmail.com", "elanvanbiljon@gmail.com", "l.francis@instadeep.com", "f.azeez@instadeep.com", "~Alexandre_Laterre1", "kb@instadeep.com"], "authors": ["Arnu Pretorius", "Scott Cameron", "Andries Petrus Smit", "Elan van Biljon", "Lawrence Francis", "Femi Azeez", "Alexandre Laterre", "Karim Beguir"], "keywords": [], "abstract": "The human imagination is an integral component of our intelligence. Furthermore, the core utility of our imagination is deeply coupled with communication. Language, argued to have been developed through complex interaction within growing collective societies serves as an instruction to the imagination, giving us the ability to share abstract mental representations and perform joint spatiotemporal planning. In this paper, we explore communication through imagination with multi-agent reinforcement learning. Specifically, we develop a model-based approach where agents jointly plan through recurrent communication of their respective predictions of the future. Each agent has access to a learned world model capable of producing model rollouts of future states and predicted rewards, conditioned on the actions sampled from the agent's policy. These rollouts are then encoded into messages and used to learn a communication protocol during training via differentiable message passing. We highlight the benefits of our model-based approach, compared to a set of strong baselines, by developing a set of specialised experiments using novel as well as well-known multi-agent environments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pretorius|learning_to_communicate_through_imagination_with_modelbased_deep_multiagent_reinforcement_learning", "pdf": "/pdf/8e517b8c1c71632aecca36506992a30f69b4031e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-XuhlIUaIY", "_bibtex": "@misc{\npretorius2021learning,\ntitle={Learning to communicate through imagination with model-based deep multi-agent reinforcement learning},\nauthor={Arnu Pretorius and Scott Cameron and Andries Petrus Smit and Elan van Biljon and Lawrence Francis and Femi Azeez and Alexandre Laterre and Karim Beguir},\nyear={2021},\nurl={https://openreview.net/forum?id=boZj4g3Jocj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3553/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073885, "tmdate": 1606915773333, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3553/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review"}}}, {"id": "cpG8cCYuoD6", "original": null, "number": 2, "cdate": 1606231000666, "ddate": null, "tcdate": 1606231000666, "tmdate": 1606231000666, "tddate": null, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "invitation": "ICLR.cc/2021/Conference/Paper3553/-/Official_Comment", "content": {"title": "Response to comments", "comment": "We would like to start by thanking every reviewer for their valuable feedback. We have taken note of everything that was said and made the necessary improvements to our system. As can be seen in our updated paper, in Figure 4 and 5, the MACI algorithm is more stable than before, while still outperforming all other algorithms we tested against. This is due to our system now training the world model, policy and encoder in an iterative fashion. Previously the world model was only trained once. We provide a more detailed algorithmic setup of our new MACI algorithm on page 6.\n\nSome of the reviewers rightfully pointed out that our digit game environment was quite limiting as agent actions did not influence future observations. We, therefore, created a new grid world environment that is much closer to the original RL setting. This environment allows us to test communication and navigation, where every action can influence future observations of all agents. With our new algorithm, we are able to scale beyond our previous limitation of 2 agents. We see that 4 agents can effectively communicate in this grid world. We also see that a combination of communication and a world model is needed for good performance. \n\nThe reviewers also pointed out that our paper lacks details on network architectures used for this work. We, therefore, added an Appendix with these specifications. Unfortunately, due to our main author falling sick, we did not get to address all the envisioned improvements and write them up in time. We will, however, keep on improving the algorithm in the coming weeks and release our update results. \n\nThank you for your time and consideration."}, "signatures": ["ICLR.cc/2021/Conference/Paper3553/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3553/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to communicate through imagination with model-based deep multi-agent reinforcement learning", "authorids": ["~Arnu_Pretorius1", "scott.a.cameron@live.co.uk", "dries.epos@gmail.com", "elanvanbiljon@gmail.com", "l.francis@instadeep.com", "f.azeez@instadeep.com", "~Alexandre_Laterre1", "kb@instadeep.com"], "authors": ["Arnu Pretorius", "Scott Cameron", "Andries Petrus Smit", "Elan van Biljon", "Lawrence Francis", "Femi Azeez", "Alexandre Laterre", "Karim Beguir"], "keywords": [], "abstract": "The human imagination is an integral component of our intelligence. Furthermore, the core utility of our imagination is deeply coupled with communication. Language, argued to have been developed through complex interaction within growing collective societies serves as an instruction to the imagination, giving us the ability to share abstract mental representations and perform joint spatiotemporal planning. In this paper, we explore communication through imagination with multi-agent reinforcement learning. Specifically, we develop a model-based approach where agents jointly plan through recurrent communication of their respective predictions of the future. Each agent has access to a learned world model capable of producing model rollouts of future states and predicted rewards, conditioned on the actions sampled from the agent's policy. These rollouts are then encoded into messages and used to learn a communication protocol during training via differentiable message passing. We highlight the benefits of our model-based approach, compared to a set of strong baselines, by developing a set of specialised experiments using novel as well as well-known multi-agent environments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pretorius|learning_to_communicate_through_imagination_with_modelbased_deep_multiagent_reinforcement_learning", "pdf": "/pdf/8e517b8c1c71632aecca36506992a30f69b4031e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-XuhlIUaIY", "_bibtex": "@misc{\npretorius2021learning,\ntitle={Learning to communicate through imagination with model-based deep multi-agent reinforcement learning},\nauthor={Arnu Pretorius and Scott Cameron and Andries Petrus Smit and Elan van Biljon and Lawrence Francis and Femi Azeez and Alexandre Laterre and Karim Beguir},\nyear={2021},\nurl={https://openreview.net/forum?id=boZj4g3Jocj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "boZj4g3Jocj", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3553/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3553/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3553/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3553/Authors|ICLR.cc/2021/Conference/Paper3553/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3553/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836383, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3553/-/Official_Comment"}}}, {"id": "MpVs55kCNJr", "original": null, "number": 1, "cdate": 1603709112109, "ddate": null, "tcdate": 1603709112109, "tmdate": 1605023980290, "tddate": null, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "invitation": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review", "content": {"title": "The presented algorithm is interesting, but the paper needs reframing and improved experimental method", "review": "\nThis paper claims to present an algorithm which enables a population of (two) agents\nto learn to communicate and coordinate to solve a task, and thus positions itself\nin the field of multi-agent Deep RL. After a long but rather vague and unspecific introduction\nand related work (see below), it describes the algorithm, then presents experiments where\nthe introduced algorithm is compared with model-free MARL baselines.\n\nWhile the algorithm presented is interesting and has potentially some novelties compared to\nthe state-of-the-art (e.g. differentiability of message passing in model-based MARL), it has\nalso a number of weaknesses:\n\n1) Globally, I had a lot of difficulty understanding clearly what are the aims of this paper:\nWhat are the problems it aims to solve? What are the scientific questions adressed?\nNeither the abstract nor the text provide sharp explanations of these aims.\n\n2) The paper uses very loaded but undefined vocabulary like \"imagination\", \"language\" and \"communication\".\nWhile in general I think it can be sometimes useful to use concepts and terms from human cognitive sciences\nto describe AI systems, in this particular case I found it very far fetched to speak of \"imagination\" and \"language\",\neven \"communication\". It seems in practice authors might simply mean something like \"prediction of future states\"\nwhen they use the term \"imagination\". \"Language\" and \"communication\" are also far-fetched because in cognitive\nscience and linguistics it refers to systems that enable different individuals, with different world views, to\ncommunicate an intent to each other. Here, the \"agents\" share the same world model, so they are not really\ndifferent individuals with their own world representations, and their communication is rather like \nmessage passing in GNNs, which is pretty far from \"language\" or \"human-like communication\".\n\n3) It is not even clear whether it is meaningful to call the presented system as \"multi-agent\", since in addition\nto a centralized shared reward, there is also a shared world model. To me, the system looks rather like an RL\nsystem that controls a multi-component body with local controllers that synchronize through message passing,\nquite similary to graph neural network controllers (also including message passing) used for e.g. in Pathak et al. 2019.\nA discussion of the similarities and differences with work such as Pathak et al. is needed.\n\n4) the authors are right to say that there is little research on model-based MARL, and cite one exception:\nKrupnik et al. However, it is not justified why this closely related work is not included in the baselines,\nor at least compared in discussion more thoroughly. Authors might also want to discuss another model-based MARL\npaper: Zhang et al. 2020.\n\n5) A large part of the related work section is not relevant to this paper, in particular about Deep RL and model-based RL,\nwhich are much broader topics than the one addressed in this paper\n\n6) The description of the method lacks sufficient technical details for reproducibility, in particular it lacks detailed\npseudo-code (some refs are said to be in an appendix, but I did not find an appendix), and no links to code is provided.\nFurthermore, there is no sufficient information on how hyperparameters selection for baselines was made.\n\n7) The two environments in the experiments are not sufficiently well motivated: why did you need to introduce them rather\nthan reuse existing test environments? E.g. which particular problems did you want to address that was not possible with\nexisting environments ?\n\n8) Since the claimed topic of the paper is about the emergence of a \"communication system\", one would expect a detailed\nanalysis of the emergent communication code (currently only figure 5 gives a quite superficial qualitative analysis).\n\n9) The quantitative comparison of algorithms is not made using a sufficiently strong statistical method (only 5 seeds,\nno tests such as Welch t-tests)\n\nFor these reasons, while the particular algorithms studied is in itself interesting, I think the paper would need a major\nconceptual reframing and a better experimental methodology and justification before publication.\n\nReferences:\n\nPathak et al. (2019) Learning to Control Self-Assembling Morphologies: A Study of Generalization via Modularity\nhttps://arxiv.org/pdf/1902.05546.pdf\n\nZhang, K., Kakade, S. M., Ba\u015far, T., & Yang, L. F. (2020). Model-based multi-agent rl in zero-sum markov games with near-optimal sample complexity. arXiv preprint arXiv:2007.07461.\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3553/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3553/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to communicate through imagination with model-based deep multi-agent reinforcement learning", "authorids": ["~Arnu_Pretorius1", "scott.a.cameron@live.co.uk", "dries.epos@gmail.com", "elanvanbiljon@gmail.com", "l.francis@instadeep.com", "f.azeez@instadeep.com", "~Alexandre_Laterre1", "kb@instadeep.com"], "authors": ["Arnu Pretorius", "Scott Cameron", "Andries Petrus Smit", "Elan van Biljon", "Lawrence Francis", "Femi Azeez", "Alexandre Laterre", "Karim Beguir"], "keywords": [], "abstract": "The human imagination is an integral component of our intelligence. Furthermore, the core utility of our imagination is deeply coupled with communication. Language, argued to have been developed through complex interaction within growing collective societies serves as an instruction to the imagination, giving us the ability to share abstract mental representations and perform joint spatiotemporal planning. In this paper, we explore communication through imagination with multi-agent reinforcement learning. Specifically, we develop a model-based approach where agents jointly plan through recurrent communication of their respective predictions of the future. Each agent has access to a learned world model capable of producing model rollouts of future states and predicted rewards, conditioned on the actions sampled from the agent's policy. These rollouts are then encoded into messages and used to learn a communication protocol during training via differentiable message passing. We highlight the benefits of our model-based approach, compared to a set of strong baselines, by developing a set of specialised experiments using novel as well as well-known multi-agent environments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pretorius|learning_to_communicate_through_imagination_with_modelbased_deep_multiagent_reinforcement_learning", "pdf": "/pdf/8e517b8c1c71632aecca36506992a30f69b4031e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-XuhlIUaIY", "_bibtex": "@misc{\npretorius2021learning,\ntitle={Learning to communicate through imagination with model-based deep multi-agent reinforcement learning},\nauthor={Arnu Pretorius and Scott Cameron and Andries Petrus Smit and Elan van Biljon and Lawrence Francis and Femi Azeez and Alexandre Laterre and Karim Beguir},\nyear={2021},\nurl={https://openreview.net/forum?id=boZj4g3Jocj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3553/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073885, "tmdate": 1606915773333, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3553/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review"}}}, {"id": "hD-pR4tcjv1", "original": null, "number": 2, "cdate": 1603744387479, "ddate": null, "tcdate": 1603744387479, "tmdate": 1605023980222, "tddate": null, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "invitation": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review", "content": {"title": "A neat idea that requires further investigation", "review": "Thank you very much for sharing these cool ideas. I enjoyed the clear writing and excellent related work sections, and I genuinely believe this paper presents interesting concepts that warrant further investigation. Unfortunately, in its current state, this manuscript is not ready for to be shared with the wider community at ICLR.\n\nI will leave here a few suggestions for improvement and ideas on how to strengthen your argument. I sincerely hope you will find these useful as you continue your research on this topic.\n\nThe manuscript describes Multi-Agent Communication through Imagination (MACI). MACI is an imagination-inspired communication protocol that allows two sub-modules to exchange information about their non-overlapping observations.\n\nThe manuscript is well written and easy to follow, and the authors properly place their contributions in the context of existing ideas.\n\nWhile the experiments presented are clear and the results are encouraging, I think the experimental section could benefit from additional experiments, here is why:\n\n- The tasks presented here are extremely simple. I understand the need of didactic environments, but in a purely methods paper, the reader is left to wonder if this method scales to more complex environments, if it can work with more than two agents, and if it can handle non-cooperative settings. This is especially acute here, given that Fig. 6 suggests MACI only helps in 1 out of 2 environments, as the performance gains in Invisible Spread are obviously attributable to partial observability in the baselines.\n\n- The tasks presented are purely cooperative, and the communication system is differentiable. This means that by setting WorldModel, Encoder and Aggregator to the identity function, one would recover exactly a single-agent architecture that has access to the combined observations and operates in the product of the actions spaces. The only difference might lie in how the the system is supervised (it is unclear from the manuscript how WorldModel is trained). This is similar to what is presented in Fig. 6 in the ablation study, but would add including a shared world-model to produce an \"ideal\" agent. How does this perform? The baselines provided are at an obvious disadvantage as the environments are partially observable. This performance ceiling would guide the reader in understanding how much of the gap is recovered by MACI.\n\nAdditional minor remarks: \n\n- I cannot find in the methods section how WorldModel is trained. Could this be made clearer in the text?\n- How accurate is WorldModel? How important is this accuracy? What happens if we replace our learned WorldModel module with an ideal oracle?\n- There is a bunch of work in modeling MARL (see, e.g. Hierarchical Policy Models [Zheng 2016], VAIN [Hoshen 2017], NRI [Kipf 2018] and RFM [Tacchetti 2018]). In particular RFM introduces on-board imagination models that influence the decisions of each agent. It might be good to add these to your references.\n\nThank you again for sharing these cool ideas, I hope to see more of this soon and that you'll find some of this feedback useful.\nAll the best.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3553/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3553/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to communicate through imagination with model-based deep multi-agent reinforcement learning", "authorids": ["~Arnu_Pretorius1", "scott.a.cameron@live.co.uk", "dries.epos@gmail.com", "elanvanbiljon@gmail.com", "l.francis@instadeep.com", "f.azeez@instadeep.com", "~Alexandre_Laterre1", "kb@instadeep.com"], "authors": ["Arnu Pretorius", "Scott Cameron", "Andries Petrus Smit", "Elan van Biljon", "Lawrence Francis", "Femi Azeez", "Alexandre Laterre", "Karim Beguir"], "keywords": [], "abstract": "The human imagination is an integral component of our intelligence. Furthermore, the core utility of our imagination is deeply coupled with communication. Language, argued to have been developed through complex interaction within growing collective societies serves as an instruction to the imagination, giving us the ability to share abstract mental representations and perform joint spatiotemporal planning. In this paper, we explore communication through imagination with multi-agent reinforcement learning. Specifically, we develop a model-based approach where agents jointly plan through recurrent communication of their respective predictions of the future. Each agent has access to a learned world model capable of producing model rollouts of future states and predicted rewards, conditioned on the actions sampled from the agent's policy. These rollouts are then encoded into messages and used to learn a communication protocol during training via differentiable message passing. We highlight the benefits of our model-based approach, compared to a set of strong baselines, by developing a set of specialised experiments using novel as well as well-known multi-agent environments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pretorius|learning_to_communicate_through_imagination_with_modelbased_deep_multiagent_reinforcement_learning", "pdf": "/pdf/8e517b8c1c71632aecca36506992a30f69b4031e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-XuhlIUaIY", "_bibtex": "@misc{\npretorius2021learning,\ntitle={Learning to communicate through imagination with model-based deep multi-agent reinforcement learning},\nauthor={Arnu Pretorius and Scott Cameron and Andries Petrus Smit and Elan van Biljon and Lawrence Francis and Femi Azeez and Alexandre Laterre and Karim Beguir},\nyear={2021},\nurl={https://openreview.net/forum?id=boZj4g3Jocj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3553/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073885, "tmdate": 1606915773333, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3553/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review"}}}, {"id": "TM1J4kPTxnU", "original": null, "number": 4, "cdate": 1603919979803, "ddate": null, "tcdate": 1603919979803, "tmdate": 1605023980087, "tddate": null, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "invitation": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review", "content": {"title": "Insufficient evidence for an otherwise interesting take on MARL algorithms", "review": "The paper talks about developing a model-based method for cooperative multi-agent reinforcement learning. The proposed approach utilizes communication as a tool for mitigating the partial observability induced by the non-stationary task while also helping agents reason about other agents' behaviors. The authors present their motivation for using language as a medium in model-based RL stemming from early literature in psychology and linguistics.\n\nThe setup consists of decentralized agents each of which is equipped with a world model similar to Ha et al. 2018. Further, each agent also has a separate message input that is received from the other players. Each agent does a form of decision-time planning where it produces rollouts for K steps before taking a real action. The message is then the encoding produced by the concatenation of the observations, rewards, and the actions taken during the rollouts.\n\nThe approach is novel and one of the first works that combine model-based RL in a dec-POMDP. The paper does a good job of explaining prior work in related domains. The schematic diagram also depicts the setup in an efficient and standalone manner.\n\nStill, I have some qualms related to the experimental setup that arguably makes the contribution of the proposed imagination framework inconclusive.\n\n- In the digits game, the agents need to produce actions that represent the next observation of the other agent. The transition dynamics are defined in a way such that the next observation for an agent i is independent of the action taken at the current timestep. I find this formulation to be incoherent with the way MACI works. Specifically, \n     a) The AgentController that produces the action doesn't need to depend on the current observation since it has no effect on the action. \n    b) The WorldModel produces the next observations, next hidden states, and the rewards given the current observation, current action, and current hidden state. Similar to the above, the information about the current action is not needed to produce the next observation. Moreover, the rewards, in this case, are only tied to the action. So it would make sense to produce it along with the action in the AgentController with a recurrent network.\nOverall I believe this game is not aligned with the objectives of MACI, although I would love to have the authors clarify this.\n\n- There is no information about the objective functions used for optimization or any detail about the learning process without which it makes it hard to reproduce.\n\n- The choice of baselines doesn't seem to be appropriate for the task. Since all the baseline methods used do not use explicit communication in their original forms, the comparison thus becomes unfair. I would like the authors to reference if the baselines were modified in a way to accommodate this. This is important specifically in the two tasks chosen since I believe just adding communication should yield sufficient improvement.\n\n- The current approach is only applicable for a two-agent cooperative game narrowing down the scalability of the method. I believe the approach has the potential to extend to multiple agents either by having a confluence of messages or explicit grouping of agents. \n\n- An important missing ablation experiment is comparing comm+world model with only world model. This is crucial since it will determine whether the performance gain is due to the abstract planning or the communication.\n\n- The overall compute required is more than running a real-time experiment since the planning uses K-step rollouts. Some ablation of the choice of K would be interesting to look at especially in terms of wall time.\n\ntypo: Fig 6-A title\n", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3553/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3553/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to communicate through imagination with model-based deep multi-agent reinforcement learning", "authorids": ["~Arnu_Pretorius1", "scott.a.cameron@live.co.uk", "dries.epos@gmail.com", "elanvanbiljon@gmail.com", "l.francis@instadeep.com", "f.azeez@instadeep.com", "~Alexandre_Laterre1", "kb@instadeep.com"], "authors": ["Arnu Pretorius", "Scott Cameron", "Andries Petrus Smit", "Elan van Biljon", "Lawrence Francis", "Femi Azeez", "Alexandre Laterre", "Karim Beguir"], "keywords": [], "abstract": "The human imagination is an integral component of our intelligence. Furthermore, the core utility of our imagination is deeply coupled with communication. Language, argued to have been developed through complex interaction within growing collective societies serves as an instruction to the imagination, giving us the ability to share abstract mental representations and perform joint spatiotemporal planning. In this paper, we explore communication through imagination with multi-agent reinforcement learning. Specifically, we develop a model-based approach where agents jointly plan through recurrent communication of their respective predictions of the future. Each agent has access to a learned world model capable of producing model rollouts of future states and predicted rewards, conditioned on the actions sampled from the agent's policy. These rollouts are then encoded into messages and used to learn a communication protocol during training via differentiable message passing. We highlight the benefits of our model-based approach, compared to a set of strong baselines, by developing a set of specialised experiments using novel as well as well-known multi-agent environments.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "pretorius|learning_to_communicate_through_imagination_with_modelbased_deep_multiagent_reinforcement_learning", "pdf": "/pdf/8e517b8c1c71632aecca36506992a30f69b4031e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=-XuhlIUaIY", "_bibtex": "@misc{\npretorius2021learning,\ntitle={Learning to communicate through imagination with model-based deep multi-agent reinforcement learning},\nauthor={Arnu Pretorius and Scott Cameron and Andries Petrus Smit and Elan van Biljon and Lawrence Francis and Femi Azeez and Alexandre Laterre and Karim Beguir},\nyear={2021},\nurl={https://openreview.net/forum?id=boZj4g3Jocj}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "boZj4g3Jocj", "replyto": "boZj4g3Jocj", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3553/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073885, "tmdate": 1606915773333, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3553/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3553/-/Official_Review"}}}], "count": 7}