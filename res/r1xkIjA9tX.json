{"notes": [{"id": "r1xkIjA9tX", "original": "H1eUrJRFt7", "number": 139, "cdate": 1538087751097, "ddate": null, "tcdate": 1538087751097, "tmdate": 1545355376859, "tddate": null, "forum": "r1xkIjA9tX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators", "abstract": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives, with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.\n", "keywords": ["q-calculus", "neural activation function"], "authorids": ["frank.nielsen@acm.org", "sunk.edu@gmail.com"], "authors": ["Frank Nielsen", "Ke Sun"], "TL;DR": "q-calculus helps build simple and scalable neural activation functions", "pdf": "/pdf/e7eaffe9345c90bc41e728e2f31f9207e11168c1.pdf", "paperhash": "nielsen|qneurons_neuron_activations_based_on_stochastic_jacksons_derivative_operators", "_bibtex": "@misc{\nnielsen2019qneurons,\ntitle={q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators},\nauthor={Frank Nielsen and Ke Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xkIjA9tX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJl841yMgE", "original": null, "number": 1, "cdate": 1544838957929, "ddate": null, "tcdate": 1544838957929, "tmdate": 1545354531969, "tddate": null, "forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper139/Meta_Review", "content": {"metareview": "This paper proposes a new type of activations function based on q-calculus. The reviewers found that the papers is significantly lacking in its presentation, in clarity, and in its experimental evaluation. The motivation of the method raises several significant questions to the reviewers, and the proposed method is not sufficiently compared to existing approaches for (noisy) activation functions. After reviews, the authors have failed to present any updates to their paper.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Lacking in presentation and in experimental evaluation"}, "signatures": ["ICLR.cc/2019/Conference/Paper139/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper139/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators", "abstract": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives, with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.\n", "keywords": ["q-calculus", "neural activation function"], "authorids": ["frank.nielsen@acm.org", "sunk.edu@gmail.com"], "authors": ["Frank Nielsen", "Ke Sun"], "TL;DR": "q-calculus helps build simple and scalable neural activation functions", "pdf": "/pdf/e7eaffe9345c90bc41e728e2f31f9207e11168c1.pdf", "paperhash": "nielsen|qneurons_neuron_activations_based_on_stochastic_jacksons_derivative_operators", "_bibtex": "@misc{\nnielsen2019qneurons,\ntitle={q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators},\nauthor={Frank Nielsen and Ke Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xkIjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper139/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353324133, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper139/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper139/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper139/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353324133}}}, {"id": "rklyCuc_37", "original": null, "number": 2, "cdate": 1541085383153, "ddate": null, "tcdate": 1541085383153, "tmdate": 1543245312908, "tddate": null, "forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper139/Official_Review", "content": {"title": "My neurons activate unanimously to vote NO for this submission", "review": "\n############ Updated Review #################\n\nI have read the author(s)' rebuttal. My decision stays unchanged. In my opinion, this first step is not significant enough, and the presentation is clearly below the acceptance threshold for ICLR. Additionally, the author(s) did not update their submission to reflect the changes. I thereby recommending rejection to this submission. \n\n##########################################\n\nThis work proposes to replace the regular deterministic activation functions used in artificial neural nets with stochastic variants. In particular, the author(s) considered the q-derivatives of standard activation functions. \n\nThe author(s) claimed that ``By Proposition 2, the p-derivative of the q-activation g_q(x) agrees with the original activation function f.'' I have trouble understanding this. I assume by original activation function the author(s) meant f(x), then how can Eqn (4) agree with f(x)?\n\nAt the bottom of pp. 3, the author(s) wrote: ``q-neuron ... combines stochasticity and some second-order information in an easy-to-compute way.'' I definitely can not agree with this point. Basically, q-neuron is the ``derivative'' of the original activation function, so there is no surprise that its derivative links to the second derivative of f(x). I can always use the high order derivative of some function as activation and claim now we are combining even higher order information into the neural network, but does that help? I don't think so. \n\nIt really annoys me to see that four out of the eight pages are occupied by gigantic figures, which should be placed in supplementary material in my opinion. A simple table could do the job equally well in the text. We are not interested in nitty-gritty details on how the training evolves. Let alone the datasets tested are all small-scale image classification tasks. At least the author(s) should diversify their test beds (e.g., NLP tasks and ImageNet scale experiments) and model architectures (e.g., RNN, ResNet). \n\nWhat's also missing from their experiments is a fair comparison with the real counterparts. I do not see comparisons with dropouts, and to more direct activation function randomization schemes (additive noise to regular activation functions). \n\nTo summarize, I can not approve this paper as it falls well below the acceptance level of an ICLR. In its current form, it's more like a sketchy note rather than a serious academic paper. I would encourage the authors to significantly enrich the content of this writing before considering resubmitting to another venue. \n", "rating": "2: Strong rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper139/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators", "abstract": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives, with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.\n", "keywords": ["q-calculus", "neural activation function"], "authorids": ["frank.nielsen@acm.org", "sunk.edu@gmail.com"], "authors": ["Frank Nielsen", "Ke Sun"], "TL;DR": "q-calculus helps build simple and scalable neural activation functions", "pdf": "/pdf/e7eaffe9345c90bc41e728e2f31f9207e11168c1.pdf", "paperhash": "nielsen|qneurons_neuron_activations_based_on_stochastic_jacksons_derivative_operators", "_bibtex": "@misc{\nnielsen2019qneurons,\ntitle={q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators},\nauthor={Frank Nielsen and Ke Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xkIjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper139/Official_Review", "cdate": 1542234529625, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper139/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335657450, "tmdate": 1552335657450, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper139/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkeiFZZKC7", "original": null, "number": 2, "cdate": 1543209347119, "ddate": null, "tcdate": 1543209347119, "tmdate": 1543209347119, "tddate": null, "forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper139/Official_Comment", "content": {"title": "Rebuttal", "comment": "\nWe would like to thank all reviewers for the helpful comments.\nWe would like to make the following points to improve the understanding of this work:\n\n-- q-neuron vs deterministic neurons + noise  (reviewer 3)\nIt is not straightforward to write q-neuron as a deterministic neuron plus noise.\nBy Figure 2, one can see that the noise has different conditional distributions\nwrt the input x and is therefore *not* independent to x.\n\n-- adoption of q-activation (reviewer 2)\nPractically, one may use q-neurons for\n(1) potentially better performance;\n(2) extremely-easy-to-implement noise-injected NN;\nTheoretically, q-neuron could be a first step of q-derivative into the deep learning realm.\n\n-- lack of monotonicity leads to more local optima (reviewer 2)\nIs it a very interesting direction to explore theoretically and thank you for pointing this out.\nWe would like to make the following points:\n(1) the number of local optima is different from the quality of local optima.\n(2) recent developments on deterministic neurons, such as Swish\n(Searching for activation functions, Ramachandran et al. 2018)\ndo not satisfy monotonicity as well.\n\n--proposition 2 (reviewer 1)\nThank you for pointing this out. It should be better phrased as\n\"The p-derivative of g_q (LHS) agrees with an affine combination of the pq-derivative of f (RHS).\"\n\n-- proposition 3 (reviewer 1)\nq-derivative is not simply the derivative of the original activation,\nIt has a profound theoretical background known as q-calculus \n(see e.g. Kac and Cheung 2001). We do appreciate your comments\non our empirical evaluation and presentation that we agree to\nimprove in further versions.\n\n-- dropout (reviewer 1/2/3)\nWe have already made experimental comparision with/without dropout,\nas presented in the left/right of Figure 3/4/5.\nOur observation is using q-neuron can bring more performance gains \nthan dropout.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper139/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper139/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper139/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators", "abstract": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives, with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.\n", "keywords": ["q-calculus", "neural activation function"], "authorids": ["frank.nielsen@acm.org", "sunk.edu@gmail.com"], "authors": ["Frank Nielsen", "Ke Sun"], "TL;DR": "q-calculus helps build simple and scalable neural activation functions", "pdf": "/pdf/e7eaffe9345c90bc41e728e2f31f9207e11168c1.pdf", "paperhash": "nielsen|qneurons_neuron_activations_based_on_stochastic_jacksons_derivative_operators", "_bibtex": "@misc{\nnielsen2019qneurons,\ntitle={q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators},\nauthor={Frank Nielsen and Ke Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xkIjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper139/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621622229, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1xkIjA9tX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper139/Authors", "ICLR.cc/2019/Conference/Paper139/Reviewers", "ICLR.cc/2019/Conference/Paper139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper139/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper139/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper139/Authors|ICLR.cc/2019/Conference/Paper139/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper139/Reviewers", "ICLR.cc/2019/Conference/Paper139/Authors", "ICLR.cc/2019/Conference/Paper139/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621622229}}}, {"id": "rklk9KW9hX", "original": null, "number": 3, "cdate": 1541179783046, "ddate": null, "tcdate": 1541179783046, "tmdate": 1541534249980, "tddate": null, "forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper139/Official_Review", "content": {"title": "Interesting theory paper, needs emphasis on usefulness in practice.", "review": "The authors introduce concept of q-calculus into neural networks along with its advantages. They define a family of stochastic activation functions based on standard functions together with q-calculus.\n\nI have a single question, if the proposed stochastic activation functions can also be achieved through deterministic neurons together with noise schemes like dropout (or any others)? If yes, is it still useful to use q-neurons. sorry, if I missed something obvious.\n\nAs mentioned by the authors the experiments only showcase a slight improvement in performance which may not be consistent when tried across larger set of experiments.", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper139/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators", "abstract": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives, with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.\n", "keywords": ["q-calculus", "neural activation function"], "authorids": ["frank.nielsen@acm.org", "sunk.edu@gmail.com"], "authors": ["Frank Nielsen", "Ke Sun"], "TL;DR": "q-calculus helps build simple and scalable neural activation functions", "pdf": "/pdf/e7eaffe9345c90bc41e728e2f31f9207e11168c1.pdf", "paperhash": "nielsen|qneurons_neuron_activations_based_on_stochastic_jacksons_derivative_operators", "_bibtex": "@misc{\nnielsen2019qneurons,\ntitle={q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators},\nauthor={Frank Nielsen and Ke Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xkIjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper139/Official_Review", "cdate": 1542234529625, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper139/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335657450, "tmdate": 1552335657450, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper139/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1e4gE9Oh7", "original": null, "number": 1, "cdate": 1541084140132, "ddate": null, "tcdate": 1541084140132, "tmdate": 1541534249551, "tddate": null, "forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "invitation": "ICLR.cc/2019/Conference/-/Paper139/Official_Review", "content": {"title": "On the adoption of q-activations", "review": "The authors describe q-activation functions, stochastic relatives of common activation functions used in neural networks.  It seems like the main argument is to use them because you get a performance improvement with them. \n\nWhile the experiments appear to show better training at early epochs, none of the models appear to have been trained to convergence.  Additional justifications for why (or when) to use this should be described.\n\nWhy does the method outperform particularly when dropout is included?\n\nI also expect the lack of monotonicity in the q-activation functions to lead to the creation of (exponentially) more local minima.  Any comments?\n\nQuality: the experiments need some further work.\nClarity: aside from a few points, the paper is written clearly.\nOriginality: the work appears original to me\nSignificance: TBD, but the main argument appears to be that it leads to empirical comparative gains (but on networks not designed to be SOTA).\n\nSmall points:\n\"By prop 2, g_q(x) agrees with with original activation function\".  What does \"agrees with\" mean?\n\"Fig 2. Darker color --> lighter color?\"\n\"(Conclusion) ... can goes[sic] deeper on the error surface.\" To me, the experiments only show marginally better performance", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper139/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators", "abstract": "We propose a new generic type of stochastic neurons, called $q$-neurons, that considers activation functions based on Jackson's $q$-derivatives, with stochastic parameters $q$. Our generalization of neural network architectures with $q$-neurons is shown to be both scalable and very easy to implement. We demonstrate experimentally consistently improved performances over state-of-the-art standard activation functions, both on training and testing loss functions.\n", "keywords": ["q-calculus", "neural activation function"], "authorids": ["frank.nielsen@acm.org", "sunk.edu@gmail.com"], "authors": ["Frank Nielsen", "Ke Sun"], "TL;DR": "q-calculus helps build simple and scalable neural activation functions", "pdf": "/pdf/e7eaffe9345c90bc41e728e2f31f9207e11168c1.pdf", "paperhash": "nielsen|qneurons_neuron_activations_based_on_stochastic_jacksons_derivative_operators", "_bibtex": "@misc{\nnielsen2019qneurons,\ntitle={q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative Operators},\nauthor={Frank Nielsen and Ke Sun},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xkIjA9tX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper139/Official_Review", "cdate": 1542234529625, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1xkIjA9tX", "replyto": "r1xkIjA9tX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper139/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335657450, "tmdate": 1552335657450, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper139/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 6}