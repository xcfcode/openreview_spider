{"notes": [{"id": "MuSYkd1hxRP", "original": "IUqWH_Q50dH", "number": 1019, "cdate": 1601308115302, "ddate": null, "tcdate": 1601308115302, "tmdate": 1616002771362, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1x48R1Ny_W", "original": null, "number": 1, "cdate": 1610040499187, "ddate": null, "tcdate": 1610040499187, "tmdate": 1610474105764, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "This is a solid paper providing the first theoretical convergence result for NAS and showing promising empirical results.\nThe authors' proposed GAEA method can be combined with different types of weight-sharing algorithms (DARTS, PC-DARTS, etc) and is likely to reduce the impact of the architecture discretization step due to finding sparser solutions.\nI clearly recommend acceptance and would expect this to make a nice spotlight."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040499174, "tmdate": 1610474105747, "id": "ICLR.cc/2021/Conference/Paper1019/-/Decision"}}}, {"id": "PqHnx49s1k", "original": null, "number": 10, "cdate": 1606230931880, "ddate": null, "tcdate": 1606230931880, "tmdate": 1606230931880, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "swNrP9spIgf", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your follow-up message.  \n\nWe would like to clarify the connection between single-level NAS, mirror descent, and our convergence result. First, note that we argue in favor of studying the single-level algorithm in theory, not that it should be better in practice. It is by studying the single-level optimization problem that we are able to extend recent results in non-convex mirror descent and obtain a convergence guarantee.  \n\nAs you correctly point out, this convergence guarantee can be used for a variety of algorithms. However, it is also prescriptive, in that it suggests using a method that has better guarantees over the geometry of interest; for the simplex case that arises in NAS the suggested method is GAEA. Thus our single-level analysis is directly connected to our proposed method.  \n\nOf course, as with any such analysis this approach has its limitations in practice; we discuss this in point 2 of our response to your original review. Still, this type of algorithmic comparison is not possible using results in existing work such as XNAS, and our geometric understanding of the problem enables the application of GAEA to numerous NAS methods, not just one.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "swNrP9spIgf", "original": null, "number": 9, "cdate": 1606188726161, "ddate": null, "tcdate": 1606188726161, "tmdate": 1606188726161, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "P2b9GWmeidm", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Response ", "comment": "Thank the authors for the detailed response to my original review. The explanation has clearly answer some of my concerns. In my opinion, the paper mainly states three things before the experiment: single-level NAS, mirror descent and the theorem of convergence. The paper discusses why single-level NAS is good. Although the algorithm is similar with XNAS, the paper gives another perspective to interpret the algorithm by mirror descent way. The paper also provides a theorem of convergence of NAS problem.\nHowever, I think the problem here is the is no strong connections stated among the three things. Lots of NAS algorithms are single level. Since \\phi in assumption 1 is general, the theorem 1 can be applied on other methods as well (if I understand correctly), such as single-level DARTS. These two parts seems separated with GAEA, and there is no story provided to smoothly contact these parts together, which makes your paper not so smooth, and readers cannot easily understand the main contribution of your paper.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "9GFhxyhsYay", "original": null, "number": 1, "cdate": 1602790043235, "ddate": null, "tcdate": 1602790043235, "tmdate": 1605641359907, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Review", "content": {"title": "Paper Review", "review": "## Summary\nThe submission presents a modification to the DARTS family of efficient Neural Architecture Search algorithms. The authors claim their modification (i) leads to better empirical performance, and (ii) is theoretically well-motivated. DARTS is a Neural Architecture Search algorithm which aims to find the most accurate network architecture within a human-defined search space.\n\nThe original DARTS algorithm uses gradient descent to optimize both (i) a set of shared model weights $\\mathbf{w}$ which are used to rank different candidate architectures within a search space and (ii) a vector $\\theta \\in \\mathbf{R}^n$ which corresponds to a continuous (real-valued) relaxation the network architecture. At the end of the search, $\\theta$ is converted into a discrete architecture within the search space; this architecture is the output of the search process. The authors' proposed variant of DARTS -- which they call GAEA (Geometry-Aware Gradient Algorithm) -- updates the architectural parameters $\\theta$ using *exponentiated* gradient descent instead of standard gradient descent. (Details are provided in Equation (8) in Section 3.3 of their paper, but the modification seems straightforward to implement.) The authors claim this modification favors sparse values of $\\theta$, and helps decrease the gap between discrete architectures and their continuous relaxations.\n\nOn the applied side: The authors evaluate efficient NAS algorithms with and without their modification on three different benchmark tasks. On two of the tasks, the performance of their modified algorithm seems quite close to that of a baseline without the modification. On the third task (NASBench-201), the improvements appear to be quite substantial in one case. In most (but not all) cases, the authors directly compare the results of architecture searches obtained with and without their proposed modification. This type of control experiment is important and very helpful for a NAS paper, since it makes it possible to directly quantify the helpfulness of the proposed changes.\n\nOn the theoretical side: I had trouble understanding the significance of the authors' theoretical results. In particular: the paper claims to prove stationary-point convergence bounds for a variant of their algorithm but doesn't explain what stationary-point convergence means or why it's important. I'm very much on the applied side of machine learning, so it's possible that someone with a stronger theory background would get more out of the theoretical results. However, I also think the theoretical results could be much better-explained than they currently are. (I read through Section 3, but have not tried to check the theoretical results carefully.)\n\n*Pros:*\n* Paper provides experiments on three different benchmark tasks, and reports results for multiple NAS algorithms (mostly based on DARTS).\n* Strong empirical results for some of the NASBench-201 experiments.\n* Paper appears to provide solid baselines for most experiments.\n* Proposed approach is not overly complex, and seems straightforward to implement.\n\n*Cons:*\n* Novelty is limited, since the authors are proposing a relatively limited change to an existing family of Efficient NAS algorithms. (This would, however, be a non-issue if the proposed changes were effective enough.)\n* Unclear presentation of theoretical results.\n* Quality improvements seem quite limited on 2 of the 3 benchmark tasks. (Part of the problem might -- as the authors suggest -- be due to limited headroom.)\n* For the benchmark task with the largest improvement -- NASBench-201 -- the authors only provide their own baseline for 1 of the 3 NAS algorithms they evaluate, and for this variant (GDAS), the improvements over the baseline are quite small. So it's not clear to me how much of the improvements for the other two algorithms can be attributed to GAEA and how much can be attributed to NAS setups that are better tuned than the ones from the original NASBench-201 paper.\n\n## Notes on Experimental Evaluation\nThe abstract makes a strong claim about the quality improvements brought about by the authors' proposed modification, so I'll evaluate the paper relative to that claim: \"we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100.\"\n\nThe submission evaluates their method on three tasks: (i) the search space from the DARTS paper, (ii) the NASBench-1shot1 benchmark task, and (iii) the NASBench-201 benchmark task. On the plus side: the authors appear to provide direct comparisons between DARTS-based NAS algorithms with and without their proposed GAEA modifications (although this requires some confirmation of details which are currently missing from the paper). On the minus side: the improvements of GAEA algorithms over their non-GAEA equivalents seem quite small in most cases. (The authors attribute this to limited headroom in the search spaces, which seems plausible to me.)\n\nOn the original DARTS search space, the authors' proposed GAEA PC-DARTS algorithm produces architectures with an average test error rate of 2.50%, compared with 2.57% for standard PC-DARTS. The authors do not provide numbers for their own reproduction of the original PC-DARTS results; the writing implies that the hyper-parameters and search space are comparable. Providing numbers for the authors' reproduction of PC-DARTS would strengthen these results. Assuming the current numbers are in fact comparable: the performance improvements appear to be extremely small, and are probably within the range of statistical error.\n\nOn the NASBench-1shot1 search space, the authors claim that their proposed GAEA PC-DARTS algorithm works slightly better than an unmodified PC-DARTS, although the improvements appear to be small. While this seems reasonable given the data they provided, I think the presentation of results can be improved; the results presented in Figure 2 are difficult for me to interpret because the plots are visually noisy and contain many overlapping lines of different colors.\n\nNASBench-201 is the most interesting (and potentially most appealing) case. The authors provide accuracy numbers for both their reproduction of the existing GDAS algorithm and their proposed GAEA-GDAS variant, and performance of the two seems quite similar (e.g., 93.55% test accuracy on CIFAR-10 for GDAS-GAEA compared with 93.52% for GDAS). The gap is larger on CIFAR-100, but primarily because the authors' reproduction of GDAS has lower accuracy than the published GDAS numbers from the NASBench-201 paper. For other DARTS variants, the accuracy improvements are quite impressive (e.g., 94.10% accuracy for GAEA-DARTS-ERM), but the authors do not provide a non-GAEA baseline for this configuration. Furthermore, the accuracies that the original NASBench-201 paper provides for baselines such as ENAS and DARTS are extremely low, so I think that providing a careful reproduction of DARTS-ERM with properly tuned hyper-parameters would help make the results more convincing.\n\n## Notes on Clarity\nWhile the paper seems reasonably well-organized and well-proofread, I found parts of it difficult to understand. I had trouble understanding Section 3 (which presents the paper's main theoretical results) in particular. The section is notation and terminology-heavy, and would be easier to follow if it spent more time explaining relevant terms.\n\nI found Table 1 difficult to interpret because different rows of the table contain the results of architecture searches performed using different search spaces and/or model training hyper-parameters. For example, ProxylessNAS uses a very different search space than PC-DARTS and GAEA PC-DARTS. While the authors note this particular example in the text under the table, the table would be much easier to interpret if it was organized to show which subsets of the results were comparable to each other (i.e., used the same model training hyper-parameters and search space).\n\nI also had trouble understanding Figure 2, which contains 8-10 overlapping red/blue lines. I also had trouble understanding the relationship between the \"PC-DARTS\" and \"Best from Zela et al.\" lines (since Zela et al. evaluate PC-DARTS in their paper). It would be helpful to clarify the distinction between the two.\n\n\n## Additional Notes\n\nI had trouble understand this sentence from the end of Section 2.2: \"Furthermore, for methods that adapt architecture parameters during search, it [using a first-order method] makes clear that we need not worry about rank disorder as long as we can optimize and generalize\"\nWhy don't first-order methods need to worry about rank disorder? This needs to be better explained.\n\nIn Equation (5), what does the $\\bigodot$ symbol denote? My best guess is that it refers to an elementwise product, but it would be helpful to state the meaning more clearly.\n\nIn Section 3.2: It's not clear to me what the significance of Theorem 1 is in practice. What is the significance of having a small expected value for $\\Delta$ or of being close to an approximate stationary point? What does it mean, and why is it desirable? This needs to be better-explained.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129037, "tmdate": 1606915799899, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1019/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Review"}}}, {"id": "6qUkDc2W8r2", "original": null, "number": 8, "cdate": 1605641345224, "ddate": null, "tcdate": 1605641345224, "tmdate": 1605641345224, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "9GFhxyhsYay", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Review Updates", "comment": "I'd like to thank the authors for their detailed response to my original review. Based on the authors' feedback and their updates to the submission, I plan to update my score for the paper from 4 to 7.\n\nHere are the two biggest reasons for my score change:\n\n* The authors added accuracy numbers for their own reproductions of DARTS (ERM) and DARTS (bilevel) in Table 2. The addition of these baselines makes me much confidence in the effectiveness of the authors' proposed algorithmic modifications, since they allow me directly compare the results of the authors' GAEA DARTS results against the non-GAEA equivalents.\n* The authors improved their exposition of theoretical results, and added a few paragraphs in Section 3.2 to explain the significance of their $\\epsilon$-stationary-point convergence results. (I'd also like to thank the authors for the explanation they provided in their OpenReview comments.)\n\nThe authors also addressed many of my lower-level comments. For example:\n\n* Formally defining the $\\odot$ symbol in Section 3.1 of the paper.\n* Cleaning up Figure 2 and adding additional information to its caption.\n* Clarifying in an OpenReview post that \"GAEA-based modifications of DARTS are not tuned on NAS-Bench-201.\""}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "CbDV9XjETg", "original": null, "number": 6, "cdate": 1605572881756, "ddate": null, "tcdate": 1605572881756, "tmdate": 1605572881756, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "9GFhxyhsYay", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 1: main concerns)", "comment": "Thank you for the detailed feedback. Below we respond to your questions/criticisms.\n\nResponses to main concerns:\n1. _\u201cNovelty is limited, since the authors are proposing a relatively limited change to an existing family of Efficient NAS algorithms. (This would, however, be a non-issue if the proposed changes were effective enough.)\u201d_  \nThe novelty of the paper is not just in the empirical results and technical method but also in the optimization perspective on NAS with weight-sharing. As we argue below, we also believe our proposed changes are effective.\n\n2. _\u201cUnclear presentation of theoretical results.\u201d \u201cIn particular: the paper claims to prove stationary-point convergence bounds for a variant of their algorithm but doesn't explain what stationary-point convergence means or why it's important.\u201d_  \n\u03b5-stationary-point convergence is the standard measure of success in non-convex optimization; in the unconstrained Euclidean case it corresponds to the expected norm of the gradient being smaller than \u03b5. This is significant because it implies we have reached a point that satisfies a necessary condition of global optimality (small gradient) and where most first-order algorithms do not make further progress, thus measuring the speed at which a first-order method effectively terminates. In the revision we have a substantially more detailed discussion of this in Section 3.2. \n\n3. _\u201cQuality improvements seem quite limited on 2 of the 3 benchmark tasks. (Part of the problem might -- as the authors suggest -- be due to limited headroom.)\u201d \u201cOn two of the tasks, the performance of their modified algorithm seems quite close to that of a baseline without the modification. On the third task (NASBench-201), the improvements appear to be quite substantial in one case.\u201d_  \nWe disagree that the improvement is small on the DARTS Search Space benchmark. While the changes on CIFAR-10 are small numerically due to the saturation on this dataset, the GAEA PC-DARTS average is one standard deviation better than the PC-DARTS average, and the best of 10 seeds is more than 3.5 standard deviations better than the PC-DARTS average; given the overall small remaining room for improvement and the fact that PC-DARTS was the state-of-the-art on this benchmark, we believe these results are important. Furthermore, we observe larger improvements on the ImageNet evaluation, especially in the setting of CIFAR-10 -> ImageNet transfer NAS, where GAEA PC-DARTS improves upon DARTS by 0.8% in terms of Top-1 error. We agree that on the NAS-Bench-1Shot1 search space the improvement due to GAEA is small, with existing algorithms already approaching optimality. \n\n4. _\u201cFor the benchmark task with the largest improvement -- NASBench-201 -- the authors only provide their own baseline for 1 of the 3 NAS algorithms they evaluate, and for this variant (GDAS), the improvements over the baseline are quite small. So it's not clear to me how much of the improvements for the other two algorithms can be attributed to GAEA and how much can be attributed to NAS setups that are better tuned than the ones from the original NASBench-201 paper.\u201d_  \nWe did not report our own reproductions of the other algorithms because we had no issues reproducing them, whereas we did for GDAS. Note that we did compare to our own evaluation of bilevel DARTS in the Appendix (Figure 4), showing that it exhibited the same poor performance. In revision we have added our own experiments for DARTS (ERM and bilevel) to Table 2; they show that the substantial improvements on both using GAEA are a result of our method and not different setups. Note that GAEA-based modifications of DARTS are not tuned on NAS-Bench-201: the only hyperparameter is the architecture learning rate, which we set to 0.1 for all DARTS/PC-DARTS experiments on all search spaces."}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "bST0oaw3Ibl", "original": null, "number": 5, "cdate": 1605572779961, "ddate": null, "tcdate": 1605572779961, "tmdate": 1605572779961, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "9GFhxyhsYay", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Response to AnonReviewer1 (Part 2: notes and questions)", "comment": "- _\u201c[T]he improvements of GAEA algorithms over their non-GAEA equivalents seem quite small in most cases. (The authors attribute this to limited headroom in the search spaces, which seems plausible to me.) [..] The authors do not provide numbers for their own reproduction of the original PC-DARTS results; the writing implies that the hyper-parameters and search space are comparable. Providing numbers for the authors' reproduction of PC-DARTS would strengthen these results.\u201d_  \nPlease see our response to concern 3 above regarding significance on the DARTS search space. We did not fully reproduce PC-DARTS results because we were able to recover the reported performance in initial tests and our settings are identical both in terms of the search space as well as the hyperparameter used for evaluation. Note that a full re-evaluation on this search space is very costly (close to $2,000 on AWS).\n\n- _\u201c[T]he results presented in Figure 2 are difficult for me to interpret because the plots are visually noisy and contain many overlapping lines of different colors. [..] I also had trouble understanding the relationship between the \"PC-DARTS\" and \"Best from Zela et al.\" lines (since Zela et al. evaluate PC-DARTS in their paper).\u201d_  \nIn the revision we have slightly enlarged the plots and filled in the error bounds to have fewer individual lines. The \u201cPC-DARTS\u201d line is our reproduction of PC-DARTS on NAS-Bench-1Shot1, which performed better than the best result in Zela et al. (2020b) on search spaces 1 and 3 (this is why the dashed line is higher than the blue line in those two cases). We have clarified this point in the revised caption.\n\n- _\u201cFor other DARTS variants, the accuracy improvements are quite impressive (e.g., 94.10% accuracy for GAEA-DARTS-ERM), but the authors do not provide a non-GAEA baseline for this configuration. Furthermore, the accuracies that the original NASBench-201 paper provides for baselines such as ENAS and DARTS are extremely low, so I think that providing a careful reproduction of DARTS-ERM with properly tuned hyper-parameters would help make the results more convincing.\u201d_  \nPlease see our response to concern 4.\n\n- _\u201cI had trouble understanding Section 3 (which presents the paper's main theoretical results) in particular. The section is notation and terminology-heavy, and would be easier to follow if it spent more time explaining relevant terms.\u201d_  \nWe have significantly expanded upon the discussion of stationarity in Section 3.2 of the revision.\n\n- _\u201cI found Table 1 difficult to interpret because different rows of the table contain the results of architecture searches performed using different search spaces and/or model training hyper-parameters. For example, ProxylessNAS uses a very different search space than PC-DARTS and GAEA PC-DARTS. While the authors note this particular example in the text under the table, the table would be much easier to interpret if it was organized to show which subsets of the results were comparable to each other (i.e., used the same model training hyper-parameters and search space).\u201d_  \nIn the revision we have divided Table 1 into methods evaluated on the DARTS search space (bottom) and on others (top). The only other option we see is to remove the top three methods entirely, but we believe they serve as a useful comparison. While evaluation routines for all methods on the DARTS search space are largely comparable, we specifically use the PC-DARTS routine, which differs slightly from DARTS (drop-path probability changed from 0.2 to 0.3). We have edited the caption to reflect this.\n\n- _\u201cI had trouble understand this sentence from the end of Section 2.2: \"Furthermore, for methods that adapt architecture parameters during search, it [using a first-order method] makes clear that we need not worry about rank disorder as long as we can optimize and generalize\" Why don't first-order methods need to worry about rank disorder? This needs to be better explained.\u201d_  \nThe \u201cit\u201d here is \u201cthe single-level formulation,\u201d not \u201cusing a first-order method.\u201d Our argument is that all (bilevel and single-level) weight-sharing methods (except those like RS-WS that do not update architecture parameters) do not need to worry about rank-disorder, since the main requirement is a well-generalizing solution to an optimization problem, which requires outputting a single point rather than a ranking. We have modified this discussion in the revision.\n\n- _\u201cIn Equation (5), what does the \u2a00 symbol denote? My best guess is that it refers to an elementwise product, but it would be helpful to state the meaning more clearly.\u201d_  \nYes, it stands for elementwise product. We have defined this in the revision.\n\n- _\u201cIn Section 3.2: It's not clear to me what the significance of Theorem 1 is in practice. What is the significance of having a small expected value for \u0394 or of being close to an approximate stationary point? What does it mean, and why is it desirable?\u201d_  \nPlease see our response to concern 2."}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "i8iUCFs0RhK", "original": null, "number": 2, "cdate": 1605570847014, "ddate": null, "tcdate": 1605570847014, "tmdate": 1605571924912, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Revision summary", "comment": "We thank all three reviewers for thoughtful reviews. We have uploaded a revision incorporating the feedback, including the following changes:\n\n- Improved readability of Figure 2 (following comments by reviewers 1 and 2).\n- Expanded discussion in Sections 2 and 3 (following comments by reviewers 1 and 2).\n- Cleaning up the presentation of Table 1 (following comments by reviewer 1).\n- Additional baseline results in Table 2 (following comments by reviewer 1).\n- A conclusion section (following comments by reviewer 2).\n- Minor fixes.\n\nWe are happy to answer any further questions and would appreciate additional feedback."}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "jpY7vV9CmjI", "original": null, "number": 4, "cdate": 1605571908739, "ddate": null, "tcdate": 1605571908739, "tmdate": 1605571908739, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "xV6KPHQkNn", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for the positive feedback. We would be happy to answer any further questions stemming from the revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "P2b9GWmeidm", "original": null, "number": 3, "cdate": 1605571814520, "ddate": null, "tcdate": 1605571814520, "tmdate": 1605571814520, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "NXPecoAyuyU", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for the positive feedback. Below we respond to your questions/criticisms.\n\n1.  \n   - _\u201cUsing EG to update architecture parameters can only accelerate convergence, which has nothing to do with improving the NAS performance. [...] It is not clear that the slight improvement on the performance is due to your algorithm or accident.\u201d_  \nAs we discuss in Sections 3.3 and 4.1, EG does not just affect convergence speed but is also known to encourage sparsity; in our case this means it is more likely to output sparse architecture parameters that do not require as much post-search discretization. We show in Figure 1 that NAS methods modified by GAEA do indeed yield sparser architecture parameters. Loss due to post-search discretization is a well-known issue of DARTS-like NAS methods, and we give quantitative evidence in Section 4.1 that the drop in validation accuracy due to discretization is indeed smaller for GAEA.  \n   - _\u201cIt is still confusing that why single-level optimization can resolve rank disorder and poor performance.\u201d_  \nOur argument in Section 2 is not that single-level optimization resolves rank disorder, but rather that we do not need to worry about rank disorder so long as we can use optimization and regularization techniques to output a single good architecture. The usefulness of single-level optimization is (1) making this point more clear by expressing weight-sharing NAS as a regular deep learning optimization problem needing only a point solution and (2) serving as a useful object for theoretical study. We have modified our discussion of this in the revision.\n\n2. _\u201cEfficiency is claimed as an important point in this paper. However, only the results in Table 2 shows GAEA shorten the time cost. In Table 1, it cannot be detected that your method is more efficient. Is that because updating architecture parameters does not cost too much time in these experiment? If so, the contribution may be less.\u201d_  \nOn NAS-Bench-201 (Table 2), we found that GAEA-modified methods found good architectures much faster than the original methods and could be stopped early, leading to speedups. On the DARTS Search Space (Table 1), we found that both PC-DARTS and GAEA PC-DARTS yield good architectures when stopped early (GAEA is somewhat better earlier on, but not significantly so), so for fairness and consistent comparison with previous papers we used the same number of search epochs in both cases. Finally, note that most supernet training procedures are effectively early-stopped (as evidenced by fewer training epochs during architecture search than architecture evaluation) since the network has to be retrained anyway, so the faster EG convergence likely manifests itself in better final accuracy since the supernet is closer to convergence at the end of search: we give evidence of this in Section 4.1, where the GAEA PC-DARTS supernet has a 2.9% better final validation accuracy than the PC-DARTS supernet, and in Figure 4 of the appendix, which shows that GAEA DARTS converges on NAS-Bench-201 while DARTS does not.\n\n3. _\u201cIn Figure 2, it is not easy to detect the performance difference between your method and the baseline. You should also explain the meaning of lines with deeper colors.\u201d_  \nIn the revision we have slightly enlarged the plots, filled in the error bounds to have fewer individual lines, and explained the meaning of the different lines in the caption.\n\n4. _\u201cIt is better to add discussion or conclusion at the end of your paper, which can help readers to better understand your work.\u201d_  \nThank you for the suggestion; we have added a conclusion in the revision.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "MuSYkd1hxRP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1019/Authors|ICLR.cc/2021/Conference/Paper1019/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864595, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Comment"}}}, {"id": "xV6KPHQkNn", "original": null, "number": 2, "cdate": 1603862695763, "ddate": null, "tcdate": 1603862695763, "tmdate": 1605024550191, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Review", "content": {"title": "Well written and SOTA methods.", "review": "This paper introduces the geometry-aware framework that can be adapted to any existing weight-sharing NAS methods optimized over gradient descent. The authors focus on the aspect of optimizing the architecture parameters to overcome the criticism of weight-sharing methods. The author's method relies on the mirror descent supporting their methods with a theoretical guarantee for the fast convergence. The author also supports their methods on various datasets such as CIFAR-10, ImageNet, NAS-Bench-201 (Dong & Yang), and NAS-Bench-1Shot (Zela et al.).\n\nThe paper supports their ideas not only supported by theoretical background, but outperforming results consistently with extensive empirical experiments such as CIFAR-10, ImageNet, NAS-Bench-1Shot1, and NAS-Bench-201. Furthermore, the GAEA can easily be applied to existing NAS methods which I believe making this work more valuable. \n\nStrength\n1. Their methods easily apply to existing NAS methods with gradient-based methods. \n2. Theoretically supported methods with convergence guarantee. \n3. Extensive empirical experiments on CIFAR-10, ImageNet, NAS-Bench-1Shot1, and NAS-Bench-201. Moreover, detailed experiment descriptions and fair setups that are critical in NAS comparison are provided.\n4. Novel perspective of view (optimization perspective) to overcoming the weight-sharing methods' criticism of recent works. \n5. Reproducible code included along with the paper. \n\nOverall, I recommend clear acceptance. This paper will provide new insights/perspective to NAS algorithms which adopts weight-sharing methods. \n\nReference\n1. Dong & Yang. NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search (ICLR 2020)\n2. Zela et al. NAS-Bench-1Shot1: Benchmarking and Dissecting One-Shot Neural Architecture Search (ICLR 2020)", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129037, "tmdate": 1606915799899, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1019/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Review"}}}, {"id": "NXPecoAyuyU", "original": null, "number": 3, "cdate": 1603904528421, "ddate": null, "tcdate": 1603904528421, "tmdate": 1605024550125, "tddate": null, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "invitation": "ICLR.cc/2021/Conference/Paper1019/-/Official_Review", "content": {"title": "This paper argues for using a single level objective on weight-sharing NAS, and proposes GAEA, which uses exponentiated gradient to update architecture parameters, to accelerate the convergence. The paper gives a proof to guarantee finite-time convergence. The experiment results show this method is efficient and can slightly improve the performance.", "review": "Pros:\n1.\tThis paper gives a proof of finite-time convergence, which is the first paper working on this. Besides, the paper gives corresponding analysis of ENAS and DARTS. This is a new perspective of NAS methods.\n2.\tThis work uses EG method to update architecture parameter, which takes the advantage of EG method and is reasonable to be applied on NAS problem.\n3.\tThe experiment results show the efficiency and effectiveness of the proposed method.\n\nCons:\n1.\tUsing EG to update architecture parameters can only accelerate convergence, which has nothing to do with improving the NAS performance. It is still confusing that why single-level optimization can resolve rank disorder and poor performance. It is not clear that the slight improvement on the performance is due to your algorithm or accident.\n2.\tEfficiency is claimed as an important point in this paper. However, only the results in Table 2 shows GAEA shorten the time cost. In Table 1, it cannot be detected that your method is more efficient. Is that because updating architecture parameters does not cost too much time in these experiment? If so, the contribution may be less.\n3.\tIn Figure 2, it is not easy to detect the performance difference between your method and the baseline. You should also explain the meaning of lines with deeper colors.\n4.\tIt is better to add discussion or conclusion at the end of your paper, which can help readers to better understand your work.\n\nOverall Review:\nThis paper gives a theory about the convergence time of NAS methods, which provides new perspectives on NAS problem. The paper find that EG method is appropriate on updating architecture parameters and this method can improve the efficiency of NAS problem. There are also some questions mentioned above in this paper. With some modifications, this paper could be an excellent paper.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1019/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1019/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Geometry-Aware Gradient Algorithms for Neural Architecture Search", "authorids": ["~Liam_Li1", "~Mikhail_Khodak1", "~Nina_Balcan1", "~Ameet_Talwalkar1"], "authors": ["Liam Li", "Mikhail Khodak", "Nina Balcan", "Ameet Talwalkar"], "keywords": ["neural architecture search", "automated machine learning", "weight-sharing", "optimization"], "abstract": "Recent state-of-the-art methods for neural architecture search (NAS) exploit gradient-based optimization by relaxing the problem into continuous optimization over architectures and shared-weights, a noisy process that remains poorly understood. We argue for the study of single-level empirical risk minimization to understand NAS with weight-sharing, reducing the design of NAS methods to devising optimizers and regularizers that can quickly obtain high-quality solutions to this problem. Invoking the theory of mirror descent, we present a geometry-aware framework that exploits the underlying structure of this optimization to return sparse architectural parameters, leading to simple yet novel algorithms that enjoy fast convergence guarantees and achieve state-of-the-art accuracy on the latest NAS benchmarks in computer vision. Notably, we exceed the best published results for both CIFAR and ImageNet on both the DARTS search space and NAS-Bench-201; on the latter we achieve near-oracle-optimal performance on CIFAR-10 and CIFAR-100. Together, our theory and experiments demonstrate a principled way to co-design optimizers and continuous relaxations of discrete NAS search spaces.", "one-sentence_summary": "Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "li|geometryaware_gradient_algorithms_for_neural_architecture_search", "supplementary_material": "/attachment/40d1293e30dfebc4910bedf126623e6a574d2638.zip", "pdf": "/pdf/110552d41d9f40c3d50988fde09b3b5038c2bebd.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nli2021geometryaware,\ntitle={Geometry-Aware Gradient Algorithms for Neural Architecture Search},\nauthor={Liam Li and Mikhail Khodak and Nina Balcan and Ameet Talwalkar},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=MuSYkd1hxRP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "MuSYkd1hxRP", "replyto": "MuSYkd1hxRP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1019/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129037, "tmdate": 1606915799899, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1019/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1019/-/Official_Review"}}}], "count": 13}