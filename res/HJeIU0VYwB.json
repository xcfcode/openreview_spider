{"notes": [{"id": "HJeIU0VYwB", "original": "SkxTebwuwr", "number": 1140, "cdate": 1569439310213, "ddate": null, "tcdate": 1569439310213, "tmdate": 1577168246543, "tddate": null, "forum": "HJeIU0VYwB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE", "authors": ["Yue Zhao", "Xiangsheng Huang", "Ludan Kou"], "authorids": ["oasis.random.time@gmail.com", "xiangsheng.huang@ia.ac.cn", "2015019051@mail.buct.edu.cn"], "keywords": ["Optimization", "Adaptive Methods", "Convergence", "Convolutional Neural Network"], "TL;DR": "This work proposes a novel generic framework, in which we explicitly analyze different behaviors brought by various types of \u03a6(\u00b7),  and based on the framework we propose a more adaptive optimization algorithm.", "abstract": "Although adaptive algorithms have achieved significant success in training deep neural networks with faster training speed, they tend to have poor generalization performance compared to SGD with Momentum(SGDM). One of the state-of-the-art algorithms, PADAM, is proposed to close the generalization gap of adaptive methods while lacking an internal explanation. This work pro- poses a general framework, in which we use an explicit function \u03a6(\u00b7) as an adjustment to the actual step size, and present a more adaptive specific form AdaPlus(Ada+). Based on this framework, we analyze various behaviors brought by different types of \u03a6(\u00b7), such as a constant function in SGDM, a linear function in Adam, a concave function in Padam and a concave function with offset term in AdaPlus. Empirically, we conduct experiments on classic benchmarks both in CNN and RNN architectures and achieve better performance(even than SGDM).\n", "pdf": "/pdf/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "code": "https://anonfiles.com/daV7Ed6enb/AdaPlus_zip", "paperhash": "zhao|ada_a_generic_framework_with_more_adaptive_explicit_adjustment_for_learning_rate", "original_pdf": "/attachment/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "_bibtex": "@misc{\nzhao2020ada,\ntitle={{\\{}ADA{\\}}+: A {\\{}GENERIC{\\}} {\\{}FRAMEWORK{\\}} {\\{}WITH{\\}} {\\{}MORE{\\}} {\\{}ADAPTIVE{\\}} {\\{}EXPLICIT{\\}} {\\{}ADJUSTMENT{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}RATE{\\}}},\nauthor={Yue Zhao and Xiangsheng Huang and Ludan Kou},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIU0VYwB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2sH993qYbi", "original": null, "number": 1, "cdate": 1576798715543, "ddate": null, "tcdate": 1576798715543, "tmdate": 1576800920982, "tddate": null, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "invitation": "ICLR.cc/2020/Conference/Paper1140/-/Decision", "content": {"decision": "Reject", "comment": "In this paper, the authors proposed a general framework, which uses an explicit function as an adjustment to the actual learning rate, and presented a more adaptive specific form Ada+. Based on this framework, they analyzed various behaviors brought by different types of the function. Empirical experiments on benchmarks demonstrate better performance than some baseline algorithms. The main concern of this paper is: (1) lack of justification or interpretation for the proposed framework; (2) the performance of the proposed algorithm is on a par with Padam; (3) missing comparison with some other baselines on more benchmark datasets. Plus, the authors did not submit response.  I agree with the reviewers\u2019 evaluation.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE", "authors": ["Yue Zhao", "Xiangsheng Huang", "Ludan Kou"], "authorids": ["oasis.random.time@gmail.com", "xiangsheng.huang@ia.ac.cn", "2015019051@mail.buct.edu.cn"], "keywords": ["Optimization", "Adaptive Methods", "Convergence", "Convolutional Neural Network"], "TL;DR": "This work proposes a novel generic framework, in which we explicitly analyze different behaviors brought by various types of \u03a6(\u00b7),  and based on the framework we propose a more adaptive optimization algorithm.", "abstract": "Although adaptive algorithms have achieved significant success in training deep neural networks with faster training speed, they tend to have poor generalization performance compared to SGD with Momentum(SGDM). One of the state-of-the-art algorithms, PADAM, is proposed to close the generalization gap of adaptive methods while lacking an internal explanation. This work pro- poses a general framework, in which we use an explicit function \u03a6(\u00b7) as an adjustment to the actual step size, and present a more adaptive specific form AdaPlus(Ada+). Based on this framework, we analyze various behaviors brought by different types of \u03a6(\u00b7), such as a constant function in SGDM, a linear function in Adam, a concave function in Padam and a concave function with offset term in AdaPlus. Empirically, we conduct experiments on classic benchmarks both in CNN and RNN architectures and achieve better performance(even than SGDM).\n", "pdf": "/pdf/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "code": "https://anonfiles.com/daV7Ed6enb/AdaPlus_zip", "paperhash": "zhao|ada_a_generic_framework_with_more_adaptive_explicit_adjustment_for_learning_rate", "original_pdf": "/attachment/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "_bibtex": "@misc{\nzhao2020ada,\ntitle={{\\{}ADA{\\}}+: A {\\{}GENERIC{\\}} {\\{}FRAMEWORK{\\}} {\\{}WITH{\\}} {\\{}MORE{\\}} {\\{}ADAPTIVE{\\}} {\\{}EXPLICIT{\\}} {\\{}ADJUSTMENT{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}RATE{\\}}},\nauthor={Yue Zhao and Xiangsheng Huang and Ludan Kou},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIU0VYwB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795727602, "tmdate": 1576800279872, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1140/-/Decision"}}}, {"id": "SygSmKDatB", "original": null, "number": 2, "cdate": 1571809565075, "ddate": null, "tcdate": 1571809565075, "tmdate": 1572972507733, "tddate": null, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "invitation": "ICLR.cc/2020/Conference/Paper1140/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a general framework for adaptive algorithms, and presents a specific form: ADAPLUS. In the theory part, this work gives convergence analysis of ADAPLUS. For experiments, this work analyzes several algorithm's empirical performances including SGDM, ADAM, AMSGRAD, PADAM, ADAPLUS on CV and NLP tasks. \n\n1. This paper's analysis is not solid enough to support its claim. Since this paper gives a general framework and claims that offset term can achieve superior performance, it is better to give the convergence analysis of general algorithm in the framework and discuss the benefit of the offset term theoretically. Actually, the author gives almost the same theoretical result as ADAM type algorithms, from which I did not see the advantage of using ADAPLUS. \n\n2. And the experiment shows that ADAPLUS performs on par with PADAM on CV task. \nI wonder why the authors did not give the experimental result of PADAM on NLP task? \n\n3.The notation in this paper is quite confusing. In page 2 definition (2.1), v is a vector so the L^p norm of g is actually a vector. But in Figure on page 2, the paper treat it as a scalar and I cannot understand it without further explanation from the authors.\n\nTherefore, I think this work doesn't make enough contribution and the novelty is not enough for ICLR standard.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1140/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1140/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE", "authors": ["Yue Zhao", "Xiangsheng Huang", "Ludan Kou"], "authorids": ["oasis.random.time@gmail.com", "xiangsheng.huang@ia.ac.cn", "2015019051@mail.buct.edu.cn"], "keywords": ["Optimization", "Adaptive Methods", "Convergence", "Convolutional Neural Network"], "TL;DR": "This work proposes a novel generic framework, in which we explicitly analyze different behaviors brought by various types of \u03a6(\u00b7),  and based on the framework we propose a more adaptive optimization algorithm.", "abstract": "Although adaptive algorithms have achieved significant success in training deep neural networks with faster training speed, they tend to have poor generalization performance compared to SGD with Momentum(SGDM). One of the state-of-the-art algorithms, PADAM, is proposed to close the generalization gap of adaptive methods while lacking an internal explanation. This work pro- poses a general framework, in which we use an explicit function \u03a6(\u00b7) as an adjustment to the actual step size, and present a more adaptive specific form AdaPlus(Ada+). Based on this framework, we analyze various behaviors brought by different types of \u03a6(\u00b7), such as a constant function in SGDM, a linear function in Adam, a concave function in Padam and a concave function with offset term in AdaPlus. Empirically, we conduct experiments on classic benchmarks both in CNN and RNN architectures and achieve better performance(even than SGDM).\n", "pdf": "/pdf/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "code": "https://anonfiles.com/daV7Ed6enb/AdaPlus_zip", "paperhash": "zhao|ada_a_generic_framework_with_more_adaptive_explicit_adjustment_for_learning_rate", "original_pdf": "/attachment/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "_bibtex": "@misc{\nzhao2020ada,\ntitle={{\\{}ADA{\\}}+: A {\\{}GENERIC{\\}} {\\{}FRAMEWORK{\\}} {\\{}WITH{\\}} {\\{}MORE{\\}} {\\{}ADAPTIVE{\\}} {\\{}EXPLICIT{\\}} {\\{}ADJUSTMENT{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}RATE{\\}}},\nauthor={Yue Zhao and Xiangsheng Huang and Ludan Kou},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIU0VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575714306355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1140/Reviewers"], "noninvitees": [], "tcdate": 1570237741775, "tmdate": 1575714306371, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1140/-/Official_Review"}}}, {"id": "S1gUbdrTFH", "original": null, "number": 1, "cdate": 1571801086056, "ddate": null, "tcdate": 1571801086056, "tmdate": 1572972507685, "tddate": null, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "invitation": "ICLR.cc/2020/Conference/Paper1140/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This work proposes a modification to the ADAM optimizer by introducing an adjustment function, which consists of a square root function and an extra parameter delta. Although the modification is very simple and easy to implement, the theoretical analysis is weak and the empirical performances of the proposed method are similar to the previous adaptive methods.   \n\nHere are my main concerns of the current paper:\n1. The presentation is a little bit confusing in the motivation section. According to equation (2.1), the $L^p$ norm of $\\mathbf{g}_t$ is defined as a vector. However, it seems that the authors treat $\\|\\mathbf{g}_t\\|_p$ as a scalar when presenting the intuition of the proposed method in Figure 1.\n\n2. I do not understand why Figure 1 says the proposed method is better than Padam. It seems to me that if Padam choose a specific p, it can recover the proposed $\\Phi$ function, or even better than the proposed method. Therefore, I do not think the intuition of the proposed method is correct.\n\n3. For the convergence analysis, the authors only consider the convex setting, which I think is meaningless. Because the proposed method is designed for training neural networks, such convergence guarantee in convex setting is not enough. There exist some work such as [1] have proved the convergence guarantee of the adaptive algorithms including Padam in the nonconvex setting. \n\n4. There is one missing baseline Yogi [2] in the current paper.\n\n5. For experimental results, the performance of the proposed method is very similar to the Padam. Due to the close formulation of the proposed method and Padam, it seems to me that the proposed method is  just a more careful hyperparameter tuning process.\n\n6. In NMT experiments, why there is no Padam baseline? In addition, the authors should also report the test perplexity to validate the generalization performance of the proposed optimizer.\n\n7. To fully evaluate the performance of the proposed method, the authors should at least conduct an experiment on the task of language model.\n\nMinor comments:\nThere is an unknown citation in section 5.1.\n\nReference:\n[1]. Zhou, Dongruo, et al. \"On the convergence of adaptive gradient methods for nonconvex optimization.\" arXiv preprint arXiv:1808.05671 (2018).\n[2]. Zaheer, Manzil, et al. \"Adaptive methods for nonconvex optimization.\" Advances in Neural Information Processing Systems. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1140/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1140/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE", "authors": ["Yue Zhao", "Xiangsheng Huang", "Ludan Kou"], "authorids": ["oasis.random.time@gmail.com", "xiangsheng.huang@ia.ac.cn", "2015019051@mail.buct.edu.cn"], "keywords": ["Optimization", "Adaptive Methods", "Convergence", "Convolutional Neural Network"], "TL;DR": "This work proposes a novel generic framework, in which we explicitly analyze different behaviors brought by various types of \u03a6(\u00b7),  and based on the framework we propose a more adaptive optimization algorithm.", "abstract": "Although adaptive algorithms have achieved significant success in training deep neural networks with faster training speed, they tend to have poor generalization performance compared to SGD with Momentum(SGDM). One of the state-of-the-art algorithms, PADAM, is proposed to close the generalization gap of adaptive methods while lacking an internal explanation. This work pro- poses a general framework, in which we use an explicit function \u03a6(\u00b7) as an adjustment to the actual step size, and present a more adaptive specific form AdaPlus(Ada+). Based on this framework, we analyze various behaviors brought by different types of \u03a6(\u00b7), such as a constant function in SGDM, a linear function in Adam, a concave function in Padam and a concave function with offset term in AdaPlus. Empirically, we conduct experiments on classic benchmarks both in CNN and RNN architectures and achieve better performance(even than SGDM).\n", "pdf": "/pdf/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "code": "https://anonfiles.com/daV7Ed6enb/AdaPlus_zip", "paperhash": "zhao|ada_a_generic_framework_with_more_adaptive_explicit_adjustment_for_learning_rate", "original_pdf": "/attachment/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "_bibtex": "@misc{\nzhao2020ada,\ntitle={{\\{}ADA{\\}}+: A {\\{}GENERIC{\\}} {\\{}FRAMEWORK{\\}} {\\{}WITH{\\}} {\\{}MORE{\\}} {\\{}ADAPTIVE{\\}} {\\{}EXPLICIT{\\}} {\\{}ADJUSTMENT{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}RATE{\\}}},\nauthor={Yue Zhao and Xiangsheng Huang and Ludan Kou},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIU0VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575714306355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1140/Reviewers"], "noninvitees": [], "tcdate": 1570237741775, "tmdate": 1575714306371, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1140/-/Official_Review"}}}, {"id": "H1g4Pa9zcS", "original": null, "number": 3, "cdate": 1572150620499, "ddate": null, "tcdate": 1572150620499, "tmdate": 1572972507640, "tddate": null, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "invitation": "ICLR.cc/2020/Conference/Paper1140/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a new variation on adaptive learning rate algorithm that builds on a prior work Padam, which is also a concurrent submission to this conference. The method is empirically validated through through various domains and neural networks architecture. Though the empirical results are extensive, I am leaning towards reject because (1) The method is a very small variation; no insight is provided for why. (2) The theory is not very useful in justifying the method. (3) The empirical results are weak.\n\n(1) This work builds on the prior work Padam, which is not a well justified algorithm. Same criticisms hence can be made to this paper. I don't see any strong arguments from the paper that well justifies the methodology either.\nThe problems are listed below:\n\nHere's what I can find of the authors' interpretation of the prior work Padam: \n\n\"The internal cause is that a concave function is applied rather than the linear function in ADAM.\nOnce \u03b5 is extremely small and |g| \u2208 (0, \u03b5), the mapping value of \u03a6(\u00b7) would be much\nlarger in PADAM than in ADAM; therefore, PADAM can adapt to larger learning rate \u03b1,\nthus flexibly adapting to the variable learning rate scheme.\"\n\n(i) I don't think phrasing the cause as a result of concavity gives arise to any new insights. Simply, what this is trying to say is that \u03a6(\u00b7) is large when |g| is small, that's why PADAM can adapt to large learning rate. \n\nBuilding on Padam, the paper further justifies the proposed method by:\n\n\" This form of \u03a6(\u00b7) not only directly inherits advantages of PADAM,\nas is depicted in Figure 1, but also makes a better guarantee for larger learning rates. The\noffset \u2206 makes sure that \u03a6(\u00b7) can altogether avoid the extreme situation. Even when\n|g| \u2192 0, a more extensive learning rate \u03b1t is allowed. \"\n\n(ii) The paper inherits a serious problem from Padam, that is to assume large learning rate is important for learning rate decay. Padam didn't explain this, nor does this paper. So the justification is very weak. \n(iii) The introduction of the offset is not novel. Just as the author noted, this is almost the same term epsilon in original updates. It's not rare that one also tunes epsilon for some optimization problems.\n\n(2) Continuing the last point, the theoretical analysis focus on the convergence proof of the algorithm. The proof is not new. Theory also is not useful in justifying the method.\n\n(3) The method seems not to be able to beat the previous baseline Padam, which makes it questionable as a practical algorithm. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1140/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1140/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "ADA+: A GENERIC FRAMEWORK WITH MORE ADAPTIVE EXPLICIT ADJUSTMENT FOR LEARNING RATE", "authors": ["Yue Zhao", "Xiangsheng Huang", "Ludan Kou"], "authorids": ["oasis.random.time@gmail.com", "xiangsheng.huang@ia.ac.cn", "2015019051@mail.buct.edu.cn"], "keywords": ["Optimization", "Adaptive Methods", "Convergence", "Convolutional Neural Network"], "TL;DR": "This work proposes a novel generic framework, in which we explicitly analyze different behaviors brought by various types of \u03a6(\u00b7),  and based on the framework we propose a more adaptive optimization algorithm.", "abstract": "Although adaptive algorithms have achieved significant success in training deep neural networks with faster training speed, they tend to have poor generalization performance compared to SGD with Momentum(SGDM). One of the state-of-the-art algorithms, PADAM, is proposed to close the generalization gap of adaptive methods while lacking an internal explanation. This work pro- poses a general framework, in which we use an explicit function \u03a6(\u00b7) as an adjustment to the actual step size, and present a more adaptive specific form AdaPlus(Ada+). Based on this framework, we analyze various behaviors brought by different types of \u03a6(\u00b7), such as a constant function in SGDM, a linear function in Adam, a concave function in Padam and a concave function with offset term in AdaPlus. Empirically, we conduct experiments on classic benchmarks both in CNN and RNN architectures and achieve better performance(even than SGDM).\n", "pdf": "/pdf/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "code": "https://anonfiles.com/daV7Ed6enb/AdaPlus_zip", "paperhash": "zhao|ada_a_generic_framework_with_more_adaptive_explicit_adjustment_for_learning_rate", "original_pdf": "/attachment/275ad1a553639cbc285ab8fa660944b8a10b0715.pdf", "_bibtex": "@misc{\nzhao2020ada,\ntitle={{\\{}ADA{\\}}+: A {\\{}GENERIC{\\}} {\\{}FRAMEWORK{\\}} {\\{}WITH{\\}} {\\{}MORE{\\}} {\\{}ADAPTIVE{\\}} {\\{}EXPLICIT{\\}} {\\{}ADJUSTMENT{\\}} {\\{}FOR{\\}} {\\{}LEARNING{\\}} {\\{}RATE{\\}}},\nauthor={Yue Zhao and Xiangsheng Huang and Ludan Kou},\nyear={2020},\nurl={https://openreview.net/forum?id=HJeIU0VYwB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJeIU0VYwB", "replyto": "HJeIU0VYwB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1140/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575714306355, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1140/Reviewers"], "noninvitees": [], "tcdate": 1570237741775, "tmdate": 1575714306371, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1140/-/Official_Review"}}}], "count": 5}