{"notes": [{"id": "5k8F6UU39V", "original": "odrzsGq-PRh", "number": 1115, "cdate": 1601308125367, "ddate": null, "tcdate": 1601308125367, "tmdate": 1615804902538, "tddate": null, "forum": "5k8F6UU39V", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "HvNdqpvH0ic", "original": null, "number": 1, "cdate": 1610040494727, "ddate": null, "tcdate": 1610040494727, "tmdate": 1610474100916, "tddate": null, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "This is a novel, simple, and experimentally well-supported new idea for entity linking.  The key insight is to perform entity linking by producing meaningful entity names with seq2seq approaches, and the big surprise is how well this works experimentally (at least for wikipedia-style entities).  Very nice paper!\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040494712, "tmdate": 1610474100901, "id": "ICLR.cc/2021/Conference/Paper1115/-/Decision"}}}, {"id": "Df8kqVymbd1", "original": null, "number": 6, "cdate": 1605648001859, "ddate": null, "tcdate": 1605648001859, "tmdate": 1605648001859, "tddate": null, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment", "content": {"title": "General Author Response", "comment": "As also pointed out by all reviews, an analysis for rare or \u201ctail\u201d entities would add value to the paper. We thank all reviewers for suggesting this. We added 3 new plots (Figures 3, 4 and 5) for analysing such \u201ctail entities\u201d\n\n**Analysis of \u201ctail entities\u201d**\n\nFigure 3 shows the accuracy per mention-entity pair frequency in Wikipedia for the KILT entity disambiguation tasks. The accuracy for pairs that do not appear in Wikipedia is substantially lower than the average (~51% vs ~82%)  suggesting that those are harder cases (the very end tail of the distribution). However, for pairs that are present at least one time in Wikipedia, the accuracy is ~80% meaning that rare (but not absent) mention-entity pairs are predicted well by our model. Note that when a pair appears at least one time in Wikipedia it does not necessary implies we have trained with that pair. But it means that during masked language model pre-training the model has seen that mention at least one time (but not the corresponding entity as we fine-tune for this). Also note that other systems (e.g., BLINK and DPR -- Wu et al., 2019; Karpukhin et al., 2020) have to see all entity descriptions after training to make predictions where we do not.\n\nFigure 4 shows the accuracy per number of BPE tokens in the entity. We also show the data distribution of token lengths. Here GENRE has an average accuracy of 78.6% but it is higher for short titles (e.g., <10) and it is lower for long titles (e.g., >=10). Degradation in performance does not directly follow the data distribution of the token lengths. Indeed, even if long titles are rare performance is not heavily affected (e.g., for length >15).\n\nFigure 5 shows the accuracy per number of incoming links in Wikipedia (i.e., how many times they are referred from other pages) for all KILT tasks. Intuitively, a page/entity with few incoming links has been observed less than highly connected pages/entities. Indeed, for pages/entities never linked the average accuracy is ~20% lower than the global average (78.6%). However, for pages/entities linked at least once it is above the global average.\n\nThese three analysis indicates that GENRE seems effective on linking rare (but not absent) entities.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5k8F6UU39V", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1115/Authors|ICLR.cc/2021/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863499, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment"}}}, {"id": "16xWaLYRUJF", "original": null, "number": 5, "cdate": 1605647881798, "ddate": null, "tcdate": 1605647881798, "tmdate": 1605647931627, "tddate": null, "forum": "5k8F6UU39V", "replyto": "gmngTl8-zQe", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment", "content": {"title": "Reply to R4", "comment": "We thank R4 for the useful comments.\n\n**Lack of clarity at many places**\n\na) Thanks for pointing our that the input formulation was not clear. We actually do show how we reformulate the input and output for the 3 tasks in the Appendix (see Figure 3, 4, and 5 of our first submission - on the revision are Figure 6, 7, and 8). We will further clarify this aspect in the final version.\n\nb) We will include brief descriptions of baseline models in further versions (we haven\u2019t included them here due to space constraints as we compare to ~20 other systems from the literature across the 3 tasks).\n\nc) The log-probabilities for all of the 3 tasks are computed according to the equation at the beginning of Section 3: $score(e|x) = p(y|x) = \\prod_{i=1}^N p(y_i|y_{<i},x)$. When we do entity disambiguation x is the source document with an annotated mention and y is the entity name. For end-to-end entity linking x is the source document and y is the markup annotated source document (as defined in Section 3.2). The log-probability of an annotation is computed using the equation above token-by-token.\n\n**Is it possible to generate all entities simultaneously as what GENRE does in end-to-end entity linking?**\n\nIn the entity disambiguation task GENRE only generates one entity at a time as we process the sequence left-to-right. However, we batch the generation to speed up the computation. Some documents have too many entities to allow inference for all of them simultaneously. Also for end-to-end entity linking GENRE only generates one entity at a time since it is an autoregressive model. The model predicts the markup annotated source token by token so it cannot generate all entities simultaneously. Also for this task we batched documents to speed up computation.\n\nIn case you meant whether we have a \u201cglobal\u201d model (in which predictions depend on each other but where mentions are given) then do not have a \u201cglobal\u201d model as predictions are only conditioned on previously generated tokens (for the end-to-end entity linking task). We could extend the entity disambiguation model to be global condition the generation of the next token dependent on all the mention/entity pairs. Although this was not a goal of this work.\n\n**What causes the difference between our method and baseline systems as all should benefit from pre-training?**\n\nWe consider several baselines that use pre-trained language models (e.g., all those in Table 3 except tf-idf). As we argue in the introduction, we believe that the difference in performance is due to the novel autoregressive way of formulating the output in GENRE. In fact, all current solution estimate the match between input and entity label though a bi-encoder --- a dot product between dense vector encodings of the input and the entity\u2019s meta information --- that can miss fine-grained interactions between input and entity meta information. Our autoregressive formulation allows to cross-encode mention context and entity candidates directly capturing fine-grained interactions between the two.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5k8F6UU39V", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1115/Authors|ICLR.cc/2021/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863499, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment"}}}, {"id": "YPE-3CuYyHp", "original": null, "number": 4, "cdate": 1605647729852, "ddate": null, "tcdate": 1605647729852, "tmdate": 1605647781232, "tddate": null, "forum": "5k8F6UU39V", "replyto": "YPITSP2P0IC", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment", "content": {"title": "Reply to R3", "comment": "We thank R3 for the useful comments.\n\n**Analysis of ablations**\n\nThe focus of the paper is on a new autoregressive way of predicting entity identifiers based on their unique names. We use a standard transformer architecture as several of the baselines we considered (e.g.,  Wu et al., 2019; Karpukhin et al., 2020). The key difference is that current solutions estimate the match between input and entity label though a bi-encoder --- a dot product between dense vector encodings of the input and the entity\u2019s meta information --- that can miss fine-grained interactions between input and entity meta information. Our autoregressive formulation allows to cross-encode mention context and entity candidates directly capturing fine-grained interactions between the two.\n\nThanks for pointing our that our baselines and setting where not detailed enough, we added more details on the ablation experiments and commenting result tables. We also provide details on which datasets we pre-train our models, for how many steps and other training hyper-parameters (Appendix A). In the next revision we will use extra space in the main paper to report these details.\n\n**Analysis of \u201ctail entities\u201d**\n\nThanks for suggesting such an analysis, we believe it strengthened our paper. In our last revision, we added 3 new plots (Figures 3, 4 and 5). It turns out that there is a drop in performance for unseen entities but not dramatic. Besides, it seems that GENRE is robust at identifying entities that appear even just once in Wikipedia.  Please refer to the new draft and the general response for further details.\n\n**References**\n\nWu, L., Petroni, F., Josifoski, M., Riedel, S., & Zettlemoyer, L. ; Scalable Zero-shot Entity Linking with Dense Entity Retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n\nKarpukhin, V., O\u011fuz, B., Min, S., Wu, L., Edunov, S., Chen, D., & Yih, W. T. ; Dense Passage Retrieval for Open-Domain Question Answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5k8F6UU39V", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1115/Authors|ICLR.cc/2021/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863499, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment"}}}, {"id": "j5qBiIrcdZ", "original": null, "number": 3, "cdate": 1605647452996, "ddate": null, "tcdate": 1605647452996, "tmdate": 1605647452996, "tddate": null, "forum": "5k8F6UU39V", "replyto": "MdMUmOHa5d", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment", "content": {"title": "Reply to R2", "comment": "We thank R2 for the useful comments.\n\n**Analysis of \u201ctail entities\u201d**\n\nThanks for suggesting such an analysis, we believe it strengthened our paper. In our last revision, we added 3 new plots (Figures 3, 4 and 5). It turns out that there is a drop in performance for unseen entities but not dramatic. Besides, it seems that GENRE is robust at identifying entities that appear even just once in Wikipedia.  Please refer to the new draft and the general response for further details.\n\n**Ablating the constrained beam decoding**\n\nConstrained beam decoding is a central part of our system as without it the model may generate entity names that cannot be mapped to a specific entity (e.g., if the model predicts \u201cMichael Jordan (basketball player)\u201d it will not be mapped to the Michael Jordan entity in Wikipedia as the label for that is simply \u201cMichael Jordan\u201d: https://en.wikipedia.org/wiki/Michael_Jordan).  We thank the reviewer for suggesting investigating its importance. We added two new ablations experiments for entity disambiguation and page-level retrieval (on all 6 dataset for ED and all 11 datasets for retrieval \u2014 see \u201cGENRE w/o constrains\u201d in Table 7 and 8 respectively). Without  constrained decoding we have an average drop of ~9 F1 points for entity linking (88.8 -> 79.6) and ~7 R-precision points for retrieval (69.7 -> 63.0). Due to space constrains we kept ablations in the Appendix. For future versions (e.g., camera ready) we will move them in the main paper accompanied with comments in the main text.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5k8F6UU39V", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1115/Authors|ICLR.cc/2021/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863499, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment"}}}, {"id": "1DwC6HuVoWJ", "original": null, "number": 2, "cdate": 1605647279247, "ddate": null, "tcdate": 1605647279247, "tmdate": 1605647340563, "tddate": null, "forum": "5k8F6UU39V", "replyto": "JMSBRkLIT5i", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment", "content": {"title": "Reply to R1", "comment": "We thank R1 for the useful comment!\n\n**How can you improve the model to link rare entities not in the Wikipedia pages?**\n\nAn assumption of our method (and generally all entity linking/ retrieval systems) is that entities belongs to a  Knowledge Base (e.g., entity linking has to point to an entity in the KB). Thus, we require as part of our system to have a set of entity names one for each entity in the KB. However, we do not required the vocabulary of the entities to be the ones that have a Wikipedia page. One can define a different set of entities and it has been the case in some of our experiments (e.g., for end to end entity linking in Table 2, we used the candidate sets provided by Kolitas et al., 2018 which are not all Wikipedia entities). We will clarify this aspect in the paper.\n\nThanks for suggesting a rare or \u201ctail\u201d entities analysis, we believe it strengthened our paper. In our last revision, we added 3 new plots (Figures 3, 4 and 5). It turns out that there is a drop in performance for unseen entities but not dramatic. Besides, it seems that GENRE is robust at identifying entities that appear even just once in Wikipedia.  Please refer to the new draft and the general response for further details.\n\n**How is the efficiency of your decoding strategy?**\n\nAlthough time efficiency was not an objective of our work, we agree that it is an interesting thing to know/ add. We measured time efficiency (sentences/sec) on the Entity Disambiguation task against BLINK (Wu et al., 2019). The experiment was done on CPU (since the index occupies ~24GB  we could not run the dot product search on GPU) with top-k=100 for BLINK and beam-size=5 for our method. BLINK has a bi-encoder that retrieves the top-k entires with maximum inner product search and then a cross-encoder that re-ranks the entities with a more complex function. We compare to both versions on the AIDA dataset (note that BLINK bi-encoder only is fast but it has lower performance than BLINK full \u2014 see Wu et al., 2019):\n\n| Model |examples/sec |\n|---|---|\n| BLINK (bi-encoder only) | 6.6 |\n| BLINK (full) | 0.6 |\n| GENRE (ours) | 2.9 |\n\nGENRE is ~2x slower than BLINK (bi-encoder only) but ~5x faster than the full BLINK system. We will add these results to the main paper and perhaps with additional comparisons. In future work we might try some techniques from neural machine translation to make the decoder faster.\n\n**References**\n\nWu, L., Petroni, F., Josifoski, M., Riedel, S., & Zettlemoyer, L. ; Scalable Zero-shot Entity Linking with Dense Entity Retrieval. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020."}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "5k8F6UU39V", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1115/Authors|ICLR.cc/2021/Conference/Paper1115/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923863499, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Comment"}}}, {"id": "YPITSP2P0IC", "original": null, "number": 1, "cdate": 1603841411616, "ddate": null, "tcdate": 1603841411616, "tmdate": 1605024526873, "tddate": null, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review", "content": {"title": "Official Review #3", "review": "The paper introduces a new method to retrieve entity by auto regressively generating unique entity name as a sequence of word pieces, instead of pinpointing the ID representing an entity. This method stands out in novelty compared to existing various entity retrieval methods, which always assigns a single ID to each entity. Practically, the proposed method has two nice properties: (1) When the entity vocabulary is very large, this approach requires less parameter space and memory compared to other methods (as shown clearly in Table 4) (2) The model can address novel entities, which was unseen during the training. The paper is clearly written and extensively evaluated on three relevant tasks, entity disambiguation, entity linking, and entity retrieval.\n\nI have one big concern with the current format of the presentation. In the current draft, It is not very clear whether the strong gain is coming from large scale pretr aining or the proposed method itself.  To this end, the ablations shown in Table 7 and Table 8 should be reported in the main paper, with clear explanations. As we all know, the model architecture (which is the focus of this paper) cannot be properly evaluated when the training set up is different (i.e., how much pretraining has been done, on what dataset?).  Could you elaborate on this? In appendix, there's only result tables without in-line explanations. \n\nThe experiments on cold start, as well as table 5 which shows performances on entities divided by name match is pretty cool!\n\nIf space allows, adding some more analysis on what types of entities do this model do better compared to other methods would be interesting (would lengthier names easier or harder? would it do better on popular entities or more long tail ones? are these systems complementary to existing methods or mostly succeed and fail on the same set of examples?) \n\n ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126550, "tmdate": 1606915798704, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1115/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review"}}}, {"id": "MdMUmOHa5d", "original": null, "number": 2, "cdate": 1603857519971, "ddate": null, "tcdate": 1603857519971, "tmdate": 1605024526811, "tddate": null, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review", "content": {"title": "Novel idea on entity retrieval/linking with well executed experiments", "review": "### Summary\n\nThis paper proposes to tackle the entity linking task using a sequence-to-sequence neural model, trained by producing unique entity names, in autoregressive fashion. The paper makes a case that this approach can scale better with larger entity vocabularies than previous methods with dedicated entity representations both in terms of memory as well as computation costs. The model is studied under a number of tasks including entity disambiguation, entity linking and document retrieval for question answering.\n\n### Strong and weak points\n\nStrong points:\n- The work is well motivated: I believe there are, or will be, many systems where retrieving information about billions of entities is useful. Making simpler and more efficient models to deal with larger entity vocabularies is important.\n- Doing seq2seq entity linking is really a novel idea and is surprising that it works so well for several of the datasets presented.\n- The idea of constrained decoding using a Trie is neat and makes intuitive sense.\n- The empirical evaluation in the paper is quite exhaustive, in terms of number of datasets.\n\nWeak points:\n- There is no discussion or analysis on the performance of this new model on \u201ctail entities\u201d (entities that have few examples in the training set). A believe such a discussion would be interesting for two reasons: (1) if one wishes to use this type of model for much larger entity vocabularies, it is likely that a larger fraction of entities will have low number of examples, and (2) one effect of contrastive learning (e.g., negative sampling) has on systems with explicit entity representations is some implicit training of _all_ entities, which is lacking in the described autoregressive proposal.\nFinally on this topic of tail entities: the \u201cIDs\u201d experiment, shown in Table 5, indicates that when entity mention and decode target are not related, performance suffers. Similarly, for \u201ctail entities\u201d that are ambiguous with another \u201cpopular entity\u201d, the model may be biased with the popular entity (results from \u201cCold Start\u201d may support this hypothesis). Hard to say without some analysis.\n\n- While the constrained decoding is really interesting, it was not clear from the paper how crucial this was for good performance. Is this something absolutely critical for performance overall, or does it provide a modest performance improvement? While it may be clear to the authors, it would be highly informative to explicitly describe the performance impact on unconstrained beam decoding.\n\n### Recommendation\n\nOverall, my recommendation is to accept this paper to ICLR. I believe the problem of representing entities in natural language systems is of high interest to the ICLR and NLP communities. The ideas in this paper are novel, the paper is well written and the empirical evaluation is well executed.\n\n### Questions for authors\n\n- See comment above regarding analysis of \u201ctail entities\u201d. Could any further insights on this topic be added to the paper? \n- See comment above about ablating the constrained beam decoding. This would not need to be a long or complex analysis. Simply 1 or 2 data points for the reader to understand the magnitude of the importance of this decoding.\n", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126550, "tmdate": 1606915798704, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1115/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review"}}}, {"id": "JMSBRkLIT5i", "original": null, "number": 3, "cdate": 1603902479555, "ddate": null, "tcdate": 1603902479555, "tmdate": 1605024526751, "tddate": null, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review", "content": {"title": "Review #1: Great paper with convincing results.", "review": "The paper proposed to use autoregressive approach to solve entity-based problems. They proposed a uniform framework and showed that their model achieved the state of the art performance on 3 different types of tasks (~20 datasets). The GENRE model also significantly reduced the memory usage compared to previous models that stored a big memory table. It's also capable of linking novel entities at inference time. This paper is clearly written. The experiment results are convincing.\n\nOne limitation of this paper is that they required the vocabulary of the entities to be the ones that have a Wikipedia page. And that their model relied on copying the surface form of entities (as suggested in the paper). From the experiments, the \"copying\" approach worked very well on Wikipedia entities, that are often common entities. How can you improve the model to link rare entities not in the Wikipedia pages? \n\nAnother concern is the efficiency at inference time. Compared to the models with a large entity memory whose retrieval is performed with Maximum inner product search, how is the efficiency of your decoding strategy?", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126550, "tmdate": 1606915798704, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1115/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review"}}}, {"id": "gmngTl8-zQe", "original": null, "number": 4, "cdate": 1603974254110, "ddate": null, "tcdate": 1603974254110, "tmdate": 1605024526691, "tddate": null, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "invitation": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review", "content": {"title": "The paper proposes a brand new approach for entity retrieval, which leverages an encoder-decoder architecture to generate the target entity directly. Intensive experiments on entity disambiguation, entity linking, and document retrieval tasks prove the effectiveness of the approach.", "review": "The paper proposes a brand new approach for entity retrieval, which leverages an encoder-decoder architecture to generate the target entity directly. To tackle the problem of invalid generation, a trie-constrained beam search is used for decoding. The author performs intensive experiments on entity disambiguation, entity linking, and document retrieval tasks and achieves new SOTA or competitive results on over 20 datasets. Although the paper does not come up with new architecture or elaborately designed neural components, I believe this paper is worth reading for the community, including how this paper redefines the problem.\n\nStrengths:\n1. A brand new perspective on entity retrieval. Clever use of text generation methods.\n2. Compared to previous methods, GENRE is memory saving.\n3. This paper provides comprehensive experiments on 3 important tasks, over 20 datasets, showing the robustness of this method. \n\nWeaknesses: \n1. Lack of clarity at many places: \na) It's clearer to demonstrate how you reformulate the input and output for the 3 tasks, as they seem very different compared to previous methods\nb) It's better to add some description about the baseline systems\nc) I'm especially interested in the decoding details on end-to-end entity linking,  it's better to use a formular to demonstrate how you compute the log-probabilities.\n\nQuestions:\n1. In the entity disambiguation task, it seems that for each inference step, GENRE only generates one entity. Is it possible to generate all entities simultaneously as what GENRE does in end-to-end entity linking? \n2. From table 5, an impressive result is that it seems that the pre-trained model is good at memorizing, even if the identifier is numeric. But from table 3, some of the baseline systems also benefit from pretrained models, while achieve much lower results. What do you think causes this difference? \n", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1115/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1115/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Autoregressive Entity Retrieval", "authorids": ["~Nicola_De_Cao1", "~Gautier_Izacard1", "~Sebastian_Riedel1", "~Fabio_Petroni2"], "authors": ["Nicola De Cao", "Gautier Izacard", "Sebastian Riedel", "Fabio Petroni"], "keywords": ["entity retrieval", "document retrieval", "autoregressive language model", "entity linking", "end-to-end entity linking", "entity disambiguation", "constrained beam search"], "abstract": "Entities are at the center of how we represent and aggregate knowledge. For instance, Encyclopedias such as Wikipedia are structured by entities (e.g., one per Wikipedia article). The ability to retrieve such entities given a query is fundamental for knowledge-intensive tasks such as entity linking and open-domain question answering. One way to understand current approaches is as classifiers among atomic labels, one for each entity. Their weight vectors are dense entity representations produced by encoding entity meta information such as their descriptions. This approach leads to several shortcomings: (i) context and entity affinity is mainly captured through a vector dot product, potentially missing fine-grained interactions between the two; (ii) a large memory footprint is needed to store dense representations when considering large entity sets; (iii) an appropriately hard set of negative data has to be subsampled at training time. In this work, we propose GENRE, the first system that retrieves entities by generating their unique names, left to right, token-by-token in an autoregressive fashion and conditioned on the context. This enables us to mitigate the aforementioned technical issues since: (i) the autoregressive formulation allows us to directly capture relations between context and entity name, effectively cross encoding both; (ii) the memory footprint is greatly reduced because the parameters of our encoder-decoder architecture scale with vocabulary size, not entity count; (iii) the exact softmax loss can be efficiently computed without the need to subsample negative data. We show the efficacy of the approach, experimenting with more than 20 datasets on entity disambiguation, end-to-end entity linking and document retrieval tasks, achieving new state-of-the-art or very competitive results while using a tiny fraction of the memory footprint of competing systems. Finally, we demonstrate that new entities can be added by simply specifying their unambiguous name. Code and pre-trained models at https://github.com/facebookresearch/GENRE.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "cao|autoregressive_entity_retrieval", "one-sentence_summary": "We address entity retrieval by generating their unique name identifiers, left to right, in an autoregressive fashion, and conditioned on the context showing SOTA results in more than 20 datasets with a tiny fraction of the memory of recent systems.", "pdf": "/pdf/921ba67c80871fda61a4c0cf8f889b1c381a2a78.pdf", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\ncao2021autoregressive,\ntitle={Autoregressive Entity Retrieval},\nauthor={Nicola De Cao and Gautier Izacard and Sebastian Riedel and Fabio Petroni},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=5k8F6UU39V}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "5k8F6UU39V", "replyto": "5k8F6UU39V", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1115/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538126550, "tmdate": 1606915798704, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1115/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1115/-/Official_Review"}}}], "count": 11}