{"notes": [{"id": "r1gzoaNtvr", "original": "rJeWlRADvS", "number": 736, "cdate": 1569439130257, "ddate": null, "tcdate": 1569439130257, "tmdate": 1577168290878, "tddate": null, "forum": "r1gzoaNtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "IByNbfUjSE", "original": null, "number": 1, "cdate": 1576798704599, "ddate": null, "tcdate": 1576798704599, "tmdate": 1576800931463, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Decision", "content": {"decision": "Reject", "comment": "This paper explores the emergence of language in environments that demand agents communicate, focusing on the compositionality of language, and the cultural transmission of language.\n\nReviewer 1 has several suggestions about new experiments that are possible. The AC does think there is value in many of the suggested experiments, if not to run, then just to acknowledge their possibility and leave for future work. The reviewers also point to some previous work that is very similar.  E.g. \"Ease-of-Teaching and Language Structure from Emergent Communication\", Funshan Li et al", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728691, "tmdate": 1576800281145, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper736/-/Decision"}}}, {"id": "BJgwl453sS", "original": null, "number": 18, "cdate": 1573852143053, "ddate": null, "tcdate": 1573852143053, "tmdate": 1573852143053, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "HJe3VbL3sS", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment", "content": {"title": "Responses to R3", "comment": "__Why 8 generations:__\nWe agree that additional temporal analysis would strengthen the paper. In our initial experiments we focused on the Overcomplete setting and found that agent populations tended to converge by 8 generations. We've now performed some additional experiments that analyze test accuracy over time, though these are only preliminary due to other pending deadlines (CVPR). See [here](https://pasteboard.co/IGPZzwY.png) for a plot of test accuracy vs q-bot generation. See [here](https://pasteboard.co/IGQ0iZQ.png) for a plot of test accuracy vs a-bot generation. In both cases accuracy increases over generations, especially for the models that do better in the end. This more recent analysis suggests performance may improve for more generations for some models.\n\n__Memory:__\nThe agent memory settings are a point of comparison with prior work (sorry for citing again but Kottur et al.), where they were shown to be useful in inducing more compositional language. As we also duplicate the work of Kottur et al. and introduce a more rigorous evaluation (both in terms of the compositional split and computing variance over multiple runs), we felt these settings were useful to the community for comparison. We note that our findings are consistent with this past work and that our approach is shown to be complementary. In updated drafts we will try to make this point clearer or deemphasize these results -- as the reviewer said Kottur et al. is not the only valid experimental setting.\n\nAgain, thank you for your active participation in the review process!"}, "signatures": ["ICLR.cc/2020/Conference/Paper736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gzoaNtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper736/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper736/Authors|ICLR.cc/2020/Conference/Paper736/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167001, "tmdate": 1576860530945, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment"}}}, {"id": "HJe3VbL3sS", "original": null, "number": 14, "cdate": 1573835059904, "ddate": null, "tcdate": 1573835059904, "tmdate": 1573835078303, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "SJl3j-ScjB", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment", "content": {"title": "Response", "comment": "Thank you very much for these clarifications. They are clear and I greatly appreciated that the authors expect to add temporal figures. \n\nTwo concerned were not fully answered:\n - why 8 generations (and the potential discussions/experiments that may follow). Maybe the temporal plots would answer those questions, but they were not uploaded.\n - I cannot help myself to think that there are two directions in this paper. Transmission and Memory agents. Therefore, the overall message may be not as impactful as it should be (even if the claim is sound and the experiments seems sound). \n\nIn the end, I am willing to increase my score from weak accept (6/10) to 7/10 (even if it does really exist :-/) but I cannot vouch for a clear accept (8/10).\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer3", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gzoaNtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper736/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper736/Authors|ICLR.cc/2020/Conference/Paper736/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167001, "tmdate": 1576860530945, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment"}}}, {"id": "SkgQjoTV9S", "original": null, "number": 3, "cdate": 1572293531458, "ddate": null, "tcdate": 1572293531458, "tmdate": 1573787915955, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "N/A", "title": "Official Blind Review #4", "review": "This paper proposes a simple extension to the training of emergent communication protocols in multi-agent settings. \nThe central hypothesis is that learnability will favor more 'compressible' and therefor more compositional languages to emerge. \n\nThis hypothesis is tested by training a population of listens and speakers in an emergent communication task and comparing a number of different strategies for reinitializing agents in the population. Since the new agents start from a random initialization, they provide a learning signal that reinforces protocols which can be learned quickly. \n\nExperiments:\nWhile the experimental results largely confirm the hypothesis there are a few issues:\n-all of the plots show mean and standard deviations, rather than error of the mean. This makes it difficult to understand which differences are statistically significant and which ones are not. \n- The replacement strategy 'epsilon-greedy' replaces agents based on their validation loss, which seems like an unfair advantage. \n- It is currently unclear how much we can learn from Section 5.2: Naturally, agents trained in the same population will develop more similar protocols than those trained independently. This result is obvious and it is unclear whether reinitializing the agents makes any significant difference to the similarity. Again, confidence intervals would help.  \n- The experiments are also extremely toy. I would be more convinced if the authors tested their method on a more challenging task, though I am aware this is a common problem in this field. \n\nNovelty:\nThe single biggest issues with the current form of the paper is the related work section. In this section two papers [1,2] are mentioned as \"concurrent\" when at least one of them [1] has been available online since June 2019. I think it is important to clearly point out the novelty of the current work compared to those two previous papers. \nIn particular [1] seems to be extremely close to the ideas and methods proposed here. Saying that these papers \"confirm the hypothesis\" simply is not enough. \n\nReferences:\n[1]: \"Ease-of-Teaching and Language Structure from Emergent Communication\", Funshan Li et al\n[2]: \"Co-evolution of language and agents in referential games\", Gautier Dagan et al \n\n[Updated score based on the rebuttal]\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575846327787, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper736/Reviewers"], "noninvitees": [], "tcdate": 1570237747836, "tmdate": 1575846327802, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Review"}}}, {"id": "Bkg40O5jiB", "original": null, "number": 11, "cdate": 1573787851691, "ddate": null, "tcdate": 1573787851691, "tmdate": 1573787851691, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "HkeqAfN9sr", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment", "content": {"title": "Response", "comment": "These comments do indeed address my major concerns. I would have preferred for the paper to be already updated with the new plots but will increase my score based on the response, in particular regarding the novelty of the result and the significance tests."}, "signatures": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gzoaNtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper736/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper736/Authors|ICLR.cc/2020/Conference/Paper736/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167001, "tmdate": 1576860530945, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment"}}}, {"id": "SJl3j-ScjB", "original": null, "number": 7, "cdate": 1573700003867, "ddate": null, "tcdate": 1573700003867, "tmdate": 1573700003867, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "rkx4Qvul9B", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment", "content": {"title": "Responses to R3", "comment": "Thanks for the well balanced review and depth of feedback! We generally agree with the comments and suggestions and add clarifications, additional information, and some rebuttal below.\n\n__Comment (referencing Kottur et al)__: \u201cThe authors made were careful not to take ownership of Kottur et al. 's works. Yet, the writing sometimes gives the feeling that their work is solely an extension of Kottur's work, which is not the case.\u201d\n__Response__: Thanks! This is something we struggled with, so it\u2019s nice to get feedback. We will try to better isolate references and clearly indicate the differences.\n\n__Comment (population dynamics)__: \u201cPopulation dynamics: I am missing a key element in the paper: an analysis of the population dynamics. Although the paper deals with generational transmissions, there are no experiments that analyze the evolution of language generations after generations. Most of the experiments deal with the final convergence state.\u201d\n__Response__: In initial experiments we measured D (the language dissimilarity metric from section 5.2) over training iterations. We will compute this plot for our final experiments and report it in the appendix. Initial experiments looked as expected: After some initial stabilization D looks like a typical learning curve for the 3 Multi Agent replacement methods, decreasing quickly at first then continuing to decrease slowly in later generations. The No Replacement strategy converges immediately and then either stays fixed throughout training or sometimes (for Small Vocab models) actually increases over the course of training.\n\n__Comment (literature)__: \u201cLiterature side: The authors did an excellent job [...] I think that it is worth extending[...]\u201d\n__Response__: Thanks for pointing out the relations to game theory and dynamic distillation. Previous drafts did briefly discuss the relation to evolutionary algorithms, but we ended up cutting that discussion to help meet the page limit. We will add these changes as space allows.\n\n__Commen (qualitative figure clarification)__: \u201cI had some difficulties in understanding Fig4, and the final-take away correctly. Would it be possible to give me one or two examples to correctly parse the table? More generally, I would recommend[...]\u201d\n__Response__: Thanks for the suggestion. We will add further explanation along the lines of what follows to the paper: Consider the top left example of Fig 4a. In this case A-bot is presented with a dashed blue circle. Due to space constraints, the figure leaves out what Q-bot says. A-bot\u2019s first response is the symbol \u201c2\u201d and A-bot\u2019s 2nd response is the symbol \u201c0\u201d. The checkmark indicates Q-bot guessed correctly. The solid green star is a shape in the test set (because the text is in bold). After the first generation (Fig 4a) Q-bot guessed some other shape. After subsequent generations (Figs 4b and 4c) Q-bot guessed it correctly, qualitatively demonstrating the improvement in test accuracy we see after many generations. We will clarify our explanation in the final version.\n\n__Comment (meaning of D)__: \u201cIn a similar spirit, it is hard to interpret the distance in Figure 3.\u201d\n__Response__: We only intended this metric to be used to compare models and we find it hard to ground in an absolute sense. The Single Agents Combined (roughly most different language) and Random Initialization (roughly most similar language) baselines can be compared to to get a sense of the range of performance for a particular model.\n\n__Comment (raw results table)__: \u201creproducible: having a final table in the array in the appendix could be very helpful\u201d\n__Response__: We will add this.\n\n__Comment__: Number of seeds and average length.\n__Response__: Dialogs all take 2 rounds and we use 4 different random seeds. This makes 16  runs per experiment because there are also 4 splits. (As we mention in sections 2 and 4)\n\n__Comment__: \u201cLast point... but it does not undermine the soundness of the experimental protocol! In the end, Is 25 accuracy points really compositionality?[...]\u201d\n__Response__: Our models are usually not completely compositional in the sense of 100% test accuracy, but we seem to agree that an intermediate sense of compositionality (between 0% and 100% test accuracy) is useful. What simple strategies should we consider in addition those we already reported? With respect to modalities, is the reviewer asking about single attribute accuracy (One as in the One vs Both accuracy from Kottur et al)?\n\n__Comment (grounding)__ \u201c[...]I am not sure whether we can speak of grounded language here[...]\u201d\n__Response__: The observations made by A-bot are of simple attribute tokens fed through a learned embedding. They are not perceptual observations, so the language is not grounded in any perceptual environment in the normal sense. We think it is reasonable to say the language is grounded in these tokens, but agree the perspective is more controversial than the normal sense of grounding. We will make this discussion a bit more delicate."}, "signatures": ["ICLR.cc/2020/Conference/Paper736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gzoaNtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper736/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper736/Authors|ICLR.cc/2020/Conference/Paper736/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167001, "tmdate": 1576860530945, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment"}}}, {"id": "BklQy3N5iH", "original": null, "number": 6, "cdate": 1573698522633, "ddate": null, "tcdate": 1573698522633, "tmdate": 1573698522633, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "BJeUQfre5B", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment", "content": {"title": "Responses to R1", "comment": "Thanks for the review. It raises some serious concerns, however we think they can be appropriately addressed and have attempted to do so in our responses below.\n\n__Comment (insufficient experiments)__: \u201cIn my experience, the emergence of compositionality (or lack of) in the setup learned here is very sensitive[...]\u201d \u201cThe current paper does improve the evaluation protocol[...] but does not explore the parameter space more systematically, hence I am concerned that it may suffer from similar fragility.\u201d\n\n\u201cIn this light, significantly more evidence should be provided to convince that the experimental results are stable and can be reproduced.\u201d\n\n\u201cThe \"evolutionary\" part has a similar issue. The paper draws conclusions from three anecdotal rules for replacing the population. There is no systematic analysis of the \"evolution\" process[\u2026] Drawing conclusions based on anecdotal evidence is bad scientific practice, that ICLR should discourage.\u201d\n\n\u201c\"Variations in replacement strategy tend to not affect performance.\" This is a key result of the paper and mush be quantified and analyzed.\u201d\n\n__Response__:\nWe are also concerned that our experiments may suffer from some fragility, partially because of the fragility we found in the experiments of Kottur et. al. That is exactly why we extended our experiments to capture much of this variance, as mentioned. We further agree that our claims should be softer, perhaps changing \"cultural transmission induces compositionality\" to \u201cour replacement strategies increase compositionality.\u201d\n\nHowever, as R3 puts it, in papers like this there are \u201cnever-ending experiments\u201d and the question is whether or not \u201cthis paper [has] enough of these never-ending experiments.\u201d\nWe agree with R3 that this paper does have enough of these never ending experiments. We think Figure 2 and the p values reported in A.3 show a clear increase in compositionality as a result of our replacement strategies and we systematically consider variations in Memory, Vocab Size, Number of Agents, and Replacement Strategy:\n\nGrid search dimensions\n(Model) Memory: Memory vs No Memory\n(Model) Vocab Size: Small vs Large/Overcomplete Vocab\n(Method) Number of Agents: Single vs Multiple Agents\n(Method) Replacement Strategy: No Replacement vs Random vs Epsilon Greedy vs Oldest\n\nWe did search over how often our agents are replaced in our initial experiments and found that replacement every 25000 epochs is a good tradeoff between allowing agents to converge in a generation and enabling us to run multiple generations. New agents usually converge in this length of time. Increasing this parameter is expensive because it must be increased for every generation, making each increase 8x more expensive (given our 8 generations).\n\nWe\u2019d also like to point out that just the experiments we reported in the paper took over 3 GPU months to run. That\u2019s a conservative back of the envelope calculation that assumes each single agent model takes 20min to train and each multi-agent model takes 6hr40min to train (about right for our implementation). This does not include the iterations our models went through before converging on the models presented in the paper, which likely multiply that figure by at least 2x or 3x. Running the additional experiments would take a long time because they would have to include all of the hyperparameter variations we have already considered.\n\nBoth R4 (\u201cthe experimental results largely confirm the hypothesis\u201d) and R3 (\u201cThe claim is clear, the hypotheses are well-stated, and the experiments look solid\u201d) agree that our experiments support our main claim. Is our main claim invalid without the additional experiments requested here?\n\n\n__Comment__: \u201cThe statistical analysis is not well explained, not even in the supplemental, so it is hard to tell which differences are significant. If data is paired, it would be useful to view data as a scatter plot, instead of a barplot which hides the pairing.\u201d\n__Response__: We pair the 16 different random seed/split variations. We report dependent paired t-test p values between each pair of experiments (114 tests with 16 instances in each case) in appendix A.3. We\u2019re not sure we quite understand the plotting suggestion here. Should we plot model performance with random seed and split values on the x and y axes?\n\n__Comment__: \u201cBTW, p<0.05 is not a \"strong support\", but rather is the most permissive threshold. The results in the supplemental may be stronger.\u201d\n__Response__: In almost every case p<0.01 (comparing Replacement and No Replacement models), as reported in A.3.\n\n\n__Comment__: \u201cOther parts of the paper make additional claims, that should similarly be systematically analyzed and supported with data-driven evidence.\u201d\n__Response__: Could the reviewer point out the specific parts of the paper that make additional claims without evidence?"}, "signatures": ["ICLR.cc/2020/Conference/Paper736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gzoaNtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper736/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper736/Authors|ICLR.cc/2020/Conference/Paper736/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167001, "tmdate": 1576860530945, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment"}}}, {"id": "HkeqAfN9sr", "original": null, "number": 5, "cdate": 1573696210437, "ddate": null, "tcdate": 1573696210437, "tmdate": 1573696210437, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "SkgQjoTV9S", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment", "content": {"title": "Responses to R4", "comment": "Thanks for the review! We hope these responses satisfactorily address the raised concerns, especially those about novelty.\n\n__Comment (novelty)__: \u201cThe single biggest issues with the current form of the paper is the related work section. In this section two papers [1,2] are mentioned as \"concurrent\" when at least one of them [1] has been available online since June 2019. I think it is important to clearly point out the novelty of the current work compared to those two previous papers. In particular [1] seems to be extremely close to the ideas and methods proposed here. Saying that these papers \"confirm the hypothesis\" simply is not enough.\u201d\n__Response__: Actually, our work has been available online since April 2019. Since this is prior to both [1] and [2], we do think it is fair to call our work \u201cconcurrent\u201d and we do not think this is a valid reason for rejection. Neither of these works have been published yet, though [1] will appear at NeurIPS 2019. Furthermore, [2] builds on and compares to our work though we urge reviewers to avoid searching out identifying details.\n\n__Comment (statistical significance)__: \u201call of the plots show mean and standard deviations, rather than error of the mean. This makes it difficult to understand which differences are statistically significant and which ones are not.\u201d\n__Response__: We take the reviewer\u2019s point and will update our paper with standard error instead of standard deviation. However, we also point out that we tested the statistical significance of our results by applying t-tests to each pair of experiments. We find that most differences in performance are quite significant (p much less than 0.05). We use these results to support our discussion throughout section 5.1 and report all pairwise p-values in section A.3 of the appendix.\n\n__Comment (epsilon greedy)__: \u201cThe replacement strategy 'epsilon-greedy' replaces agents based on their validation loss, which seems like an unfair advantage.\u201d\n__Response__: Absolutely. We agree this gives the Epsilon Greedy replacement strategy an advantage over the others because it is the only one that has access to validation performance. However, this is not an unrealistic advantage. Our agents can be given access to validation data as is done throughout the Meta Learning literature. Indeed, an interesting result of our experiments is that Epsilon Greedy did not help more.\n\n__Comment (language similarity)__: \u201cIt is currently unclear how much we can learn from Section 5.2: Naturally, agents trained in the same population will develop more similar protocols than those trained independently. This result is obvious and it is unclear whether reinitializing the agents makes any significant difference to the similarity. Again, confidence intervals would help.\u201d\n__Response (obvious)__: We also were not very surprised that the Multi Agent settings had more similar language than the Single Agents Combined setting. We reported the result mainly to help orient the reader\u2019s interpretation of the metric D and partially out of a desire to verify our intuition.\n__Response (significance)__: We will report standard error instead of standard deviation in Figure 3. We have computed p values and found that all pairs of No Replacement methods with Replacement methods have quite low t-test p values (much less than 0.05). This suggests the differences are significant. We will report these p values in the appendix."}, "signatures": ["ICLR.cc/2020/Conference/Paper736/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gzoaNtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper736/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper736/Authors|ICLR.cc/2020/Conference/Paper736/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167001, "tmdate": 1576860530945, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper736/Authors", "ICLR.cc/2020/Conference/Paper736/Reviewers", "ICLR.cc/2020/Conference/Paper736/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Comment"}}}, {"id": "BJeUQfre5B", "original": null, "number": 1, "cdate": 1571996190223, "ddate": null, "tcdate": 1571996190223, "tmdate": 1572972558757, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper continues the line of work on emergent compositionality in dialogs, and here it is extended to handle groups of interacting agents that pass language in an evolutionary way from one generation to another. The key idea is that with group of interacting agents, if some agents are replaced with new ones, then the newbies would learn the same language as the group. \n\nGeneral assessment: \n\nThe setup of the paper is is interesting, and the paper covers a lot of ground. The paper makes bold claims like \"cultural transmission induces compositionality\" and \"Variations in replacement strategy tend to not affect performance\". Unfortunately, the paper does not systematic provide experimental evidence to adequately support its claims.\n\nIn my experience, the emergence of compositionality (or lack of) in the setup learned here is very sensitive to various aspects of the learning setup, hence are hard to reproduce. Specifically, they the task-and-talk paper by Kottur et al may yield different conclusions, if parameters of the original experiments are modified, even slightly. The current paper does improve the evaluation protocol of Kottur 2017 by reporting variance over runs, but does not explore the parameter space more systematically, hence I am concerned that it may suffer from similar fragility. \n\nIn this light, significantly more evidence should be provided to convince that the experimental results are stable and can be reproduced.  First, one needs to show the experiments repeated over the full range of setup parameters. Including, the vocabulary sizes V_Q and V_A, the number of tasks, the number of attributes per task, and number of agents etc. Similarly, the current paper introduces a new compositional split, generated in one specific way. The effect of the split on what aspects of language emerge should be studied systematically, instead of using one \"hard\" and one random split. \nThe \"evolutionary\" part has a similar issue. The paper draws conclusions from three anecdotal rules for replacing the population. There is no systematic analysis of the \"evolution\" process, not even studying a range of the parameter epsilon, which is set 0.8 (arbitrarily? to fit the story? we do not know).  Drawing conclusions based on anecdotal evidence is bad scientific practice, that ICLR should discourage.\n\nWhile the ideas in this paper are innovative and exciting, the paper promises much more than its analysis supports, and the paper is not ready for publication.\n\nOther comments: \n-- The paper states that \"darker blue bars\" in figure 2 are higher. The statistical analysis is not well explained, not even in the supplemental, so it is hard to tell which differences are significant. If data is paired, it would be useful to view data as a scatter plot, instead of a barplot which hides the pairing. BTW, p<0.05 is not a \"strong support\", but rather is the most permissive threshold. The results in the supplemental may be stronger. \n-- \"Variations in replacement strategy tend to not affect performance.\" This is a key result of the paper and mush be quantified and analyzed. Authors should define some space of replacements strategies (e.g. in in parametric ways like  how often and how many agents are replaced), then compute performance difference as a function of grid search over the parameter space and show a figure. \n-- \"We stop after 8 generations\". Justify with data. \n-- Other parts of the paper make additional claims, that should similarly be systematically analyzed and supported with data-driven evidence.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575846327787, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper736/Reviewers"], "noninvitees": [], "tcdate": 1570237747836, "tmdate": 1575846327802, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Review"}}}, {"id": "rkx4Qvul9B", "original": null, "number": 2, "cdate": 1572009755888, "ddate": null, "tcdate": 1572009755888, "tmdate": 1572972558667, "tddate": null, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "invitation": "ICLR.cc/2020/Conference/Paper736/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper studies whether language composition may emerge by partially re-sampling new agents inside a pool of language agents. They set up a consistent experimental setting for assessing compositionality,  assess different agent architectures, e.g., memory vs. memoryless agents,  and explore how the language remains close to each other by re-sampling new agents.\n\nThe paper is well-motivated with substantial background literature on the cognitive science and emergent communication side. The claim is clear, the hypotheses are well-stated, and the experiments look solid (I particularly appreciated the paragraph on shortcoming evaluation). In the end, I enjoy reading the paper despite its density, and I could see that the authors made quite some effort in that direction.\n\nImprovement direction, questions:\n - The authors made were careful not to take ownership of Kottur et al. 's works. Yet, the writing sometimes gives the feeling that their work is solely an extension of Kottur's work, which is not the case. It also gives the feeling that Kottur et al. is the only valid experimental setting, which is not the case (at the authors pointed out in the related work section). Thus, I would recommend to summarise at some point the similarity/difference between the two papers, or at least stop referring the paper every two lines! \n - Replacement strategy: the authors use simple replacement strategies, and conclude that it has little impact. Althought It sounds reasonable in the current setting, the conclusion may be a bit premature. I would recommend to discuss further this result with complementary experiments could be the following: see the impact of epsilon, trying tournament strategies, why 8 populations (this sounds a bit arbitrary too). I would also like to put those observations in perspective with the evolutionary literature [1], and even provide a full paragraph in the related work section.\n - Population dynamics: I am missing a key element in the paper: an analysis of the population dynamics. Although the paper deals with generational transmissions, there are no experiments that analyze the evolution of language generations after generations. Most of the experiments deal with the final convergence state. Again, I would recommend having a look at the evolutionary literature to see which protocol they use to analyze such behavior.\n - Literature side: The authors did an excellent job on the emergent communication and cognitive science side. I think that it is worth extending the comparison further. For instance: \n    * generational transmission can be studied in the light of game theory [2] where compositionality can be seen as a Nash Equilibrium between agent. \n    * generational transmission is a form of dynamic distillation [3]\n    * and evolutionary algorithms!\n - I had some difficulties in understanding Fig4, and the final-take away correctly. Would it be possible to give me one or two examples to correctly parse the table? More generally, I would recommend to add a few lines with some concrete and cherry-picked examples from the experiments to help the reader to have more intuition). \n - In a similar spirit, it is hard to interpret the distance in Figure 3. What would correspond to an increase of 1pt of distance? Having said that, the experiment is sound, and it is insightful.\n - reproducible: having a final table in the array in the appendix could be very helpful \n - crazy experiment: even if I am also a DRL addict, I would be curious to train one of the models with evolutionary algorithms (CMA-ES over parameters, for instance) to assess whether RL has an impact on compositionality (or it is solely the experimental protocol that matters). \n - I may have missed this point, but how many seeds did you use to run your experiments? \n - I may have also missed this point, what is the average length of the dialogue. Can you upload (non-understandable) dialogue example? \n\nLast point... but it does not undermine the soundness of the experimental protocol! \n - In the end, Is 25 accuracy points really compositionality? What would be the score of simple strategies with overcomplete tokens? What is the score of the minimal vocab if we are only correct with one modality, two modalities?\n\n\n\nRemarks:\n - in the introduction, you mention that previous old agents have grounded language, I am not sure whether we can speak of grounded language here, they have a predefined language, but it is not grounded. \n - Please remove the bold sentence in the introduction :) The claim is clear!\n - P11: Alg undefined\n - P12: the legend cannot be read\n \n\nConclusion\nI am familiar with this type of experimental protocols, and I am well aware that they are never-ending works. There are always more experiments to do, more parameters to analyze. The final question is the following: is this paper have enough of these never-ending experiments? I think that this paper is just above this threshold by a short margin, and I vouch for weak accept.\n\nHowever, I am missing at least one dynamic figure (to see the impact of the population along time, which is one of the core concepts of the paper), and there are several links with other ML communities that still have to be highlighted (especially evolutionary algorithms). \nBesides, I somehow feel that the authors pursue two different goals in this paper: they both analyze memory/memoryless complete/overcomplete agents, which is somehow orthogonal to the general transmission hypothesis. Maybe, It would have made more sense to focus on one (or two) of the models and change the experimental setting on them (population size, training time, etc.) \n\nIn the end, I would favor a weak accept. \nI am open to discussion regarding this scoring.\n\n[1] B\u00e4ck, Thomas, and Frank Hoffmeister. \"Extended selection mechanisms in genetic algorithms.\" (1991).\n[2] Lanctot, Marc, et al. \"A unified game-theoretic approach to multiagent reinforcement learning.\" Advances in Neural Information Processing Systems. 2017.\n[3] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. \"Distilling the knowledge in a neural network.\" arXiv preprint arXiv:1503.02531 (2015)."}, "signatures": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper736/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["cogswell@gatech.edu", "jiasenlu@gatech.edu", "steflee@gatech.edu", "parikh@gatech.edu", "dbatra@gatech.edu"], "title": "Emergence of Compositional Language with Deep Generational Transmission", "authors": ["Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra"], "pdf": "/pdf/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "TL;DR": "We use cultural transmission to encourage compositionality in languages that emerge from interactions between neural agents.", "abstract": "Recent work has studied the emergence of language among deep reinforcement learning agents that must collaborate to solve a task. Of particular interest are the factors that cause language to be compositional---i.e., express meaning by combining words which themselves have meaning. Evolutionary linguists have found that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of  compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization.", "keywords": ["Cultural Evolution", "Deep Learning", "Language Emergence"], "paperhash": "cogswell|emergence_of_compositional_language_with_deep_generational_transmission", "original_pdf": "/attachment/b2118fab2d3373c0154c53caca2c74f3a542d2d4.pdf", "_bibtex": "@misc{\ncogswell2020emergence,\ntitle={Emergence of Compositional Language with Deep Generational Transmission},\nauthor={Michael Cogswell and Jiasen Lu and Stefan Lee and Devi Parikh and Dhruv Batra},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gzoaNtvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gzoaNtvr", "replyto": "r1gzoaNtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper736/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575846327787, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper736/Reviewers"], "noninvitees": [], "tcdate": 1570237747836, "tmdate": 1575846327802, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper736/-/Official_Review"}}}], "count": 11}