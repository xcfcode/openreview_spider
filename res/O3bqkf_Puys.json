{"notes": [{"id": "O3bqkf_Puys", "original": "pCfWmoHr635", "number": 75, "cdate": 1601308017439, "ddate": null, "tcdate": 1601308017439, "tmdate": 1615728221231, "tddate": null, "forum": "O3bqkf_Puys", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "epJRc26e9aF", "original": null, "number": 1, "cdate": 1610040415415, "ddate": null, "tcdate": 1610040415415, "tmdate": 1610474013455, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper presents a construction for deep learning on point clouds that evolve over time. The key characteristics of the data are irregular sampling in the spatial domain and regular sampling in the temporal domain. The presented construction addresses both these aspects of the data. The review by R3 was negative but was addressed by the authors and R3 did not participate in the discussion. The AC supports acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040415401, "tmdate": 1610474013438, "id": "ICLR.cc/2021/Conference/Paper75/-/Decision"}}}, {"id": "DThyUilwG-Q", "original": null, "number": 3, "cdate": 1604279151469, "ddate": null, "tcdate": 1604279151469, "tmdate": 1606356028861, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Review", "content": {"title": "Principled and clearly presented; empirical justification needs improvement.", "review": "This paper introduces a new convolutional approach to directly process raw spatiotemporal (ST) point cloud data. The proposed point spatio-temporal (PST) convolution operates on \"point tubes\" and decouples space and time through a shared spatial convolution at each timestep, followed by a temporal convolution. It also introduces a transposed PST to enable point-wise predictions in an encoder-decoder framework (PSTNet). The presented experiments demonstrate the effectiveness of these convolutions by using PSTNet for action recognition and semantic segmentation on point cloud sequences, showing improvement over relevant recent work.\n\nStrengths:\n+ The approach is technically novel. Processing raw 4D point clouds of arbitrary length is a relatively new direction, and I have not seen a convolutional approach before.\n+ I like the idea of decoupling spatial and temporal convolutions. As the paper points out, this should allow better handling of varied spatial sampling over time.\n+ Though the ST neighborhood size (i.e. temporal kernel size and spatial search radius) must be manually set and tuned, these neighborhoods are defined and structured in a principled way similar to grid-based CNNs which could make tuning easier than in prior work (e.g. MeteorNet).\n+ Experiments show slight improvement over the most relevant SOTA MeteorNet and MinkowskiNet for action recognition and semantic segmentation from depth sequences. I also appreciate the analysis of sensitivity to ST neighborhood parameters and the informative visualization of learned features (Fig 4, 12, and 13).\n+ The paper is, for the most part, well-written and nicely presented (e.g. Fig 2). The detailed supplementary material and promised code release enabled reproducibility. \n\nWeaknesses:\n- The paper could better situate the proposed method in the context of relevant prior work (see \"Related Work\" below).\n- The proposed PST convolution is described as \"generic\", but as far as I can tell it would struggle with irregular temporal sampling. The architecture operates on ordered point cloud sequences but does not leverage timestamps, so there is an underlying assumption that point clouds are sampled at a fixed rate that does not change from training to test time.\n- Various claims and design decisions could be better supported through additional experiments; in particular, the choice to decouple space and time and the spatial kernel design (see \"Experiments\" below). \n- There is no discussion about the limitations of the proposed method in the paper.\n\nBased on these points and the ones detailed below, I initially lean towards accepting the paper. The proposed method is technically novel and principled, and the experiments are sufficient to show PSTNet can extract useful features. The ability to handle irregular timestamps and additional experiments to characterize the advantages of space/time decoupling would improve the paper, but their absence does not outweigh the contributions for me. My other concerns about related work and a discussion of limitations can be addressed in a revision.\n\nRelated work:\n- PSTNet should be compared and contrasted to MeteorNet in more detail since this is the most relevant prior work. Currently, MeteorNet is described in an oversimplified way as applying PointNet++ to 4D points, but there are actually many similarities to the proposed method (\"point tubes\" are analogous to neighborhood grouping and the PST convolution to PointNet aggregation).\n- The proposed spatial kernel (Eq 5) is not compared to or motivated by related static point cloud convolutional methods in the Related Work or Methods sections. Additionally, some important relevant works are not cited, e.g., [1][2].\n- Other relevant works that consider spatiotemporal point clouds in a less general setting may be worth mentioning: e.g. in scene flow estimation [3] or learning from spatiotemporal object point clouds [4].\n\n[1] PointCNN: Convolution on X-Transformed Points, Li et al., 2018\n\n[2] Dynamic Graph CNN for Learning on Point Clouds, Wang et al., 2019\n\n[3] HPLFlowNet: Hierarchical Permutohedral Lattice FlowNet for Scene Flow Estimation on Large-scale Point Clouds, Gu et al., 2019\n\n[4] CaSPR: Learning Canonical Spatiotemporal Point Cloud Representations, Rempe et al., 2020\n\nExperiments:\n- The decision to decouple spatial/temporal convolutions would be more convincing with: (1) a comparison to a baseline PST convolution that does not fully decompose space and time (Eq 2), and (2) an evaluation on LIDAR data where spatial sampling can be highly irregular and variable over time. For example, the task of scene flow estimation from LIDAR would indicate if PSTNet can deal with high spatial irregularity while still encoding necessary local details.\n- The design of the learned spatial kernel f (Eq 5 and following paragraph) is not extensively motivated or justified in the text or by experimental results. How does the displacement/sharing kernel design affect performance or compare to using an MLP or other prior work like PointConv which considers point density? \n- If possible, a runtime comparison to MeteorNet/MinkowskiNet would be useful since these are similar generic ST point processing approaches.\n\nMinor Suggestions:\n- The formulation in Sec 3.2 unclear and inconsistent at times: e.g. (1) the displacement vector is described as a real number following Eq 3, but as a vector following Eq 5, and (2) the shape of F is not detailed for Eq 2-5. In general, I think the exposition in Sec 3.2 would be improved by intuitively describing the \"Point Tube\" idea before the precise mathematical formulation.\n- In Fig 5, GPU runtime is longer than CPU?\n- Typo in Sec 4.3, second paragraph: \"kennel size\" -> \"kernel size\".", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150806, "tmdate": 1606915776922, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper75/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Review"}}}, {"id": "i2BcsL92mDR", "original": null, "number": 5, "cdate": 1605937306394, "ddate": null, "tcdate": 1605937306394, "tmdate": 1606230107088, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "8t-Q7ob-GH_", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment", "content": {"title": "Highlight differences between PSTNet and prior works and efficiency analysis (Part 2/2)", "comment": "Overall, we believe that our PSTNet is significantly different from MeteorNet or PointNet++. Note that radius/neighboring search and FPS sampling are commonly-used operations in point cloud processing. In this paper, we did not claim they are our contribution. The decomposition of spatio-temporal information in representing dynamic point clouds and the hierarchical temporal modeling are our major contributions.\n\n**2.  Efficiency of stacking multiple layers**\n\nStacking multiple layers to form hierarchical architectures is a common technique in deep neural networks, not only used in our PSTNet. In Fig. 14, the running time of PSTNet with different numbers of layers are shown as follows, \n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Table 2 (Appendix Section M)\n\n| # layers $\\ $ | $\\ \\ $1$\\ \\ $ | $\\ \\ \\ $2$\\ \\ $ | $\\ \\ \\ $3$\\ \\ \\ $ |  $\\ \\ \\ $4 $\\ \\ \\ $ |  $\\ \\ $5 $\\ \\ \\ $ |  $\\ \\ \\ $6 $\\ \\ \\ $ |\n\n|time (ms) |  5.67 | 9.76 | 15.95 | 19.17 | 27.64 | 31.92 |\n\nAs PSTNet becomes deep, its running time does not increase dramatically. Because our PSTNet is spatio-temporally hierarchical, points are exponentially reduced along both spatial and temporal dimensions, thus saving running time.\n\n**3.  Efficiency of FPS sampling and nearest neighbor search**\n\nFirst, our spatial convolutions use radius search instead of nearest neighbor search. Compared with nearest neighbor search, radius search does not need time-consuming sort. The running time of main operations in our PSTNet is shown in Tab. 3.\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Table 3  (Appendix Section M)\n\n| operation $\\ \\ \\ \\ \\ $ | time (ms) | percentage |\n\n| convolution $\\ \\ $ | $\\ \\ \\ $12.45$\\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ $39%$\\ \\ \\ \\ \\ $ |\n\n| FPS sampling | $\\ \\ \\ \\ \\ $5.43$\\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ $17%$\\ \\ \\ \\ \\ $ |\n\n| radius  search | $\\ \\ \\ \\ \\ $7.02$\\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ $22%$\\ \\ \\ \\ \\ $ |\n\n| others $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ |$\\ \\ \\ \\ \\ \\ $7.03$\\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ $22%$\\ \\ \\ \\ \\ $ |\n\nThe running time is mainly spent on convolutions.  Note that, if replacing our convolutions with MLPs, the MLP operations take longer time, i.e., 32.78 ms. Then, the running time percentage is MLP : FPS : radius search: others = 63% : 10% : 13.5% : 13.5%.  Therefore, in point cloud processing, the main running time is not caused by FPS sampling or radius search. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "O3bqkf_Puys", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper75/Authors|ICLR.cc/2021/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874797, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment"}}}, {"id": "ofNU-rd2PTS", "original": null, "number": 6, "cdate": 1605937904735, "ddate": null, "tcdate": 1605937904735, "tmdate": 1606200440525, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "lnY3AUdv8g", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment", "content": {"title": "Reason for temporal modeling in the semantic segmentation and correlated points", "comment": "We thank you for acknowledging that our method is  **\u201cvery interesting\u201d** and **\u201cvery versatile\u201d**, and our results are **\u201cpromising\u201d**. We also thank you for your valuable comments.  \n\n**1.  Reason for temporal modeling in the semantic segmentation**\n\nWhen capturing temporal correlation, our PSTNet involves a temporal kernel size for temporal correlation modeling and a temporal stride for frame subsampling. We follow MinkNet14 and MeteorNet, and take 3 frames as input. Since the clip is too short, we do not exploit striding to subsample frames, but set temporal kernel size to 3 to capture temporal correlation. Although semantic segmentation can be achieved from a single frame without temporal correlation, by exploiting temporal information we can constrain the consistency of segmented scene structure across frames, thus improving segmentation accuracy and robustness to noise. \n\n**2.  Correlated points**\n\nWhen points are correlated across frames, both anchors and neighbors can be accurately tracked. In this case, our point tubes can be constructed according to tracked anchor points. \n\nIn the real world, it is impossible to obtain point IDs, because points emerge inconsistently and may flow in and out. To evaluate our PSTNet in the scenario where points can be tracked across frames, we conduct **motion recognition**  on a synthetic Moving MNIST Point Cloud dataset. Each synthetic sequence consists of 16 consecutive point cloud frames.  Each frame contains one handwritten digit moving and bouncing inside a $64 \\times 64$ area. The digits are chosen randomly from the MNIST dataset. We sample 128 points for each digit. The point order is maintained across frames so that tracking can be employed.  We design 144 motions, including 9 initial locations, 8 velocities and 2 kinds of digit distortion.  Because motion and appearance are independent of each other, it is challenging to recognize motion while avoiding interference from digit appearance.  We use the same training/test split as the original MNIST dataset. For details, please refer to Appendix Section P. \n\nTo exploit point correlation, anchor points and their neighbors are selected in the first frame, which are then propagated to other frames.  In this fashion, anchors and their neighbors are tracked across frames.\n\n$\\ \\ \\ \\ \\ $Table 1  (Appendix Section P)\n\n| Method$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ | Acc (%)  |\n\n| PSTNet (original)  $ \\ \\ \\ \\ \\ \\ \\ \\ $ | $\\ \\ $97.15$\\ $ |\n\n| PSTNet (with tracking)  | $\\ \\ $98.74$\\ $ |\n\nAs shown in Tab. 1,  both original and tracking based PSTNet achieve promising accuracy on the Moving MNIST Point Cloud dataset. Our original PSTNet achieves similar accuracy with tracking based PSTNet in this simulated case, demonstrating that our PSTNet does not heavily rely on point IDs. \n\n**3.  Related work**\n\nThank you for pointing out these two interesting works that also attempt to model spatio-temporal point clouds. We have discussed them in our revision. Niemeyer [1] learned a temporal and spatial vector field in which every point is assigned with a motion vector of space and time for 4D reconstruction. Prantl et al. [2] learned stable and temporal coherent feature spaces for point-based super-resolution. \n\n[1] Michael Niemeyer, Lars M. Mescheder, Michael Oechsle and Andreas Geiger. Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics. ICCV (2019)\n\n[2] Lukas Prantl, Nuttapong Chentanez, Stefan Jeschke and Nils Thuerey. Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds. ICLR (2020)\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "O3bqkf_Puys", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper75/Authors|ICLR.cc/2021/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874797, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment"}}}, {"id": "ZEmbKMU3a72", "original": null, "number": 3, "cdate": 1605935568751, "ddate": null, "tcdate": 1605935568751, "tmdate": 1606137743480, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "MdgIUlImWze", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment", "content": {"title": "More discussions and experiments (Part 2/2)", "comment": "**3. Effect of displacement/sharing kernel in the proposed lightweight spatial convolution.**\n\nThe goal of displacement and sharing kernels is to generate a unique spatial kernel for each displacement so that the spatial convolution is able to capture spatial structure like conventional convolutions.  To this end, we first use a sharing kernel to increase point feature dimension to improve the feature representation ability. Then, a displacement kernel is to capture spatial local structure based on point displacements.  We conduct 3D action recognition on MSR-Action3D to study the effect of displacement/sharing kernel.\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Table 3 (Appendix Section L)\n\n| displacement | sharing | 16-frame | 24-frame |\n\n| $\\ \\ \\ \\ \\ \\ \\ \\ \\ $\u2713$\\ \\ \\ \\ \\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$ | $\\ \\ \\$58.25%$\\ \\$ | $\\ \\ $61.67%$\\ \\ $ |\n\n| $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ $\u2713$\\ \\ \\ \\ $ | $\\ \\ \\$86.35%$\\ \\$ | $\\ \\ $87.46%$\\ \\ $ |\n\n| $\\ \\ \\ \\ \\ \\ \\ \\ \\ $\u2713$\\ \\ \\ \\ \\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ $\u2713$\\ \\ \\ \\ $ | $\\ \\ \\$89.90%$\\ \\$ | $\\ \\ $91.20%$\\ \\ $ |\n\nAs shown in Tab. 3, both displacement and sharing kernel is important in our spatial convolution. \n\n**4. Irregular frame sampling**\n\n We assume the timesteps of dynamic point clouds are regular because commercial widely-used LiDARs and depth sensors capture point cloud sequences at a fixed FPS (frames per second). For instance, the frame rates of Intel RealSense and Kinetic V2 are 30 FPS. However, our method is not restricted to this. When the frames are not irregularly sampled, we can employ frame interpolation to achieve evenly sampled frames along the temporal dimension. \n\nHere, we conduct 3D action recognition on MSR-Action3D with irregularly sampled frames. Clip length is originally 24 and we randomly remove 8 frames from each clip. Replication and Iterative Closest Point (ICP) are used to interpolate missing frames. \n\n $\\ \\ \\ $Table 4 (Appendix Section O)\n\n|Method$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ | Acc (%)|\n\n|MeteorNet$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ | $\\ \\ $88.40 |\n\n|MeteorNet (full)$\\ \\ \\ \\ \\ \\ $ | $\\ \\ $88.50 |\n\n|PSTNet (replication) | $\\ \\ $90.56 |\n\n|PSTNet (ICP)$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ | $\\ \\ $90.94 |\n\nEven with the replication interpolation, our PSTNet achieves higher performance than MeteorNet using all frames. This also indicates that PSTNet is able to model irregularly sampled dynamic point clouds.\n\n**5. Comparison to PST convolution without decoupling (Appendix Section L)**\n\nAs shown in Fig. 6, the decoupled PST convolutions achieve better accuracy than the non-decoupled version. For example, with 24 frames, the decoupled PSTNet achieves 91.20% on MSR-Action, while the non-decoupled PSTNet only achieves 89.56%. \n\n**6. Scene flow estimation**\n\nTo show the ability for point cloud sequence modeling, we follow MeteorNet to estimate a flow vector for every point in the last frame.  Scene flow estimation is conducted on the KITTI scene flow dataset. Point tracking is not used. For more details, please refer to  Appendix Section P.\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Table 5 (Appendix Section P)\n\n| Method$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $ | #Frames | End-Point-Error |\n\n| FlowNet3D$\\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ $2$\\ \\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ \\ \\ $0.287$\\ \\ \\ \\ \\ \\ \\ \\ $ |\n\n| MeteorNet$\\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ $3$\\ \\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ \\ \\ $0.282$\\ \\ \\ \\ \\ \\ \\ \\ $ |\n\n| PSTNet (ours) | $\\ \\ \\ \\ \\ \\ $3$\\ \\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ \\ \\ \\ $0.278$\\ \\ \\ \\ \\ \\ \\ \\ $ |\n\n As expected, our method achieves promising accuracy and outperforms the state-of-the-art.\n\n**7. Limitation**\n\nAs our method only exploits local operations and does not explicitly capture global dependencies, this may limit the ability of understanding scenes or spotting periodic actions. A potential improvement is to adopt non-local techniques to enhance the feature representations in spatial and temporal dimensions.\n\n**8. Missing related works and minor problems**\n\nThe minor problems and missing related works have been fixed and discussed in our revision. \n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "O3bqkf_Puys", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper75/Authors|ICLR.cc/2021/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874797, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment"}}}, {"id": "8t-Q7ob-GH_", "original": null, "number": 4, "cdate": 1605936091736, "ddate": null, "tcdate": 1605936091736, "tmdate": 1606006887365, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "jkW1n7SJ8x8", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment", "content": {"title": "Highlight differences between PSTNet and prior works and efficiency analysis (Part 1/2)", "comment": "We thank you for acknowledging **\u201cthe extensive experiments verify the effectiveness\u201d**  of our method and **\u201cachieve state-of-the-art results on multiple benchmark datasets.\u201d** We also thank you for the suggestion of efficiency comparison.\n\n**1. Comparison to MeteorNet or PointNet++**\n\nTo clarify the differences, we formulate PointNet++, MeteorNet and our method as follows,\n\nPointNet++:         $\\textit{F}'^{(x,y,z)} = \\mathrm{MAX}\\_{||(\\delta_{x}, \\delta_y, \\delta_z)|| \\le r}   \\mathrm{MLP}(\\textit{F}^{(x,y,z)}, \\textit{F}^{(x+\\delta_x,y+\\delta_y,z+\\delta_z)}, \\delta_{x}, \\delta_y, \\delta_z)$\n\nMeteorNet:          $\\textit{F}\u2019^{(x,y,z)}\\_t = \\mathrm{MAX}\\_{||(\\delta_{x}, \\delta_y, \\delta_z)|| \\le r}\\mathrm{MLP}(\\textit{F}_t^{(x,y,z)}, \\textit{F}_\\{t+\\delta\\_t}^{(x+\\delta_x,y+\\delta_y,z+\\delta_z)}, \\delta_x,\\delta_y,\\delta_z, \\delta_t)$\n\nPSTNet (ours):     spatial         $\\ \\ \\ \\ \\textit{M}^{(x,y,z)}\\_t = \\sum\\_{||(\\delta_{x}, \\delta_y, \\delta_z)|| \\le r} \\textit{S}^{(\\delta_{x}, \\delta_{y}, \\delta_{z})}  \\cdot \\textit{F}^{(x+\\delta_x,y+\\delta_y,z+\\delta_z)}\\_t$\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $temporal    $\\textit{F}\u2019^{(x,y,z)}\\_t = \\sum\\_{k=-\\lfloor l/2 \\rfloor}^{\\lfloor l/2 \\rfloor}    \\textit{T}\\_k \\cdot \\textit{M}^{(x,y,z)}\\_{t+k}$ \n\nwhere $r$ is spatial search radius and $l$ is temporal kernel size.  Note that, MeteorNet does not involve temporal search radius or temporal kernel size. \n\nFirst of all, PointNet++ only models spatial information of static point clouds and does not have a mechanism to model temporal information of point cloud sequences. On the contrary, our PSTNet and MeteorNet are designed to address dynamic point clouds. \n\nFurthermore, there are three major differences between our proposed PSTNet and MeteorNet:\n\n- **Decoupling spatio-temporal information for point cloud sequence modeling.**\n  - Considering space and time are orthogonal and independent of each other, our PSTNet decouples spatio-temporal information of point cloud sequences. This decomposition significantly facilitates us to model the spatial and temporal information explicitly according to their distinct characteristics. Specifically, our PSTNet models temporal dynamics by leveraging the temporal order while addressing the spatial structure of point clouds by taking their spatial irregularity into account. On the contrary, MeteorNet simply stacks timestamps and coordinates together, and neglects the temporal order in timestamps. Thus, it may not well exploit temporal information, leading to inferior performance.\n  - The scales of spatial displacements and temporal differences in point cloud sequences may not be compatible. Treating them equally like MeteorNet is not in favor of network optimization and thus leads to inferior feature extraction. In contrast, by modeling these two modalities separately, our PSTNet achieves superior feature representation ability. \n- **Hierarchical spatio-temporal modeling benefiting from our point tube.** \nThe grouping in MeteorNet only considers spatial radius. As MeteorNet neglects the local dependencies of neighboring frames, it models temporal information by using the sequence length as its temporal receptive field.\nWithout constructing temporal hierarchy with temporal kernels, pooling or stride, MeteorNet faces two issues. \n(i) As points flow in and out of the region, especially for long sequences and fast motions, embedding points in a spatially local area along an entire sequence handicaps capturing instant local dynamics of point clouds in MeteorNet. (ii) Without the temporal stride or pooling, MeteorNet needs to process all the frames in each layer and thus is not efficient. In contrast, benefiting from our point tube, the proposed PSTNet is both spatially and temporally hierarchical, and thus is efficient to model long sequences. As shown in Tab. 1, from 16 frames to 24 frames, our method achieves 1.30% improvement while MeteorNet only obtains 0.29% improvement, demonstrating our method is more effective in modeling longer sequences. \n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Table 1 (Appendix Section M)\n\n$\\ \\ \\ \\ \\ \\ $| Method$\\ \\ \\ \\ \\ \\ $|  # Params | 16-frame time | 16-frame acc | 24-frame acc | \n\n$\\ \\ \\ \\ \\ \\ $| MeteorNet | $\\ \\ $17.60 M$\\ $ | $\\ \\ \\ \\ \\ $54.56 ms$\\ \\ \\ $ | $\\ \\ \\ \\ \\ $88.21%$\\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ $88.50%$\\ \\ \\ \\ $ | \n\n$\\ \\ \\ \\ \\ \\ $| PSTNet$\\ \\ \\  \\ \\ \\ \\$  | $\\ \\ \\ $8.26 M $\\ $   | $\\ \\ \\ \\ $31.92 ms $\\ \\ \\ $ | $\\ \\ \\ \\ \\ $89.90%$\\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ $91.20%     $\\ \\ \\ $ | \n\n- **Operators.** To the best of our knowledge, our PSTNet is the first attempt to construct convolutions on raw point cloud sequences in a unified framework. Our convolution-based network outperforms MLP-based MeteorNet on all the tasks. Moreover, as shown in Tab. 1, our PSTNet uses fewer parameters and takes less time than MeteorNet, demonstrating our convolutional operations are effective and efficient. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "O3bqkf_Puys", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper75/Authors|ICLR.cc/2021/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874797, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment"}}}, {"id": "MdgIUlImWze", "original": null, "number": 2, "cdate": 1605933524692, "ddate": null, "tcdate": 1605933524692, "tmdate": 1605971793880, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "DThyUilwG-Q", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment", "content": {"title": "More discussions and experiments (Part 1/2)", "comment": "We thank you for acknowledging our work is **\u201ctechnically novel and principled\u201d** and experiments are **\u201csufficient to show PSTNet can extract useful features\u201d**. We are glad to see that you **\u201clike the idea of decoupling spatial and temporal convolutions\u201d**.  We also thank you for your constructive and detailed comments. \n\n**1. Differences from MeteorNet**  \n\nThere are three major differences between our proposed PSTNet and MeteorNet, as follows:\n- **Decoupling spatio-temporal information for point cloud sequence modeling.**\n  - Considering space and time are orthogonal and independent of each other, our PSTNet decouples spatio-temporal information of point cloud sequences. This decomposition significantly facilitates us to model the spatial and temporal information explicitly according to their distinct characteristics. Specifically, our PSTNet models temporal dynamics by leveraging the temporal order while addressing the spatial structure of point clouds by taking their spatial irregularity into account. On the contrary, MeteorNet simply stacks timestamps and coordinates together, and neglects the temporal order in timestamps. Thus, it may not well exploit temporal information, leading to inferior performance.\n  - The scales of spatial displacements and temporal differences in point cloud sequences may not be compatible. Treating them equally like MeteorNet is not in favor of network optimization and thus leads to inferior feature extraction. In contrast, by modeling these two modalities separately, our PSTNet achieves superior feature representation ability. \n- **Hierarchical spatio-temporal modeling benefiting from our point tube.** \nThe grouping in MeteorNet only considers spatial radius. As MeteorNet neglects the local dependencies of neighboring frames, it models temporal information by using the sequence length as its temporal receptive field.\nWithout constructing temporal hierarchy with temporal kernels, pooling or stride, MeteorNet faces two issues. \n(i) As points flow in and out of the region, especially for long sequences and fast motions, embedding points in a spatially local area along an entire sequence handicaps capturing instant local dynamics of point clouds in MeteorNet. (ii) Without the temporal stride or pooling, MeteorNet needs to process all the frames in each layer and thus is not efficient. In contrast, benefiting from our point tube, the proposed PSTNet is both spatially and temporally hierarchical, and thus is efficient to model long sequences. As shown in Tab. 1, from 16 frames to 24 frames, our method achieves 1.30% improvement while MeteorNet only obtains 0.29% improvement, demonstrating our method is more effective in modeling longer sequences. \n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Table 1 (Appendix Section M)\n\n$\\ \\ \\ \\ \\ \\ $| Method$\\ \\ \\ \\ \\ \\ $|  # Params | 16-frame time | 16-frame acc | 24-frame acc | \n\n$\\ \\ \\ \\ \\ \\ $| MeteorNet | $\\ \\ $17.60 M$\\ $ | $\\ \\ \\ \\ \\ $54.56 ms$\\ \\ \\ $ | $\\ \\ \\ \\ \\ $88.21%$\\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ $88.50%$\\ \\ \\ \\ $ | \n\n$\\ \\ \\ \\ \\ \\ $| PSTNet$\\ \\ \\  \\ \\ \\ \\$  | $\\ \\ \\ $8.26 M $\\ $   | $\\ \\ \\ \\ $31.92 ms $\\ \\ \\ $ | $\\ \\ \\ \\ \\ $89.90%$\\ \\ \\ \\ \\ $ | $\\ \\ \\ \\ \\ $91.20%     $\\ \\ \\ $ | \n\n- **Operators.** To the best of our knowledge, our PSTNet is the first attempt to construct convolutions on raw point cloud sequences in a unified framework. Our convolution-based network outperforms MLP-based MeteorNet on all the tasks. Moreover, as shown in Tab. 1, our PSTNet uses fewer parameters and takes less time than MeteorNet, demonstrating our convolutional operations are effective and efficient. \n\n**2. Comparison to existing static point cloud convolutional methods**\n\nAlthough we propose a specific lightweight implementation for the spatial convolution, we did not claim it was our key novelty or contribution. The goal of this paper is to decouple spatio-temporal information, and construct spatial and temporal hierarchy networks for point cloud sequence modeling. \n\nCompared to existing static point cloud convolutions which usually contain MLPs and other layers, our lightweight spatial convolutions contain fewer parameters and are more efficient. We replace our lightweight spatial convolution in our PSTNet with PointConv, and conduct 3D action recognition on MSR-Action3D with 16 frames.\n\n$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $Table 2 (Appendix Section N)\n\n| Method $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ $| # Params | time (ms) | Acc (%)|\n\n| PSTNet (original)  $\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\$ | $\\ \\ \\ $8.26 M$\\ $ | $\\ \\ \\ $31.92$\\ \\ \\ \\ $ | $\\ $89.90$\\ $ |\n\n| PSTNet (with PointConv) | $\\ $ 20.46 M$\\ $ | $\\ $102.72$\\ \\ \\ \\ \\ \\$| $\\ $90.24$\\ $ |\n\nAs shown in Tab. 2, using PointConv as spatial operations only improves performance slightly compared to our lightweight spatial convolutions, but significantly increases parameters and running time.  \n\n\n\n\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper75/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "O3bqkf_Puys", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper75/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper75/Authors|ICLR.cc/2021/Conference/Paper75/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923874797, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Comment"}}}, {"id": "lnY3AUdv8g", "original": null, "number": 1, "cdate": 1603663542645, "ddate": null, "tcdate": 1603663542645, "tmdate": 1605024768401, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Review", "content": {"title": "Promising results and well elaborated", "review": "The paper introduces point spatio-temporal convolutions, which are used for the feature extraction of point cloud sequences. A trainable kernel is used which is applied locally as a continuous convolution. An important aspect is that the temporal dimension is processed separately, with an additional convolution, instead of simply using a 4D convolution. The authors claim that in this way the network will achieve a better understanding of the dynamics of the input.\n\nThe presented method is very interesting in my eyes. I find the argumentation concerning the construction of the spatio-temporal conclusive and it is supported by the experiments. Together with the presented striding, based on farthest point sampling, and the transposed convolution, the method seems to be very versatile, as it was shown in the results. However, a few points are still not quite clear to me:\n* Why is striding used for the semantic segmentation test? Theoretically, this problem could be solved much easier without striding and transposed convolutions.\n* The results were mainly sequences where the points of the frames do not correlate. What about data where the points correlate, like in physical simulations? Would the presented point tubes make sense there?\n\nThe paper is written understandably and the evaluations are sufficient, IMO, to confirm the claims given by the authors. Maybe the following papers could be relevant as related work: Occupancy Flow: 4D Reconstruction by Learning Particle Dynamics and Tranquil Clouds: Neural Networks for Learning Temporally Coherent Features in Point Clouds, since they also deal with point sequences. \nIn section 3.1 there is a small typo (... CNNs usually adopt small ~~kennel~~ **kernel** sizes ...), just like at the beginning of section 4.3 (Temporal ~~kennel~~ **kernel** size...).\n\nAll in all I find the method simple but promising. The paper seems well elaborated and I would tend to accept it.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150806, "tmdate": 1606915776922, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper75/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Review"}}}, {"id": "jkW1n7SJ8x8", "original": null, "number": 2, "cdate": 1604066815062, "ddate": null, "tcdate": 1604066815062, "tmdate": 1605024768322, "tddate": null, "forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "invitation": "ICLR.cc/2021/Conference/Paper75/-/Official_Review", "content": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "review": "This paper aims to process the point cloud data in a convolution manner. The authors propose the PST convolution and deconvolution operations to handle the different tasks such as the classification and segmentation on point cloud. The extensive experiments verify the effectiveness of the proposed method and achieve state-of-the-art results on multiple benchmark datasets. Overall, the paper is well-written and organized. \n \nStrength:\n \u2022\tIn this paper, the authors introduce a method named PST convolution which could directly handle the sequential point cloud data. The PST convolution could capture the spatio-temporal in a convolution manner. They decompose the spatial and temporal information and conduct convolution on them respectively. \n\u2022\tIn order to handle both the abstraction level and point-wise level tasks, they build a complete convolution system including convolution and deconvolution. Also, the point tube concept is introduced here to preserve the spatio-temporal structure. \n\u2022\tThe experiments are exhaustive and impressive. Especially the author adopts various kinds of demonstration to show the effectiveness. \n \n Weakness:\n \u2022\tEven through the experiments results look pretty good, the novelty of this paper is still limited. In this paper, compared to the previous paper such as MeteorNet, the authors explicitly promotes the concept of capturing the spatial and temporal information. There is no significant difference with MeteorNet or even PointNet++. The MeteorNet also extracts the spatio-temporal information by chain-flow radius search. No matter the point tube or the anchor points extracted in the spatial and temporal domain, the main idea is finding a good neighbor for the current point. It should be described clearly in details for the novelty in the future revised version. \n\u2022\tAlso, even the convolution adopted in this paper could make the point processing more convenient, I have a concern about the efficiency of the proposed method. The computation cost including stacked convolution layers, the FPS sampling, and the nearest neighbor search are all heavy. So the authors may provide some efficiency comparison in the future. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper75/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper75/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "PSTNet: Point Spatio-Temporal Convolution on Point Cloud Sequences", "authorids": ["~Hehe_Fan1", "~Xin_Yu1", "~Yuhang_Ding1", "~Yi_Yang4", "~Mohan_Kankanhalli1"], "authors": ["Hehe Fan", "Xin Yu", "Yuhang Ding", "Yi Yang", "Mohan Kankanhalli"], "keywords": ["Point cloud", "spatio-temporal modeling", "video analysis", "action recognition", "semantic segmentation", "convolutional neural network"], "abstract": "Point cloud sequences are irregular and unordered in the spatial dimension while exhibiting regularities and order in the temporal dimension. Therefore, existing grid based convolutions for conventional video processing cannot be directly applied to spatio-temporal modeling of raw point cloud sequences. In this paper, we propose a point spatio-temporal (PST) convolution to achieve informative representations of point cloud sequences. The proposed PST convolution first disentangles space and time in point cloud sequences.  Then, a spatial convolution is employed to capture the local structure of points in the 3D space, and a temporal convolution is used to model the dynamics of the spatial regions along the time dimension.  Furthermore, we incorporate the proposed PST convolution into a deep network, namely PSTNet, to extract features of point cloud sequences in a hierarchical manner.  Extensive experiments on widely-used 3D action recognition and 4D semantic segmentation datasets demonstrate the effectiveness of PSTNet to model point cloud sequences.", "one-sentence_summary": "This paper proposes a point spatio-temporal (PST) convolution, which decomposes space and time, to learn representations of raw point cloud sequences in a spatio-temporally hierarchical manner.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "fan|pstnet_point_spatiotemporal_convolution_on_point_cloud_sequences", "pdf": "/pdf/777730e0844c663a556004d8da8b42ea97743fd0.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nfan2021pstnet,\ntitle={{\\{}PSTN{\\}}et: Point Spatio-Temporal Convolution on Point Cloud Sequences},\nauthor={Hehe Fan and Xin Yu and Yuhang Ding and Yi Yang and Mohan Kankanhalli},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=O3bqkf_Puys}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "O3bqkf_Puys", "replyto": "O3bqkf_Puys", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper75/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538150806, "tmdate": 1606915776922, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper75/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper75/-/Official_Review"}}}], "count": 10}