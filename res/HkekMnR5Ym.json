{"notes": [{"id": "HkekMnR5Ym", "original": "BkemFt6ctQ", "number": 1230, "cdate": 1538087943500, "ddate": null, "tcdate": 1538087943500, "tmdate": 1545355437376, "tddate": null, "forum": "HkekMnR5Ym", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BygYT82Hl4", "original": null, "number": 1, "cdate": 1545090753372, "ddate": null, "tcdate": 1545090753372, "tmdate": 1545354479762, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Meta_Review", "content": {"metareview": "This work proposes and interesting approach to learn approximate set membership. While the proposed architecture is rather closely related to existing work, it is still interesting, as recognized by reviewers. Authors's substantial rewrites has also helped make the paper clearer. However, the empirical merits of the approach are still a bit limited; when combined with the narrow novelty compared to existing work, this makes the overall contribution a bit too thin for ICLR. Authors are encouraged to strengthen their work by showing more convincing practical benefit of their approach.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting approach, empirical validation needs strengthening"}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1230/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352914957, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352914957}}}, {"id": "HJlfcMvI3m", "original": null, "number": 1, "cdate": 1540940426042, "ddate": null, "tcdate": 1540940426042, "tmdate": 1543180599165, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Review", "content": {"title": "Details of the architecture not well motivated", "review": "The paper proposes a learnable bloom filter architecture. While the details of the architecture seemed a bit too complicated for me to grasp (see more on this later), via experiments the authors show that the learned bloom filters are more compact that regular bloom filters and can outperform other neural architectures when it comes to retrieving seen items.\n\nA bloom filter is fairly simple, K hash functions hash seen items into K bit vectors. During retrieval, if all of the bits hashed to are 1 then we say we've seen the query. I think there's simpler ways to derive a continuous, differentiable version of this which begs the question why the authors chose a relatively more elaborate architecture involving ZCA transform and first/second moments. Perhaps the authors need to motivate their architecture a bit better.\n\nIn their experiments, a simple LSTM seems to perform remarkably well (it is close to the best in 2 (a), (b); and crashes in (c) but the proposed technique is also outperformed by vanilla bloom filters in (c)). This is not surprising to me since LSTMs are remarkably good at remembering patterns. Perhaps the authors would like to comment on why they did not develop the LSTM further to remedy it of its shortcomings. Some of the positive results attained using neural bloom filters is a bit tempered by the fact that the experiments were using a back up bloom filter. Also, the neural bloom filters do well only when there is some sort of querying pattern. All of these details would seem to reduce the applicability of the proposed approach.\n\nThe authors have addressed most (if not all) of my comments in their revised version. I applaud the authors for being particularly responsive. Their explanations and additional experiments go a long way towards lending the insights that were missing from the original draft of the paper. I have upped my rating to a 7.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Review", "cdate": 1542234275571, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335901414, "tmdate": 1552335901414, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hkl9AlNPA7", "original": null, "number": 14, "cdate": 1543090385768, "ddate": null, "tcdate": 1543090385768, "tmdate": 1543090385768, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Updated speed benchmark with LSTM", "comment": "To supplement the discussion on \"why create this model versus use an LSTM or variant\". Aside from the fact that we could not get any RNN to solve the database task with 5,000 elements; we ran an LSTM on the speed benchmark for this task to determine how much slower/faster it would be ... If we could somehow train it to solve this task.\n\nInsertion throughput for Bloom Filter on: ~60K (CPU)\nInsertion throughput for Neural Bloom Filter: ~4K (CPU), ~58K (GPU)\nInsertion throughput for LSTM: ~2K (CPU), ~5K (GPU).\n\nSo as discussed earlier, a Neural Bloom Filter can match the throughput of an Bloom Filter when run on a GPU. A couple of GPUs reserved along with the thousand of CPU cores for a large Bigtable database seems feasible. However if we look at the LSTM numbers, the insertion throughput is about 10x less (5K insertions per sec vs 60K). Thus the sequential write scheme of RNNs (in this case, an LSTM) is not only a problem from an optimization-perspective - as the LSTM fails to learn the task - but also reduced the throughput of insertions by an order of magnitude during evaluation. We have added these numbers to the Appendix G and discussed this point in the main paper, Section 5.4. This is just extra empirical evidence that there is room for a memory model with a feed-forward & compressive write scheme, alike to a bloom filter. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "ByxXh6ZPAQ", "original": null, "number": 11, "cdate": 1543081387340, "ddate": null, "tcdate": 1543081387340, "tmdate": 1543081387340, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "ryeKu7-wCX", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Re. Re.", "comment": "Thanks R3 for reading the revision and rebuttal. \n\nSmall point re. analytical bounds --- we have these two sentences in S 5.1 which was perhaps not pushed into the revision (although I see it now) however we could also put (analytical) in the plot legends if you think this is worthwhile.\n\n\"The false positive rate is measured empirically over a sample of queries for the learned models; for the Bloom Filter we employ the analytical false positive rate. Beating a Bloom Filter\u2019ss pace usage with the analytical false positive rate implies better performance for any given BloomFilter library version (as actual Bloom Filter hash functions are not uniform), thus the comparison is fair.\"\n\nAside from the text motivating the model, what do you think could be added or amended in this study to make it more clearly worthwhile of publishing going forward? We ask because you appear to be interested in the subject area. \n\nE.g.\n- One-shot learning for real applications is not sufficiently motivated?\n- Some experiments are missing (from your perspective)?\n- You think there are flaws in the model or comparison approach?\n\nAny further feedback would be highly appreciated."}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "H1lg3Mjdn7", "original": null, "number": 2, "cdate": 1541087911978, "ddate": null, "tcdate": 1541087911978, "tmdate": 1543078830129, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Review", "content": {"title": "Interesting topic, some concerns", "review": "SUMMARY\nThe paper proposes a neural network based architecture to solve the approximate set membership problem, in the distributional setting where the in-set and out-of-set elements come from two unknown and possibly different distributions.\n\n\nCOMMENTARY\nThe topic of the paper is interesting, and falls into the popular trend of enhancing classical data structures with learning algorithms. For the approximate set membership problem, this approach was already suggested by (Kraska et al. 2018) and studied further in (Mitzenmacher 2018a,b). The difference in the current paper is that the proposed approach relies on \"meta-learning\", apparently to facilitate online training and/or learning across multiple sets arising from the same distribution; this is what I gather from the introduction, even though as I write below, I feel this point is not properly explained.\n\nMy main issue with the paper is that its conceptual contribution seems limited and unclear. It suggests a specific architecture whose details seem mostly arbitrary, or at least this is the impression the reader is left with, as the paper does rather little in terms of discussing and motivating them or putting them in context. Moreover, since the solution ultimately relies on a backup Bloom Filter as in (Kraska et al. 2018), it is hard to not view it as just an instantiation of the model in (Kraska et al. 2018, Mitzenmacher 2018a) with a different plugging of learning component. It would help to flesh out and highlight what the authors claim are the main insights of the paper.\n\nAnother issue I suggest revising pertains to the writing. The problem setting is only loosely sketched but not properly defined. How exactly do different subsets coming into play? Specifically, the term \"meta-learning\" appears in the title and throughout the paper, but is never defined or explained. The authors should write out what exactly they mean by this notion and what role it plays in the paper. This is important since to my understanding, this is the main point of departure from the aforementioned recent works on learning-enhanced Bloom Filters.\n\nThe experiments do not seem to make a strong case for the empirical advantage of the Neural Bloom Filter. They show little to no improvement on the MNIST tasks, and some improvement on a non-standard database related task. One interesting thing to look at would be the workload partition between the learning component and the backup filter, meaning what is the rate of false negatives emitted by the former and caught by the latter, and how the space usage breaks down between them (vis-a-vis the formula in Appendix B). For example, it seems plausible that on the class familiarity task, the learning component simply learns to be a binary classifier for the chosen two MNIST classes and mostly ignores the backup filter, whereas in the uniform distribution setting, the learning component only memorizes a small number of true and false positives and defers almost the entire task to the backup filter. I am not sure what to expect on the intermediate exponential distribution task.\n\nOther comments/questions:\n1. For the classical Bloom Filter, do the results reported in the experimental plots reflect the empirical false-positive rate measured in the experiment, or just the analytic bound?\n2. On that note, it is worth noting that the false positive rate of the classical Bloom Filter is different than the one you report for the neural-net based architectures. The Bloom Filter FP probability is over its internal randomness (i.e. its hash functions) and is independent of the distribution of queries, which need not be randomized at all. For the neural-net based architectures, the measured FP rate is w.r.t. a specific distribution of queries. See the discussion in (Mitzenmacher 2018a), sections B-C.\n3. The works (Mitzenmacher 2018a,b) should probably at least be referenced in the related work section.\n\n\nCONCLUSION\nWhile I like the overall topic of the paper, I currently find the conceptual contribution to be too thin, raising doubts on novelty and significance. In addition, the presentation is somewhat lacking in clarity, and the practical merit is not well established. Notwithstanding the public nature of ICLR submissions, I would suggest more work on the paper prior to publication.\n\n\nREFERENCES\nM. Mitzenmacher, A Model for Learned Bloom Filters and Related Structures, 2018, see https://arxiv.org/pdf/1802.00884.pdf.\nM. Mitzenmacher, Optimizing Learned Bloom Filters by Sandwiching, 2018, see https://arxiv.org/pdf/1803.01474.pdf.\n\n(Update: score revised, see below.)", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Review", "cdate": 1542234275571, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335901414, "tmdate": 1552335901414, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ryeKu7-wCX", "original": null, "number": 10, "cdate": 1543078768969, "ddate": null, "tcdate": 1543078768969, "tmdate": 1543078768969, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "BklHAWcapX", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Response to revision", "comment": "I thank the authors for their detailed responses and revision.\n- The revision to section 5 explaining the training procedure is helpful.\n- The revision to section 3 is also helpful. It may have helped to go even further in explaining the objectives of the memory access learning task (as in the response to Rev2) with analogs to BF, and distinguishing them from some aspects that seem like engineering details, as the former is (to me) the conceptually significant portion of the paper.\n- I still could not find where it is stated that the BF plots are analytic; my apology if I missed it. This is not a major issue, and I understand the choice to use the theoretical bound, but there is some discord in including an analytic curve next to empirical curves on the same plot without clearly marking it as such, as it may give a wrong impression as to what the reader is seeing (not an actual experiment, but an estimate of what an experiment would have yielded based on probabilistic concentration). \n- I have revised my score to 6."}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "S1lXv_J707", "original": null, "number": 7, "cdate": 1542809690623, "ddate": null, "tcdate": 1542809690623, "tmdate": 1542809690623, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Feedback on revision", "comment": "Dear reviewers,\n\nGiven the consensus was that the model motivation and the one-shot learning setup was not clear enough, versus fundamental disagreements with the subject area or potential impact, it would be very valuable for feedback on the paper revision.  The principal changes are found in Section 3, Section 5 intro (+ Algorithm 1) & Section 5.1,  box. We have surveyed several of our peers unfamiliar with meta-learning and they said they understood the training regime much better and felt 80%+ sure of how the model was trained and evaluated. So we would be very grateful if you could consider our response and paper revision! "}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "B1llq3T-0Q", "original": null, "number": 6, "cdate": 1542737032191, "ddate": null, "tcdate": 1542737032191, "tmdate": 1542737032191, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "BylFBY5aa7", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Backup Bloom Filter", "comment": "Sorry we realize we did not address this comment,\n\n\"Some of the positive results attained using neural bloom filters is a bit tempered by the fact that the experiments were using a back up bloom filter.\"\n\nActually when comparing the space of our one-shot model versus a Bloom Filter; we compute the size of the state (in bits) *plus* the size of the backup Bloom Filter which stores the false negatives. The backup Bloom Filter (which *only* stores false negatives) thus must be very small in comparison with the original Bloom Filter for the total space of the neural bloom filter to be smaller. In the case of the database task where we see >30x space reduction, it is clearly negligible. \n\nWe only use a backup filter to ensure an apples-to-apples comparison between the neural bloom filter and bloom filter (i.e. a guaranteed 0% false negative rate). For applications where a small false negative rate is acceptable, one could avoid using the backup bloom filter completely. We have clarified this in the text in the experiments section. In terms of speed, the backup bloom filter does not add latency to queries because it can be queried in parallel to the neural bloom filter. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "rJeha6qa6m", "original": null, "number": 5, "cdate": 1542462916261, "ddate": null, "tcdate": 1542462916261, "tmdate": 1542462916261, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "BkxdWIvphQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Explanation of training setup and why it's one-shot classification.", "comment": "Thank you for reading the paper, and we apologize for its opacity upon first pass. We completely agree the paper has mis-judged its audience and was not easy to read straight-through, this feedback is very useful in correcting this. We wrote the paper for someone highly familiar with meta-learning memory-augmented neural networks but not familiar with bloom filters; this left out an important audience.\n\n--- Re. \u201cI had a hard time understanding how the model is trained...\u201d\n\nThe model learns in one-shot because it observes a set S = (k1, k2, \u2026 kn) and writes it to a memory (or state) M with only one observation of this dataset. It then answers queries \u201cis my query x in S\u201d using the read operation, conditioning on the memory, M. It is the same one-shot classification approach as \"Matching Networks\" Vinyals et al. 2016 however we focus on classifying familiarity versus image or text class. We have added several paragraphs and an algorithm box with further explanation of the meta-learning training setup. We will just briefly summarize it here. \n\nWe have a collection of sets Strain1, Strain2, , \u2026 Strainm reserved for training; and a collection of queries Q = {q1, q2, \u2026, qL} and targets yi = 1 if qi in S and 0 otherwise. In the example of a database we can think of a given set Si = {k1, \u2026, kN} as a set of rowkeys for a given file on disk (e.g. SSTable). We have many sets because we have many files; for training we have reserved some for an offline training routine.\n\nDuring training we calculate M = fwrite(S), and then we calculate oi = fread(S, qi). We calculate the cross-entropy loss L = \\sumi yi log(oi) + (1 - yi)log(1-oi) and backprogate through the network (through the parameters controlling both the read, write, and encoder networks). One can consider the creation of M = fwrite(S) as a fast one-shot learning procedure; the network learns a state which can help it solve the classification problem, \u201cis q in S?\u201d. The slow-moving \u2018meta-learning\u2019 process is in the network parameters, which are slowly being optimized over several set membership tasks, i.e. several different sets S1:m, to be effective at one-shot classification. At test time, when we observe a new subset (or stream of elements) we can insert them with fwrite in one-shot and the resulting data-structure is the external memory, M.\n\n-- Re. \u201cA lot of details are relegated to the Appendix. For instance B.2 talks about the encoder architecture for one of the experiments.\u201d \n\nThis is a good point. We have removed B. 2 from the appendix and promoted the details to the model section. Furthermore we have given an example instantiation of the full architecture in the model section, so one does not need to consult the appendix. We have not completely removed the appendix as some details are tangential discussion points (e.g. how to implement the model in sub-linear time) but other details, such as space comparison, are now described in more detail in the experiments section.\n\nWe have significantly re-written the paper\u2019s model and experiments section to remedy this --- please take a look and let us know if this addresses concerns."}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "BylFBY5aa7", "original": null, "number": 4, "cdate": 1542461760940, "ddate": null, "tcdate": 1542461760940, "tmdate": 1542461760940, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HJlfcMvI3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Model motivation and applicability.", "comment": "Thank you for your review, and for your keen eye to detail.\n\nRe. \u201cwhy not develop further an LSTM\u201d. We are principally interested in whether it is possible to learn a compressive set membership data-structure in one-shot. Because many applications of Bloom Filters are in highly dynamic settings (e.g. databases), the requirement that a network may be able to beat a Bloom Filter with only a single computational pass over the data is quite important. It wasn\u2019t clear from the beginning of this research project (to us and our peers) whether it would be possible, and if so - in what setting. Thus we feel that it would be a worthwhile scientific contribution to show this is the case with any model --- even an LSTM. \n\nFirstly, it is worth noting the LSTM is non-trivially less efficient for the database task even when the sequence length is quite short. But the real issue with an LSTM and other RNNs (partially covered in the Reviewer 3 response) is that it they are difficult to scale to larger set sizes, because one has to BPTT over the entire input sequence linearly (elements of our storage size S) during training. Say S contains 5,000 elements\u2026 One would have to train with sequences of length 5,000, insert all elements sequentially and BPTT over the 5,000 long sequence. Training is way too slow, and the optimization problem becomes intractable (network fails to learn). Furthermore the LSTM has quadratic computation cost with respect to the hidden state size. Since set membership is order-invariant, it seemed preferable to try out a memory architecture which does not rely on sequential computation and BPTT (like a memory network) that is still very compressive (unlike a memory network).\n\nOur mistake in the original exposition, which you rightly point out, is that we have presented our solution (the architecture) without much of the motivation that lead to its incarnation. We have re-written the model section to remedy this. But we will also briefly state the model motivation here: \n\n- We want a simple write scheme and no BPTT -> additive write. (it\u2019s order-invariant, and alike to the Bloom Filter\u2019s logical-or write).\n\n- We want the network to choose where to write, as well as what to write -> address is a softmax over memory based on content. (alike to the Kanerva Machine Wu et al. (2018))\n\n- We want the network\u2019s network trainable parameters to be small and independent of memory size -> make the addressing matrix A non-trainable.\n\n- We want the addressing to be efficient -> make it sparse (alike to Rae et al. (2016)).\n\n- We found a sparse address led to the network fixating on a subset of memory -> whiten the query vector.\n\nWhitening (or sphering) may appear complex but was only necessary if one adopts the sparse attention for efficiency. We implemented it in four lines of TensorFlow code, so at least it is not too complex from an engineering standpoint. Whitening has been used within deep learning literature before, e.g. \u201cnatural neural networks\u201d [1] . An alternative to whitening would be to use a \u201cflow\u201d such as real NVP [2] which actually transforms the query to something which appears to be truly gaussian. Crucially, this was a trick to get sparse attention working, if one wishes to avoid sparse attention and just use the full softmax over memory then this side-detail of whitening can be ignored. \n\n-- Re. \u201cAlso, the neural bloom filters do well only when there is some sort of querying pattern. All of these details would seem to reduce the applicability of the proposed approach.\u201d \n\nFortunately the proposed approach does well if there is structure to the query pattern *or* storage set. In the case of the database task, our queries are picked uniformly from the universe --- there is not much structure. However there is structure to the storage sets (which represent row keys in an disk file within a database) and this is why our approach outperforms the classical data-structures so significantly. \n\nMore generally we think the research area of using neural networks to replace data-structures, in this case a bloom filter, is so exciting because (we would argue) they are very rarely applied to data that contains no structure. Using a neural network to exploit redundancy and save space feels like a very impactful thing to do, and thought leaders within Computer Science (e.g. Jeff Dean, a co-author of the kraska et al. 2018 paper) appear to believe so. There are patterns to the rowkey schema that is used within our databases, there are patterns to blacklist URLs and IPs within our firewalls, there are patterns to our search queries. \n\nWe have re-written the model and experiment section to address your concerns!\n\n[1] https://deepmind.com/research/publications/natural-neural-networks/\n[2] https://arxiv.org/abs/1605.08803 \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "BklHAWcapX", "original": null, "number": 3, "cdate": 1542459853367, "ddate": null, "tcdate": 1542459853367, "tmdate": 1542459853367, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "H1lg3Mjdn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "R3 response", "comment": "Thank you for this thoughtful and comprehensive review.\n\n-- We agree the recent Mitzenmacher arxiv posts should have been included in the related works, and they have now been added. \n\n-- Re. \u2018strong empirical case for NBF\u2026\u2019 The fact that an LSTM does well on the MNIST class-based familiarity task is a useful data-point. However we do see a substantial gain for the database task. However the main problem with RNNs such as the LSTM (and DNC) is that they are not scalable. need to be trained to store N items by ingesting the N elements sequentially, and then backpropagating over the entire sequence. For large N this does not end up being scalable; e.g. for the large database task (Table 1) where N = 5,000. Thus we develop a memory model that does not rely on BPTT (alike to memory networks) but is compressive (unlike memory networks). \n\nThe crucial design-point of the model is that it uses a commutative write operation (addition) which is much simpler than the DNC & LSTM write (e.g. no gating, no squashing of the state) and is like a continuous relaxation of the Bloom Filter\u2019s write (logical or). A simple additive write scheme also means the model will produce the same external memory M regardless of the ordering of the inputs (because addition is commutative) which makes sense given that familiarity does not depend upon input ordering, thus we also do not get strange effects where older inputs have much worse performance than newer inputs (which will occur with an RNN).  We discuss the model\u2019s motivation more explicitly in the revised text. \n\n-- Re. \u201cOne interesting thing to look at would be the workload partition between the learning component and the backup filter\u201d. This is a very interesting question you ask here. Your intuition is absolutely pretty well for class-based familiarity, the backup filter is used where the encoder essentially miss-classifies a character (so it is very lightly used). For uniform sampling, the model essentially captures a small random subset of inputs but mostly relies on the backup bloom filter. For the imbalanced data the model appears to store and characterise well the \u2018heavy hitter\u2019 i.e. frequent elements in the state memory and uses the backup bloom filter for infrequent elements. \n\n-- Re. \u2018problem setting is loosely sketched\u2026\u2019 - the reviewer is correct, we originally wrote the paper for readers familiar with the recent one-shot memory-augmented meta-learning literature (e.g. matching networks [vinyals et al. 2016], MANN [santoro et al. 2016]) but unfamiliar with Bloom Filters. This was an unfortunate choice, we have thus expanded on what we mean by meta-learning and described how the training regime works. It is the exact same training regime as that in vinyals et al. 2016 and many follow-on works, only the classification problem is set membership, versus image classification. We have added a subsection with further explanation and an algorithm box with a succinct summary of the meta-learning training setup.\n\nWe will  just briefly summarize the training setup here. We have a collection of sets {S_1, S_2, , \u2026 S_m} reserved for training (each set contains n points to insert); and a collection of queries Q = {q1, q2, \u2026, qL} and targets yi = 1 if qi in S and 0 otherwise. In the example of a database we can think of a given set Si = {k1, \u2026, kN} as a set of rowkeys for a given file on disk (e.g. SSTable). We have many sets because we have many files; for training we have reserved some for an offline training routine.\n\nDuring training we calculate M = f_write(S), and then we calculate oi = f_read(S, qi), our query responses having observed the set S only once. We calculate the cross-entropy loss L = \\sumi yi log(oi) + (1 - yi)log(1-oi) and backprogate through the network (through the parameters controlling both the read, write, and encoder networks). One can consider the creation of M = f_write(S) as a fast one-shot learning procedure; the network learns a state which can help it solve the classification problem, \u201cis q in S?\u201d in one-shot. The slow-moving \u2018meta-learning\u2019 process is in the network parameters, which are slowly being optimized over several set membership tasks, i.e. several different sets S_1:m, to be effective at one-shot classification. At test time, when we observe a new subset (or stream of elements) we can insert them with f_write in one-shot and the resulting data-structure is the external memory, M.\n\n-- Re. Bloom Filter space usage, we indeed used the analytical bound. We have clarified this in the text. We feel this is fair as it makes the task of beating a Bloom Filter\u2019s space performance slightly more difficult (as the analytic bound is slightly more compressive than in-practice), and it absolves any dispute over the choice of Bloom Filter library / choice of hash function etc.\n\n-- We have clarified the false positive rate is with respect to the distribution of queries in the text. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "HyeHtaK6pm", "original": null, "number": 2, "cdate": 1542458748872, "ddate": null, "tcdate": 1542458748872, "tmdate": 1542458748872, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "S1gCw_-T6X", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Re: \"How is this different to Kraska et al. 2018\"", "comment": "One concern that reviewer 2 and 3 raised that we would like to quickly address is, \u2018what is the point of this model vs kraska et al. 2018?\u2019 The simple answer is that kraska et al. 2018 learns a set membership classifier by training a feed-forward neural classifier from scratch over many (hundreds to thousands) epochs of the storage set S, and it is compressed into the weights of the network. We propose a method where a neural network learns to produce a classifier with a single pass over S, and the set is represented by an external memory M of compressed activations. \n\nIn the case of a banned URL list, where S may not change very much, it may be tenable to use the kraska et al. 2018 approach with multiple epochs of gradient descent. In the case of databases that uses Bloom Filters (e.g. Google Bigtable, Apache Cassandra, Redis) where one may have thousands of separate bloom filters (one per disk file, say) which are dynamically updating, it is impractical to train thousands of separate networks from scratch. Thus a one-shot approach (our paper) is absolutely necessary, and this paper serves as an existence proof that significant compression can be obtained in this challenging setting."}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "S1gCw_-T6X", "original": null, "number": 1, "cdate": 1542424677653, "ddate": null, "tcdate": 1542424677653, "tmdate": 1542424677653, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "content": {"title": "Substantial revision - thank you!", "comment": "Thank you for reading the paper and leaving your detailed feedback. The unified message from all three of you is that the paper could have done a better job in motivating the model, and describing the training regime. We have perhaps \u2018regularized\u2019 the paper\u2019s contents too heavily in the endeavor to be succinct. We provide an updated manuscript with additions to \u2018model\u2019, \u2018experiments\u2019, and \u2018related work\u2019 --- an extra page of text. The proposed architecture is now better explained and the training regime is much more explicit. It\u2019s a much better paper thanks to your comments.\n\nIf you think the problem setting has no potential for impact, or if you think there are fundamental flaws in our research approach then we would really appreciate feedback on this (and a rejection). Otherwise we would ask you to read the updated manuscript and update your response. We will also respond to each comment individually.\n\n----\n\nKey changes:\n\n- Re-written \u2018model\u2019 section with a much clearer motivation. Added more specific details (e.g. encoder architecture) to model section, less reliance on appendix.\n\n- Re-written experiments: explained meta-learning training in detail with algorithm box, explained why this is meta-learning / one-shot learning, added space comparison info, less reliance on appendix.\n\n- Added speed comparison benchmarks (some peers were interested in us adding these numbers). The summary of these numbers is the latency of the neural bloom filter is much higher than a bloom filter, but the throughput can be comparable if the model is run on a gpu.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621613923, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkekMnR5Ym", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1230/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1230/Authors|ICLR.cc/2019/Conference/Paper1230/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers", "ICLR.cc/2019/Conference/Paper1230/Authors", "ICLR.cc/2019/Conference/Paper1230/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621613923}}}, {"id": "BkxdWIvphQ", "original": null, "number": 3, "cdate": 1541400064013, "ddate": null, "tcdate": 1541400064013, "tmdate": 1541533311491, "tddate": null, "forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "invitation": "ICLR.cc/2019/Conference/-/Paper1230/Official_Review", "content": {"title": "Unclear paper, difficult to understand how the algorithm works or why", "review": "The paper proposes a method whereby a neural network is trained and used as a data structure to assess approximate set membership. Unlike the Bloom filter, which uses hand-constructed hash functions to store data and a pre-specified method for answering queries, the Neural Bloom Filter learns both the Write function and the Read function (both are \"soft\" values rather than the hard binary values used in the Bloom filter). Experiments show that, when there is structure in the data set, the Neural Bloom Filter can achieve the same false positive rate with less space.\n\nI had a hard time understanding how the model is trained. There is an encoding function, a write function, and a query function. The paper talks about one-shot meta-learning over a stream of data, but doesn't make it clear how those functions are learned. A lot of details are relegated to the Appendix. For instance B.2 talks about the encoder architecture for one of the experiments. But even that does not contain much detail, and it's not obvious how this is related to one-shot learning. Overall, the paper is written from the perspective of someone fully immersed in the details of the area, but who is unable to pop out of the details to explain to people who are not already familiar with the approach how it works. I would suggest rewriting to give an end-to-end picture of how it works, including details, without appendices. The approach sounds promising, but the exposition is not clear at all.", "rating": "3: Clear rejection", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Conference/Paper1230/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Meta-Learning Neural Bloom Filters", "abstract": "There has been a recent trend in training neural networks to replace data structures that have been crafted by hand, with an aim for faster execution, better accuracy, or greater compression.  In this setting, a neural data structure is instantiated by training a network over many epochs of its inputs until convergence. In many applications this expensive initialization is not practical, for example streaming algorithms --- where inputs are ephemeral and can only be inspected a small number of times.  In this paper we explore the learning of approximate set membership over a stream of data in one-shot via meta-learning. We propose a novel memory architecture, the Neural Bloom Filter, which we show to be more compressive than Bloom Filters and several existing memory-augmented neural networks in scenarios of skewed data or structured sets.", "keywords": ["meta-learning", "memory", "one-shot learning", "bloom filter", "set membership", "familiarity", "compression"], "authorids": ["jwrae@google.com", "bartunov@google.com", "countzero@google.com"], "authors": ["Jack W Rae", "Sergey Bartunov", "Timothy P Lillicrap"], "TL;DR": "We investigate the space efficiency of memory-augmented neural nets when learning set membership.", "pdf": "/pdf/27beb2b6d5ae8192a098883237183cb5bce6c91f.pdf", "paperhash": "rae|metalearning_neural_bloom_filters", "_bibtex": "@misc{\nrae2019metalearning,\ntitle={Meta-Learning Neural Bloom Filters},\nauthor={Jack W Rae and Sergey Bartunov and Timothy P Lillicrap},\nyear={2019},\nurl={https://openreview.net/forum?id=HkekMnR5Ym},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1230/Official_Review", "cdate": 1542234275571, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkekMnR5Ym", "replyto": "HkekMnR5Ym", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1230/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335901414, "tmdate": 1552335901414, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1230/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}