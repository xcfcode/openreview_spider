{"notes": [{"id": "BkNUFjR5KQ", "original": "SJexq5b5Ym", "number": 450, "cdate": 1538087806441, "ddate": null, "tcdate": 1538087806441, "tmdate": 1545355426017, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJgBsJ0mlN", "original": null, "number": 1, "cdate": 1544966045497, "ddate": null, "tcdate": 1544966045497, "tmdate": 1545354489792, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Meta_Review", "content": {"metareview": "This paper proposes a genetic algorithm to search neural network architectures with locally dense and globally sparse connections. A population-based genetic algorithm is used to find the sparse, connections between dense module units. The local dense but global sparse architecture is an interesting idea, yet is not well studied in the current version, e.g. overfitting and connections with other similar architecture search methods. Based on reviewers\u2019 ratings (5,5,6), the current version of paper is proposed as borderline lean reject.\n\n", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "A genetic algorithm to search locally-dense and globally-sparse neural network architectures. "}, "signatures": ["ICLR.cc/2019/Conference/Paper450/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper450/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353213228, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper450/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper450/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper450/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353213228}}}, {"id": "BJgK3rS80X", "original": null, "number": 7, "cdate": 1543030193386, "ddate": null, "tcdate": 1543030193386, "tmdate": 1543030193386, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "content": {"title": "Paper Updated", "comment": "Thanks to all reviewers, we take a lot of time on proofreading and update a new version. Detailed modifications are separately listed under each review.\nThank you very much."}, "signatures": ["ICLR.cc/2019/Conference/Paper450/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614406, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkNUFjR5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper450/Authors|ICLR.cc/2019/Conference/Paper450/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614406}}}, {"id": "HJlWjGHUA7", "original": null, "number": 6, "cdate": 1543029400830, "ddate": null, "tcdate": 1543029400830, "tmdate": 1543029400830, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "rklBnne02X", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "content": {"title": "Thank you for your thoughtful review", "comment": "We are very excited about the positive and enthusiastic support of our core idea. We totally agree with you that interpretability of deep neural network is of vital importance. Actually, our original motivation is exactly the brain\u2019s modularity. Human brain performs density by function cells. Thank you for point out this good paper https://www.nature.com/articles/nmeth.4627. We also think hierarchical structures might be a key point for complicated human brain functions.  We will concentrate on this topic and come up with more works in this area.\n \nWe agree with your second review comment that explores how to separate a whole network into several small networks with different functionalities could be a great work.  Due to the page limit, we only did an analysis of how specific connections will influence the final result in Section 4.4.  We think the paper \u201cUsing deep learning to model the hierarchical structure and function of a cell\u201d is pretty interesting; we will focus on sub-networks and functionality cells in our next paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper450/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614406, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkNUFjR5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper450/Authors|ICLR.cc/2019/Conference/Paper450/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614406}}}, {"id": "Hyl9rfr8Am", "original": null, "number": 5, "cdate": 1543029314190, "ddate": null, "tcdate": 1543029314190, "tmdate": 1543029314190, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "SygozEUAnX", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "content": {"title": "Detailed reply about novelty and improvement of our writing.", "comment": "We are very excited about the positive and enthusiastic support of our core idea. Thank you for your feedback about our strong part. We totally agree with you that our strong part is Section 4.4.\n \nAbout your main concerns:\nWe belive we have enough novelty for our work.\n \nPaper [2] claimed that they use a genetic algorithm for searching network structures. As I understand, their work mostly concentrates on searching skip connections of layers. As it is shown in Fig. 2 in [2], the optimization object is only connections between layers, however, strictly speaking, they didn\u2019t change the structure of the network.  Our focus is combining the locally dense and externally sparse property of the human brain into the neural network. Our optimization object is sparse connections between dense modules. In our paper, we figure out a method to achieve local density and global sparsity and demonstrate it with our solid experiments.  We have typical hierarchical structures, and our experiment figures out how different parts of the network will influence the final result. Yes, many papers could use genetic algorithms, but they all have their own contributions. Moreover, according to the experiment part in page 8 of [2], we acquire more solid experiment results.   As these two papers have different core ideas, we believe that our paper have enough novelty.\n \nPaper [3] focuses on minimizing human participation as much as possible. They search all parameters including learning rate, identity, reset weights, insert & remove convolutions.  We think paper [3] has the same motivation and idea as paper [2] that reduce human participation as much as possible. We think paper [3] is even better than paper [2] as they are in the same direction.\nOur motivation is different from these two papers.  Our focus is combining locally dense and globally sparse properties of network structures.  We do analysis about how different parts of the network or the different types of connections will influence the final performance in Section 4.4.\n \n \nPaper [4] has a similar idea of hierarchical structures as our paper. But our basic elements are modules which contain several dense layers. We notice that in their paper, evolving algorithm could form cliques in the end. We think it might have some interesting conclusions if they look into properties like density and which connections are important. We think searching network structure is a big topic. It worth many good papers on this topic. But all of them have different contributions.\nDifferent from their work, we focus on the implement of human-like locally dense but externally sparse structures in our paper. And we make a detailed analysis of how each long-distance connection will influence the final result.\n \nPaper [1] is a good NLP paper with special layers and searching strategy. This paper is also under the network search topic. But we focus on totally different aspects.\n \nThank you for mentioning some typos and grammar mistakes. We apologize for this. We spend a lot of time doing several rounds of proofreading and revising. We hope this version may make you feel better.\n \nIn all, although there are some papers having similar topics to our paper (network searching, hierarchical network structures, network pruning), we think a good topic worth many good papers to contribute to it. Also, we think we have enough novelty as present above. In that case, we think it worth to be accepted. Thank you very much.\n "}, "signatures": ["ICLR.cc/2019/Conference/Paper450/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614406, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkNUFjR5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper450/Authors|ICLR.cc/2019/Conference/Paper450/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614406}}}, {"id": "SyloJ8NLCm", "original": null, "number": 4, "cdate": 1543026147084, "ddate": null, "tcdate": 1543026147084, "tmdate": 1543026147084, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "BJg8FrE80m", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "content": {"title": "Detailed Reply about our paper 2", "comment": "Problem: \"More importantly, while some of step jumps in Figure 6~9 are suspicious, it turns out that all the step jumps happen at the same number of steps, which are identical to the change of learning rates described in Section 4.2. Thee clear explanation about that phenomena is required.\"\n\nAnswer: Yes, we think the step jumps are caused by the learning-rate change strategy. We originally thought this was a common phenomenon. So, we didn\u2019t discuss this (due to the page limit). But thanks to your advice,  we have explained this phenomenon in the revised paper, and updated it on the website.\n\nProblem: \"Even though the sparse connection is enforced for some reasons, overfitting, variance, or any other benefits that slim structure can bring in has not been evaluated. They need to be presented to verify the hypothesis that the authors claim.\"\n\nAnswer: Thank you very much for pointing this out! Yes, claiming that sparse connections could alleviate over-fitting does\u2019t seem to have experiment support as much as other claims. Yet, I think it\u2019s mostly the problem of presentation. \n\nOur focus is on how internal density and global sparsity could be implemented in deep neural networks and how these sparse connections will contribute to final performance such as Section 4.4.  Actually, we did some prototype experiments on comparing anti-over-fitting ability between ResNet and our network structures, and the results supported our hypothesis. But due to the page limit, we didn\u2019t put this part in the current paper. We agree that this part needs more strong support from experiments, so we just remove the claim that sparse connections could improve anti-over-fitting ability in conclusion and put it in future works. We think locally dense but globally sparse structure is a big topic; one paper may not be enough to explore all the aspects. We will do a solid analysis of its benefit in follow-up papers.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper450/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614406, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkNUFjR5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper450/Authors|ICLR.cc/2019/Conference/Paper450/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614406}}}, {"id": "BJg8FrE80m", "original": null, "number": 3, "cdate": 1543026046256, "ddate": null, "tcdate": 1543026046256, "tmdate": 1543026046256, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "B1liWFj7Tm", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "content": {"title": "Detailed Reply about our paper", "comment": "First of all, thank you very much for writing a detailed review of our paper!\nWe are excited with the positive and enthusiastic support of our core experiments.\n\nWe feel very sorry for the typos and grammar mistakes in our previous version. After a long time of proofreading and revising, we believe that the current version is much better. For instance, we have provided brief explanations with references of terms like \u2018growth rate\u2019 and \u2018population\u2019 in our current version as suggested by you.\n\nMoreover, we have added a clear explanation about step jumps in Figures 6~9 in our experiment section. Yes, these step jumps are caused by the change of learning rates. As we use same learning-rate change strategy in all experiments, the step jumps of all experiments happen at the same epochs. We originally thought it was a common phenomenon. Thanks to your advice, we have added an explanation of this phenomenon in our experiment section.  \n\nAs we have corrected all of the typos and errors that we can find, we believe that this paper is still worth being seen by more people.\n\nOur direction is not merely using genetic algorithm to search network structure, but also is about the internal dense and external sparse network structures.  How to combine the trend of being dense and being sparse is an interesting area. Moreover, the internal dense and external sparse structure also coincides with the modularity observed in human brain. As such, we believe this paper together with its research insights might be helpful to those who want to build hierarchical network structures. We really put a lot of work in this paper. It might not be perfect right now, but we think it\u2019s worth being seen by more people.\n\nProblem:  \n\n\"Please represent the blocks (e.g. 1*1conv) better. Current representation is quite confusing to read. Maybe proper spacing and different style of fonts may help\"\n\nAnswer:\n\nThank you very much for this suggestion! We have used a specific format and another font to improve the represents of blocks. For example on page 5. Section Long Distance Connections.  We\u2019ve corrected this part and updated the paper.  \n\nProblem:\n\n\"In Page 5, \"C_{m}ax\" is a typo. It should be \"C_{max}\u201d.\"\n\nAnswer:\n\nThank you very much for pointing out the typo. We\u2019ve corrected it and updated the paper accordingly.  \n\nProblem:\n\n \"Regarding the C_max, does sum(C_max) represent (D * W)^2 where D is the total depth and W is the total indicies in each layer? If so, specifying it will help. Otherwise, please explain its meaning clearly.\"\n\nAnswer:\n\nThe element of this matrix only denotes whether there is a connection or not, and sum (C_max) and sum (C_i) just denote the summation of all elements\u2019 values in the matrix.   \n\nFor example, if the element C_i[M_11,M_21] equals to 1, it means M_11 and M_21 are connected to each other.  Our original thought is that sum(C_i) denotes the number of the connections in C_i, so it\u2019s just simply the summation of all elements\u2019 value in the matrix. In this case, density D is between 0 and 1, where D_max=sum(C_max)/sum(C_max) reaches value 1.  Thank you for the advice, we\u2019ve illustrated it more clearly and updated the paper.\n\nProblem: \"In Figure 4(a), it would be better if we reuse M_{d,w} notation instead of Module {d_w}.\"\n\nAnswer: We\u2019ve correct this part and updated the paper for all similar images.\n\nProblem: \"Please briefly explain or provide references to the terms like \"growth rate\", \"population\", and \"individuals\u201d.\"\n\nAnswer: Thank you very much for the advice! We\u2019ve provided references to the concepts, \u2018growth rate\u2019, \u2018population\u2019 and \u2018individual\u2019 and updated the paper.\n\nGrowth rate k is a concept in the paper \"Densely Connected Convolutional Neural networks\". The l_{th} layer has k0 + k \u00d7 (l \u2212 1) input feature-maps according to paper [1], where k0 is the number of channels in the input layer. In that case growth rate k denotes how fast the feature maps will growth when the depth increases. Population and individual are concepts in genetic algorithm. Candidate solutions are called individuals or phenotypes. A population of individuals is called \u2018population\u2019.\n\nProblem: \"Different mutations may favor different hyper-parameters. How the authors control the hyperparameters other than the number of epochs will be useful to know.\"\n\nAnswer: We think different mutations may favor different hyper-parameters, too. But we didn\u2019t get a discipline of how different hyper-parameters will influence the model so far. We keep the hyper parameters the same during the whole experiment section, and the hyper-parameters are presented in first paragraph of Section 4.2. Our experiments show that weight decay 5*10^-4 is better than 1* 10^-4, so we suggested 5*10^-4 in our paper. "}, "signatures": ["ICLR.cc/2019/Conference/Paper450/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621614406, "tddate": null, "super": null, "final": null, "reply": {"forum": "BkNUFjR5KQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper450/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper450/Authors|ICLR.cc/2019/Conference/Paper450/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers", "ICLR.cc/2019/Conference/Paper450/Authors", "ICLR.cc/2019/Conference/Paper450/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621614406}}}, {"id": "B1liWFj7Tm", "original": null, "number": 3, "cdate": 1541810435206, "ddate": null, "tcdate": 1541810435206, "tmdate": 1541810435206, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Review", "content": {"title": "Interesting topic and insights, but requiring more improvements", "review": "The authors present the interesting and important direction in searching better network architectures using the genetic algorithm. Performance on the benchmark datasets seems solid. Moreover, the learned insights described in Section 4.4 would be very helpful for many researchers.\n\nHowever, the overall paper needs to be polished more. There are two many typos and errors that imply that the manuscript is not carefully polished. Explanations about some terms like growth rate, population, etc. are necessary for broader audience. \n\nMore importantly, while some of step jumps in Figure 6~9 are suspicious, it turns out that all the step jumps happen at the same number of steps, which are identical to the change of learning rates described in Section 4.2. Thee clear explanation about that phenomena is required.\n\n* Details\n- Please represent the blocks (e.g. 1*1conv) better. Current representation is quite confusing to read. Maybe proper spacing and different style of fonts may help.\n- In Page 5, \"C_{m}ax\" is a typo. It should be \"C_{max}\".\n- Regarding the C_max, does sum(C_max) represent (D * W)^2 where D is the total depth and W is the total indicies in each layer? If so, specifying it will help. Otherwise, please explain its meaning clearly.\n- In Figure 4(a), it would be better if we reuse M_{d,w} notation instead of Module {d_w}.\n- Please briefly explain or provide references to the terms like \"growth rate\", \"population\", and \"individuals\". \n- Different mutations may favor different hyper-parameters. How the authors control the hyperparameters other than the number of epochs will be useful to know.\n- Even though the sparse connection is enforced for some reasons, overfitting, variance, or any other benefits that slim structure can bring in has not been evaluated. They need to be presented to verify the hypothesis that the authors claim. \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper450/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Review", "cdate": 1542234458805, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335726892, "tmdate": 1552335726892, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SygozEUAnX", "original": null, "number": 2, "cdate": 1541461010976, "ddate": null, "tcdate": 1541461010976, "tmdate": 1541533985759, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Review", "content": {"title": "This paper proposes a neural network architectures with locally dense and globally sparse connections. Using dense units a population-based evolutionary algorithm is used to find the sparse connections between modules.", "review": "The problem is of increasing practical interest and importance. \n\nThe ablation study on the contribution and effects of each constituent  part is a strong part of the experiment section and the paper. \n\nOne major concern is about the novelty of the work. There are many similar works under the umbrella of Neural Architecture search who are trying to connect different building blocks (modules) to build larger CNNs. One example that explicitly makes sparse connections between them is [1]. Other examples of very similar works are [2,3,4].\n\nThe presentation of the paper can be improved a lot. In the current setup it\u2019s very similar to a collection of ideas and tricks and techniques combined together. \n\nThere are some typos and errors in the writing. A thorough grammatical  proofreading is necessary. \n\nIn conclusion there is a claim about tackling overfitting. It\u2019s not well supported or discussed in the experiments. \n\n[1] Shazeer, Noam, et al. \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer.\" arXiv preprint arXiv:1701.06538 (2017).\n[2] Xie, Lingxi, and Alan L. Yuille. \"Genetic CNN.\" ICCV. 2017.\n[3] Real, Esteban, et al. \"Large-scale evolution of image classifiers.\" arXiv preprint arXiv:1703.01041 (2017).\n[4] Liu, Hanxiao, et al. \"Hierarchical representations for efficient architecture search.\" arXiv preprint arXiv:1711.00436 (2017).\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper450/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Review", "cdate": 1542234458805, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335726892, "tmdate": 1552335726892, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkgXDnxCn7", "original": null, "number": 1, "cdate": 1541438554908, "ddate": null, "tcdate": 1541438554908, "tmdate": 1541533985547, "tddate": null, "forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "invitation": "ICLR.cc/2019/Conference/-/Paper450/Official_Review", "content": {"title": "LEARNING INTERNAL DENSE BUT EXTERNAL SPARSE STRUCTURES OF DEEP NEURAL NETWORK", "review": "The authors bridge two components (density of CNNs and sparsity structures) by proposing a new network structure with locally dense yet externally sparse connections. \n\n+ Combination of being dense and sparse is an interesting area.\n- Although experiment results demonstrate evolving sparse connection could reach competitive results, it would be interesting to show how separating a network into several small networks is useful, for example, interpretablity of deep neural network. There is an interesting work: \"Using deep learning to model the hierarchical structure and function of a cell\" https://www.nature.com/articles/nmeth.4627\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper450/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Internal Dense But External Sparse Structures of Deep Neural Network", "abstract": "Recent years have witnessed two seemingly opposite developments of deep convolutional neural networks (CNNs). On one hand, increasing the density of CNNs by adding cross-layer connections achieve higher accuracy. On the other hand, creating sparsity structures through regularization and pruning methods enjoys lower computational costs. In this paper, we bridge these two by proposing a new network structure with locally dense yet externally sparse connections. This new structure uses dense modules, as basic building blocks and then sparsely connects these modules via a novel algorithm during the training process. Experimental results demonstrate that the locally dense yet externally sparse structure could acquire competitive performance on benchmark tasks (CIFAR10, CIFAR100, and ImageNet) while keeping the network structure slim.", "keywords": ["Convolutional Neural Network", "Hierarchical Neural Architecture", "Structural Sparsity", "Evolving Algorithm"], "authorids": ["duanyiquncc@gmail.com"], "authors": ["Yiqun Duan"], "TL;DR": "In this paper, we explore an internal dense yet external sparse network structure of deep neural networks and analyze its key properties.", "pdf": "/pdf/34d98aef59177a53161263186d14c02468f6dcc9.pdf", "paperhash": "duan|learning_internal_dense_but_external_sparse_structures_of_deep_neural_network", "_bibtex": "@misc{\nduan2019learning,\ntitle={Learning Internal Dense But External Sparse Structures of Deep Neural Network},\nauthor={Yiqun Duan},\nyear={2019},\nurl={https://openreview.net/forum?id=BkNUFjR5KQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper450/Official_Review", "cdate": 1542234458805, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "BkNUFjR5KQ", "replyto": "BkNUFjR5KQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper450/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335726892, "tmdate": 1552335726892, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper450/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}