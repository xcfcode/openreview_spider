{"notes": [{"id": "r1zmVhCqKm", "original": "BygO_bA9tm", "number": 1438, "cdate": 1538087979413, "ddate": null, "tcdate": 1538087979413, "tmdate": 1545355393387, "tddate": null, "forum": "r1zmVhCqKm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Text Infilling", "abstract": "Recent years have seen remarkable progress of text generation in different contexts, including the most common setting of generating text from scratch, the increasingly popular paradigm of retrieval and editing, and others. Text infilling, which fills missing text portions of a sentence or paragraph, is also of numerous use in real life. Previous work has focused on restricted settings, by either assuming single word per missing portion, or limiting to single missing portion to the end of text. This paper studies the general task of text infilling, where the input text can have an arbitrary number of portions to be filled, each of which may require an arbitrary unknown number of tokens. \nWe develop a self-attention model with segment-aware position encoding for precise global context modeling.\nWe further create a variety of supervised data by masking out text in different domains with varying missing ratios and mask strategies. Extensive experiments show the proposed model performs significantly better than other methods, and generates meaningful text patches.", "keywords": ["text generation", "text infilling", "self attention", "sequence to sequence"], "authorids": ["zhuwr56@gmail.com", "zhitinghu@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wanrong Zhu", "Zhiting Hu", "Eric P. Xing"], "TL;DR": "We study a general task of text infilling that fills missing portions of given text; an self-attention model is developed.", "pdf": "/pdf/734c0abdadc7d3ec99c79257c376f6dbac0c65ae.pdf", "paperhash": "zhu|text_infilling", "_bibtex": "@misc{\nzhu2019text,\ntitle={Text Infilling},\nauthor={Wanrong Zhu and Zhiting Hu and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1zmVhCqKm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Sylig4qxl4", "original": null, "number": 1, "cdate": 1544754163422, "ddate": null, "tcdate": 1544754163422, "tmdate": 1545354517855, "tddate": null, "forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1438/Meta_Review", "content": {"metareview": "although the problem of text infilling itself is interesting, all the reviewers were not certain about the extent of experiments and how they shed light on whether, how and why the proposed approach is better than existing approaches. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "rejection"}, "signatures": ["ICLR.cc/2019/Conference/Paper1438/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1438/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Infilling", "abstract": "Recent years have seen remarkable progress of text generation in different contexts, including the most common setting of generating text from scratch, the increasingly popular paradigm of retrieval and editing, and others. Text infilling, which fills missing text portions of a sentence or paragraph, is also of numerous use in real life. Previous work has focused on restricted settings, by either assuming single word per missing portion, or limiting to single missing portion to the end of text. This paper studies the general task of text infilling, where the input text can have an arbitrary number of portions to be filled, each of which may require an arbitrary unknown number of tokens. \nWe develop a self-attention model with segment-aware position encoding for precise global context modeling.\nWe further create a variety of supervised data by masking out text in different domains with varying missing ratios and mask strategies. Extensive experiments show the proposed model performs significantly better than other methods, and generates meaningful text patches.", "keywords": ["text generation", "text infilling", "self attention", "sequence to sequence"], "authorids": ["zhuwr56@gmail.com", "zhitinghu@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wanrong Zhu", "Zhiting Hu", "Eric P. Xing"], "TL;DR": "We study a general task of text infilling that fills missing portions of given text; an self-attention model is developed.", "pdf": "/pdf/734c0abdadc7d3ec99c79257c376f6dbac0c65ae.pdf", "paperhash": "zhu|text_infilling", "_bibtex": "@misc{\nzhu2019text,\ntitle={Text Infilling},\nauthor={Wanrong Zhu and Zhiting Hu and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1zmVhCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1438/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352839099, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1438/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1438/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1438/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352839099}}}, {"id": "ryxzxgQc37", "original": null, "number": 3, "cdate": 1541185513887, "ddate": null, "tcdate": 1541185513887, "tmdate": 1541533132366, "tddate": null, "forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1438/Official_Review", "content": {"title": "The authors present an interesting new conditional generation task, but the paper lacks critical implementation details and the experimental setting is too restricted to fully support the claims.", "review": "The paper proposes a setting for evaluation of a text infilling task, where a system needs to fill in the blanks in a provided incomplete sentences. The authors select sentences from three different sources, Yahoo Reviews, fairy tales, and NBA scripts, and blank out words with varying strategies, ranging from taking out prepositions and articles to removing all but two anchor words from a sentence. On this data, they compare the performances of a GAN model, Recurrent Seq2seq model with attention, and Transformer model in terms of BLEU, perplexity, and human evaluation.\n\nThe setting is certainly interesting, and the various data creation strategies are reasonable, but the paper suffers from two main flaws. First, the size of the data set is far from sufficient. Unless the authors are trying to show that the transformer is more data-efficient (which is doubtful), the dataset needs to be much larger than the 1M token it appears to be now. The size of the vocabularies is also far from being representative of any real world setting.\n\nMore important however is the fact that the authors fail to describe there baseline systems in any details. What are the discriminator and generator used in the GAN? What kind of RNN is used in Seq2seq? What size? Why not use a transformer seq2seq? How exactly is the data fed in / how does the model know which blank it's generating? It would be absolutely impossible for anyone to reproduce the results presented in the paper.\n\nThere are some other problems with the presentation, including the fact that contrary to what is suggested in the introduction, the model seems to have access to the ground truth size of the blank (since positional encodings are given), making it all but useless in a real world application setting, but it is really difficult to evaluate the proposed task and the authors' conclusions without a much more detailed description of the experimental setting.", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1438/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Infilling", "abstract": "Recent years have seen remarkable progress of text generation in different contexts, including the most common setting of generating text from scratch, the increasingly popular paradigm of retrieval and editing, and others. Text infilling, which fills missing text portions of a sentence or paragraph, is also of numerous use in real life. Previous work has focused on restricted settings, by either assuming single word per missing portion, or limiting to single missing portion to the end of text. This paper studies the general task of text infilling, where the input text can have an arbitrary number of portions to be filled, each of which may require an arbitrary unknown number of tokens. \nWe develop a self-attention model with segment-aware position encoding for precise global context modeling.\nWe further create a variety of supervised data by masking out text in different domains with varying missing ratios and mask strategies. Extensive experiments show the proposed model performs significantly better than other methods, and generates meaningful text patches.", "keywords": ["text generation", "text infilling", "self attention", "sequence to sequence"], "authorids": ["zhuwr56@gmail.com", "zhitinghu@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wanrong Zhu", "Zhiting Hu", "Eric P. Xing"], "TL;DR": "We study a general task of text infilling that fills missing portions of given text; an self-attention model is developed.", "pdf": "/pdf/734c0abdadc7d3ec99c79257c376f6dbac0c65ae.pdf", "paperhash": "zhu|text_infilling", "_bibtex": "@misc{\nzhu2019text,\ntitle={Text Infilling},\nauthor={Wanrong Zhu and Zhiting Hu and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1zmVhCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1438/Official_Review", "cdate": 1542234229594, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1438/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335946974, "tmdate": 1552335946974, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1438/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HygOvpY_2X", "original": null, "number": 2, "cdate": 1541082464454, "ddate": null, "tcdate": 1541082464454, "tmdate": 1541533132161, "tddate": null, "forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1438/Official_Review", "content": {"title": "questions about experiments", "review": "Summary\nThis paper proposes an approach to fill in gaps in sentences. While usually generation assumes a provided left context, this paper generalizes generation to any missing gap of any number of words.\nThe method is based on Transformer which does attention on all the available context and generates in a left to right manner.\nThe authors validate their approach on rather small scale datasets. I am unclear about several details of their empirical validation.\n\nRelevance: this work is on text generation, which is very relevant to this conference.\n\nClarity: the description of the method is very clear, less so the description of the empirical validation.\n\nNovelty: The method per se is not novel, but the combination of method and application is.\n\nEmpirical validation: The empirical validation is not very clear and potentially weak.\nFirst, I did not understand the baselines the authors considered: seq2seq and GAN. There are LOTS of variants of these methods and the references cited by the authors do not immediately apply to filling-in tasks. The authors should explain in detail what these baselines are and how they work. \nSecond, MaskGAN ICLR 2018 was proposed to solve the filling task and code is publicly available by the authors of that paper. A direct comparison to MaskGAN seems a must.\nThird, there are lots of details that are unclear, such as : how are segments defined in practice? the ranking metric is not clearly defined (pair-wise ranking or ranking of N possible completion?). Could the authors be a little bit more formal?\nSpeaking of metrics, why don't the authors considered precision at K for the evaluation of small gaps?\nFourth, the datasets considered, in particular the Grimm's dataset, is very small. There are only 16K training sentences and the vocabulary size is only 7K. How big is the model? How comes it does not overfit to such a small dataset? Did the authors measure overlap between training and test sets? \n\nMore general comments\nThe beauty of the proposed approach is its simplicity. However, the proposed model feels not satisfactory as generation proceeds left to right, while the rightmost and the leftmost missing word in the gap should be treated as equal citizens.\n\nMinor note: positional embeddings are useful also with convolutional models. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1438/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Infilling", "abstract": "Recent years have seen remarkable progress of text generation in different contexts, including the most common setting of generating text from scratch, the increasingly popular paradigm of retrieval and editing, and others. Text infilling, which fills missing text portions of a sentence or paragraph, is also of numerous use in real life. Previous work has focused on restricted settings, by either assuming single word per missing portion, or limiting to single missing portion to the end of text. This paper studies the general task of text infilling, where the input text can have an arbitrary number of portions to be filled, each of which may require an arbitrary unknown number of tokens. \nWe develop a self-attention model with segment-aware position encoding for precise global context modeling.\nWe further create a variety of supervised data by masking out text in different domains with varying missing ratios and mask strategies. Extensive experiments show the proposed model performs significantly better than other methods, and generates meaningful text patches.", "keywords": ["text generation", "text infilling", "self attention", "sequence to sequence"], "authorids": ["zhuwr56@gmail.com", "zhitinghu@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wanrong Zhu", "Zhiting Hu", "Eric P. Xing"], "TL;DR": "We study a general task of text infilling that fills missing portions of given text; an self-attention model is developed.", "pdf": "/pdf/734c0abdadc7d3ec99c79257c376f6dbac0c65ae.pdf", "paperhash": "zhu|text_infilling", "_bibtex": "@misc{\nzhu2019text,\ntitle={Text Infilling},\nauthor={Wanrong Zhu and Zhiting Hu and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1zmVhCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1438/Official_Review", "cdate": 1542234229594, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1438/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335946974, "tmdate": 1552335946974, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1438/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SklOXypTjm", "original": null, "number": 1, "cdate": 1540374304471, "ddate": null, "tcdate": 1540374304471, "tmdate": 1541533131949, "tddate": null, "forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "invitation": "ICLR.cc/2019/Conference/-/Paper1438/Official_Review", "content": {"title": "Lack of novelty", "review": "Pros:\nThis paper targets an interesting and important task, i.e. general text filling, where the incomplete sentence can have arbitrary number of blanks and each blank can have arbitrary number of missing words.  Convincing experiments are conducted both qualitatively and quantitatively.\n\nCons:\n1. Lack of novelty in the proposed method. Specifically, such impression mainly comes from the writing in Sec 3.2. When discussing details of the proposed method, authors keep referring to [A], indicating heavily that authors are applying an existing method, i.e. yet another application of [A].  This limits the novelty. On the other hand, this also limits the readability for anyone that not familiar with [A]. Moreover, this prevents authors discuss the motivation behinds their architectural choices. Whether [A] is the optimal choice for this task, and can there be alternative options for its components. For example,  could we use a rnn to encode x_template_i and use the encoding as a condition to fill  s_i? \n\n[A] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in Neural Information Processing Systems. 2017.\n\n2. Discussion in Sec 3.2 and Figure 2 are not clear enough. I didn't get a picture of how the proposed method interacts with requirements of the task. For example, in sec 3.2.3, what represent queries, keys and values respectively are unclear. And authors mention \"template-decoder attention\" layers, where  I didn't find in Figure 2.\n\n3. Is not very straightforward that how the baselines Seq2Seq and GAN are applied to this task, where necessary information is missed in the experiment section.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1438/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Text Infilling", "abstract": "Recent years have seen remarkable progress of text generation in different contexts, including the most common setting of generating text from scratch, the increasingly popular paradigm of retrieval and editing, and others. Text infilling, which fills missing text portions of a sentence or paragraph, is also of numerous use in real life. Previous work has focused on restricted settings, by either assuming single word per missing portion, or limiting to single missing portion to the end of text. This paper studies the general task of text infilling, where the input text can have an arbitrary number of portions to be filled, each of which may require an arbitrary unknown number of tokens. \nWe develop a self-attention model with segment-aware position encoding for precise global context modeling.\nWe further create a variety of supervised data by masking out text in different domains with varying missing ratios and mask strategies. Extensive experiments show the proposed model performs significantly better than other methods, and generates meaningful text patches.", "keywords": ["text generation", "text infilling", "self attention", "sequence to sequence"], "authorids": ["zhuwr56@gmail.com", "zhitinghu@gmail.com", "epxing@cs.cmu.edu"], "authors": ["Wanrong Zhu", "Zhiting Hu", "Eric P. Xing"], "TL;DR": "We study a general task of text infilling that fills missing portions of given text; an self-attention model is developed.", "pdf": "/pdf/734c0abdadc7d3ec99c79257c376f6dbac0c65ae.pdf", "paperhash": "zhu|text_infilling", "_bibtex": "@misc{\nzhu2019text,\ntitle={Text Infilling},\nauthor={Wanrong Zhu and Zhiting Hu and Eric P. Xing},\nyear={2019},\nurl={https://openreview.net/forum?id=r1zmVhCqKm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1438/Official_Review", "cdate": 1542234229594, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1zmVhCqKm", "replyto": "r1zmVhCqKm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1438/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335946974, "tmdate": 1552335946974, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1438/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 5}