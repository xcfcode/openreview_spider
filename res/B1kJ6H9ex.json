{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1491578465214, "tcdate": 1478282455161, "number": 271, "id": "B1kJ6H9ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1kJ6H9ex", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "content": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 34, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396473346, "tcdate": 1486396473346, "number": 1, "id": "Hk-r3GUug", "invitation": "ICLR.cc/2017/conference/-/paper271/acceptance", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Reviewers agree that this a high-quality and interesting paper exploring important connections between widely used RL algorithms. It has the potential to be an impactful paper, with the most positive comment noting that it \"will likely influence a broad swath of RL\". \n \n Pros:\n - The main concepts of the paper came through clearly, and all reviewers felt the idea was interesting and novel.\n - The empirical part of the paper was convincing and \"empirical section is very well explained\" \n \n Cons:\n - There and some concerns about the writing and notation. None of these were too critical though, and the authors responded \n - Reviewers asked for some more comparisons with alternative formulations.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396473876, "id": "ICLR.cc/2017/conference/-/paper271/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396473876}}}, {"tddate": null, "tmdate": 1484686421698, "tcdate": 1484686421698, "number": 19, "id": "rk0INZ3Ug", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "H1BxzUzVl", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "Interesting links between policy-based and value-based methods", "comment": "Thankyou for your detailed and insightful comments.                             \n                                                                                \n- \\tilde Q is always to be interpreted as an estimate of the Q-values, eq. 5    \n  holds exactly because it is how we define \\tilde Q, as a function of the      \n  policy. We are basically saying we are going to use the policy as an estimate \n  of the Q-values.                                                              \n- We feel section 3.2 is important to bridge the gap between the tabular        \n  case, where we can show that \\tilde Q is equal to Q exactly at the fixed      \n  point, to the general case where we can't show that but at least we can say   \n  that the error between the two is minimized in an l2 sense, so that \\tilde Q  \n  is a valid estimate of the Q-values still.                                    \n- We have made the connections and differences between dueling and our method   \n  more explicit in the prior work section. Yes, the Q-learning variant we use in\n  the examples is very similar to dueling, we made that more explicit. We added \n  the interpretation of PGQ as a combination of expected SARSA and Q-learning to\n  sect 4.1.                                                                     \n- We have made an effort to make sect 3.3 clearer.                              \n                                                                                \n                                                                                \nMinor:                                                                          \n- Updated the expectation to depend on r.                                       \n- Clarified the definition of the Boltzmann policy.                               \n- Stated in sect 4.3 that it is only for the tabular case.                             \n- The grid world example has been updated (there was a bug where the learning   \n  rates were too small originally) and they now converge to the same policy,    \n  in the original draft they all converged to same policy performance given     \n  enough time (roughly double the iterations was sufficient).                   \n                                                                                \nTypos:                                                                          \nAll fixed! Thankyou!                                                            \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1484686336630, "tcdate": 1484686336630, "number": 18, "id": "SkY-VW2Ix", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "ry3QEnzVx", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "Review", "comment": "Thankyou for your comments and suggestions.                                                     \n                                                                                \n- We have made more explicit the differences and similarities between our method\n  and the dueling architecture in the prior work section. You are correct that  \n  policy gradient methods can be interpreted as a generalization of the dueling \n  architecture combined with a SARSA style update.                              \n                                                                                \n- In that equation we're saying to consider the euclidean projection problem of \n  the log-\\pi s onto a set of values under some arbitrary measure, the KKT      \n  conditions of that problem is the next equation (along with the \\pi s being   \n  probability measures). If we compare that equation to eq. 3 we see that they  \n  match up if the measure is the same and q = Q, so we can interpret eq. 3 as a \n  projection. This is not the same as solving the optimization problem in eq. 7 \n  with the measure \\pi and Q^\\pi, since they are both  \n  functions of \\pi and thus we would need to consider how changing \\pi affects  \n  them which would change the KKT conditions. However we are not   \n  claiming to solve that problem, which is more complicated than a simple       \n  projection.                                                           \n                                                                                \n- The Q-learning example we compare to in the results is using the same network \n  and parameterizing the architecture as in eq. 10, and so provides a point of\n  comparison between PGQ and the closest pure Q-learning method, that naturally\n  includes dueling.          \n                                                                                \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1484686105420, "tcdate": 1484686105420, "number": 17, "id": "B1WX7Z28g", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "Hk2oimlSe", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "An excellent paper pointing out connections between existing algorithms, which also leads to new algorithms with excellent results", "comment": "Thankyou for your comments. You are totally correct that the use of the stationary (i.e. non-discounted) policy was glossed over originally. We have updated the paper to make the distinction more explicit.                                \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1484686030361, "tcdate": 1484686030361, "number": 16, "id": "H18Az-28e", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "Thankyou to the reviewers", "comment": "Thankyou to the three reviewers, and apologies for the delay in responding. We have updated the paper in response to your comments. Further responses below."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1482861641961, "tcdate": 1482861641961, "number": 2, "id": "BkzL3Xxre", "invitation": "ICLR.cc/2017/conference/-/paper271/pre-review/question", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer4"], "content": {"title": "No questions", "question": "No questions"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482861642601, "id": "ICLR.cc/2017/conference/-/paper271/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper271/AnonReviewer1", "ICLR.cc/2017/conference/paper271/AnonReviewer4"], "reply": {"forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482861642601}}}, {"tddate": null, "tmdate": 1482861475737, "tcdate": 1482861475737, "number": 3, "id": "Hk2oimlSe", "invitation": "ICLR.cc/2017/conference/-/paper271/official/review", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer4"], "content": {"title": "An excellent paper pointing out connections between existing algorithms, which also leads to new algorithms with excellent results", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This is a very nicely written paper which unifies some value-based and policy-based (regularized policy gradient) methods, by pointing out connections between the value function and policy which have not been established before. The theoretical results are new and insightful, and will likely be useful in the RL field much beyond the specific algorithm being proposed in the paper. This being said, the paper does exploit the theory to produce a unified version of Q-learning and policy gradient, which proves to work on par or better than the state-of-art algorithms on the Atari suite. The empirical section is very well explained in terms of what optimization were done.\nOne minor comment I had was related to the stationary distribution used for a policy - there are subtleties here between using a discounted vs non-discounted distribution which are not crucial in the tabular case, but will need to be addressed in the long run in the function approximation case. This being said, there is no major problem for the current version of the paper. \nOverall, the paper is definitely worthy of acceptance, and will likely influence a broad swath of RL, as it opens the door to further theoretical results as well as algorithm development.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482861476651, "id": "ICLR.cc/2017/conference/-/paper271/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper271/AnonReviewer1", "ICLR.cc/2017/conference/paper271/AnonReviewer3", "ICLR.cc/2017/conference/paper271/AnonReviewer4"], "reply": {"forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482861476651}}}, {"tddate": null, "tmdate": 1481978916491, "tcdate": 1481978916491, "number": 2, "id": "ry3QEnzVx", "invitation": "ICLR.cc/2017/conference/-/paper271/official/review", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer3"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "Nice paper, exploring the connection between value-based methods and policy gradients, formalizing the relation between the softmax-like policy induced by the Q-values and a regularized form of PG.  \n\nPresentation: \nAlthough that seems to be the flow in the first part of the paper, I think it could be cast as a extension/ generalization of the dueling Q-network \u2013 for me that would be a more intuitive exposition of the new algorithm and findings. \n\nSmall concern in general case derivation: \nSection 3.2: Eq. (7) the expectation (s,a) is wrt to \\pi, which is a function of \\theta -- that dependency seems to be ignored, although it is key to the PG update derivation. If these policies(the sampling policy for the expectation and \\pi) are close enough it's usually okay -- but except for particular cases (trust-region methods & co), that's generally not true. Thus, you might end up solving a very different problem than the one you actually care solving.\n\nResults:\nA comparison with the dueling architecture could be added as that would be the closest method (it would be nice to see if and in which game you get an improvement)\n\nOverall: strong paper, good theoretical insights. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482861476651, "id": "ICLR.cc/2017/conference/-/paper271/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper271/AnonReviewer1", "ICLR.cc/2017/conference/paper271/AnonReviewer3", "ICLR.cc/2017/conference/paper271/AnonReviewer4"], "reply": {"forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482861476651}}}, {"tddate": null, "tmdate": 1481953773328, "tcdate": 1481953773328, "number": 1, "id": "H1BxzUzVl", "invitation": "ICLR.cc/2017/conference/-/paper271/official/review", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "Interesting links between policy-based and value-based methods", "rating": "7: Good paper, accept", "review": "This paper shows how policy gradient and Q-Learning may be combined together, improving learning as demonstrated in particular in the Atari Learning Environment. The core idea is to note that entropy-regularized policy gradient leads to a Boltzmann policy based on Q values, thus linking pi & Q together and allowing both policy gradient and Q-Learning updates to be applied.\n\nI think this is a very interesting paper, not just for its results and the proposed algorithm (dubbed PGQ), but mostly because of the links it draws between several techniques, which I found quite insightful.\n\nThat being said, I also believe it could have done a better job at clearly exposing these links: I found it somewhat difficult to follow, and it took me a while to wrap my head around it, even though the underlying concepts are not that complex. In particular:\n- The notation \\tilde{Q}^pi is introduced in a way that is not very clear, as \"an estimate of the Q-values\" while eq. 5 is an exact equality (no estimation)\n- It is not clear to me what section 3.2 is bringing exactly, I wonder if it could just be removed to expand some other sections with more explanations.\n- The links to dueling networks (Wang et al, 2016) are in my opinion not explicit and detailed enough (in 3.3 & 4.1): as far as I can tell the proposed architecture ends up being very similar to such networks and thus it would be worth telling more about it (also in experiments my understanding is that the \"variant of asynchronous deep Q-learning\" being used is essentially such a dueling network, but it is not clearly stated). I also believe it should be mentioned that PGQ can also be seen as combining Q-Learning with n-step expected Sarsa using a dueling network: this kind of example helps better understand the links between methods\n- Overall I wish section 3.3 was clearer, as it draws some very interesting links, but it is hard to see where this is all going when reading the paper for the first time. One confusing point is w.r.t. to the relationship with section 3.2, that assumes a critic outputting Q values while in 3.3 the critic outputs V. The \"mu\" distribution also comes somewhat out of nowhere.\n\nI hope the authors can try and improve the readability of the paper in a final version, as well as clarify the points raised in pre-review questions (in particular related to experimental details, the derivation of eq. 4, and the issue of the discounted distribution of states).\n\nMinor remarks:\n- The function r(s, a) used in the Bellman equation in section 2 is not formally defined. It looks a bit weird because the expectation is on s' and b' but r(s, a) does not depend on them (so either it should be moved out of the expectation, or the expectation should also be over the reward, depending on how r is defined)\n- The definition of the Boltzmann policy at end of 2.1 is a bit confusing since there is a sum over \"a\" of a quantity that does not depend (clearly) on \"a\"\n- I believe 4.3 is for the tabular case but this is not clearly stated\n- Any idea why in Fig. 1 the 3 algorithms do not all converge to the same policy? In such a simple toy setting I would expect it to be the case.\n\nTypos:\n- \"we refer to the classic text Sutton & Barto (1998)\" => missing \"by\"?\n- \"Online policy gradient typically require an estimate of the action-values function\" => requires & value\n- \"the agent generates experience from interacting the environment\" => with the environment\n- in eq. 12 (first line) there is a comma to remove near the end, just before the dlog pi\n- \"allowing us the spread the influence of a reward\" => to spread\n- \"in the off-policy case tabular case\" => remove the first case\n- \"The green line is Q-learning where at the step an update is performed\" => at each step\n- In Fig. 2 it says A2C instead of A3C\n\nNB: I did not have time to carefully read Appendix A", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482861476651, "id": "ICLR.cc/2017/conference/-/paper271/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper271/AnonReviewer1", "ICLR.cc/2017/conference/paper271/AnonReviewer3", "ICLR.cc/2017/conference/paper271/AnonReviewer4"], "reply": {"forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482861476651}}}, {"tddate": null, "tmdate": 1481823737935, "tcdate": 1481823737927, "number": 15, "id": "rkfWLUeEl", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "HkAmxZp7l", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "Discounted weighting of states?", "comment": "Very good point, I had not considered that. I will have to fix it in the paper, thanks for bringing it to our attention."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481605305027, "tcdate": 1481605305022, "number": 10, "id": "rk-Te-aXg", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "ByLiWic7e", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "A few more questions", "comment": "Thanks, that makes sense."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1481605157661, "tcdate": 1481605157653, "number": 9, "id": "HkAmxZp7l", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "BJQm4i9Ql", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "Discounted weighting of states?", "comment": "Yes it seems to me that a correction may be needed to be technically correct. Intuitively, if better estimating Q* at a state very far from the start state comes at the expense of a worse estimation early on, it may not be worth doing it.\nI just had a look at the classical RL book 2nd edition (https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf) and eq. 13.6 in section 3.3, the classical REINFORCE policy gradient, indeed has a weighting by gamma^t.\n\nThat being said, that's in order to maximize the expected discounted return from the start, and in the end here the agent is evaluated using the total game score, so trying to apply such a correction could very well lead to poorer performance.\n\nBut I think it may be worth checking exactly what this implies w.r.t. the links between the various methods discussed in this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1481450522830, "tcdate": 1481450522823, "number": 14, "id": "BJQm4i9Ql", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "ByPmaZc7g", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "Discounted weighting of states?", "comment": "Yes, that should be made clear in the paper, I will fix that. In practice the online algorithm just 'samples' from the distribution of states encountered under the policy, and applies SGD. Are you suggesting that some correction should be applied? The Q-learning update is from a replay buffer, so the measure is definitely 'wrong' in that case. The value of gamma is the same as the A3C paper, which is 0.99."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481449886379, "tcdate": 1481449886370, "number": 13, "id": "ByLiWic7e", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "Sy3n-CK7g", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "A few more questions", "comment": "1. I think that reducing alpha could definitely improve performance in testing, we haven't tried that yet though since we wanted to be as close as possible to the A3C paper, which doesn't change alpha in the testing phase.\n\nI'm not sure why people don't train a using a weighting averaging of the target Q-values, possibly it's just simpler to do the easy thing and that works well enough in practice. There are counter-examples (which I'm sure you're aware of, but just posting this here in case others are interested) where training a greedy policy but testing with some noise can arbitrarily hurt performance, e.g. the cliff walking example here: https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html. Your suggestion would alleviate that, so it's something to think about in the future. \n\n\n4. Yes, that's basically my understanding too. The only slight difference is that in the original dueling paper the measure that normalizes the quasi-advantage function is fixed, where in this paper (and policy gradient in general) it is always equal to the policy and so changes at each iteration (i.e. closer to the real advantage function)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481411870942, "tcdate": 1481411870930, "number": 8, "id": "ByPmaZc7g", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "Discounted weighting of states?", "comment": "When initially reading the paper I assumed you just forgot to mention in section 2 that the distribution d^pi used a discounted weighted of states, for the policy gradient theorem from Sutton et al (1999) to hold (see above eq. 2 in that paper), however it seems like no such discounting is being applied when actually doing the updates in practice. Obviously in the tabular case it does not matter, but could it be a problem with DQNs? (which value of gamma is being used by the way?)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1481407438129, "tcdate": 1481396659625, "number": 7, "id": "Sy3n-CK7g", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "SyuCMUK7e", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "A few more questions", "comment": "1. Ok thanks. I was just re-reading the DDQN paper and I noticed that for some of their results (the \"tuned version\") they decrease their epsilon (as in epsilon-greedy) a lot during evaluation, so it looks like you might also benefit from decreasing alpha (in particular since I guess Q values may have different scales between games, so it could help to tune the alpha for each game). In addition, and this may be a bit off-topic, it makes me wonder why people use the Q-Learning target y = r + gamma max_a Q(s', a; theta-) if they are not going to use the greedy policy during evaluation... I would have expected something more like (expected-)SARSA, which I would write (taking inspiration from DDQN as well): y = r + gamma E_{a ~ pi_theta(s')}[Q(s', a; theta-)], where pi_theta(s') is the policy to be used during evaluation; or possibly replacing the expectation by a sample. I'm curious if you have any idea why this is not being done in DQN papers (even without the DDQN flavor, i.e. y = r + gamma E_{a ~ pi_theta-(s')}[Q(s', a; theta-]).\n\n2 & 3: thanks for the clarification.\n\n4. One (hopefully) last question: am I right that your approach can also be viewed as a dueling Q-network (Wang et al, 2016) trained by combining the updates of (off-policy) Q-Learning and (on-policy) n-step expected SARSA? (with a Boltzmann exploration policy controlled by alpha)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1481391602018, "tcdate": 1481364618891, "number": 12, "id": "HyXcV8t7l", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "rkepRFu7g", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "Page 6, last paragraph (a small doubt)", "comment": "All we are saying there is that for a small (non-zero) choice of alpha there is a fixed point of the entropy regularized policy gradient update, and that fixed point policy induces Q-values with a 'small' Bellman error (small in the sense that it converges to zero as alpha converges to zero). If the entropy penalty is equal to zero, then the set of fixed points is the set of optimal policies (which are unbounded in log-space and so might not technically be considered fixed-points), for which we have the optimal Q-values and therefore zero Bellman error."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481364226403, "tcdate": 1481364226397, "number": 11, "id": "Sy5WQ8K7g", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "S15GTXPme", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "From eq.3 to eq.4", "comment": "Exactly, thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481364176358, "tcdate": 1481364176353, "number": 10, "id": "SyuCMUK7e", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "BJZ-L1Pmg", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "A few more questions", "comment": "1. Yes, alpha is fixed throughout and used in testing as well. We did not try the greedy policy, but it is common to use a stochastic policy in testing to prevent the agent getting trapped in a cycle of actions (the same is done in the DQN atari paper).\n2. In the grid world the critic is the 1-step TD-error from a learned value function. In Atari it is the n-step return with a bootstrap, exactly as in A3C.\n3. We are freezing the target network for around 5000 steps, but not doing clipping, no particular reason other than that was the simplest to implement."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481323746383, "tcdate": 1481313975953, "number": 9, "id": "rkepRFu7g", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Page 6, last paragraph (a small doubt)", "comment": "The authors write at the page 6, last paragraph \"Under standard policy gradient the bellman residual will be small, then it follows that adding a term that reduces that error should not make much difference at the fixed point.\" \n\nThis point is not clear. Let's consider the standard policy gradient without the entropy-bonus. In this case, I was wondering if there is a way to determine the fixed-policy? I did the same maths inspired by the paper but I did not reach to any concluding result."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481223573705, "tcdate": 1481223441664, "number": 6, "id": "S15GTXPme", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "Hyol_mPmx", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "From eq.3 to eq.4", "comment": "I'll take that one as I also had to derive it on my own (taking inspiration from the paper). The answer is yes. The way I obtained it is by saying (let's take alpha=1 and consider a single state for simpler notations) there exists a constant C such that pi(a) = exp(Q(a) - C) and sum_a pi(a) = 1. Thus C = Q(a) - ln pi(a). If you multiply both sides by pi(a) and sum over a you get C = V + H. And thus pi(a) = exp(Q(a) - V - H) is the probability distribution."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1481222131517, "tcdate": 1481222131511, "number": 8, "id": "Hyol_mPmx", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "HkUZch8ml", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "From eq.3 to eq.4", "comment": "Thank you very much for the clarification.\n\nI have a one related question. Can we show that if we add equation (4) over actions, the result would be one?\n\nThank you."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481205241180, "tcdate": 1481205241174, "number": 5, "id": "BJZ-L1Pmg", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "A few more questions", "comment": "Hi,\n\nCould you please clarify the following:\n  1. It sounds like alpha is kept fixed in experiments and results are reported for the \"exploratory\" policy (softmax with temperature alpha) rather than the greedy one. Is that correct and if yes, why?\n  2. In both the grid world and Atari, is the critic estimate of Q(s, a) obtained by summing the (discounted) observed rewards for up to t_max timesteps after taking action a in state s, plus the (discounted) estimated V(last observed state)? (= as in A3C)\n  3. For the Q-learning step are you freezing the target network and clipping the error as in the Nature DQN paper? If not, why?\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1481193981561, "tcdate": 1481193981556, "number": 7, "id": "HkUZch8ml", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "HJmFbgLml", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "From eq.3 to eq. 4", "comment": "Keeping the same notation I used above, a fixed point is one where we can no longer move in the direction of \\nabla f without violating one of the constraints g_s(pi) = 1, this is equivalent to finding a point x where \\nabla f(x) is in the span of the vectors \\nabla g_s(x), i.e., finding a point x such that \\nabla f(x) = \\sum_s \\lambda_s \\nabla g_s(x) where the \\lambda_s are real numbers and Lagrange multipliers and the values must ensure that g_s(x) = 1 for every s. From this we can derive equation 3. \n\nThis is a standard result, see here for more information: https://en.wikipedia.org/wiki/Lagrange_multiplier#Multiple_constraints\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481161272373, "tcdate": 1481161272368, "number": 4, "id": "HyeBcEIQg", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "Byq7XNN7x", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "Independence assumption", "comment": "I see, thanks for the clarification."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1481142821724, "tcdate": 1481142651137, "number": 6, "id": "HJmFbgLml", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "ryR4Pl1Qx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "From eq.3 to eq. 4", "comment": "I am still not convinced with the above answer. I think the way that you reach to equation (3) is by finding the fixed-point of the regularized policy and then applying the baseline trick. \n\nWe use the Lagrange multiplier when we try to solve a constrained minimization problem. However, you have not shown that how finding the fixed point is equivalent to solving a constrained minimization problem for the tabular case. We cannot use the result of section 3.2 because then the argument will be circular.\n\nPlease clarify.\n\nThank you,"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1481028386438, "tcdate": 1481028386431, "number": 5, "id": "Byq7XNN7x", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "ryJNg-X7l", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "Independence assumption", "comment": "We're saying to consider the euclidean projection problem of the log-\\pi s onto a set of values under some arbitrary measure, the KKT conditions of that problem is the next equation (along with the \\pi s being probability measures). If we compare that equation to eq. 3 we see that they match up if the measure is the same and q = Q, so we can interpret eq. 3 as a projection. This is not the same as solving the optimization problem in eq. 7 with the measure \\mu_\\pi (let's call it that) and Q^\\pi, since they are both functions of \\pi and thus we would need to consider how changing \\pi affects them, as you say, which would change the KKT conditions. However we are not claiming to solve that problem, which is more complicated than a simple projection. If your objection is that we can't retroactively substitute \\mu_\\pi and Q^\\pi back in once we have the KKT conditions then that would be true if we were claiming to solve the more complicated problem, but we're only trying to interpret eq. 3 as a projection, which it is for whichever measure appears in eq. 3.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1480949799203, "tcdate": 1480949799197, "number": 3, "id": "ryJNg-X7l", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "Independence assumption", "comment": "Hi,\n\nIn 3.2 you consider the case where \"the measure in the expectation (is) independent of theta\". However this is not the case in practice since both the state and action distributions depend on theta. Could you please comment on how this affects the proposed interpretation? I believe it would be important to explain it in the paper.\n\nThanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1480939862954, "tcdate": 1480939862948, "number": 4, "id": "H11PK0fQx", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "BJLW1aJ7e", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "From eq. 3 to eq. 4", "comment": "I just re-read it and I agree that section is unclear at the moment, we will rewrite it. Thanks for the feedback!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1480736509592, "tcdate": 1480736509588, "number": 1, "id": "BJLW1aJ7e", "invitation": "ICLR.cc/2017/conference/-/paper271/official/comment", "forum": "B1kJ6H9ex", "replyto": "ryR4Pl1Qx", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "Thanks!", "comment": "Got it, thanks! I'd suggest to explicitly derive the Lagrangian view in the paper, as I personally find it clearer (but that might just be me)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646337, "id": "ICLR.cc/2017/conference/-/paper271/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646337}}}, {"tddate": null, "tmdate": 1480685366402, "tcdate": 1480685366396, "number": 3, "id": "ryR4Pl1Qx", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "rJcswr0zg", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "From eq. 3 to eq. 4", "comment": "One way to think of c is as a Lagrange dual variable, there exists a c such that\nwhen you solve (3) you get a probability distribution in the end. So we can     \nconsider the pi to be unconstrained (so you get the delta function) but the     \nchoice of c will enforce that sum(pi) = 1.                                      \n                                                                                \nMore specifically, we have a gradient we want to follow upwards (namely the sum \nof the gradient of the cost function and the gradient of the entropy), let's    \ndenote this as \\nabla f, but we also want to ensure that sum(pi) = 1 for each   \nstate, let's call this  g_s(pi) = 1 for each s. Finding a critical point that   \nsatisfies the constraints is equivalent to finding a point pi such that \\nabla  \nf(pi) = \\sum_s \\lambda_s \\nabla g_s(pi). From this you can derive eq. 3 by      \nrolling all the per-state constants into a single constant called c, and        \nthere must be a choice of c that ensures the constraint holds.             \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1480640418050, "tcdate": 1480640418043, "number": 1, "id": "rJcswr0zg", "invitation": "ICLR.cc/2017/conference/-/paper271/pre-review/question", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper271/AnonReviewer1"], "content": {"title": "From eq. 3 to eq. 4", "question": "Hi,\n\nI'm having some trouble following the reasoning to reach eq. 4 from eq. 3. Equation 3 is valid \"for any c\", and yet it leads you to a specific value for c, which does not make sense to me. More precisely, the specific point causing this problem seems to be \"the gradient of the policy with respect to the parameters is the indicator function\": I believe this is incorrect since the parameters are not all free, they are linked together by the constraint sum_a pi(s, a) = 1. For instance if we have two actions then there is a single parameter, and the gradient of the policy with respect to this parameter is either 1 or -1, but not zero.\n\nPlease let me know if I'm missing something here, thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482861642601, "id": "ICLR.cc/2017/conference/-/paper271/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper271/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper271/AnonReviewer1", "ICLR.cc/2017/conference/paper271/AnonReviewer4"], "reply": {"forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "writers": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper271/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482861642601}}}, {"tddate": null, "tmdate": 1478606452206, "tcdate": 1478606452188, "number": 2, "id": "Byh_RNyZg", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "r1T71wAxe", "signatures": ["~Brendan_ODonoghue1"], "readers": ["everyone"], "writers": ["~Brendan_ODonoghue1"], "content": {"title": "ICLR Paper Format", "comment": "Just re-uploaded, thanks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}, {"tddate": null, "tmdate": 1478554349732, "tcdate": 1478549284844, "number": 1, "id": "r1T71wAxe", "invitation": "ICLR.cc/2017/conference/-/paper271/public/comment", "forum": "B1kJ6H9ex", "replyto": "B1kJ6H9ex", "signatures": ["~Tara_N_Sainath1"], "readers": ["everyone"], "writers": ["~Tara_N_Sainath1"], "content": {"title": "ICLR Paper Format", "comment": "Dear Authors,\n\nPlease resubmit your paper in the ICLR 2017 format with the correct margin spacing for your submission to be considered. Thank you!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Combining policy gradient and Q-learning", "abstract": "Policy gradient is an efficient technique for improving a policy in a reinforcement learning setting. However, vanilla online variants are on-policy only and not able to take advantage of off-policy data. In this paper we describe a new technique that combines policy gradient with off-policy Q-learning, drawing experience from a replay buffer. This is motivated by making a connection between the fixed points of the regularized policy gradient algorithm and the Q-values. This connection allows us to estimate the Q-values from the action preferences of the policy, to which we apply Q-learning updates. We refer to the new technique as \u2018PGQL\u2019, for policy gradient and Q-learning. We also establish an equivalency between action-value fitting techniques and actor-critic algorithms, showing that regularized policy gradient techniques can be interpreted as advantage function learning algorithms. We conclude with some numerical examples that demonstrate improved data efficiency and stability of PGQL. In particular, we tested PGQL on the full suite of Atari games and achieved performance exceeding that of both asynchronous advantage actor-critic (A3C) and Q-learning.               \n", "pdf": "/pdf/9dceb23b2c811b0566d9025dcdd13b037e3f3f6f.pdf", "TL;DR": "We combine a policy gradient style update with a Q-learning style update into a single RL algorithm we call PGQL.", "paperhash": "odonoghue|combining_policy_gradient_and_qlearning", "conflicts": ["google.com"], "keywords": ["Deep learning", "Reinforcement Learning"], "authors": ["Brendan O'Donoghue", "Remi Munos", "Koray Kavukcuoglu", "Volodymyr Mnih"], "authorids": ["bodonoghue@google.com", "munos@google.com", "korayk@google.com", "vmnih@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287646461, "id": "ICLR.cc/2017/conference/-/paper271/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1kJ6H9ex", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper271/reviewers", "ICLR.cc/2017/conference/paper271/areachairs"], "cdate": 1485287646461}}}], "count": 35}