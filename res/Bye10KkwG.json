{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124441891, "tcdate": 1518472663594, "number": 335, "cdate": 1518472663594, "id": "Bye10KkwG", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Bye10KkwG", "signatures": ["~Itay_Hubara6"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients", "abstract": "Binarized Neural networks (BNNs) have been shown to be effective in improving network efficiency during the inference phase, after the network has been trained. However, BNNs only binarize the model parameters and activations during propagations.\nWe show there is no inherent difficulty in training BNNs using \"Quantized BackPropagation\" (QBP), in which we also quantized the error gradients and in the extreme case ternarize them. To avoid significant degradation in test accuracy, we apply stochastic ternarization and increase the number of filter maps in a each convolution layer. Using QBP has the potential to significantly improve the execution efficiency (\\emph{e.g.}, reduce dynamic memory footprint and computational energy and speed up the training process, even after such an increase in network size.", "paperhash": "hubara|quantized_backpropagation_training_binarized_neural_networks_with_quantized_gradients", "keywords": ["Neural Network Acceleration", "Neural Network Compression"], "_bibtex": "@misc{\n  hubara2018quantized,\n  title={Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients},\n  author={Itay Hubara and Elad Hoffer and Daniel Soudry},\n  year={2018},\n  url={https://openreview.net/forum?id=Bye10KkwG}\n}", "authorids": ["itay.hubara@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "authors": ["Itay Hubara", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "By quantizing only the sequential error gradients we can accelerate the DNNs training while receiving  high accuracy results.", "pdf": "/pdf/58feefd31fe12d6a408cd52f2ec3cd8fdcd293e6.pdf"}, "nonreaders": [], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582891930, "tcdate": 1520524291228, "number": 2, "cdate": 1520524291228, "id": "Byi-3R0dM", "invitation": "ICLR.cc/2018/Workshop/-/Paper335/Official_Review", "forum": "Bye10KkwG", "replyto": "Bye10KkwG", "signatures": ["ICLR.cc/2018/Workshop/Paper335/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper335/AnonReviewer2"], "content": {"title": "good work but not well written", "rating": "5: Marginally below acceptance threshold", "review": "The authors explored the possibility of using reduced precision in backpropagation. Experimental results support the idea of QBP or even TBP, showing that models trained with quantized gradients are able to get good performances.\n\nThe manuscript needs much more work to be in a good form. For example, \n0. The term MAC has appeared before its definition.\n1. On page 2, end of Section 2, the authors mentioned Algorithm 1, but there is no algorithm table in the paper. \n2. \"for the layer activation gradients derivation\" appeared twice in the same sentence.\n\nAlso, there is already a related work which is exactly doing quantized back propagation: Lin, Z., Courbariaux, M., Memisevic, R. and Bengio, Y., 2015. Neural networks with few multiplications. arXiv preprint arXiv:1510.03009. It would be great if the authors could describe the advantages of the current method over the previous one.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients", "abstract": "Binarized Neural networks (BNNs) have been shown to be effective in improving network efficiency during the inference phase, after the network has been trained. However, BNNs only binarize the model parameters and activations during propagations.\nWe show there is no inherent difficulty in training BNNs using \"Quantized BackPropagation\" (QBP), in which we also quantized the error gradients and in the extreme case ternarize them. To avoid significant degradation in test accuracy, we apply stochastic ternarization and increase the number of filter maps in a each convolution layer. Using QBP has the potential to significantly improve the execution efficiency (\\emph{e.g.}, reduce dynamic memory footprint and computational energy and speed up the training process, even after such an increase in network size.", "paperhash": "hubara|quantized_backpropagation_training_binarized_neural_networks_with_quantized_gradients", "keywords": ["Neural Network Acceleration", "Neural Network Compression"], "_bibtex": "@misc{\n  hubara2018quantized,\n  title={Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients},\n  author={Itay Hubara and Elad Hoffer and Daniel Soudry},\n  year={2018},\n  url={https://openreview.net/forum?id=Bye10KkwG}\n}", "authorids": ["itay.hubara@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "authors": ["Itay Hubara", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "By quantizing only the sequential error gradients we can accelerate the DNNs training while receiving  high accuracy results.", "pdf": "/pdf/58feefd31fe12d6a408cd52f2ec3cd8fdcd293e6.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582891735, "id": "ICLR.cc/2018/Workshop/-/Paper335/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper335/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper335/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper335/AnonReviewer2"], "reply": {"forum": "Bye10KkwG", "replyto": "Bye10KkwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper335/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper335/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582891735}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573588565, "tcdate": 1521573588565, "number": 195, "cdate": 1521573588226, "id": "r1p0CR0Kz", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Bye10KkwG", "replyto": "Bye10KkwG", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients", "abstract": "Binarized Neural networks (BNNs) have been shown to be effective in improving network efficiency during the inference phase, after the network has been trained. However, BNNs only binarize the model parameters and activations during propagations.\nWe show there is no inherent difficulty in training BNNs using \"Quantized BackPropagation\" (QBP), in which we also quantized the error gradients and in the extreme case ternarize them. To avoid significant degradation in test accuracy, we apply stochastic ternarization and increase the number of filter maps in a each convolution layer. Using QBP has the potential to significantly improve the execution efficiency (\\emph{e.g.}, reduce dynamic memory footprint and computational energy and speed up the training process, even after such an increase in network size.", "paperhash": "hubara|quantized_backpropagation_training_binarized_neural_networks_with_quantized_gradients", "keywords": ["Neural Network Acceleration", "Neural Network Compression"], "_bibtex": "@misc{\n  hubara2018quantized,\n  title={Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients},\n  author={Itay Hubara and Elad Hoffer and Daniel Soudry},\n  year={2018},\n  url={https://openreview.net/forum?id=Bye10KkwG}\n}", "authorids": ["itay.hubara@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "authors": ["Itay Hubara", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "By quantizing only the sequential error gradients we can accelerate the DNNs training while receiving  high accuracy results.", "pdf": "/pdf/58feefd31fe12d6a408cd52f2ec3cd8fdcd293e6.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1520536190375, "tcdate": 1520536190375, "number": 1, "cdate": 1520536190375, "id": "Sy8F9-1YM", "invitation": "ICLR.cc/2018/Workshop/-/Paper335/Official_Comment", "forum": "Bye10KkwG", "replyto": "Bye10KkwG", "signatures": ["ICLR.cc/2018/Workshop/Paper335/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper335/AnonReviewer1"], "content": {"title": "good paper", "comment": "This paper tackles quantization of the backward-pass in DNNs. The paper focuses mostly on vision/CNN workloads.\n\nThe idea is interesting - most works till now have looked into very low-precision for the forward pass. For backward-pass, FP16 gradients are SOTA now. This paper looks at precision below 16bits - ternary and 4bits.\n\nWriting quality of the paper can be significantly improved. For example, sin'g' in equation-1, no algorithm description (Alg 1.) in end of Section 2, etc.\n\nAlso, the paper mentions backward pass requires the same amount of multiplications as the forward pass - this is incorrect, backward pass requires 2x more compute than forward pass (one for weight in current layer and one for loss propagation to the previous layer).\n\nI can buy into the argument of training at low-precision (forward+backward pass) for most of the epochs and then train with full-precision the last few epochs, i.e. accelerated training time. This paper sets this direction.\n\nAlso, it would be good to get more data-points on why stochastic ternarization helps - why noisy gradients help and why accuracy does not go up without this.\n\nOverall, the first steps in this paper and initial results are promising."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients", "abstract": "Binarized Neural networks (BNNs) have been shown to be effective in improving network efficiency during the inference phase, after the network has been trained. However, BNNs only binarize the model parameters and activations during propagations.\nWe show there is no inherent difficulty in training BNNs using \"Quantized BackPropagation\" (QBP), in which we also quantized the error gradients and in the extreme case ternarize them. To avoid significant degradation in test accuracy, we apply stochastic ternarization and increase the number of filter maps in a each convolution layer. Using QBP has the potential to significantly improve the execution efficiency (\\emph{e.g.}, reduce dynamic memory footprint and computational energy and speed up the training process, even after such an increase in network size.", "paperhash": "hubara|quantized_backpropagation_training_binarized_neural_networks_with_quantized_gradients", "keywords": ["Neural Network Acceleration", "Neural Network Compression"], "_bibtex": "@misc{\n  hubara2018quantized,\n  title={Quantized Back-Propagation: Training Binarized Neural Networks with Quantized Gradients},\n  author={Itay Hubara and Elad Hoffer and Daniel Soudry},\n  year={2018},\n  url={https://openreview.net/forum?id=Bye10KkwG}\n}", "authorids": ["itay.hubara@gmail.com", "elad.hoffer@gmail.com", "daniel.soudry@gmail.com"], "authors": ["Itay Hubara", "Elad Hoffer", "Daniel Soudry"], "TL;DR": "By quantizing only the sequential error gradients we can accelerate the DNNs training while receiving  high accuracy results.", "pdf": "/pdf/58feefd31fe12d6a408cd52f2ec3cd8fdcd293e6.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222445449, "id": "ICLR.cc/2018/Workshop/-/Paper335/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Bye10KkwG", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper335/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper335/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper335/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper335/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper335/Reviewers", "ICLR.cc/2018/Workshop/Paper335/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222445449}}}], "count": 4}