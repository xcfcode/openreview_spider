{"notes": [{"id": "r1gIa0NtDH", "original": "SkeJdWquPr", "number": 1394, "cdate": 1569439421760, "ddate": null, "tcdate": 1569439421760, "tmdate": 1577168269609, "tddate": null, "forum": "r1gIa0NtDH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "-2u2Au4kY4", "original": null, "number": 1, "cdate": 1576798722177, "ddate": null, "tcdate": 1576798722177, "tmdate": 1576800914413, "tddate": null, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "invitation": "ICLR.cc/2020/Conference/Paper1394/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposed an autoregressive model with a multiscale generative representation of the spectrograms to better modeling the long term dependencies in audio signals. The techniques developed in the paper are novel and interesting. The main concern is the validation of the method. The paper presented some human listening studies to compare long-term structure on unconditional samples, which as also mentioned by reviewers are not particularly useful. Including justifications on the usefulness of the learned representation for any downstream task would make the work much more solid. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795719840, "tmdate": 1576800270559, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1394/-/Decision"}}}, {"id": "S1gKu8yCFB", "original": null, "number": 2, "cdate": 1571841649031, "ddate": null, "tcdate": 1571841649031, "tmdate": 1573759241265, "tddate": null, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "invitation": "ICLR.cc/2020/Conference/Paper1394/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "In this paper the authors present a new generative model for audio in the frequency domain to capture better the global structure of the signal. For this, they use an autoregressive  procedure combined with a multiscale generative model for two-dimensional time-frequency visual representation (STFT spectrogram). The proposed method is tested across a diverse set of audio generation tasks\n\nOverall, The idea of generating audio from 2D spectrogram is original but in my point of view the use of STFT is not appropriate in this context, especially with its lossy criterion. \n\nGiven the clarifications and the author\u2019s responses below, I increased the score from 3 to 6. \n\n\n\nDetailed comments:\n\nPro: \n\nMitigate the problem of generating signal using only local dependencies (on a narrow time scale) and this by capturing high level dependency that emerges on larger timescale (several seconds) using spectrogram.\n\n\n\nCons: \n\n(1)The use of  STFT is not justified why not wavelet spectrogram to capture both scale and time?\n(2)It is still confusing how the use of high resolution spectrogram improve the lossy representation?\n(3)If you increase the STFT hope size you come back to the main problem that you are trying to resolve (i.e,  the bias towards capturing local dependencies) \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1394/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1394/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574927461263, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1394/Reviewers"], "noninvitees": [], "tcdate": 1570237738022, "tmdate": 1574927461275, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1394/-/Official_Review"}}}, {"id": "Syx1z6__jH", "original": null, "number": 3, "cdate": 1573584135513, "ddate": null, "tcdate": 1573584135513, "tmdate": 1573584135513, "tddate": null, "forum": "r1gIa0NtDH", "replyto": "Skxk8C7CFH", "invitation": "ICLR.cc/2020/Conference/Paper1394/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for the comments and suggestions.\n\n> While the network architecture is described in detail and some figures, the full network structure itself is non-trivial and still somewhat opaque from the plain text description. A schematic diagram of the full network architecture, even in the appendix, could help clarify how many layers are present connecting each component of the model, which would improve reproducibility.\n\nWe spent a fair amount of time discussing how to show the architecture schematically, but ultimately did not come up with anything particularly satisfying.  Most of the intricacy lies in the implementation of the multi-dimensional recurrence and the connectivity between the time and frequency stacks, so we focused on including equations and figures which provide clarity in these areas.\n\n\n> The paper is a bit thin on metrics. Human listening studies compare long-term structure, but not short-scale fidelity. For TTS, there are clear artifacts from the spectrogram inversion process. Mean opinion scores on conditional samples could help to quantify the importance of each element of the network for audio quality. For instance, how does MOS compare between Griffin-Lim MelNet, Gradient Inversion MelNet, and WaveNet? How does MelNet compare to Linear scaled spectrograms?\n\nWe generally agree with your comments here.  As we don\u2019t provide any experiments regarding short-term fidelity, we made sure to restrict our claims to state that the benefit of MelNet relates to its ability to capture long-range structure.  In cases where audio fidelity is paramount, time-domain models could be used to invert spectrograms generated by MelNet.\n\n\n>  Generating MelSpectrograms to model long-term structure is a fairly established technique, most notably employed by all of the Tacotron variants (https://google.github.io/tacotron/). These models are perhaps a more appropriate comparison for MelNet in many ways, and opt for spectrogram inversion by smaller WaveRNN models. One of the claims of the paper is that it is important to model the fine-scale structure of spectrograms, but it is not clear if that really is the case. A proper comparison to Tacotron models (where spectrograms are generated at the same resolution / the same inversion methods are used) would help clarify the importance of end-2-end training, vs. the learned inversion approach. \n\nThe main distinction between MelNet and models such as Tacotron is that MelNet utilizes a much more flexible probabilistic model that allows it to be applied to unconditional generation tasks such as music generation.  The experiments in this paper are restricted to unconditional audio generation, so direct comparison with models such as Tacotron would not be possible.  We\u2019ve revised the paper to include a discussion of existing TTS models.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1394/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gIa0NtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference/Paper1394/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1394/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1394/Reviewers", "ICLR.cc/2020/Conference/Paper1394/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1394/Authors|ICLR.cc/2020/Conference/Paper1394/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156672, "tmdate": 1576860539778, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference/Paper1394/Reviewers", "ICLR.cc/2020/Conference/Paper1394/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1394/-/Official_Comment"}}}, {"id": "HJxDu3u_jr", "original": null, "number": 2, "cdate": 1573583983461, "ddate": null, "tcdate": 1573583983461, "tmdate": 1573583983461, "tddate": null, "forum": "r1gIa0NtDH", "replyto": "S1gKu8yCFB", "invitation": "ICLR.cc/2020/Conference/Paper1394/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for the comments and suggestions.\n\n> The use of  STFT is not justified why not wavelet spectrogram to capture both scale and time?\n\nThanks for suggesting the use of wavelet-based representations. Our primary aim was to demonstrate that autoregressive models of time-frequency representations offer advantages (e.g. better modelling of long-range structure) in comparison to existing autoregressive models of time-domain waveforms.  For this reason, our study does not focus on variations within the class of time-frequency representations and we instead opted (somewhat arbitrarily) to use an STFT-based representation.  While beyond the scope of our paper, we believe that exploring alternative time-frequency representations would be interesting future work.\n\n\n>It is still confusing how the use of high resolution spectrogram improve the lossy representation?\n\nThe STFT itself is invertible, but we use mel-spectrograms which discard information through a) the removal of phase and b) the compression of the frequency axis via the mel transform.   Ideally we\u2019d like to minimize the amount of information removed from the STFT by these transformations.  Intuitively, we can minimize the information loss due to the mel transform by utilizing a larger number of frequency bins.  Similarly, to minimize the information loss due to the removal of phase, we can use a smaller STFT hop size which eases the task of phase estimation by algorithms such as Griffin-Lim.  Empirically, this can be validated by inverting spectrograms at various resolutions and observing the that higher resolution spectrograms can be inverted to higher fidelity audio.  \n\n\n> If you increase the STFT hope size you come back to the main problem that you are trying to resolve (i.e,  the bias towards capturing local dependencies)\n\nCorrect, and this is precisely the motivation for the multiscale approach described in the paper.  By separately modeling the spectrogram at different scales, it is possible to largely decouple the tasks of modeling high-level and low-level structure.\n\n\n> Given the clarifications and the author\u2019s responses, I would be willing to increase the score.\n\nLet us know if there are any further concerns you would like us to address.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1394/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gIa0NtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference/Paper1394/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1394/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1394/Reviewers", "ICLR.cc/2020/Conference/Paper1394/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1394/Authors|ICLR.cc/2020/Conference/Paper1394/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156672, "tmdate": 1576860539778, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference/Paper1394/Reviewers", "ICLR.cc/2020/Conference/Paper1394/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1394/-/Official_Comment"}}}, {"id": "H1ejPouOoS", "original": null, "number": 1, "cdate": 1573583715132, "ddate": null, "tcdate": 1573583715132, "tmdate": 1573583715132, "tddate": null, "forum": "r1gIa0NtDH", "replyto": "rylkOqJptr", "invitation": "ICLR.cc/2020/Conference/Paper1394/-/Official_Comment", "content": {"title": "Author Response", "comment": "Thank you for the detailed review.\n\n> MelNet is not a \"fully end-to-end generative model of audio\". It generates the spectrogram and relies on other algorithmic component (Griffin-Lim or gradient-based inversion) to generate raw audio.\n\nWe used the phrase \u2018end-to-end\u2019 to draw a distinction between MelNet and other generative models which rely on labeled intermediate representations e.g. Wave2Midi2Wave which is conditioned on symbolic music representations or text-to-speech models which use intermediate linguistic features.  Nonetheless, since the phrase is ambiguous and potentially confusing, we\u2019ve removed it from the paper.\n\n\n> It's still unclear the conv2d is useful or undesirable for modeling spectrograms in generative tasks. Even in recognition tasks, I have seen different results from different papers for different settings,  e.g., In Deep Speech 2, conv2d is useful to reduce WER in ASR. \n\nOur phrasing on this could have been more clear.  Deep Speech 2 shows that 2d convolution is preferable to 1d convolution.  Our intention was to state that 2d convolution is in theory less appropriate than other 2d primitives such as 2d recurrence which would not assume invariance along the frequency axis.  We\u2019ve clarified this in the revised paper.\n\n\n> Missing connection in related work: previous conditional generation methods (e.g., Tacotron, Deep Voice 3) are autoregressive over time, but assume conditional independence over frequency bins. \n\nWe\u2019ve added a paragraph in related work which references these and other related works.  We originally did not include discussion of these works since our work focuses on a broader class of generative models that are flexible enough to model arbitrary distributions of audio.   Models such as Tacotron/DV3 would not be capable of complex generation tasks such as unconditional generation for various reasons, including a) they assume conditional independence as you mentioned b) they assume unimodality (they utilize L1/L2 or other convex losses) and aim to capture a single mode of the distribution rather than capture the full variability of the distribution c) most variants of these models are deterministic.\n\n\n> There is TTS experiments on the demo website, but I didn't find any details. For example, where does the conditional information (**aligned** linguistic feature) come from?\n\nThe TTS audio samples can be ignored.  The site is also used as a supplement for an extended version of the paper which also covers TTS.\n\n\n> MelNet is autoregressive over both time and frequency. Thus, it is as slow as autoregressive waveform models at synthesis with worse audio fidelity, which make it less preferred in potential TTS applications.\n\nSampling speed is certainly one of the largest disadvantages of autoregressive models such as ours.  The original version of WaveNet also suffered from this problem but this was largely overcome following contributions in subsequent works.  We believe similar techniques can help improve the sampling speed of MelNet, but as this is a nontrivial task we believe this line of research is better suited for future work.\n\n\n> Unconditional speech generation is an uncommon & less useful task in general. If the task is purposely constructed, the learned representation is more useful than the generation itself (e.g., van den Oord et al. 2017). However, the authors have not demonstrated the usefulness of the learned representation for any downstream task. \n\nWhile we have not directly demonstrated the usefulness of the learned representations, we believe that the samples provide some evidence that the model has learned representations which contain useful information.  For example, when priming the state with a sequence of audio, the generated continuation preserves the characteristics of the priming sequence, suggesting that the latent characteristics of the priming sequence have been encoded into the hidden state.  \n\nRegarding the usefulness of our contribution, we agree that unconditional generation is not a particularly useful task.  However, our primary goal is to present a fundamental contribution to generative models of audio which can facilitate further applications.  Unconditional image and text generation are similarly frivolous, but research in these areas lays a foundation for subsequent application-oriented research.  For example, advances in language modelling for text have led directly to improvements in translation and summarization.\n\n\nPlease let us know if there are any further changes we could make for you to consider increasing your score.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1394/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "r1gIa0NtDH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference/Paper1394/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1394/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1394/Reviewers", "ICLR.cc/2020/Conference/Paper1394/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1394/Authors|ICLR.cc/2020/Conference/Paper1394/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156672, "tmdate": 1576860539778, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1394/Authors", "ICLR.cc/2020/Conference/Paper1394/Reviewers", "ICLR.cc/2020/Conference/Paper1394/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1394/-/Official_Comment"}}}, {"id": "rylkOqJptr", "original": null, "number": 1, "cdate": 1571777126623, "ddate": null, "tcdate": 1571777126623, "tmdate": 1572972474529, "tddate": null, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "invitation": "ICLR.cc/2020/Conference/Paper1394/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This work treats 2-D spectrogram as image and uses an autoregressive models which factorizes over both time and frequency dimensions. \n\nDetailed comments:\n\n- MelNet is not a \"fully end-to-end generative model of audio\". It generates the spectrogram and relies on other algorithmic component (Griffin-Lim or gradient-based inversion) to generate raw audio.\n\n- MelNet can model the long range structure for unconditional generation of speech, but its audio fidelity is not as good as autoregressive or non-autoregressive models on raw waveforms. The major reason is that MelNet discards the phase information which is useful for high-fidelity speech synthesis. It would be more interesting if MelNet jointly models the magnitude and phase information. \n\n- The mixture density networks are well known. One may omit the details (or put them in Appendix) in Section 3 for space reason. Overall, the paper is clearly written, but it can be shortened in several ways.\n\n- \"making the use of 2D convolution undesirable.\"\nIt's still unclear the conv2d is useful or undesirable for modeling spectrograms in generative tasks. Even in recognition tasks, I have seen different results from different papers for different settings,  e.g., In Deep Speech 2, conv2d is useful to reduce WER in ASR. \n\n- Missing connection in related work: previous conditional generation methods (e.g., Tacotron, Deep Voice 3) are autoregressive over time, but assume conditional independence over frequency bins. \n\n- There is TTS experiments on the demo website, but I didn't find any details. For example, where does the conditional information (**aligned** linguistic feature) come from?\n\nMy major concern is about the usefulness of the model:\n\n1) The unconditional speech generation is an uncommon & less useful task in general. If the task is purposely constructed, the learned representation is more useful than the generation itself (e.g., van den Oord et al. 2017). However, the authors have not demonstrated the usefulness of the learned representation for any downstream task. \n\n2) The MelNet is autoregressive over both time and frequency. Thus, it is as slow as autoregressive waveform models at synthesis with worse audio fidelity, which make it less preferred in potential TTS applications."}, "signatures": ["ICLR.cc/2020/Conference/Paper1394/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1394/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574927461263, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1394/Reviewers"], "noninvitees": [], "tcdate": 1570237738022, "tmdate": 1574927461275, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1394/-/Official_Review"}}}, {"id": "Skxk8C7CFH", "original": null, "number": 3, "cdate": 1571860039272, "ddate": null, "tcdate": 1571860039272, "tmdate": 1572972474428, "tddate": null, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "invitation": "ICLR.cc/2020/Conference/Paper1394/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The authors introduce MelNet, an autoregressive model of Mel-frequency scaled spectrograms. They convert audio into high resolution spectrograms to reduce the audio artifacts introduced by inverting spectrograms (here they use gradient-based inversion over Griffin-Lim). To improve modeling of long term dependencies, they perform multi-scale splitting of the spectrograms and maximize the likelihood at each scale (avoiding dominance of noise at higher resolutions). They condition generation at finer scales from coarser scales, enabling sampling through an ancestral process. The authors also highlight the difference between temporal and frequency dimensions, creating different conditioning stacks for the past in time vs. the \"past\" in frequency (lower frequencies), and mixing conditioning between the two stacks through layers of the network. Multilayer RNNs are used throughout the network and external conditioning is incorporated at the input. \n\nThe challenge the authors are attempting to address is modeling of audio structure on both long and short timescales. As the authors demonstrate with strong baselines, WaveNet models, while superior on fine-scale fidelity, fail to capture dynamics more than a couple hundred milliseconds. The experiments demonstrate improvements on state-of-the-art for unconditional generation on text-to-speech datasets (generating coherent words and phrases) and the MAESTRO piano dataset (generating sections with consistent dynamics/timing/motifs). The continuations of primed examples in both domains are particularly impressive qualitatively, as they maintain much of the character of the priming sample. Ablation experiments qualitatively demonstrate the importance of multi-scale modeling for unconditional generation. Human listener studies support the claims made from qualitative evaluation of long term structure.\n\nThis paper should be accepted because it represents a non-trivial adaptation of autoregressive modeling to handle multi-scale structure in audio. The baselines comparisons and strong, and experiments validate the claims of the paper. \n\nThat said, several things could be done to improve the clarity and significance of the paper.\n\n* While the network architecture is described in detail and some figures, the full network structure itself is non-trivial and still somewhat opaque from the plain text description. A schematic diagram of the full network architecture, even in the appendix, could help clarify how many layers are present connecting each component of the model, which would improve reproducibility.\n\n* The paper is a bit thin on metrics. Human listening studies compare long-term structure, but not short-scale fidelity. For TTS, there are clear artifacts from the spectrogram inversion process. Mean opinion scores on conditional samples could help to quantify the importance of each element of the network for audio quality. For instance, how does MOS compare between Griffin-Lim MelNet, Gradient Inversion MelNet, and WaveNet? How does MelNet compare to Linear scaled spectrograms?\n\n* Generating MelSpectrograms to model long-term structure is a fairly established technique, most notably employed by all of the Tacotron variants (https://google.github.io/tacotron/). These models are perhaps a more appropriate comparison for MelNet in many ways, and opt for spectrogram inversion by smaller WaveRNN models. One of the claims of the paper is that it is important to model the fine-scale structure of spectrograms, but it is not clear if that really is the case. A proper comparison to Tacotron models (where spectrograms are generated at the same resolution / the same inversion methods are used) would help clarify the importance of end-2-end training, vs. the learned inversion approach. \n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1394/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1394/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "MelNet: A Generative Model for Audio in the Frequency Domain", "authors": ["Sean Vasquez", "Mike Lewis"], "authorids": ["seanjv@mit.edu", "mikelewis@fb.com"], "keywords": [], "TL;DR": "We introduce an autoregressive generative model for spectrograms and demonstrate applications to speech and music generation", "abstract": "Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps.  While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms.  By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales which time-domain models have yet to achieve.  We demonstrate that our model captures longer-range dependencies than time-domain models such as WaveNet across a diverse set of unconditional generation tasks, including single-speaker speech generation, multi-speaker speech generation, and music generation.", "pdf": "/pdf/684fb7b4e9c29e405246f942f000fdd6b49a1d17.pdf", "paperhash": "vasquez|melnet_a_generative_model_for_audio_in_the_frequency_domain", "original_pdf": "/attachment/2364de412a867c9e838414ec3f977c0ffe13fbac.pdf", "_bibtex": "@misc{\nvasquez2020melnet,\ntitle={MelNet: A Generative Model for Audio in the Frequency Domain},\nauthor={Sean Vasquez and Mike Lewis},\nyear={2020},\nurl={https://openreview.net/forum?id=r1gIa0NtDH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "r1gIa0NtDH", "replyto": "r1gIa0NtDH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1394/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574927461263, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1394/Reviewers"], "noninvitees": [], "tcdate": 1570237738022, "tmdate": 1574927461275, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1394/-/Official_Review"}}}], "count": 8}