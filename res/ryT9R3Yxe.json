{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396377584, "tcdate": 1486396377584, "number": 1, "id": "r1f1hGI_e", "invitation": "ICLR.cc/2017/conference/-/paper145/acceptance", "forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The contribution of this paper generally boils down to adding a prior to the latent representations of the paragraph in the Paragraph Vector model. An especially problematic point about this paper is the claim that the original paper considered only the transductive setting (i.e. it could not induce representations of new documents). It is not accurate, they also used gradient descent at test time. Though I agree that regularizing the original model is a reasonable thing to do, I share the reviewers' feeling that the contribution is minimal. There are also some serious issues with presentation (as noted by the reviewers), I am surprised that the authors have not addressed them during the review period."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Paragraph Vector", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "pdf": "/pdf/e86e72bc2d8a038d7ca66dba38c2bf74a24391f5.pdf", "TL;DR": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "paperhash": "zhang|generative_paragraph_vector", "conflicts": ["ict.ac.cn"], "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396378160, "id": "ICLR.cc/2017/conference/-/paper145/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396378160}}}, {"tddate": null, "tmdate": 1482554610189, "tcdate": 1482554610189, "number": 3, "id": "r19g6_sEl", "invitation": "ICLR.cc/2017/conference/-/paper145/official/review", "forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "signatures": ["ICLR.cc/2017/conference/paper145/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper145/AnonReviewer3"], "content": {"title": "below borderline", "rating": "4: Ok but not good enough - rejection", "review": "While this paper has some decent accuracy numbers, it is hard to argue for acceptance given the following:\n\n1) motivation based on the incorrect assumption that the Paragraph Vector wouldn't work on unseen data\n2) Numerous basic formatting and Bibtex citation issues.\n\nLack of novelty of yet another standard directed LDA-like bag of words/bigram model.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Paragraph Vector", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "pdf": "/pdf/e86e72bc2d8a038d7ca66dba38c2bf74a24391f5.pdf", "TL;DR": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "paperhash": "zhang|generative_paragraph_vector", "conflicts": ["ict.ac.cn"], "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482554610850, "id": "ICLR.cc/2017/conference/-/paper145/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper145/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper145/AnonReviewer1", "ICLR.cc/2017/conference/paper145/AnonReviewer2", "ICLR.cc/2017/conference/paper145/AnonReviewer3"], "reply": {"forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482554610850}}}, {"tddate": null, "tmdate": 1482189359941, "tcdate": 1482189359941, "number": 2, "id": "B1_E5k84e", "invitation": "ICLR.cc/2017/conference/-/paper145/official/review", "forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "signatures": ["ICLR.cc/2017/conference/paper145/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper145/AnonReviewer2"], "content": {"title": "Not convincing", "rating": "3: Clear rejection", "review": "It feels that this paper is structured around a shortcoming of the original paragraph vectors paper, namely an alleged inability to infer representation for text outside of the training data. I am reasonably sure that this is not the case. Unfortunately on that basis, the premise for the work presented here no longer holds, which renders most of the subsequent discussion void.\n\nWhile I recommend this paper be rejected, I encourage the authors to revisit the novel aspects of the idea presented here and see if that can be turned into a different type of paper going forward.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Paragraph Vector", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "pdf": "/pdf/e86e72bc2d8a038d7ca66dba38c2bf74a24391f5.pdf", "TL;DR": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "paperhash": "zhang|generative_paragraph_vector", "conflicts": ["ict.ac.cn"], "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482554610850, "id": "ICLR.cc/2017/conference/-/paper145/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper145/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper145/AnonReviewer1", "ICLR.cc/2017/conference/paper145/AnonReviewer2", "ICLR.cc/2017/conference/paper145/AnonReviewer3"], "reply": {"forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482554610850}}}, {"tddate": null, "tmdate": 1481973213887, "tcdate": 1481973213887, "number": 1, "id": "SyL10cfNx", "invitation": "ICLR.cc/2017/conference/-/paper145/official/review", "forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "signatures": ["ICLR.cc/2017/conference/paper145/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper145/AnonReviewer1"], "content": {"title": "Very limited in novelty", "rating": "2: Strong rejection", "review": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Paragraph Vector", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "pdf": "/pdf/e86e72bc2d8a038d7ca66dba38c2bf74a24391f5.pdf", "TL;DR": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "paperhash": "zhang|generative_paragraph_vector", "conflicts": ["ict.ac.cn"], "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482554610850, "id": "ICLR.cc/2017/conference/-/paper145/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper145/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper145/AnonReviewer1", "ICLR.cc/2017/conference/paper145/AnonReviewer2", "ICLR.cc/2017/conference/paper145/AnonReviewer3"], "reply": {"forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482554610850}}}, {"tddate": null, "tmdate": 1480657646541, "tcdate": 1480652587181, "number": 1, "id": "Sy7EP_AMx", "invitation": "ICLR.cc/2017/conference/-/paper145/public/comment", "forum": "ryT9R3Yxe", "replyto": "SylrvBAGg", "signatures": ["~ruqing_zhang1"], "readers": ["everyone"], "writers": ["~ruqing_zhang1"], "content": {"title": "Thank you for your comment", "comment": "Dear Reviewer,\n    Thanks for reviewing our paper.\n    1. Paragraph vector is incomplete in that it provides no model at the text level, just the same as PLSA. So when you want to obtain the PV for a new text for some prediction task, the only way you can do is to use fold-in method to learn the vector representation for the text. That is, you take that text together with original training dataset to learn the paragraph vector. However, if there is a generative process for the vector at the text level, we can use learned word vectors to directly infer the PV for the new test text. This is much more efficient in practice and this is the exactly the motivation of our work.\n    2. Same with the learning framework in PV, we apply the negative sampling idea to accelerate the learning. And, we use MAP estimates to approximate our learning problem. Because the MAP estimation problems for different documents are independent, we can solve them on multiple machines in parallel. The number of \u201cnegative\u201d samples, the vocabulary size of your dataset, and the parallel threads decide the training time. The training of GPV is almost of the same efficiency as PV model.\n    Hope this clarifies your questions.\n    "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Paragraph Vector", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "pdf": "/pdf/e86e72bc2d8a038d7ca66dba38c2bf74a24391f5.pdf", "TL;DR": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "paperhash": "zhang|generative_paragraph_vector", "conflicts": ["ict.ac.cn"], "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287712629, "id": "ICLR.cc/2017/conference/-/paper145/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "ryT9R3Yxe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper145/reviewers", "ICLR.cc/2017/conference/paper145/areachairs"], "cdate": 1485287712629}}}, {"tddate": null, "tmdate": 1480640312233, "tcdate": 1480640312229, "number": 1, "id": "SylrvBAGg", "invitation": "ICLR.cc/2017/conference/-/paper145/pre-review/question", "forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "signatures": ["ICLR.cc/2017/conference/paper145/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper145/AnonReviewer3"], "content": {"title": "intro", "question": "However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set.\n--> This makes no sense to me. That model clearly still works and computes a presentation for test texts.\n\nHow fast is your model?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Paragraph Vector", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "pdf": "/pdf/e86e72bc2d8a038d7ca66dba38c2bf74a24391f5.pdf", "TL;DR": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "paperhash": "zhang|generative_paragraph_vector", "conflicts": ["ict.ac.cn"], "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959441114, "id": "ICLR.cc/2017/conference/-/paper145/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper145/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper145/AnonReviewer3"], "reply": {"forum": "ryT9R3Yxe", "replyto": "ryT9R3Yxe", "writers": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper145/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959441114}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478276429567, "tcdate": 1478246036671, "number": 145, "id": "ryT9R3Yxe", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "ryT9R3Yxe", "signatures": ["~ruqing_zhang1"], "readers": ["everyone"], "content": {"title": "Generative Paragraph Vector", "abstract": "The recently introduced Paragraph Vector is an efficient method for learning high-quality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.\n", "pdf": "/pdf/e86e72bc2d8a038d7ca66dba38c2bf74a24391f5.pdf", "TL;DR": "With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts.", "paperhash": "zhang|generative_paragraph_vector", "conflicts": ["ict.ac.cn"], "authors": ["Ruqing Zhang", "Jiafeng Guo", "Yanyan Lan", "Jun Xu", "Xueqi Cheng"], "keywords": ["Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning"], "authorids": ["zhangruqing@software.ict.ac.cn", "guojiafeng@ict.ac.cn", "lanyanyan@ict.ac.cn", "junxu@ict.ac.cn", "cxq@ict.ac.cn"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 7}