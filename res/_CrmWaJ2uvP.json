{"notes": [{"id": "_CrmWaJ2uvP", "original": "KoT6myxgsqk", "number": 1695, "cdate": 1601308187536, "ddate": null, "tcdate": 1601308187536, "tmdate": 1614985715382, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "dghGW6UNJav", "original": null, "number": 1, "cdate": 1610040427240, "ddate": null, "tcdate": 1610040427240, "tmdate": 1610474026845, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a new RNN architecture called Dynamic RNN which is based on dynamic system identification.\n\nReviewers questioned the expressivity of the proposed model, practical application/impact of the proposed model, and interpretability of the proposed model. Even though the authors attempted to convince the reviewers, 3 out of 4 reviewers think that this work is not ready for publication. \n\nSpecifically, R4 recommends 5 ways to strengthen the paper. I recommend the authors to incorporate this feedback and make a stronger resubmission in the future."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040427226, "tmdate": 1610474026828, "id": "ICLR.cc/2021/Conference/Paper1695/-/Decision"}}}, {"id": "wa3KldwentX", "original": null, "number": 6, "cdate": 1606171864707, "ddate": null, "tcdate": 1606171864707, "tmdate": 1606172186789, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment", "content": {"title": "Response to all Reviewers", "comment": "Dear reviewers, thank you all for reading our paper and taking the time to write such helpful and constructive feedback. While extending the  interpretation for our model during this rebuttal phase, we detected a mistake in the implementation of one variant of the network, namely the one that is trained for one sampling rate at a time (i.e. the experiment from the current Figure 9 was unchanged). The mistake entailed that the hidden state of the I subcomponent was passed incorrectly. This showed us that there are actually two potential versions of our model: One with P, I, D, PD and PT which we call DYRNN5 and one with P, PD and PT called DYRNN3. Luckily, this mistake enabled us to see that often the model acutally improved if the amount of functions is reduced, i.e. if the model just contains P, PD, PT. \nWe want to apologize for the amount of changes that the reviewers have to reread and thank them for their patience while waiting for the rebuttal version. We hope that they will approve of our efforts to fix our mistakes and provide transparency on our new results.\n\nFurthermore we implemented the following changes: \n- Fixed writing errors and changed text to EN_GB\n- Adjusted Chapter 3.1 to clarify the recurrent aspect of our model and what the weight parameters are\n- Section 3.3 interpretation has been improved to explain how a differential equation can be derived from a trained model\n- All prior experiments were redone\n- The experiments were further explained and plots for all datasets were added\n- New aspect of interpretability: By translating the trained model into the Laplace/Fourier domain, several properties can be visualized (e.g. its frequency response, root locus curve)\n\nThank you again for your help and for your patience while waiting for this new revision."}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_CrmWaJ2uvP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1695/Authors|ICLR.cc/2021/Conference/Paper1695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856801, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment"}}}, {"id": "tOQznsiorVD", "original": null, "number": 5, "cdate": 1606170892394, "ddate": null, "tcdate": 1606170892394, "tmdate": 1606170892394, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "Fk8xgggCKGE", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "\nThank you very much for reading our work and your kind feedback! We have revised our paper and have further explained the interpretability aspect of it. We will look into applying the model to more datasets in future work.\n\nWe hope that you will like these changes and that they will clarify what we wanted to express regarding the interpretability of our model. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_CrmWaJ2uvP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1695/Authors|ICLR.cc/2021/Conference/Paper1695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856801, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment"}}}, {"id": "enqWtUBmsaA", "original": null, "number": 4, "cdate": 1606170771600, "ddate": null, "tcdate": 1606170771600, "tmdate": 1606170771600, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "3oSIzHJI2HE", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you very much for reading our work and for your feedback. We are sorry that you found the paper difficult to follow and have revised large parts of it in an effort to improve this issue.\n\n> Broadly speaking the work is quite unclear, and takes several passes over to have a basic sense of the approach. There are too many shortcomings to enumerate them all, so I will just present one. Figure 5 is presented in the section \"example architecture\" which might lead one to believe the authors implement this network (which it appears they do not). I believe this is included only to indicate a hypothetical architecture, but the presentation is too poor to glean this with any quickness.\n\nThis was an error in the text (around Figure 5) and the section been rewritten now, thank you for the pointer. \n\n> As for the significance, the work clearly falls short. Although the motivation of constructing a \"more explainable model\" is a good one, this should not come at an extreme cost of model expressivity. It seems obvious that richer models, such as LSTMs etc., correctly trained, should be able to account for the linear transformations the authors include in their \"novel layer.\" That their work is competitive with these richer models is simply an indication of the simplicity of the tasks they chose, which (as far as I can tell) can all be accounted for using linear systems analysis (although it's hard to say, since they work so poorly explains the second two tasks). It's completely unclear how effective the authors' approach would be over standard, richer models, on tasks that cannot be accounted for by linear systems analysis, and I am doubtful that the suggested approach could offer much over these richer models.\n\nThis is a valid point. Firstly, we added plots of the training datasets in order to show the  datasets used and rewrote the problem descriptions of both. In machine learning -- as we all know -- we often deal with the  trade-off between model expressivity and interpretability. We know that in non-linear problems non-linear models might be necessary. Nonetheless, many real-life systems can be linearized in their operation points and therefore be modelled adequately using linear models. Linear models are (most of the time) easier to understand for humans. Furthermore, our model does not rule out non-linear activation functions, they were just not the focus of this publication. \n\n> Likewise, an alternative view of the authors' work is as a learnable filter bank applied to data to create a representation of the data better suited for post-hoc learning with a richer model, which is certainly an useful idea, but it is not clear to me (and the authors haven't shown) that their choice for this filter-bank is superior to many other choices (e.g. convolutional layers applied prior to FC layers, which is standard for deep networks).\n\nWe understand that there are different types of neural networks applicable for such tasks. \nFor dynamic systems modelling, it is necessary to take the current and past state of the system into account. \nWe did not use FC or CNN because to our knowledge, these models would only work in the following constellations:\n1) Modelling of a PDE/ODE and using a separate ODE solver to compute the output sequence\n2) One-step-ahead prediction, which can be computationally slow and cumbersome to implement\n3) Sequence-to-sequence: impractical if one deals with measurement signals of changing lengths\n4) Fully Convolutional sequence-to-sequence: while applicable to different lengths, the field of view of a convolutional kernel is limited and the kernel lacks the memory component of a recurrent network. Memory is needed e.g. for the modelling of an integrator \n\nWe have expanded the interpretability section (3.3) and added new evaluation plots our experiments where we further analyse the model that stems from our approach. We hope that it will clarify our vision on why we created this work. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_CrmWaJ2uvP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1695/Authors|ICLR.cc/2021/Conference/Paper1695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856801, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment"}}}, {"id": "nAhkzCgMdjD", "original": null, "number": 3, "cdate": 1606168707972, "ddate": null, "tcdate": 1606168707972, "tmdate": 1606168707972, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "tGHf_ON5HBh", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment", "content": {"title": "Response to AnonReviewer4", "comment": "Thank you very much for your kind and constructive review! It's exciting to have a reviewer who knows this much about the subject matter. Your questions are all very interesting and we would like to evaluate them in the future. Due to the limited time for this rebuttal, we have to postpone the answer for your points 1/ through 4/ to a later date and want to focus on your question about interpretability (5).  \n\nWe extended the according chapter (now 3.3) by explaining how one would first derive the transfer function of the network by transforming it into the Laplace domain while keeping the trained K and T weights from the subcomponents (P, I, D, PD and PT1). From there, one can derive not only the ODE of the system, but also perform different other analyses known from control engineering, like predicting the models' frequency response. We have also shown the according plots as part of our experiments, and attached example transfer functions for our experiment data in the appendix.\n\nWe hope that this is what you envisioned with your question. We revised several parts of the paper during this change, and will post a description of all changes in the comment section. Again, thank you for your feedback."}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_CrmWaJ2uvP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1695/Authors|ICLR.cc/2021/Conference/Paper1695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856801, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment"}}}, {"id": "lzkFfPeh1-C", "original": null, "number": 2, "cdate": 1606167222881, "ddate": null, "tcdate": 1606167222881, "tmdate": 1606167421576, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "I3rNKMigZLP", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you very much for reading our work and your thorough remarks. Your feedback showed us where further adjustments to the text were needed to do in order to improve comprehensibility for the reader and we have implemented adjustments of the text to try to reflect this. \n\n> The paper is correctly written, and the subject is of interest. But I don't really see the point as it looks like the method removes all of what is 'recurrent', as no weights from the new unit are learned during the training. The learning is only on some fully connected layers afterwards: I feel that the authors just made an (interesting) 'feature extraction' instead of a recurrent unit, by using standard transfer functions without any weights to be learned (or haven't I understood correctly?).\n> (...)please explain better how your network is still recurrent : where is the 'hidden memory' that is passed? And where are the weights inside the DYRNN that are learned during the training? For me it looks like only the FC model placed after, that is moreover without any non-linearity (ReLU, etc), contains weights that are updated during training.\n\nThe network can be seen as a linear feature extractor. However the layer does contain weights, namely the K and T values from the linear equations. If you revisit equations (4) through (8) in the paper, you will see that some equations are based on their result from the prior timestep at (k-1). Therefore the hidden state of the network is [x(k-1), pt(k-1), k(k-1)] per cell.\n\n>The claim that the paper is the first to present a method where the sequence sampling rate, or 'delta t', is a parameter that can be modified without needed to re-train the network seems odd. In fact, I am not very familiar with this field but it looks to me that a lot of works that are now combining neural nets and dynamical systems are by definition able to do that. For example, I know this work 'Learning Dynamical Systems from Partial Observations', Ayed et al. 2019, which has a section called 'Benefits of Continuous-Time.', where I can read 'this allows us to accommodate irregularly acquired observations, and as demonstrated by the experiments, allows interpolation between observations.' So it is not something 'new' to the community and I would guess that papers closer to yours would also have the same feature?\n\nThank you for providing this additional piece of related work. We are aware of methods using ODE solvers and have cited one of them in our Related Work section. Our work differs from these models because our model is trained in a sequence-to-sequence fashion on a complete measurement timeseries without the use of an ODE solver. Nonetheless, internally it works similarly to an ODE solver, since the equations are discretized using backwards Euler. \n\n>The number of output channels per layer n layer amounts to (base component count * n oc = 5 * n oc ), as shown in Figure 3b.' --> not clear: there is 5 output channels, so noc = 5, but what is this 'base component count'? Why do we have to multiply it, and what is a layer? Or do you mean that there are 5 DYRNN units? or that every 'component' (P, I, ..) outputs the 5 outputs? This is not what is represented here.\n\nWe also adapted this part in the paper, seeing as it was written unclearly. What this section meant is that one DYRNN layer contains 5 basic subcomponents internally, by which we mean P, I, D, PT and PD.\nIf the layer's input has one channel only, the layer output channel count is 5*1, i.e. 5 channels. If the layer's input is e.g. 3 channelled, the output results in 15 output channels in total.\n\nWe have also added more changes to the paper, which we will describe in the general comment section. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "_CrmWaJ2uvP", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1695/Authors|ICLR.cc/2021/Conference/Paper1695/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923856801, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Comment"}}}, {"id": "3oSIzHJI2HE", "original": null, "number": 1, "cdate": 1603574567889, "ddate": null, "tcdate": 1603574567889, "tmdate": 1605024380693, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review", "content": {"title": "Clear reject. Lacks clarity. Findings not likely to be very general (weak significance).", "review": "The authors present a method for incorporating basic concepts from linear systems theory into the standard structure for training artificial neural networks. They compare results of their approach against standard approaches for 3 simple datasets.\n\nBroadly speaking the work is quite unclear, and takes several passes over to have a basic sense of the approach. There are too many shortcomings to enumerate them all, so I will just present one. Figure 5 is presented in the section \"example architecture\" which might lead one to believe the authors implement this network (which it appears they do not). I believe this is included only to indicate a hypothetical architecture, but the presentation is too poor to glean this with any quickness. This is of course, in itself, not sufficient grounds for rejection, but speaks broadly to the poor presentation of the work. It does not seem ready for publication.\n\nAs for the significance, the work clearly falls short. Although the motivation of constructing a \"more explainable model\" is a good one, this should not come at an extreme cost of model expressivity. It seems obvious that richer models, such as LSTMs etc., correctly trained, should be able to account for the linear transformations the authors include in their \"novel layer.\" That their work is competitive with these richer models is simply an indication of the simplicity of the tasks they chose, which (as far as I can tell) can all be accounted for using linear systems analysis (although it's hard to say, since they work so poorly explains the second two tasks). It's completely unclear how effective the authors' approach would be over standard, richer models, on tasks that cannot be accounted for by linear systems analysis, and I am doubtful that the suggested approach could offer much over these richer models.\n\nLikewise, an alternative view of the authors' work is as a learnable filter bank applied to data to create a representation of the data better suited for post-hoc learning with a richer model, which is certainly an useful idea, but it is not clear to me (and the authors haven't shown) that their choice for this filter-bank is superior to many other choices (e.g. convolutional layers applied prior to FC layers, which is standard for deep networks).", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112839, "tmdate": 1606915780114, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1695/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review"}}}, {"id": "Fk8xgggCKGE", "original": null, "number": 2, "cdate": 1603826203599, "ddate": null, "tcdate": 1603826203599, "tmdate": 1605024380622, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review", "content": {"title": "overall this is a good submission", "review": "This paper aims at proposing Dynamic Recurrent Network to understand the underlying system properties of RNNs. By first showing five basic linear transfer functions in dynamic systems theory, the paper formulates DYRNN units. To solve the increasing number of layers issue, they concatenate inputs and intermediate results before passing into an FC layer. It is interesting to see how adjusting\n\\deta t is related to the model\u2019s robustness. Though not fully explained, this paper provides a method to partially explain the RNN insights through FC layers learnt weights.  \n\nThe paper is well written to convey the central ideas. The overall idea is interesting, and experiments are clear to demonstrate the proposed method. It will be better to test the method on some benchmark datasets so that it will be easy to compare with the state-of-the-art.\n\nA small advice: do you mean varying instead of variing in section 6?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112839, "tmdate": 1606915780114, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1695/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review"}}}, {"id": "tGHf_ON5HBh", "original": null, "number": 3, "cdate": 1603839931871, "ddate": null, "tcdate": 1603839931871, "tmdate": 1605024380545, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review", "content": {"title": "Where\u2019s the interpretability exactly?", "review": "The theory of dynamical systems has such a rich and extensive history dating to more than a century ago, that Poincar\u00e9 himself would marvel to see what we can do now with modern technology.  Perhaps he\u2019d not be pleased to see that extensive research in dynamical systems from the previous century is ignored in this paper, however the authors are aiming at reaching more interpretability on NN models as stated in the abstract.\n\nI think the paper has definitely technical merits, but I remain puzzled with the lack interpretability section (3.4). I do not see how I can use the methodology to reconstruct the underlying physical processes that drive the physical phenomena. Since the authors are claiming in the abstract that interpretability is a distinctive feature of the methodology, I was expecting to see a full fleshed content, but it\u2019s essentially just a couple of paragraphs.\n\nThese are the questions that I\u2019d love the authors to address:\n1/ Since the authors are inserting an explicit integration method by virtue of the Euler step through Eqs 3 to 8, the method inherently carries A-unstable. Can you estimate the conditions for any system that would prevent the A-stability risk?\n2/ What are the differences of the methodology for Hamiltonian and dissipative dynamical systems? \n3/ If the methodology that the authors are proposing works, it should be the case that any transient trajectory can be explained by DYRNN. It means that a trained DYRNN on a small set of trajectories (or one sufficiently large one if we have an ergodic system), can fully reproduce any other transient trajectory of the dynamical system. In other words, produce a train and test trajectories and prove that the error on the test set is small enough.\n4/ At the core of classical dynamical system theory is understanding the set of local and global bifurcations that explain the physical/chemical/biological phenomena. If DYRNN can predict the bifurcation points respect to a control parameter, then the method is rock&roll, otherwise it\u2019s just another method.\n5/ Interpretability: If the authors can map back the DYRNN to a set of ODEs/PDEs then I\u2019d be fully convinced on the value of the method. Otherwise, I remain unconvinced on the value of the interpretability.\n\nI am not ready to accept this paper in its current form, but if the authors could prove (1,2) or 3 or 4 or 5 then I\u2019ll happily change my rating.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112839, "tmdate": 1606915780114, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1695/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review"}}}, {"id": "I3rNKMigZLP", "original": null, "number": 4, "cdate": 1603904851393, "ddate": null, "tcdate": 1603904851393, "tmdate": 1605024380473, "tddate": null, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "invitation": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review", "content": {"title": "Interesting topic but the method seems to just be a feature extraction + fully connected layers.", "review": "\n\nThis paper aims at defining a new architecture, Dynamic Recurrent Neural Network, that would be based on discrete differential equations of basic linear system transfer functions known from dynamic system identifiation. They show also an application example.\n\nThe paper is correctly written, and the subject is of interest. But I don't really see the point as it looks like the method removes all of what is 'recurrent', as no weights from the new unit are learned during the training. The learning is only on some fully connected layers afterwards: I feel that the authors just made an (interesting) 'feature extraction' instead of a recurrent unit, by using standard transfer functions without any weights to be learned (or haven't I understood correctly?).\n\nThe claim that the paper is the first to present a method where the sequence sampling rate, or 'delta t', is a parameter that can be modified without needed to re-train the network seems odd. In fact, I am not very familiar with this field but it looks to me that a lot of works that are now combining neural nets and dynamical systems are by definition able to do that. For example, I know this work 'Learning Dynamical Systems from Partial Observations', Ayed et al. 2019, which has a section called 'Benefits of Continuous-Time.', where I can read 'this allows us to accommodate irregularly acquired observations, and as demonstrated by the experiments, allows interpolation between observations.' So it is not something 'new' to the community and I would guess that papers closer to yours would also have the same feature?\n\nQuestions/ remarks:\n- please explain better how your network is still recurrent : where is the 'hidden memory' that is passed? And where are the weights inside the DYRNN that are learned during the training? For me it looks like only the FC model placed after, that is moreover without any non-linearity (ReLU, etc), contains weights that are updated during training.\n- 'and are state of the art layer types for text based sequence-to-sequence problems like machine translation or text processing --> no, the recurrent NN are not state of the art in translation anymore... transformers are. But RNN could be state of the art in other problems.\n- 'The number of output channels per layer n layer amounts to (base component count * n oc = 5 * n oc ), as shown in Figure 3b.' --> not clear: there is 5 output channels, so noc = 5, but what is this 'base component count'? Why do we have to multiply it, and what is a layer? Or do you mean that there are 5 DYRNN units? or that every 'component' (P, I, ..) outputs the 5 outputs? This is not what is represented here.\n- Fig. 6 might not be necessary, we can understand the concept without it.\n- Modell --> Model\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1695/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1695/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems", "authorids": ["~Deniz_Neufeld1"], "authors": ["Deniz Neufeld"], "keywords": ["dynamic system identification", "recurrent networks", "explainable AI", "time series modelling"], "abstract": "While dynamic systems can be modelled  as sequence-to-sequence tasks  by deep learning using different network architectures like DNN, CNN, RNNs or neural ODEs, the resulting models often provide poor understanding of the underlying system properties. We propose a new recurrent network architecture, the Dynamic Recurrent Network, where the computation function is based on the discrete difference equations of basic linear system transfer functions known from dynamic system identification. This results in a more explainable model, since the learnt weights can provide insight on a system's time dependent  behaviour.  It also introduces the sequences' sampling rate as an additional model parameter, which can be leveraged, for example, for time series data augmentation and model robustness checks. The network is trained using traditional gradient descent optimization and can be used in combination with other state of the art neural network layers. We show that our new layer type yields results comparable to or better than other recurrent layer types on several system identification tasks.", "one-sentence_summary": "A new recurrent network structure consisting of basic linear building blocks from dynamic system identification.", "pdf": "/pdf/674dca5d1daa6e05f8f943716c0740db6f5c652e.pdf", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "neufeld|recurrent_neural_network_architecture_based_on_dynamic_systems_theory_for_data_driven_modelling_of_complex_physical_systems", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=7LFDmRoxnf", "_bibtex": "@misc{\nneufeld2021recurrent,\ntitle={Recurrent Neural Network Architecture based on Dynamic Systems Theory for Data Driven Modelling of Complex Physical Systems},\nauthor={Deniz Neufeld},\nyear={2021},\nurl={https://openreview.net/forum?id=_CrmWaJ2uvP}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "_CrmWaJ2uvP", "replyto": "_CrmWaJ2uvP", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1695/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538112839, "tmdate": 1606915780114, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1695/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1695/-/Official_Review"}}}], "count": 11}