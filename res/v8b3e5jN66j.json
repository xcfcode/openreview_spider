{"notes": [{"id": "v8b3e5jN66j", "original": "xU51l4X39RR", "number": 1947, "cdate": 1601308214570, "ddate": null, "tcdate": 1601308214570, "tmdate": 1615935592889, "tddate": null, "forum": "v8b3e5jN66j", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "_P0dJOrvaUo", "original": null, "number": 1, "cdate": 1610040383916, "ddate": null, "tcdate": 1610040383916, "tmdate": 1610473977247, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "This paper addresses the problem of how best to sample hard negatives during contrastive learning, a topic of importance for the recently resurgent field of metric learning / contrastive loss-based unsupervised representation learning. Backed by theoretical results for a new low-variance version of the NCE, the paper proposes an easy-to-implement \"Ring\" method for selecting negatives that are at just the right level of difficulty, neither too hard nor too easy.\n\nHappily, this is a paper that has improved significantly through the interactive peer review of a dedicated set of reviewers combined with prompt responses from the authors. Perhaps the result that tipped this paper over the line in my assessment: the new experimental results now show significant gains from applying the \"Ring\" approach for hard negative sampling to near-state-of-the-art implementations of the MoCo-v2 approach, which is among the leading unsupervised visual feature learning approaches. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040383903, "tmdate": 1610473977230, "id": "ICLR.cc/2021/Conference/Paper1947/-/Decision"}}}, {"id": "O39kBpFN7wz", "original": null, "number": 4, "cdate": 1604631895135, "ddate": null, "tcdate": 1604631895135, "tmdate": 1606802594555, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review", "content": {"title": "Interesting direction, presentation and experiments need to be enhanced", "review": "This paper propose to sample effective hard negative samples in contrastive learning, conditioned on given anchor point. Authors proved that their new objective is a more biased lower bound than InfoNCE, but with less variance. Experiments on several datasets as well as transferring datasets show some promising results.\n\nPros:\n- Conditional negative sampling in contrastive learning is currently less studied, and this paper starts on this direction.\n- The relative improvement of the new objective function upon multiple popular contrastive learning methods have been verified.\n- The strategy in final realization is easy to implemented (though this realization is not very closely coherent to their theoretical analysis). In the analysis, they specify expectation value c as bound, while in experiments, the tuning of w_l can be tricky and a bit hacky.\n- The implementation can be simple, which I appreicaite.\n\nCons:\n- The written is not very good. Specifically, I found the usage of p and q in section 3 is a bit hard to follow. Specifically, I did not follow the sentence, \"The remaining probability mass assigned by p to elements outside S_B is renormalized to sum to one...\", do you mean conditional p or marginal p, or else? Overall, the section 3 is not well clarified for me\n- While the experimental explicitly remove \"two closed\" samples, this part is completely missing in the theoretical analysis. E.g., the authors did not discuss the cases that the c threshold in Theorem 3.1 is too high such that set B only contains positive samples.\n- The authors do not prove, in experience, that whether such improvement still holds on more solid baselines with larger networks and recent contrastive learning tricks. E.g., how would MoCoRing works with MoCo v2 baseline with 800 epochs training of R-50 model on ImageNet?\n- Specifically for MoCo which maintains a FIFO queue, I am not convinced that the improvement mainly comes from hard negatives. It might because it mainly comes from removing \"too close\" negatives which are false negatives. I am imagine the case that, if we have labels to remove all false negatives in MoCo for each anchor, then all remaining negatives are true negatives. Then, does it still help to remove easy negatives to certain extent, or it's better to keep all true negatives you have in the queue? This would be some interesting experiments to see.\n- The results on MS COCO is way off. I do not believe that switching from R-50 to R-18 will lead such a significant drop in AP, e.g. AP of bounding box drops from ~38 to ~10. \n\nOverall, I have a mixed feeling over this paper, and would appreciate authors response. \n\n======= updated ======\nThe authors response partially addressed my concerns and I would raise the rating to 6.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107250, "tmdate": 1606915767850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1947/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review"}}}, {"id": "Z8RO2Nmj0xc", "original": null, "number": 16, "cdate": 1606266761344, "ddate": null, "tcdate": 1606266761344, "tmdate": 1606268377750, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Updated MoCo-v2 Results", "comment": "Multiple reviewers rightfully addressed concerns that our baseline numbers did not match prior work due to a difference in setup. In our paper, we tried to use a consistent setup that ended up sacrificing performance on baselines. We agree that it is also important to show that gains from using Ring persist on top on the best performing setups.\n\nAs such, we studied the hyperparameters used in the SimCLR repos suggested by Reviewers 2 and 3, as well as the Pytorch Lightning Bolts repo (https://github.com/PyTorchLightning/pytorch-lightning-bolts) which has a MoCo-V2 implementation to match our setup to those described in the papers as best as possible. After several attempts and changes, we arrived at the following (new) results:\n\nCIFAR10 ResNet18 | MoCo-v2 90.09 | MoCoRing-v2 91.96 (+1.87)\n\nSTL10 ResNet18 | MoCo-v2 74.81 | MoCoRing-v2 76.72 (+1.91)\n\nCIFAR100 ResNet18 | MoCo-v2 65.18 | MoCoRing-v2 67.29 (+2.11)\n\nOur baselines now more closely match prior work (~90% on CIFAR10, ~75% on STL10 and a 3% incease in CIFAR100 from our last results). Additionally, we find that using Ring still increases the performance by 1.6-2.1%. For the final revision, we will include these in the main paper.\n\nAdditionally, we know that using larger architectures is also important. As such, we show Resnet-50 results for CIFAR10:\n\nCIFAR10 ResNet50 | MoCo-v2 92.47 | MoCoRing-v2 94.10 (+1.63)\n\nThe gain from using Ring remains significant! (We also note that these two models were only trained for 200 epochs and may see further increases given the time to finish 800 epochs.). Overall, we believe these results, along with the ones in our paper which standardize the setups, to be good evidence that the Ring algorithm is improving the representations in a meaningful manner.\n\nFinally, we would like to thank the reviewers for taking the time to respond and provide continued feedback. We feel that our paper has become more clear and stronger in the rebuttal process and credit the reviewers for this."}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "VuSbP8WSwQD", "original": null, "number": 18, "cdate": 1606267218069, "ddate": null, "tcdate": 1606267218069, "tmdate": 1606267218069, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "R_WestABR2Z", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Thanks for the feedback!", "comment": "We are glad that we were able to address your initial concerns (regarding efficiency and generalization). To discuss the additional concerns about baselines, we provide some thoughts below:\n\n> Reported baseline results are too low.\n\nThis was also raised by R3 and we fully agree that it is also meaningful to show improvements with Ring on the best performing baseline setups. We showed new results with MoCo-V2 in a new  general post above and hope the reviewer can refer to the updated numbers. We tried to pick the best documented setup for MoCo-v2 on these three datasets and improved the baseline numbers to match expected ones. On top of these, we show that MoCo-v2-Ring still has a 1.6-2.1% gain, which we find significant.\n\nSecond, we want to clarify that the results in Table 6 are for SimCLR with Resnet-18 whereas the results in https://github.com/HobbitLong/SupContrast are for Resnet-50. If we refer to another repo (https://github.com/AidenDurrant/SimCLR-Pytorch) which shares results for Resnet-18 SimCLR on CIFAR10, we find 88% linear evaluation, which is what we have in Table 6. Since Table 6 is more motivation for future contributions rather than our core results, we think Resnet-18 results should suffice. However, if the reviewer would prefer to have Resnet-50, we can do so in the final revision.\n\n> The method should clarify its assumption and limitation.\n\nYes! We will make this assumption clear. I updated a new revision with some edits: (1) in the contributions bullet points, I narrowed our claims to \"contrastive algorithms that utilize a memory structure\", and (2) in the end of the background (before Section 3), I state clearly that we restrict ourselves to contrastive algorithms with a memory structure. Finally, the discussion with batch-based negative sampling should hopefully make it more clear that extending to SimCLR and the like is future work.\n\n> Significant revision during the discussion period.\n\nWe are very grateful for the opportunity to discuss and revise, making our contributions more precise and our experiments detailed. Although our revisions were significant, we have not deviated greatly from our initial claims and have only done additional work to clarify and experimentally strengthen them."}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "R_WestABR2Z", "original": null, "number": 15, "cdate": 1606197364142, "ddate": null, "tcdate": 1606197364142, "tmdate": 1606197909820, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "7SNPnc4BQYM", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Thanks to the Response", "comment": "I sincerely read the rebuttal and other reviewers' reviews.\n\n**Method/Results**\n\nMy concerns on run-time cost and batch extension are mostly addressed as the paper mainly considers the approaches using a memory bank (e.g., IR/MoCo). I also appreciate the authors' efforts to extend for SimCLR by subsampling negatives from the mini-batch, though the improvement seems to be less significant compared to IR/MoCO.\n\nDespite the partial extension for SimCLR, I believe the paper should more emphasize the primary focus is IR/MoCo, as the assumption of memory bank definitely restricts the applicability of the algorithm (though MoCo is also state-of-the-art as the rebuttal claimed).\n\nFurthermore, the performance reported in Table 6 seems too low. While the paper reports 90% on CIFAR-10 and 64% on CIFAR-100 for SimCLR, other implementations (including the original paper) report 93% for CIFAR-10 and 70% for CIFAR-100 (e.g., see https://github.com/HobbitLong/SupContrast).\n\nAs Reviewer 3 also highlighted, the proposed technique should be validated upon the carefully tuned baseline (remark that many meta-analyses discovered that the baseline method outperforms the sophisticated methods, with careful optimization).\n\n**Presentation**\n\nI found that the revised draft significantly improved the presentation. However, I'm not sure this rushing for the submission and revision during the discussion period is a good practice for OpenReview.\n\n**Overall**\n\nMy initial rating was on the unclear applicability of the method (e.g., for SimCLR), and the concern is addressed as the rebuttal clarified the primary focus of the paper is IR/MoCo.\n\nHowever, there are still some remaining concerns:\n1. Reported baseline results are too low (also highlighted by R3).\n2. The method should clarify its assumption and limitation.\n3. Significant revision during the discussion period.\n\nThus, I currently recommend rating 5 (mostly due to concern 1)."}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "mznPX1ss1Cp", "original": null, "number": 1, "cdate": 1603894242570, "ddate": null, "tcdate": 1603894242570, "tmdate": 1606197377018, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review", "content": {"title": "Reasonable direction, but needs more improvements", "review": "This paper adopts semi-hard negative mining, a sampling strategy widely used for metric learning, for contrastive self-supervised learning. Specifically, the paper chooses the negative samples in the range of $[w_l, w_u]$ percentiles (close, but not too close) in terms of the normalized feature distance. As the initial representation is not informative, the paper anneals down the percentile range. This sampling strategy improves the contrastive learning methods (IR, CMC, MoCO).\n\nThe paper has some good points:\n- Applying semi-hard negative mining for contrastive learning is reasonable.\n- Discussion on the property of the proposed estimator, CNCE.\n- Empirically validate the proposed method improves the contrastive learning methods.\n\nHowever, the paper needs more improvements for both method and presentation.\n\n\n**Concerns in method**\n\nA. Choice of the hyperparameters $[w_l,w_u]$.\n\nChoosing \"close, but not too close\" samples is ambiguous and may depend on datasets, networks, and training methods. Is there some principle to choose hyperparameters? I checked both the main text and appendix but could not find how the paper selected the hyperparameters for experiments.\n\nB. Cost of the negative mining\n\nSearching negative samples for each update is quite expensive. How much the training time increased compared to the vanilla contrastive learning methods? Providing the training trend curve of the vanilla model and negative mining (using the clock time as an x-axis) would be insightful. It would also be great to discuss how to reduce the cost, e.g., use approximated nearest neighborhood search.\n\nC. Negative mining for the *batch* setting?\n\nFor a single sample of $x_i$, it is easy to find the semi-hard negative samples. However, how to construct the batch $\\{x_i\\}$ such that each sample is effective negatives for the other samples? The batch should contain diverse samples; it would be interesting to consider the determinantal point process or submodular optimization formulation.\n\n\n**Concerns in presentation**\n\nThere are lots of imprecise or undefined terms, unclear or unkind expressions, and typos. Here are some examples:\n- Eq. (1) assumes to use $k$ negative samples ($i \\notin \\{1,...,k\\}$ for a positive sample $i$), but Theorem 3.1 assumes to use $k-1$ negative samples\n- The definition of the CNCE estimator comes after the property of it (Theorem 3.1)\n- The definition of $S_B$ comes after the property of it. Also, it would be kinder to say \"Assume $p(S_B) > 0$ for $S_B = \\sim$\" to \"Assume that the set of random variables $S_B := \\sim$ has a non-zero probability, i.e., $p(S_B) > 0$\"\n- \"For Borel $A$\" $\\to$ \"For a Borel set $A$\"\n- \"Figure 1:\" $\\to$ \"Figure 1: Visual illustration of ring discrimination\"\n- In Algorithm 1, do tx1 and tx2 receives the same input $x$?\n- The evaluation metric \"Transfer Acc.\" is not defined. Also, the term can be confused with \"transferring features\". Why not use the standard terminology \"linear evaluation\"?\n- In Table 1, \"three\" image domains $\\to$ \"four\" image domains\n\n\n**Other comments**\n\nTons of similar techniques are concurrently proposed. It would be informative to discuss the relation with those works.\n- Contrastive Learning with Hard Negative Samples\n- Are all negatives created equal in contrastive instance discrimination?\n- Self-supervised representation learning via adaptive hard-positive mining\n- What Should Not Be Contrastive in Contrastive Learning\n- Contrastive Learning with Stronger Augmentations\n\nIs the sentence \"A better representation would contain more \"object-centric\" information, thereby achieving a higher classification score.\" has some logical/empirical supports? Does \"good\" representation (in terms of downstream task performance) have some relation (in both directions) with the \"object-centric\" representation?", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107250, "tmdate": 1606915767850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1947/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review"}}}, {"id": "45xfPj70D6i", "original": null, "number": 11, "cdate": 1606066032400, "ddate": null, "tcdate": 1606066032400, "tmdate": 1606070893758, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "7SNPnc4BQYM", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Continued Response", "comment": "> What is the relation between \"object-centric\" representations and downstream tasks?\n\nIt is a good question because it is not always the case that a good contrastive representation captures object properties. In general, this relationship is set by the downstream tasks. In vision, the popular downstream tasks such as classification, detection, or segmentation are all properties of objects in the image. (For a more thorough discussion in the object detection setting, we refer to \u201cWhat makes instance discrimination good for transfer learning\u201d by Zhao et al.) As such, we would expect a representation that is highly performing to contain information about object features in the image. We will change the language in the text to be more precise that this is specific to the visual downstream tasks commonly studied in the field. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "AeZzXivjqm", "original": null, "number": 10, "cdate": 1605731543174, "ddate": null, "tcdate": 1605731543174, "tmdate": 1605731564804, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "JYyEpUBjdg6", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Linear Evaluation with MoCo-V2", "comment": "We bring an additional result to the attention of the reviewers. We compare the performance of MoCo-v2 and MoCoRing-v2 using linear evaluation on CIFAR10, CIFAR100, and STL10. We found improvements of 3.3%, 3.2%, 1.7% absolute points, respectively by using Ring. This is comparable to the benefits of MoCoRing over MoCo. Please refer to the response to AnonReviewer3 for more details. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "O69srnS8shW", "original": null, "number": 9, "cdate": 1605731189103, "ddate": null, "tcdate": 1605731189103, "tmdate": 1605731264062, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "_qhzaLJj8DI", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Added MoCo V2 Experiments", "comment": "> Do the results generalize to MoCo V2?\n\nThis is a good question as we claim our methods to be general. To test this, we compare MoCo-v2 and MoCoRing-v2 on CIFAR10, CIFAR100, and STL10. The changes from MoCo to MoCo-v2 are (1) using different image augmentations (e.g. gaussian blur and different color jittering), and (2) using a MLP projection head. No changes are needed to the Ring formulation.\n\nSTL10 | MoCo-v2  | 62.3\n\nSTL10 | MoCoRing-v2 | 64.0 (+1.7)\n\nCIFAR10 | MoCo-v2 | 84.7 \n\nCIFAR10 | MoCoRing-v2 | 88.0 (+3.3)\n\nCIFAR100 | MoCo-v2 | 62.4\n\nCIFAR100 | MoCoRing-v2 | 65.6 (+3.2)\n\nWe find consistent and significant improvements over MoCo-v2 that parallel the improvements found on MoCo. We note that MoCo-v2 / MoCoRing-v2 perform worse than MoCo / MoCoRing on STL10; we suspect this to be due to the smaller dataset size. For the next revision, we will add MoCo-v2 results for ImageNet, meta-dataset, and the object detection / segmentation experiments. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "7SNPnc4BQYM", "original": null, "number": 8, "cdate": 1605606152896, "ddate": null, "tcdate": 1605606152896, "tmdate": 1605606666697, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "mznPX1ss1Cp", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Rebuttal Response", "comment": "We thank the reviewer for their thought-provoking questions. \n\n> how are the $w_\\ell$ and $w_u$ chosen?\n\nFirst we pick numbers $0 \\leq \\ell < u \\leq 100$ to represent percentiles. For the current example $x_i$, we compute embedding distances between all $x \\in \\mathcal{D}$and $x_i$ (via memory bank). We sort and exponentiate the distances, and choose $w_\\ell$ and $w_u$ to be the distances at the $\\ell$-th and $u$-th percentiles.  In experiments, we initially choose $u$ to be 100 and decrease it over training to 10 with a linear schedule. We fix $\\ell = 1$. \n\n> What is the run-time cost of searching for negative samples?\n\nThe cost of searching for negative samples is not substantial as a happy consequence of the reliance on memory banks: (1) there is no need to re-compute embeddings and (2) we do not backpropagate through the search. Therefore, the cost of searching is a single dot product between two matrices. In Section 7 of our revision, we added timing experiments over 4 datasets. We find on average a 1.2x increase, which we judge acceptable in the context of deep learning.\n\n> How do you handle the batch setting?\n\nGreat question! We focused on IR and MoCo as sampling negatives in this setting is both simple and effective. For algorithms like SimCLR, where the negatives are the other elements in the minibatch, the problem is more nuanced.  We believe this is a substantial addition and not within the scope of this paper. Further, as MoCo and SimCLR are both state-of-the-art, we believe improving MoCo already pushes the boundaries.\n\nHowever, we are also interested and tested an idea:\n\nOur initial thoughts were also to encourage diversity in the batch. However, we realized it is less straightforward: recall that in NCE, the examples $x_i$ are picked i.i.d. from the marginal $p(x)$ (see the first expectation in Eq. 1). Using CNCE does not change this. So, if we construct the batch ourselves based on $q$, the examples $x_i$ are no longer distributed according to $p(x)$.\n\nWe opt for a simpler approach: for each element in the minibatch, we only treat a subset of the minibatch as its negatives. We choose this subset with a lower and upper percentile based on embedding distance to the current element.  We added new experiments for SimCLR + Ring in Section 7. We find consistent but modest gains in 3 datasets, leaving plenty of room for future improvements. In our opinion, even these modest gains are non-trivial as one might have expected using more negatives to always be better. \n\n> In Alg. 1, do $t_1(x)$ and $t_2(x)$ receive the same input? \n\nYes, MoCo compares embeddings of two transformations of the same image in the numerator.\n\n> A more thorough comparison with related work would be useful.\n\nWe thank the reviewer for the pointer to concurrent work. These papers were new to us and we found them interesting to read. We discuss two of the papers we found most relevant to our work.\n\nContrastive Learning with Hard Negative Samples (Robinson et. al.): this paper studies sampling hard negatives in contrastive learning as well. However, the negative distribution, called $q$ in both papers, is parameterized differently. Let $p$ be the marginal distribution. Both papers reweight the probability assigned by $p$ to every example: in theirs, this is done with an exponential term while in ours, this is done by restricting the support.  I believe there are strengths and weaknesses to both approaches. Their approach is more easily applied to SimCLR. But their approach still samples negatives from $p$ and upweights harder ones. If for example, the training dataset contains only a small set of hard negatives, choosing iid from $p$ may not find any, despite reweighting. In contrast, Ring subsets the possible samples to a set of harder negatives, so every negative must be at least a minimum level of \u201cdifficulty\u201d. Finally, it is not clear if their approach preserves a lower bound on I(X;Y). \n\nAre all negatives created equal in contrastive instance discrimination? (Cai et. al.): This work does a thorough experimentation showing that the easiest negatives can be discarded and the hardest negatives are harmful. In our Appendix E (Figure 2), we also show that using hard negatives early in training hurts the representation. This observation has also been studied in deep metric learning: Wu. et. al. 2017 uses this to motivate the necessity of \u201csemi\u201d-hard negatives. Even Robinson et. al. discusses this in Section 6.1. The algorithm in Cai et. al. is quite similar to MoCoRing. That being said, the two papers show complementary findings: they focus on efficiency of using fewer negative samples whereas we show that you can also learn stronger representations by annealing. Further, we provide theory to ground this type of approach in mutual information.\n\nTo briefly mention the other three papers, we found them less relevant as they focused on finding better augmentations, which is a complementary problem to better negatives. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "Lp2yUJZj30F", "original": null, "number": 7, "cdate": 1605604453443, "ddate": null, "tcdate": 1605604453443, "tmdate": 1605604524245, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "k7SdmUQuKOw", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Rebuttal Response", "comment": "We thank the reviewer for their clarifying questions and comments. \n\n> The background is too long and perhaps, unrelated.\n\nWe agree and have shortened the background significantly, especially the IR and MoCo explanations.\n\n> Is the approach general or specific to applications in vision?\n\nWe rewrote the introduction to cut out details not relevant to our contributions. We also explicitly listed our contributions at the end of Section 1 in the revision. \n\nTo summarize briefly, we present negative sampling specifically for contrastive learning of visual representations. We also focus on a class of algorithms that optimize NCE under lossy image transformations that are basis for recent breakthroughs (Wu et al., 2018; Oord et al., 2018; Hjelm et al., 2018; Zhuang et al., 2019; Henaff et al., 2019; Misra & Maaten, 2020; He et al., 2019; Chen et al., 2020a;b; Grill et al., 2020). \n\nThat being said, we also believe Ring Discrimination should generalize to other modalities where contrastive algorithms have been successful e.g. speech (Oord et al., 2018; Al-Tahan & Mohsenzadeh, 2020; Kharitonov et. al. 2020). To showcase this generality, we added a new suite of experiments in Section 7 that compare speech representations learned by MoCo and MoCoRing on LibriSpeech using 6 transfer tasks. We find significant gains of up to 4% from using MoCoRing. We believe this is strong evidence for the generality of our algorithm.\n\n>  How would you actually define the $A_1$ to $A_k$ in Theorem 3.1?\n\nGood catch! We rewrote the text of Theorem 3.1 to be more clear. In particular, the sets $A_1$ to $A_k$ are each from the Borel $\\sigma$-algebra over $\\mathbf{R}^d$ (assuming $d$-dimensional random variables). \n\n> Why choose the dot product as the condition to build $q$?\n\nIn Theorem 3.1, we construct $q$ according to an arbitrary similarity function $f: X \\times Y \\rightarrow \\mathbf{R}$. So for the general case, we are not limited to a dot product. \n\nWhen we apply CNCE to contrastive learning (e.g. Ring Discrimination), we purposely parameterize $f(x,y) = g(t(x))^T g(t'(y))$, the dot product between embeddings of transforms of two images, $x$ and $y$. We do this to align to prior work: IR, CMC, LA, MoCo, SimCLR and many more use dot-product between embeddings as the distance function. This is likely due to wanting to learn encoders that linearize the useful information in a domain.\n\n> How does Ring Discrimination connect to Theorem 3.1?\n\nIf possible, we encourage the reviewer to read our revised Section 3 and 4.\n\nTo clarify Theorem 3.1, suppose we have two random variables $X$ and $Y$ and fix a value $x$. The theorem asks us to choose a set $B$ of real-valued numbers. Then, the set $S_B$ contains all values of $y$ such that $e^{f(x,y)}$ is in $B$. We should think of $S_B$ as the support of the distribution we will construct. The choice of $B$ lets us decide how close we want the negative examples to be to $x$. Then we define a distribution $q(y|x)$ as equal to $p(y)$ for all y in $S_B$ but renormalized to sum to 1. The utility of Theorem 3.1 is to show that replacing $p(y)$ with $q(y|x)$ in NCE builds a new estimator that still lower bounds I(X; Y). We call this estimator CNCE.\n\nRing is a special case of CNCE for contrastive learning. First, the similarity function is parameterized as $f(x,y) = g(t(x))^T g(t\u2019(y))$. Second, we set $B = [w_\\ell, w_u]$, a closed interval defined by two real-valued thresholds $w_\\ell$ and $w_u$.  Now, $S_B =${$y: e^{g(t(x))^T g(t'(y))} \\in B$}, the set of all $y$ whose exponentiated distance to $x$ is in $B$. So, defining $q(y|x)$ on $S_B$ will guarantee that samples will not be \"too far\" from x and also not \"too close\". This is precisely Ring Discrimination. So, Theorem 3.1 covers this choice of B although it is more flexible (e.g. B need not be contiguous).\n\nIn code, we pick two numbers $0\\leq \\ell < u \\leq 100$ representing percentiles. We compute exponentiated distances of each example in the dataset with x. Sorting these, we can find the distance at the $\\ell$-th and $u$-th percentile and set these to be $w_\\ell$ and $w_u$.\n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "vPMWcq1qhdU", "original": null, "number": 6, "cdate": 1605604185654, "ddate": null, "tcdate": 1605604185654, "tmdate": 1605604185654, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "TjKkfur3d9n", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Rebuttal Response", "comment": "We thank the reviewer for the positive and useful review!\n\n> There has been a rich history of research in semi-hard negative mining from deep metric learning that is worth mentioning.\n\nIn the new revision, we now mention metric learning in the abstract and introduction to make obvious our inspiration from its literature. In Section 6, we reference several works from deep metric learning that we found to be similar to our approach, e.g. Schroff et. al. (2015), Oh Song et. al. (2016), Wu et. al. (2017), Yuan  et. al. (2017), which describe semi-hard negative mining procedures. We draw inspirations from these approaches but find the application to modern contrastive learning to be novel and relevant for future work. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "_qhzaLJj8DI", "original": null, "number": 5, "cdate": 1605603416297, "ddate": null, "tcdate": 1605603416297, "tmdate": 1605604045516, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "O39kBpFN7wz", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Rebuttal Response", "comment": "We thank the reviewer for their thoughtful comments.\n\n> The sentence \"The remaining probability mass assigned by p to elements outside $S_B$ is renormalized to sum to one...\" is unclear.\n\nSay we construct an initial distribution $q^*(y|x)$ to be equal to the marginal distribution, $p(y)$, for all $y \\in S_B$ (we should think of $S_B$ as the support we want our constructed distribution to have). But $S_B $ is a subset of the support of $p(y)$ so $q^*(y|x)$ does not sum to 1. To make it a valid probability distribution, we need to normalize $q^*(y|x)$. Let $q(y|x) = q^*(y|x) / \\sum_{y \\in S_B} q^*(y|x)$. But this is the same as $q(y|x) = q^*(y|x) / p(S_B)$.\n\n> Removing negative examples that are \u201ctoo close\u201d to the current example is not covered by Theorem 3.1.\n\nThis is a good question and a good opportunity for us to clarify Theorem 3.1 and the relationship of CNCE to Ring. \n\nTo clarify Theorem 3.1, suppose we have two random variables $X$ and $Y$ and fix a value $x$. The theorem asks us to choose a set $B$ of real-valued numbers. Then, the set $S_B$ contains all values of $y$ such that $e^{f(x,y)}$ is in $B$. We should think of $S_B$ as the support of the distribution we will construct. The choice of $B$ lets us decide how close we want the negative examples to be to $x$. Then we define a distribution $q(y|x)$ as equal to $p(y)$ for all y in $S_B$ but renormalized to sum to 1. The utility of Theorem 3.1 is to show that replacing $p(y)$ with $q(y|x)$ in NCE builds a new estimator that still lower bounds I(X; Y). We call this estimator CNCE.\n\nRing is a special case of CNCE for contrastive learning. First, the similarity function is parameterized as $f(x,y) = g(t(x))^T g(t\u2019(y))$. Second, we set $B = [w_\\ell, w_u]$, a closed interval defined by two real-valued thresholds $w_\\ell$ and $w_u$.  Now, $S_B =${$y: e^{g(t(x))^T g(t'(y))} \\in B$}, the set of all $y$ whose exponentiated distance to $x$ is in $B$. So, defining $q(y|x)$ on $S_B$ will guarantee that samples will not be \"too far\" from x and also not \"too close\". This is precisely Ring Discrimination. So, Theorem 3.1 covers this choice of B although it is more flexible (e.g. B need not be contiguous).\n\nIn code, we pick two numbers $0\\leq \\ell < u \\leq 100$ representing percentiles. We compute exponentiated distances of each example in the dataset with x. Sorting these, we can find the distance at the $\\ell$-th and $u$-th percentile and set these to be $w_\\ell$ and $w_u$.\n\n> What if the threshold $c$ is so high that the set $S_B$ only contains positive samples?\n\nGreat question! Pick a value $x$. Recall that $c$ is the average exponentiated distance between $x$ and $y$ over all $y \\sim p(y)$. So unless the data distribution is such that most examples are positive samples of $x$, it should not be the case that $c$ is so high that whatever $S_B$ we choose contains only positive samples. For example, given an image $x$ from ImageNet. For $S_B$ to only contain positive samples would mean that the \u201caverage image\u201d is as close to $x$ in representation as an image of the same class as $x$.\n\n> Do experiments generalize to larger architectures e.g. R-50?\n\nIdeally, we would be able to show experiments on a diverse range of architectures and hyperparameters. However, fitting 800 epochs of R-50 is very computationally intensive and difficult for us to do. Instead, we hoped to showcase consistent improvements on different objectives, transfer tasks, and image distributions (along with the new speech and SimCLR experiments: see Section 7). Together, we found this convincing that the method is generalizable. \n\n> How do you compare the effects of removing close negatives versus using hard negatives. \n\nThis is a great question! We actually had experiments comparing the importance of removing \"too hard\" negatives vs \"too easy\" negatives in Table 2 on page 6. The rows for \"IRing\" represent sampling negatives whose embedding distance to the current example is between the $\\ell$-th and $u$-th percentile, thereby removing those in the $[0, \\ell)$ and $(u, 100]$ percentiles. The rows for \u201cIRing ($\\ell$ = 0)\u201d therefore sample negatives in the $[0, u]$ percentiles, keeping the ones closest to the current example. Thus, these two models directly compare the effect of removing \u201ctoo close\u201d negatives. We see gains from only removing \u201ctoo far\u201d negatives (by 0.9 points in CIFAR10 and 4.1 points in ImageNet). We see additional gains from also removing the \u201ctoo close\u201d negatives (by 1.8 points in CIFAR10 and 1.1 points on ImageNet). We can reasonably conclude that both play an important role, motivating why in our main experiments, we remove both \u201ctoo far\u201d and \u201ctoo close\u201d negatives.\n\n> MS COCO results seem too low.\n\nTable 4 reported use of a \u201cfrozen\u201d R-18 encoder. That is, we do not finetune the parameters. While this results in lower performance, we do believe it to be a fair test of the quality of the representation in capturing object properties. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "JYyEpUBjdg6", "original": null, "number": 4, "cdate": 1605602757065, "ddate": null, "tcdate": 1605602757065, "tmdate": 1605602757065, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment", "content": {"title": "Updated submission with revisions.", "comment": "We thank the reviewers for their positive feedback and important suggestions. We rewrote substantially for more clarity and simpler language. Below, we provide a \u201cdiff\u201d of the main changes to text in the new revision. \n\n- Tightened the introduction to be more clear on scope. Replaced general statements with more focus on motivation of improving visual contrastive algorithms. We include a list of contributions at the end of Section 1.\n- Shortened background to focus on more essential details.\n- Rewrote Section 3 for clarity. We do not unnecessarily abbreviate terms and try to be clear about which distributions we are referencing. We simplified the notation as well.\n- Added an example below Theorem 3.1 to give the reader context.\n- Rewrote Section 4 to clarify connection to Section 3.\n- Mentioned the inspiration for our method from metric learning earlier in paper.\n- Added Figure 2 as visual support for Section 4 (Ring Discrimination).\n- Fixed many typos and simplified language.\n- Added Section 7 with three sets of additional experiments: (1) timing costs comparing IR to IRing and MoCo to MoCoRing (we find increases of only 1.2x per epoch); (2) application of MoCoRing to learning speech representations (we find significant gains of up to 4% absolute over 6 transfer tasks); (3) application of Ring to SimCLR (where we find modest but consistent improvements). \n\nWe aim to address individual concerns below. We hope reviewers can use the revised text for reference.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "v8b3e5jN66j", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1947/Authors|ICLR.cc/2021/Conference/Paper1947/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923853962, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Comment"}}}, {"id": "k7SdmUQuKOw", "original": null, "number": 2, "cdate": 1603960610293, "ddate": null, "tcdate": 1603960610293, "tmdate": 1605024322265, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review", "content": {"title": "1947 Initial review", "review": "\n\nBriefing:\n\nThis is an interesting paper that discusses the negative sample mining in visual representation learning. The authors discuss the theory and method to conditionally select the negative samples based on the dot product of representations in noise constructive estimation (NCE). Their theory shows that the NCE with negative examples sampling from a conditional distribution q is lower bounded with mutual information, and the object has higher bias and lower variance. The authors also provide the method to construct the conditional distribution by picking a ring surface where the dot product of representations is bounded within percentiles of data.\n\n#######################################################\n\nPros:\n\n\nThe topic of this paper is popular and interesting. The negative sample mining strategy in unsupervised representation learning is well discussed and found effective in recent research.\n\nThe experiments show the effectiveness of their method compared with the original NCE methods.\n\n#######################################################\n\nCons:\n\n\nThis paper's writing quality is limited, and this makes some points that are not easy to understand.\n\nWhy introduce many such details about IR and MoCo? It seems they are not heavily related to the contribution of this paper.\n\nThe paper limits the exploration of NCE to image representation learning with transform functions. This is not clear in the introduction section and may make readers confused about the contribution and purpose of the introduction. Can this theory and strategy be extended to other representation problems? \n\nThe claim of Borel sets A in theory 3.1 is not clear enough. How would you actually define the A2 to Ak? The proof of 3.1 is also unclear, and the notations here are in a mess.\n\nThe motivation to choose the way to construct q is still unclear. Why chose the dot product as the condition to build q? What's the connection between the \"ring discrimination\" and theory 3.1? It seems the sample selection strategy is not well supported by the theory the authors claim.\n\n#######################################################\n\nPlease address and clarify the cons above in the rebuttal period.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107250, "tmdate": 1606915767850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1947/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review"}}}, {"id": "TjKkfur3d9n", "original": null, "number": 3, "cdate": 1604323292612, "ddate": null, "tcdate": 1604323292612, "tmdate": 1605024322128, "tddate": null, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "invitation": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review", "content": {"title": "A good paper that tackles a very important problem in unsupervised learning", "review": "# Summary\n\nInspired by the effectiveness of hard negative mining in deep metric learning, this papers focuses on the problem of negative mining in unsupervised learning under the contrastive setting. One of the problems in this scenario is that naively selecting difficult negatives may yield an objective that no longer bounds mutual information, which is the basis for many contrastive objectives such as the Noise Contrastive Estimator. To address this problem, this paper formally defines a family of conditional distributions where negatives can be drawn from (negatives are chosen conditional on the current instance), while maintaining a lower bound on the NCE and on mutual information, resulting in a new estimator dubbed Conditional NCE. It also shows that, even though it\u2019s a looser bound than NCE, it also has lower variance, which may lead to better local optima. Finally, within this family of conditional distributions, the paper proposes the Ring model, which takes inspiration from semi-hard negative mining approaches, and that can be applied to state-of-the-art contrastive algorithms in order to sample harder negatives, resulting in better representations.\n\n\n# Pros\n\nThe paper tackles a typically disregarded problem in contrastive learning: hard negative mining. It shows the importance of selecting difficult negatives to obtain stronger unsupervised representations.\n\nThe method and contributions are very well motivated and introduced. The paper is also very well written, and includes a thorough review of the background needed in order to understand the proposed approach.\n\nThe strongest contribution of the paper is the definition of this family of conditional distributions, the CNCE, and the proof that it remains a lower bound on NCE, preserving the relation of contrastive learning to mutual information. Not less important is the proof that even though CNCE is a lower bound on NCE, it has however lower variance, leading to better local optima.\n\nThe Ring model results in a simple method that can be applied to any contrastive algorithm resulting in a better representation that outperforms existing approaches by a significant margin. This is shown in a thorough and extensive experimental analysis, which includes several benchmarks, transfer tasks, and ablation studies to validate the different components of the proposed approach.\n\n\n# Cons\n\nA minor comment is that the Ring model takes a lot of inspiration from other works in deep metric learning, which might limit its novelty. The concept of semi-hard negative mining is well-know in the field of deep metric learning and has been studied in numerous papers that have proposed more sophisticated approaches. I understand however that this wasn\u2019t the main focus of the paper, being mainly paving the road to the exploration of negative sampling in contrastive learning.\n\n\n# Recommendation\n\nMy overall recommendation is accept. The paper not only tackles a very important and typically disregarded problem in contrastive learning, but does it in a way that opens up the door to future research on this topic. It is also interesting to see hard negative mining from the mutual information point of view, making sure that by adding these conditional distributions the contrastive objective still remains a lower bound.\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1947/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1947/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Conditional Negative Sampling for Contrastive Learning of Visual Representations", "authorids": ["~Mike_Wu1", "mmosse19@stanford.edu", "~Chengxu_Zhuang1", "~Daniel_Yamins1", "~Noah_Goodman1"], "authors": ["Mike Wu", "Milan Mosse", "Chengxu Zhuang", "Daniel Yamins", "Noah Goodman"], "keywords": ["contrastive learning", "hard negative mining", "mutual information", "lower bound", "detection", "segmentation", "MoCo"], "abstract": "Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two transformations of an image. NCE typically uses randomly sampled negative examples to normalize the objective, but this may often include many uninformative examples either because they are too easy or too hard to discriminate. Taking inspiration from  metric learning, we show that choosing semi-hard negatives can yield stronger contrastive representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a \"ring\" around each positive. We prove that these estimators remain lower-bounds of mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5% absolute points in each case, measured by linear evaluation on four standard image benchmarks. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and key-point detection.", "one-sentence_summary": "Theoretical and experimental evidence that choosing difficult negative examples in contrastive learning can learn stronger representations as measured by several downstream tasks and image distributions.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|conditional_negative_sampling_for_contrastive_learning_of_visual_representations", "supplementary_material": "/attachment/e46644e3d6334c5fcda96d8be492449a03c3b633.zip", "pdf": "/pdf/4198b5cda9f9be5f974d70d54ad518d2c6893b99.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nwu2021conditional,\ntitle={Conditional Negative Sampling for Contrastive Learning of Visual Representations},\nauthor={Mike Wu and Milan Mosse and Chengxu Zhuang and Daniel Yamins and Noah Goodman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=v8b3e5jN66j}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "v8b3e5jN66j", "replyto": "v8b3e5jN66j", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1947/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538107250, "tmdate": 1606915767850, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1947/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1947/-/Official_Review"}}}], "count": 17}