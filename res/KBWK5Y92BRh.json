{"notes": [{"id": "KBWK5Y92BRh", "original": "UVMCRnFLJA", "number": 1008, "cdate": 1601308114126, "ddate": null, "tcdate": 1601308114126, "tmdate": 1614985628874, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "jyRG0R4BC0G", "original": null, "number": 1, "cdate": 1610040531352, "ddate": null, "tcdate": 1610040531352, "tmdate": 1610474140936, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a new NAS methods that when doing architecture search, returns flat minima using based on a notion of distance defined for two cells (Eq. (2)). Authors then evaluation the effectiveness of the proposed methods against prior work on several benchmarks.\n\nAs authors have discussed in the paper, the idea of using flatness notion in architecture search is not new and has been first proposed by Zela et al 2020. This paper is building on Zela et al 2020 but the proposed algorithm is novel and different than Zela et al 2020. Even though the introduced algorithm is interesting, there are several concerns/areas of improvements:\n\n1- The proposed method's performance is highly dependent to the notion of distance defined in eq. (2). However, the current choice is not well-motived and does not seem like a well-thought-out choice. See for example the issue raised by R1. I think authors need to spend more time on this choice. One other option is to meta-learn the vector representation of each operation.\n\n2- All reviewers agree that the improvements marginal and in some cases not statistically significant. Authors have responded by arguing that this is typical for this area of research. I don't find this answer satisfying. For example, consider P-DARTS (Chen et al., 2019). P-DARTS improves over NA-DARTS (the proposed method) on CIFAR-10 and ImageNet and on CIFAR-100 they are on par given the standard deviation of NA-DARTS (see Tables 4 and 5). Moreover, the search cost of P-DART is 0.27% of NA-DARTS (Table 4). So P-DARTS has clear advantage over NA-DARTS.\n\nGiven the above issues, I recommend rejecting the paper. I hope authors would take feedbacks from the reviewing process into account to improve the paper and resubmit.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040531339, "tmdate": 1610474140920, "id": "ICLR.cc/2021/Conference/Paper1008/-/Decision"}}}, {"id": "RydFGH04m0f", "original": null, "number": 2, "cdate": 1603857637710, "ddate": null, "tcdate": 1603857637710, "tmdate": 1606786840080, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review", "content": {"title": "An interesting idea for NAS, where the neighborhood of the model is considered. ", "review": "** Summary \nThe authors proposed neighborhood-aware neural architecture search, where during the evaluation phase during search, the neighborhood of an architecture is considered. Specifically, when an architecture $\\alpha$ is picked, its neighbors $\\mathcal{N}(\\alpha)$ all contribute to the performance validation. This is built upon the assumption that `` flat minima generalize better than sharp minima\u2019\u2019 and the authors verify it in Appendix C. \nThe authors conducted experiments on CIFAR-10/100 and ImageNet, and obtained promising improvements over the standard baselines. \n\n** Clarify\n1.\tTowards \"Due to the property of the total variation distance, when $d$ is an integer, the neighborhood contains all the cells that have at most $d$ edges associated with different operations from $\\alpha$\". As you reported, $\\alpha$ is a collection of $\\alpha^{(i,j)}$, where each $\\alpha^{(i,j)}$ is a one hot vector \u201c But in DARTS, $\\alpha^{i,j}$ is a distribution of all candidate operations. In this case, how to select the $\\mathcal{N}(\\alpha)$?\n\n** Significance \nOverall, I think the results are solid.\n1.\tThere is a possible \u201censemble\u2019\u201d baseline for your method. Let us take DARTS as an example. First, we independently train $n_{nbr}$ one-shot models. Each one-shot model has an individual $\\alpha$. Then, we sample an architecture from the average of all $\\alpha$\u2019s. This could be another way to leverage neighborhood information and should be compared. \n2.\tImprovement not significant: In Table 4, compared to DARTS+, the improvement is 0.1/0.4 on CIFAR10/100. Compared to PDARTS, on the three datasets, the results between your method and PDARTS are almost the same. Therefore, you should apply your method to more recent advantaged methods to show that it is orthogonal or others.\n3.    As you pointed in Section 2, (Zela et al 2020) also observed a strong correlation between the generalization error of the architecture found by DARTS. It is good to see the exploration in Table 6. I think the \"NA-DARTS-ES\" should be implemented to see whether your method and  (Zela et al 2020) are complementary to each other. \n4.\tThe code should be released to reproduce your work.\n\n== Post Rebuttal ==\n\nI am satisfied with the response \"ensemble baseline\" and \"NA-DARTS-ES\". But my concern about \"Improvement not significant\" is not addressed, which is also mentioned by R1 and R4. I will remain my score as 6.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129357, "tmdate": 1606915808811, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1008/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review"}}}, {"id": "aSWQy9FKJcC", "original": null, "number": 4, "cdate": 1603968582954, "ddate": null, "tcdate": 1603968582954, "tmdate": 1606755575292, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "This paper proposes a neighbor-aware method in the neural architecture search (NAS). The paper states that by optimizing a neighbor of the neural network, it can search the result in a flat-minima, which is more stable than the sharp minima. The experiment results in further support that the proposed NA-RS and NA-DARTS outperform the current SOTA in various tasks.\n\nOverall, I think the paper raises an interesting opinion, which stresses that during the NAS process, a stable flat-minima has a better generalization ability. The definition of the neighbor of the neural architecture is also interesting. Below are some detailed points of my opinion about this paper.\n\nPros:\n1. The paper compares with a lot of NAS methods and outperforms most of them.\n2. Stabilize the search result can reach a better performance seems reasonable for me.\n3. The ablation study in the appendix further shows the performance under different aggregate functions and distances, which makes the experiment more convincing.\n\nCons:\n1. From the paper's statement, sample-based NAS is easier to define the neighbor than the gradient-based one. However, from the empirical result, the sample-based NAS result is relatively weak compared with the differential based. DARTS also provides the result of the random sampling on their search space. It is interesting to see whether NA-RS can outperform the naive random sampling and even catch up with some other NAS method in DARTS's setting and search space. Currently, I think only test the sample-based method on NAS-Bench-201 is a little bit toy.\n\n2. PC-DARTS has reported its ImageNet performance, top1 error, 24.2%. I am a little bit confused about why the author here uses their implement result with a lower performance. In addition,  it is interesting that the author states that they can combine PC-DARTS with their neighbor-aware. However, they only show an S3 search space result in the appendix, it would be more convincing to show the neighbor-aware method's generalization ability if you can apply it on the standard search space and directly compare with PC-DARTS's public performance. Otherwise, the 'SOTA' statement looks not very strong for me.\n\n================================After Response==============================\n1. I agree that different DARTS paper usually uses some different settings, the lower PC-DARTS performance in the paper could be reasonable.\n2. Search Space design sometimes can greatly impact the result of the final result, but it makes sense to modify the search space for a better result. I am glad that you state the situation of the performance on the original DARTS space.\n3. I also read other reviewers' opinions. Based on the author's response and my previous rating, I decided to keep this score. This is an acceptable paper, but still has something to do in the future, like a more comprehensive experiment part.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129357, "tmdate": 1606915808811, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1008/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review"}}}, {"id": "ca2WTy9SOqH", "original": null, "number": 6, "cdate": 1606271628154, "ddate": null, "tcdate": 1606271628154, "tmdate": 1606276333318, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "vUsCoDVACs", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to R1 - Part 2", "comment": "**Concerns about the neighborhood-aware formulation and flat minima assumption**\n\nThere might be a misinterpretation in R1\u2019s statement that \u201cthis paper hypothesizes that similar architectures with similar performance point to a flat solution\u201d. This is not our assumption but simply describes the property of flatness, where similar inputs yield similar outputs. In our case, the input is the architecture and the output is the validation accuracy/loss.\n\nTo measure the flatness of an architecture, we adopt the aggregated validation accuracy/loss over the neighborhood as our objective function (defined in Eq. 1; see Sec. 3.2 for more details). We validate that our objective function can indeed find flat minima in Appendix C.4. \n\nFurthermore, we verified the flat minima assumption through extensive experiments that a flat-minima architecture generalizes better than a sharp-minima architecture. For more details, please refer to Sec. 3.3 and Appendix C. Note these analyses are also acknowledged by R2 (\u201cthe authors verify it in Appendix C\u201d) and R4 (\u201cThe paper supports their assumption with various empirical experiments and ablation studies\u201d).\n\n\n**Questions about Eq. 2**\n\nWe thank R1 for pointing out the intuition that a 3x3 conv is closer to a 5x5 conv than the identity operation. We agree that taking this into account will further strengthen our method. This requires us to quantitatively compute the similarity/distance between any two operations, which is an open problem and will be an interesting future direction to extend our idea.\n\nWe agree that a change in probability distributions does not always result in a different discrete architecture. This is common in differentiable NAS methods. For example, in DARTS, the architecture before and after a gradient update are different in the continuous probability space, but might still represent the same discrete architecture. Also, even if the two probability distributions represent the same discrete architecture, they are still informative as the probability change denotes the relative importance of operations.\n \n\n**Params and FLOPS of architectures in Table 1&2**\n\nThe results in Table 1&2 are obtained by experiments on NAS-Bench-201 (Dong & Yang, 2020).  It seems to be a convention for NAS-Bench-201 that researchers do not explicitly report the params/FLOPS of architectures when comparing search algorithms (e.g., Table 5 in the original NAS-Bench-201 paper). All the architectures have the same number of initial channels and cells, so most architectures should have similar params and FLOPS. In all the experiments on NAS-Bench-201, we directly query the benchmark for accuracy of each architecture without changing its size and training it.\n\n**Incremental Improvement**\n\nWe agree that the improvement is not amazing. But our improvement is also not marginal given the fact that the cell search space used in DARTS has a narrow performance range as verified in Yang et al., 2020, e.g., the test error of most methods are within [25%, 27%] on ImageNet. \nTaking two recent methods PC-DARTS [A] (ICLR 2020) and PVLL-NAS [B] (ICML 2020) for example, their improvement over DARTS on ImageNet are 1.0% and 1.1% respectively. Our improvement over DARTS on ImageNet is higher than both of them. Also, as shown in Table 6, our method can be combined with PC-DARTS to further improve the performance.\n\n[A] Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, Hongkai Xiong. PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search. ICLR 2020.\n\n[B] Yanxi Li, Minjing Dong, Yunhe Wang, Chang Xu, Neural Architecture Search in A Proxy Validation Loss Landscape. ICML 2020.\n\n\n\n**DARTS search space**\n\nAll the baselines we compare to in Table 4&5 only report results on the DARTS-like search space. To ensure consistent evaluation and fair comparison to previous work, we also evaluate our method on the DARTS search space. We agree that having results on the MobileNet block-based search space will be very useful, but due to limited resources, we leave that for the future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KBWK5Y92BRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1008/Authors|ICLR.cc/2021/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864737, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment"}}}, {"id": "uDQnwfSz5ts", "original": null, "number": 4, "cdate": 1606267792425, "ddate": null, "tcdate": 1606267792425, "tmdate": 1606273064439, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "RydFGH04m0f", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to R2", "comment": "We thank R2 for the valuable feedback.\n\nAs suggested by R2, we conduct the following two experiments:\n\n**Ensemble baseline**\n\nWe average the $\\alpha$ from 5 independent runs of DARTS and derive the final architecture. This architecture obtains 2.90% test error on CIFAR-10 and 18.07% test error on CIFAR-100. Our proposed NA-DARTS (2.63% on CIFAR-10 and 16.48% on CIFAR-100) easily outperforms this ensemble baseline on both datasets.\n\nIn our NA-DARTS experiments, $n_\\text{nbr}$=10. The reason why we only average 5 runs of DARTS are: (1) we did not have enough time to run DARTS for 10 times; and (2) the time to run DARTS for 5 times is already longer than running NA-DARTS for once (0.3x5=1.5 v.s. 1.1 GPU days).\n\n**NA-DARTS-ES**\n\nWe thank R2 for suggesting this experiment. We empirically confirm that combining our method with the early stopping strategy in Zela et al. (2020) yields further improvement.  When implementing NA-DARTS-ES, we follow Zela et al. (2020) and stop the optimization based on the dominant eigenvalue of the Hessian matrix. Here are the results:\n\n|             |         CIFAR-10       |         CIFAR-100       |\n|-------------|:----------------------:|:-----------------------:|\n| DARTS       |          $4.13\\pm0.98$ | $22.49\\pm2.62$          |\n| NA-DARTS    |          $2.97\\pm0.18$ | $18.86\\pm0.49$          |\n| NA-DARTS-ES | $\\mathbf{2.49\\pm0.02}$ | $\\mathbf{17.03\\pm0.41}$ |\n\n\n\n**Clarification: how to select $\\mathcal{N}(\\alpha)$ for DARTS?**\n\nYou are right that $\\alpha^{(i, j)}$ is a probability distribution, as the architecture representation in DARTS is relaxed to be continuous. In this case, we sample the neighboring architectures by perturbing the probability distribution $\\alpha^{(i, j)}$ with a randomly sampled noise vector. Please see Sec. 4.2.2 and Eq. 6 for more details. This strategy allows us to sample a set of neighboring architectures of $\\alpha$ and the sampled architectures are represented as a differentiable function of $\\alpha$.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KBWK5Y92BRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1008/Authors|ICLR.cc/2021/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864737, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment"}}}, {"id": "eyG137lfvFc", "original": null, "number": 2, "cdate": 1606266964983, "ddate": null, "tcdate": 1606266964983, "tmdate": 1606272687983, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "aSWQy9FKJcC", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to R3", "comment": "We thank R3 for the valuable feedback.\n\n**ImageNet performance**\n\nSome NAS methods use a different training setup to train the found architecture on ImageNet. For example, DARTS+ (Liang et al., 2019) trains for 800 epochs while DARTS only trains for 250 epochs. PC-DARTS (Xu et al., 2019)  and P-DARTS (Chen et al., 2019) use a large batch size 1024 (need 8 GPUs, infeasible cost for our research group), while DARTS uses a batch size of 128. For fair comparison, we retrain the found architecture reported by the authors in their paper using the same training setup as DARTS.\n\n**NA-PC-DARTS on the DARTS search space**\n\nWe choose the S3 space as it is a more challenging search space for DARTS and its variants, since Zela et al. (2020) shows that on S3, DARTS yields degenerate architectures with very poor test performance. On S3, our idea can be combined with PC-DARTS and gives further improvement, i.e., NA-PC-DARTS outperforms both NA-DARTS and PC-DARTS in Table 6.\n\nAs suggested by R3, we apply our neighborhood-aware formulation to PC-DARTS on the standard DARTS search space. We find that NA-PC-DARTS performs similarly to our NA-DARTS on the DARTS search space. Our experiments on S3 search space provide a more complete picture of how NA-PC-DARTS works.\n\n**Sample-based NAS on DARTS search space**\n\nWe agree that evaluating the sample-based method on more search spaces would be helpful. Due to limited computational resources and time, we haven\u2019t been able to obtain the results on the DARTS search space. We leave that for future work.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KBWK5Y92BRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1008/Authors|ICLR.cc/2021/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864737, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment"}}}, {"id": "vUsCoDVACs", "original": null, "number": 5, "cdate": 1606271582882, "ddate": null, "tcdate": 1606271582882, "tmdate": 1606271642623, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "tdO3MKS69ph", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to R1 - Part 1 (Novelty)", "comment": "We thank R1 for the valuable feedback.\n\n**Novelty**\n\nWhile the flat minima idea has been explored in Zela et al., 2020, our method differs from theirs in the following aspects:\n\n(1) Our formulation encourages the search algorithm to **actively** looking for flat minima architectures from the search space, while their method does not change the normal optimization process but stops optimization before the curvature of the minimum becomes too sharp.\n\n(2) We use the aggregated performance $g \\left(f(\\mathcal{N}(\\alpha)) \\right)$ to identify flat minima, while they use the largest eigenvalue of the Hessian matrix.\n\n(3) Our formulation is applicable to sampling-based NAS methods, while their method is not immediately applicable since they require computing the Hessian matrix, which is difficult for sampling-based NAS.\n\n(4) We provide extensive analysis to verify that the flat minima assumption also holds true in the architecture space. This is novel and one of our contributions.\n\n(5) As shown in Table 6, our method (NA-DARTS) outperforms their method (DARTS-ES). Also, we conduct further experiments as suggested by R2, where we show that our neighborhood-aware formulation is orthogonal to their early stopping strategy. Combining the two methods yields further improvement (see NA-DARTS-ES in response to R2).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KBWK5Y92BRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1008/Authors|ICLR.cc/2021/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864737, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment"}}}, {"id": "y3Q-rboTOn", "original": null, "number": 3, "cdate": 1606267026797, "ddate": null, "tcdate": 1606267026797, "tmdate": 1606267696406, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "cXLzcqsESLS", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment", "content": {"title": "Response to R4", "comment": "We thank R4 for the valuable feedback.\n\n**Incremental Improvement**\n\nWe agree that the improvement is not amazing. But our improvement is also not marginal given the fact that the cell search space used in DARTS has a narrow performance range as verified in Yang et al., 2020, e.g., the test error of most methods are within [25%, 27%] on ImageNet. \nTaking two recent methods PC-DARTS [A] (ICLR 2020) and PVLL-NAS [B] (ICML 2020) for example, their improvement over DARTS on ImageNet are 1.0% and 1.1% respectively. Our improvement over DARTS on ImageNet is higher than both of them. Also, as shown in Table 6, our method can be combined with PC-DARTS to further improve the performance.\n\n[A] Yuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, Hongkai Xiong. PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search. ICLR 2020.\n\n[B] Yanxi Li, Minjing Dong, Yunhe Wang, Chang Xu, Neural Architecture Search in A Proxy Validation Loss Landscape. ICML 2020.\n\n**Experiments on NAS Benchmarks and comparison to other baselines**\n\nWe have conducted extensive experiments on NAS-Bench-201, including evaluating our NA-RS algorithm (Table 2&10&11) and justifying the flat minima assumption (1&7&8&9). We also compare to many baselines other than DARTS/PC-DARTS, on both CIFAR-10/100 and ImageNet in Table 4&5.\n\nWe agree that evaluating our method on NAS-Bench-1SHOT1/NAS-Bench-201 would be useful, but most baselines (e.g., PC-DARTS) in Table 4&5 only report results on the DARTS search space. The DARTS search space is larger than the one used in NAS-Bench-1SHOT1 and NAS-Bench-201. To ensure consistent evaluation and fair comparison to previous work, we believe evaluating on the DARTS search space is sufficient to validate our method.\n\n**PC-DARTS in Table 6**\n\nPC-DARTS is **not** better than NA-PC-DARTS according to Table 6. Our NA-PC-DARTS outperforms PC-DARTS by 0.72% on CIFAR-100 and performs similarly to PC-DARTS on CIFAR-10. This result is positive and shows that our idea can be combined with PC-DARTS and gives further improvement for generalizing to different datasets.\n\n**ImageNet results of PC-DARTS and P-DARTS**\n\nPC-DARTS (Xu et al., 2019)  and P-DARTS (Chen et al., 2019) use a large batch size 1024 (need 8 GPUs, infeasible cost for our research group), while DARTS uses a batch size of 128. They also use different hyperparameters from DARTS, e.g. a different initial learning rate and weight decay. For fair comparison, we retrain the found architecture reported by the authors in their paper using the same training setup as DARTS.\n\n**How do you select the number of neighbors?**\n\nYes, in the DARTS search space, the neighborhood of an architecture is large. Using all the neighbors is computationally prohibitive. We set $n_\\text{nbr}$ to 10 as it empirically works well and the search cost is reasonable.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "KBWK5Y92BRh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1008/Authors|ICLR.cc/2021/Conference/Paper1008/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923864737, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Comment"}}}, {"id": "tdO3MKS69ph", "original": null, "number": 1, "cdate": 1603850613811, "ddate": null, "tcdate": 1603850613811, "tmdate": 1605024552981, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review", "content": {"title": "The results are good but novelty is not enough", "review": "Inspired by the concept that ``flat minima generalize better than sharp minima\", this paper proposes to search the flat-minima architecture by considering the performance over the neighborhood architectures. A random search and differentiable search method based on the neighborhood principle are proposed, which achieve comparable results with SOTA NAS methods. \n\nConcrete comments\n1. The biggest concern lies in the main idea of the neighborhood-aware search formulation. It is known in many previous methods that the flat minima of the loss function of neural network training generalize better. This conclusion is also extended to the eigenspectrum of the Hessian of the validation loss with respect to the architectural parameters in [1]. However, this paper hypothesizes that similar architectures with similar performance point to a flat solution. How it can be proven that nearby architectures are located in neighborhood in the loss space should be clarified. This is of critical importance to support the main concept of the paper.\n2. The proposed method is not so reasonable. The distance between architectures is defined as Eq. (2). However, many cases may fail with Eq. (2). For example, a 3x3 separable convolution is nearer to a 5x5 separable convolution, but further from the identity operation, which cannot be handled with Eq. (2). Moreover, the final operation is determined with the highest probability. [0.05, 0.6, 0.35] and [0.35, 0.6, 0.05] represent the same architecture, but different in Eq. (2).\n3. Experimental results are not so convincing. \n  - In Tab. 1 and 2, only test errors are reported but not Params or FLOPs. It is not clear whether advantages of the method actually exist. Normally, models should be compared with similar sizes.\n  - The proposed method does not show evident advantages over others, e.g., worse or similar compared with PC-DARTS but taking much more cost, 10x than PC-DARTS.\n  - The experiments are based on the DARTS-like search space. However this kind of search spaces hold high latency due to the complicated topology. What about the widely used MobileNet block-based search space?\n4. The novelty is somewhat limited. As the flat solution has been explored in [1], the main contribution seems to be the newly defined loss function ensemble with similar architectures. \n\n[1] Arber Zela, Thomas Elsken, Tonmoy Saikia, Yassine Marrakchi, Thomas Brox, and Frank Hutter. Understanding and robustifying differentiable architecture search. In ICLR, 2020.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129357, "tmdate": 1606915808811, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1008/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review"}}}, {"id": "cXLzcqsESLS", "original": null, "number": 3, "cdate": 1603862528427, "ddate": null, "tcdate": 1603862528427, "tmdate": 1605024552923, "tddate": null, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "invitation": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review", "content": {"title": "Well written but incremental improvements.", "review": "This paper introduces a searching framework of neural architectures search by modifying the objective function to optimize the aggregated performance over the neighborhood of an architecture. From the observation that flat-minima architecture $\\alpha$ generalizes better than sharp-minima architecture (Zela et al. (ICLR2020), the author proposes the objective function considering the neighborhood to enforce the flat minima. The author supports their method by providing ablation studies and architecture performances from CIFAR-10/100, ImageNet, and NAS-BENCH-201.\n\nStrength\n1. Their objective function can easily be extended to existing NAS methods including Random Search and Gradient-Based methods. \n2. The authors support their methods with various datasets such as CIFAR-10/100 and ImageNet\n3. The paper supports their assumption with various empirical experiments and ablation studies. \n\nWeakness\n1.  The experiment results are incremental improvements (or par) to the existing NAS algorithms. By listing the recently published NAS literature on CIFAR-10: P-DARTs: 2.50%, DATA: 2.59%, SGAS: 2.66%, PC-DARTs: 2.57%, RandomNAS-NSAS: 2.64%. \n2. Few experiments on NAS Benchmark. The experiments don't include NAS-Bench-201 (a popular benchmark for NAS) and small experiments with NAS-Bench-1SHOT1 only comparing with DARTs, PC-DARTs. And PC-DARTs result is better than NA-PC-DARTs according to Table 6. \n\n\nQuestion\n1. How do you select the number of neighbors? In the DARTs setup, the normal/reduce cell would have $(2+3+4+5)*7*2=196$. The possible neighboring architectures from a given subnetwork even with hamming distance 1 may be much larger than 10. Can you visualize the loss landscape based on various $n_{nbs}$?\n2. Have you tried to verify your algorithm for NA-DARTs on NAS-Bench-201 which is the popular benchmark in recent NAS literature?\n3. The reproduced ImageNet test error results on PC-DARTs and P-DARTs are higher than the reported test results. The reported test results on PC-DARTs outperforms the NA-DARTs result. Is the resulting discrepancy related to hyperparameter tuning? Can you clarify this? \n\nOverall, this paper proposes a neighborhood aware objective function that can be adopted in the existing NAS search. Despite the paper well written, the performance of their methodology is incremental improvements (or par) among various existing NAS algorithms. I would recommend a marginally below acceptance threshold for now but may change the decision based on the rebuttal. \n\nReference:\n1. Xu, Y et al. (2019, September). PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search. ICLR2020\n2. Chen, X et al. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. ICCV2019\n3. Chang, J et al. DATA: Differentiable ArchiTecture Approximation. Neurips2019\n4. Li, G et al. SGAS: Sequential Greedy Architecture Search. CVPR2020\n5. Zhang, M et al. Overcoming Multi-Model Forgetting in One-Shot NAS with Diversity Maximization. CVPR2020\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1008/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1008/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neighborhood-Aware Neural Architecture Search", "authorids": ["~Xiaofang_Wang1", "~Shengcao_Cao1", "~Mengtian_Li1", "~Kris_M._Kitani1"], "authors": ["Xiaofang Wang", "Shengcao Cao", "Mengtian Li", "Kris M. Kitani"], "keywords": ["Neural architecture search", "Flat minima"], "abstract": "Existing neural architecture search (NAS) methods often return an architecture with good search performance but generalizes poorly to the test setting. To achieve better generalization, we propose a novel neighborhood-aware NAS formulation to identify flat-minima architectures in the search space, with the assumption that flat minima generalize better than sharp minima. The phrase ``flat-minima architecture'' refers to architectures whose performance is stable under small perturbations in the architecture (\\emph{e.g.}, replacing a convolution with a skip connection). Our formulation takes the ``flatness'' of an architecture into account by aggregating the performance over the neighborhood of this architecture. We demonstrate a principled way to apply our formulation to existing search algorithms, including sampling-based algorithms and gradient-based algorithms. To facilitate the application to gradient-based algorithms, we also propose a differentiable representation for the neighborhood of architectures. Based on our formulation, we propose neighborhood-aware random search (NA-RS) and neighborhood-aware differentiable architecture search (NA-DARTS). Notably, by simply augmenting DARTS~\\cite{liu2018darts} with our formulation, NA-DARTS finds architectures that perform better or on par with those found by state-of-the-art NAS methods on established benchmarks, including CIFAR-10, CIAFR-100 and ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wang|neighborhoodaware_neural_architecture_search", "one-sentence_summary": "We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.", "pdf": "/pdf/8c361eee025ea10838049102c5890e8ed55969d2.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=3V4F1hxz7h", "_bibtex": "@misc{\nwang2021neighborhoodaware,\ntitle={Neighborhood-Aware Neural Architecture Search},\nauthor={Xiaofang Wang and Shengcao Cao and Mengtian Li and Kris M. Kitani},\nyear={2021},\nurl={https://openreview.net/forum?id=KBWK5Y92BRh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "KBWK5Y92BRh", "replyto": "KBWK5Y92BRh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1008/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538129357, "tmdate": 1606915808811, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1008/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1008/-/Official_Review"}}}], "count": 11}