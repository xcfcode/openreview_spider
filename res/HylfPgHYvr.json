{"notes": [{"id": "HylfPgHYvr", "original": "HygJ4oeFvr", "number": 2351, "cdate": 1569439833546, "ddate": null, "tcdate": 1569439833546, "tmdate": 1577168264172, "tddate": null, "forum": "HylfPgHYvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "0RDHYAzwB4", "original": null, "number": 1, "cdate": 1576798746862, "ddate": null, "tcdate": 1576798746862, "tmdate": 1576800889227, "tddate": null, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Decision", "content": {"decision": "Reject", "comment": "The paper studies the problem of modeling inter-object dynamics with occlusions. It provides proof-of-concept demonstrations on toy 3d scenes that occlusions can be handled by structured representations using object-level segmentation masks and depth information. However, the technical novelty is not high and the requirement of such structured information seems impractical real-world applications which thus limits the significance of the proposed method.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725005, "tmdate": 1576800276747, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Decision"}}}, {"id": "rJejkkaRqr", "original": null, "number": 2, "cdate": 1572945635463, "ddate": null, "tcdate": 1572945635463, "tmdate": 1573939519901, "tddate": null, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #5", "review": "This paper proposes a method to predict future trajectories by modeling partial and full occlusions. Although it is well-written and the topic sounds interesting, I failed to catch why this approach is required for this setting. So, to strengthen the message of this paper, I listed a couple of suggestions and comments below (from the most important to the least important):\n\n\n1. It is a bit hard to catch how this model handles \"diversity.\" Specifically, when predicting the futures, it should be able to generate stochastic outputs. However, I failed to find how diverse the output of the model is. If the output is not that stochastic, then it would be tough to believe that the model can \"predict\" the future; instead, it may \"extrapolate\" the current condition only. To reassure such concerns, I recommend reporting how diverse your output is. (One easy way is to report the variance of the predicted center mass values between multiple samples while reporting the l2 distance.)\n\n\n2. For the future prediction task, it would be much better if it is compared with various state-of-the-art future prediction approaches [1, 2, 3, 4, 5, 6]. For some of the models, it could not be able to compare directly with this approach (e.g., lack of 'center of mass' information). However, it would be still okay once it is compared with other state-of-the-art results without feeding some 3D information (e.g., provide projected 2D video as an input). By doing so, I believe the readers can easily catch (1) why it is better to predict physical interaction in 3D space (instead of directly predicting from a 2D space), and (2) also why predicting occlusion is essential in this problem setting.\n\n\n3. Minor comments:\n(a) It is a bit hard to catch how the author computes the \"aggregate pixel reconstruction error\" in Table S1. I recommend adding an equation number there to make it clear.\n\n(b) There are a couple of missing references: the last sentence on page 4, the first paragraph in Supplementary, the last sentence in Supplementary page 3, etc.\n\n(c) \\citep is often misused. Please replace some inappropriate \\citep with \\citet.\n\n(d) Please check the format of the reference, as well; currently, it has various styles even for the same source/conference.\n\n\n\n\n------------------------------------------------------------------\n\n[Some comments based on the authors' rebuttal]\n\n\nI thank the authors for their thorough comments and detailed explanations for each question. I carefully read the whole (not just my part), but it didn't change my mind; it would be much better if the claim comes with a more directly comparable result.\n\nSome additional comments:\nQ1-comment) I think the limitation of \"learning to extrapolate\"-style video prediction approach is partially presented in Reviewer #2's claim as well. Therefore, in this context, I recommend the author to show a better result to reassure the reader's concern. \n\nQ2-comment) I at least strongly recommend to add more experiments with other baselines, rather than relying mainly on the original model of the dataset. Although the input condition of a model could be different, I at least do believe that it will help the readers to catch the benefit of your setting more clearly.\n\nI hope this review phase would make your paper more powerful. \n\n\n\n\n[1] Liang et al., Dual Motion GAN for Future-Flow Embedded Video Prediction, in ICCV, 2017\n[2] Denton and Fergus, Stochastic Video Generation with a Learned Prior, in ICML, 2018\n[3] Wichers et al., Hierarchical Long-term Video Prediction without Supervision, in ICML, 2018\n[4] Wang et al., Video-to-Video Synthesis, in NeurIPS, 2018\n[5] Heish et al., Learning to Decompose and Disentangle Representations for Video Prediction, in NeurIPS, 2018\n[6] Minderer et al., Unsupervised Learning of Object Structure and Dynamics from Videos, in NeurIPS, 2019", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576049674976, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Reviewers"], "noninvitees": [], "tcdate": 1570237724073, "tmdate": 1576049674992, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review"}}}, {"id": "Bklh6GB2sH", "original": null, "number": 7, "cdate": 1573831364294, "ddate": null, "tcdate": 1573831364294, "tmdate": 1573831364294, "tddate": null, "forum": "HylfPgHYvr", "replyto": "HJlwAevmoB", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment", "content": {"title": "Answer to Reviewer#6", "comment": "Thank you for your helpful comments on the writing as well as the missing references.\n\nWe agree with your assessment that segmentation masks would be more difficult to obtain in real videos; probably, also the depth information would be less reliable than in the synthetic datasets we are using. Still, the main point of this paper is that a level of representation like object masks+depth, a kind of a 2.5D representation is a good level of representation to compute long term predictions, provided that occlusion can be addressed. Now, we agree that the next steps should address what happens when noisy masks are used instead of ground truth ones. \n\nAblation studies: We agree that additional ablation studies on the refinement network are interesting. Following suggestions from Reviewer#6, we trained the same Recurrent Interaction network on a forward prediction task with 15 and 25 degrees tilted views, without using position refinement with the renderer. Hence the locations of objects used as input of the renderer exactly match the centroid of each segmentation mask. Note that we choose the 15 and 25 degrees tilted views so that occlusions occur frequently (otherwise refinement is not so useful). On the 25 degree tilted view, we obtain a L2 distance (in pixels) to target of 13.1/23.8 for a prediction horizon of 5 and 10 frames respectively. On the 15 tilted view, we obtain a L2 distance to target of 18.5/31.2 for prediction horizon of 5 and 10 frames. These results are to be compared with those in Table1 in the main body. We observe that in such an environment where occlusions appear frequently, refining estimates positions with the renderer reduces prediction errors by about 30%, which confirms intuition coming from qualitative results in supplementary material.\nStochasticity: the stochasticity of the model is indeed restricted to the prediction of an uncertainty term. It seems hard to compare this with the stochasticity of the ground truth data since this data is (almost) deterministic. One way to do it would be to add white noise to the input data and estimate the distribution of the resulting trajectories. This indeed looks interesting. In the present model, we have observed that in long rollouts behind occlusions uncertainty increases, as well as after objects contact (bouncing events). "}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylfPgHYvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2351/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2351/Authors|ICLR.cc/2020/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142658, "tmdate": 1576860541476, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment"}}}, {"id": "ryed9h43oS", "original": null, "number": 6, "cdate": 1573829776319, "ddate": null, "tcdate": 1573829776319, "tmdate": 1573829776319, "tddate": null, "forum": "HylfPgHYvr", "replyto": "rJejkkaRqr", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment", "content": {"title": "Answer to Reviewer#5", "comment": "Thank you for your helpful comments on the writing as well as the missing references which we will include.\n\nOn stochasticity: Our model is not stochastic. The use of the word Proba-RIN may be misleading, but we only predicts a term of error (or uncertainty). One way to make it stochastic and have various possible outcomes would be to sample predictions from this distribution (centered around the prediction, with std equal to predicted uncertainty). We mainly used this term of error to stabilize learning and represent uncertainty, not to model possible alternative outcomes.  \n\nComparison with state of the art video prediction approaches:  Indeed it is interesting to compare our results with video prediction models. To that aim we compare with Riochet et al. baseline, trained to predict future video frames in a similar setup, and which predicts segmentation masks which we can compare with our approach. Other models that directly produce RGB images are difficult to compare because they predict in a different space where colour and texture rendering matters a lot. A nice thing with predicting in mask space is that it enables to concentrate on issues of position and interaction. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylfPgHYvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2351/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2351/Authors|ICLR.cc/2020/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142658, "tmdate": 1576860541476, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment"}}}, {"id": "HJeCH2V3iH", "original": null, "number": 5, "cdate": 1573829701638, "ddate": null, "tcdate": 1573829701638, "tmdate": 1573829701638, "tddate": null, "forum": "HylfPgHYvr", "replyto": "Bylu2aeMoB", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment", "content": {"title": "Answer to Reviewer#3", "comment": "Yes, we agree that our paper does not tackle prediction from images, but from segmentation masks. Here, we use ground truth masks, which is possible because of the synthetic nature of the dataset. Application to real videos would require to use a segmentation system. While it is true that initial position and velocity can be estimated from a pair of masks, the task we are tackling is still not trivial. First, during total or partial occlusions, position estimates are incorrect or missing. Second, masks correspondence across frames is NOT provided, which makes it challenging to recover full trajectories, especially across long occlusions. Both of these challenges are tackled by our use of the neural renderer.\n\nOn the need of ground truth segmentation of occluded objects: The model does not have access to the segmentation masks of occluded objects (at any time). During training, the position of partially occluded object is refined thanks to the renderer. Fully occluded object have no corresponding mask and therefore the refinement does not modify anything (no gradient loss). We will clarify this in the rewrite. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylfPgHYvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2351/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2351/Authors|ICLR.cc/2020/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142658, "tmdate": 1576860541476, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment"}}}, {"id": "HyxH-2NhiH", "original": null, "number": 4, "cdate": 1573829629138, "ddate": null, "tcdate": 1573829629138, "tmdate": 1573829629138, "tddate": null, "forum": "HylfPgHYvr", "replyto": "S1eWc4LTtS", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment", "content": {"title": "Answer to Reviewer#2", "comment": "End-to-end training: Yes we tried to train end-to-end, but generalization wasn\u2019t as good, and it was also longer to train (by a factor of 10). The resulting masks and trajectories were similar but generalization to longer rollouts was qualitatively not convincing. It seems therefore that the renderer trains faster than the prediction network, and that an incorrect initial renderer is detrimental to prediction learning. This pleads for a kind of a curriculum learning (giving more weights initially to rendering accuracy), but this would require many more experiments that we thought would be better suited to a followup study.\n\nRemark on Figure 3: Thank you, we will update Figure 3 to make clearer and self-explanatory.\n\nDifference from Battaglia et al.: Training in full sequence (RNN) without the supervised velocity was not explored in previous work. Interaction Networks are being widely used in synthetic dataset where ground truth velocity and position is known. Showing that velocity can be kept as a latent variable, using only supervision of the a sequence of positions to train the Interaction Networks opens doors to real world applications where position is much easier to get than velocity.\n\nClaim on similar performance with Battaglia et al: We will soften this claim, saying that our approach performs similar to Battaglia et al. on long rollouts. Also note that on short rollouts, object dynamics are close to linear and the fact that the baseline is uses ground truth velocity may explain the gap in performance.\n\nConcerning the IntPhys benchmark, and the \u201cmaximum error through the whole sequence\u201d. The mask prediction is done through the whole sequence. For each frame, we get the reconstruction loss. We give to each sequence a loss corresponding to the maximum loss of its frames (maximum across time).\n\nConcerning comparison with Riochet et al. on other tasks, it is difficult to compare on the state prediction (predicting location of each object). However it is possible to compare on mask prediction, which we did but kept in the supplementary material (see table S1). In this table we can see that our method performs better than baseline Riochet et al., trained on future mask prediction.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HylfPgHYvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2351/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2351/Authors|ICLR.cc/2020/Conference/Paper2351/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504142658, "tmdate": 1576860541476, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Authors", "ICLR.cc/2020/Conference/Paper2351/Reviewers", "ICLR.cc/2020/Conference/Paper2351/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Comment"}}}, {"id": "HJlwAevmoB", "original": null, "number": 4, "cdate": 1573249230677, "ddate": null, "tcdate": 1573249230677, "tmdate": 1573249393222, "tddate": null, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "title": "Official Blind Review #6", "review": "* Note: emergency review, done under a shorter time frame than good reviews require.\n\nIn this paper, the authors develop a highly structured model to predict motions of objects defined by segmentation masks and depths. The model trains a physics model (in the form of a slightly modified interaction network) and a renderer composed of a per-object renderer combined with an occlusion model which composes the per-object segmentation and depth into a scene segmentation and depth.\n\nPositives:\n- The jury is still out on the degree of structure required to do proper object processing (neural nets with large amounts of data, mildly structured nets like networks with attention, more structured nets like this, or a full fledged renderer-like probabilistic program); this work contributes novel work to the line of research which attempts to do object-level processing with structured models while still leveraging the power of neural networks.\n\n- The experimental section appears very thorough and convincing, even if the dataset is relatively simple.\n\nNegatives:\n- The model requires highly privileged information (segmentation mask, depth) at training and test time. Given that the segmentation/depth data are not too far from the actual images, it would have been interesting to see if it were possible to work with pixels (a variant of the occlusion model would probably still work), at least at test time. \n\n- Regarding using segmentation/depth as input to the model: for a real dataset, segmentation is more relevant: it is both less informative than positions (due to significant occlusions) and easier to measure. In this highly synthetic dataset, this feels more debatable: objects are more entangled in the segmentation (which makes using segmentation more challenging), but only weakly, with many frames with no occlusion; furthermore, segmentation provides object shapes as information.\n\n- The paper is generally well written, but could benefit from some reorganization - instead of defining each module separately, it would be better to describe the flow of information through different modules, then describe the module. I was wondering for a while how the initial positions were estimated (required as input to both the interaction net and the renderer), but this only comes at the end of the paper.\n\n- Some ablation experiments felt missing, for instance, the importance of the refinement network (also unfortunate that the details of refinement were not given in the main body).\n\n- The stochasticity of the interaction network appears a bit weak (simple Gaussians) - it would be interesting to display some data to see if the ground truth data is indeed Gaussian like .\n\n- Missing potential references:\nSequential Attend, Infer, Repeat: Generative Modelling of Moving Objects\nLearning to Decompose and Disentangle Representations for Video Prediction\nMONet: Unsupervised Scene Decomposition and Representation", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer6"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer6"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576049674976, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Reviewers"], "noninvitees": [], "tcdate": 1570237724073, "tmdate": 1576049674992, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review"}}}, {"id": "Bylu2aeMoB", "original": null, "number": 3, "cdate": 1573158320432, "ddate": null, "tcdate": 1573158320432, "tmdate": 1573158320432, "tddate": null, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "The key contribution of this paper is a model that can predict the dynamics of pre-segmented image patches under multiple frames of occlusion. The input image is processed by a CNN, the dynamics are predicted by a recurrent interaction net, and the output image is generated by a (deconv) CNN. \n\nThe key weaknesses I see are:\n\n- The objects must be pre-segmented by some externally defined mechanism. Where does this mechanism come from? Segmenting the objects is challenging, and there are various recent methods that explore how to learn to do this (van Steenkiste et al., 2018). But if one has the segmentation masks, that simplifies things considerably and also offers a good estimate of the location and velocity (if there are 2+ frames). \n\n- During training, the error is computed on all frames, including occluded ones, and backpropagated into the weights. But if I understand this correctly, this means that for training you need access to ground truth rendered trajectories. It would be better if the model didn't require the ground truth segmentations for objects that are occluded. How would they be made available to a learning system?\n\n- Generally the writing wasn't that clear and I struggled to understand some details of the model and training procedure.\n\n\nOverall I don't believe this work is ready for publication, as there isn't that much novelty and the requirements are impractical.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576049674976, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Reviewers"], "noninvitees": [], "tcdate": 1570237724073, "tmdate": 1576049674992, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review"}}}, {"id": "S1eWc4LTtS", "original": null, "number": 1, "cdate": 1571804296677, "ddate": null, "tcdate": 1571804296677, "tmdate": 1572972349818, "tddate": null, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "invitation": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "Summary:\nThis paper proposes a method that combines a recurrent neural network that predicts values that are used as inputs to a rendered which interprets them and generates an object shape map and a depth map for every step of the dynamics predicted by the recurrent neural network. The proposed method is able to handle object occlusions and interactions. In experiments, the authors show improved performance against baselines for future prediction, object tracking, and object permanence.\n\n\nPros:\n+ Rendering network used with RNN\n+ Outperforms chosen baselines\n\nWeaknesses / comments:\n- Compositional Rendering Network has to be pretrained: Did the authors try to train the model end-to-end? It would be interesting to see if this can be done so the proposed network is more unified.\n\n- Figure 3 is not self explanatory: It would be good if the authors add labels to the predicted and gt frames. It is not easy to parse this figure from just looking at it.\n\n- Difference from Battaglia et al., 2016: It seems that the only difference between the proposed method and this baseline is the change of input/outputs (including output with variance), and training in full sequence (RNN)? This looks like a minor change to me and reduces the novelty of the proposed method.\n\n- Table 1 (trained on ground-truth positions): The authors claim that their network performs similar to Battaglia et al., 2016, but it seems that the baseline is better than the proposed method for the short term predictions with a relative improvement of about 20% and for long term when the baseline is better (half of the tests)  it\u2019s by a relative improvement of about 10%. Can the authors comment on this? Am I missing something?\n\n- Implausibility score: What do the authors mean by \u201cthe maximum error through the whole sequence\u201d? How is this defined?\n\n- The authors compare with Riochet et al., 2018 in Table 4, but not in the rest of the evaluations. Can the authors comment on why this is the case? \n\n\nConclusion:\nThe paper proposes an interesting method, dataset, and seems to perform baselines in the quantitative evaluation. To the best of my knowledge, the current state of the method is novel in the rendering network. However, the rest of components have limited novelty. In addition, I have some comments about the paper which stated above.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2351/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["ronan.riochet@inria.fr", "josef.sivic@ens.fr", "ivan.laptev@inria.fr", "emmanuel.dupoux@gmail.com"], "title": "Occlusion  resistant  learning  of  intuitive physics from videos", "authors": ["Ronan Riochet", "Josef Sivic", "Ivan Laptev", "Emmanuel Dupoux"], "pdf": "/pdf/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "abstract": "To reach human performance on complex tasks, a key ability for artificial\nsystems is to understand physical interactions between objects, and predict\nfuture outcomes of a situation. This ability, often referred to as\nintuitive\nphysics\n, has recently received attention and several methods were proposed to\nlearn these physical rules from video sequences. Yet, most these methods are\nrestricted to the case where no occlusions occur, narrowing the potential areas\nof application. The main contribution of this paper is a method combining\na predictor of object dynamics and a neural renderer efficiently predicting\nfuture trajectories and explicitly modelling partial and full occlusions among\nobjects. We present a training procedure enabling learning intuitive physics\ndirectly from the input videos containing segmentation masks of objects and\ntheir depth. Our results show that our model learns object dynamics despite\nsignificant inter-object occlusions, and realistically predicts segmentation\nmasks up to 30 frames in the future. We study model performance for\nincreasing levels of occlusions, and compare results to previous work on\nthe tasks of future prediction and object following. We also show results\non predicting motion of objects in real videos and demonstrate significant\nimprovements over state-of-the-art on the object permanence task in the\nintuitive physics benchmark of Riochet et al. (2018).", "keywords": [], "paperhash": "riochet|occlusion_resistant_learning_of_intuitive_physics_from_videos", "original_pdf": "/attachment/36293ed63403e8f5a69c596c04d25a350a2c8f58.pdf", "_bibtex": "@misc{\nriochet2020occlusion,\ntitle={Occlusion  resistant  learning  of  intuitive physics from videos},\nauthor={Ronan Riochet and Josef Sivic and Ivan Laptev and Emmanuel Dupoux},\nyear={2020},\nurl={https://openreview.net/forum?id=HylfPgHYvr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HylfPgHYvr", "replyto": "HylfPgHYvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2351/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576049674976, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2351/Reviewers"], "noninvitees": [], "tcdate": 1570237724073, "tmdate": 1576049674992, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2351/-/Official_Review"}}}], "count": 10}