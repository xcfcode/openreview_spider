{"notes": [{"id": "H6ZWlQrPGS2", "original": "mHSLeOoxvM", "number": 518, "cdate": 1601308064478, "ddate": null, "tcdate": 1601308064478, "tmdate": 1614985623916, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 22, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "mPwuvfd1XVk", "original": null, "number": 1, "cdate": 1610040535989, "ddate": null, "tcdate": 1610040535989, "tmdate": 1610474145989, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "## Description \n\nThe paper asks the question whether it is possible to accelerate training a binarized neural network from scratch to a given target accuracy [by starting with training a full-precision network]. The main claimed contributions are: the idea to use *partially* pretrained networks, experimental evidence regarding the split of the training budget and measuring the speed-up.\n\n## Review Process and Decision\n\nAll four reviewers agreed in the low rating of the paper and in the opinion that the paper is not a significant contribution. The area chair supports rejection.\n\n## Details\n\nIt has been already observed that pre-training  in some form is needed for achieving the best accuracy: \nRastegari (2016) XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks\nBulat (2019), \"Improved training of binary networks for human pose estimation and image recognition\"\nMartinez (2020): \"Training Binary Neural Networks with Real-to-Binary Convolutions\"\n\nAlizadeh, (2019 fig. 4) notice that pre-training can be viewed as a speed-up, but in their setup find that training from scratch gives a better accuracy. Bulat (2019) and Martinez (2020), on the contrary do use pre-training to achieve the best accuracy.  It is questionable whether the pre-training in these works is partial or not. I believe the result largely depends on the pre-training method used, which is not discussed in depth in the submission. More generally, some graduated optimization methods such as graduated smoothing or graduated non-convexity are known to help in finding better solutions / lead to faster optimization and in fact Bulat (2019) use pre-training with gradual transition from smooth activation to the sign function.  Relative to these points the technical contribution (one paragraph in the paper) is not significant. The empirical part of the contribution shows some effect, but does not indicate a breakthrough on its own. An investigation / design of pre-training schemes could make it more substantial.\n\nThe empirical analysis proposed does not rule out, and in fact supports, the methodology that for the best final accuracy, the full rather than partial pre-training is useful.\n\nThe gain of speed-up by a factor 1.3 (diminishing to close to 1 if we are interested in the best accuracy), is of little practical interest. In particular, a slight code optimization can give a similar speed-up without the complexity and hyperparameters involved in pre-training. The authors write  \"we are not aware of any effort to exploit binarization during the training phase\"\nThere are available public implementations that can optimize the forward pass of binary networks, in particular on GPU, while backward pass can stay in full precision. It could give a similar speed-up. In particular Courbariaux (2016) provides a GPU kernel and proposes a variant of BatchNorm with bit shifts rather than multiplications, specifically used at training time.\nMaking the emphasis on a relatively small speed-up that can be obtained to train sub-optimal models, in my view is not a good strategy to present this work. Rather the phenomenon that (partial) pre-training helps with the goal to improve the training methods more substantially I find of higher interest.\n\nFinally, I agree with the reviewers that the lottery ticket hypothesis (Frankle, 2020) work speaks of the speed-up only hypothetically and its main (and fairly in-depth) contribution is in demonstrating and investigating an interesting phenomenon about training and initialization, which I do not see relevant to this submission.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040535975, "tmdate": 1610474145973, "id": "ICLR.cc/2021/Conference/Paper518/-/Decision"}}}, {"id": "WA9xIOZCZGv", "original": null, "number": 19, "cdate": 1606285330709, "ddate": null, "tcdate": 1606285330709, "tmdate": 1606285330709, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "_nFP5RfY4_n", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Re: empirical evaluation", "comment": "The concern about applicability to other binary optimizers is a reasonable point, and indeed one that our experiments do not answer. To avoid generating potentially misleading conclusions about the performance of the method on binary optimizers not fully evaluated in the work, we will add clear statements to the introduction that our results are only validated on optimizing latent weights with SGD and ADAM.\n\n\nRegarding other hyperparameters (\"different learning rate schedules, etc\"), our evaluation includes 4 different networks on 3 different datasets. Each dataset uses a different learning rate schedule, batch size, and other network hyperparameters. While we agree in principle that more validation across hyperparameters would further validate our findings, this further validation is beyond the scope of this paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "CE7Hh9gJFje", "original": null, "number": 18, "cdate": 1606284483515, "ddate": null, "tcdate": 1606284483515, "tmdate": 1606284483515, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "TTuodXc-I16", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Re: Hyperparameter choices and sweeps", "comment": "> It is natural to wonder whether the results will generalize to other kinds of binarization strategies...\n\nThis is a reasonable question, and indeed one that our experiments do not answer. To avoid generating potentially misleading conclusions about the performance of the method on binarization schemes not fully evaluated in the work, we will add statements to the introduction that our results are only validated with static weight scaling and PACT activation scaling."}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "B0RViYtspSD", "original": null, "number": 17, "cdate": 1606284424813, "ddate": null, "tcdate": 1606284424813, "tmdate": 1606284424813, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "7laaCCGKl5", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Re: Regarding Other splits", "comment": "You are correct that this corresponds to the 100% point in Fig 3(b), which we did not include in our original set of experiments. The trend in Fig 3(b) is that past a roughly 50/50 split between full-precision and low-precision training, more full-precision training time results in lower accuracy than more low-precision training time; we should expect that this trend will continue to 100% full-precision training time, replicating the results in the literature that low-precision training tends to result in higher accuracy than full-precision training followed by quantization [3,10]. We will include an evaluation of this baseline, specifically an extension of Figure 3 to full-precision training followed by quantization (i.e., the points at 100%) in the final version of the paper.\n\n### Citations\n\n[3] \"An Empirical study of Binary Neural Networks' Optimisation\", Milad Alizadeh, Javier Fern\u00e1ndez-Marqu\u00e9s, Nicholas D. Lane, Yarin Gal.\n\n[10] \"Back to Simplicity: How to Train Accurate BNNs from Scratch?\" Joseph Bethge, Haojin Yang, Marvin Bornstein, Christoph Meinel."}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "6g67t0IvZHn", "original": null, "number": 16, "cdate": 1606284306824, "ddate": null, "tcdate": 1606284306824, "tmdate": 1606284306824, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "SRdC7SfFBUS", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Re: A few things to note", "comment": "> First, [8] was not cited in [1] and [9] is a later work, so [1] did not treat [8,9] as a baseline to improve upon.\n\n[8] is cited in [1] in Sections 1, 2, 5, and 7, for a total of 8 citations throughout the paper. [9] is a comparison between a generalization of techniques proposed in [1] and prior techniques (e.g., those of [8]) showing that the ultimate accuracy/sparsity tradeoffs achieved by [1] are also achievable with the theoretically slower classical pruning techniques like [8].\n\n> The point isn't about speeding up training, it is that the fast method leads to networks that are hard to train.\n\nThis is essentially the same point that we make when we cite [1] as an example of a paper that \"propose[s] techniques to speed up the training of efficient neural networks.\" By showing that it would be possible for the fast method to train accurate networks, rather than the inaccurate networks observed in prior work, [1] in effect (and in stated motivation) proposes speeding up training of efficient neural networks.\n\nAs you point out, there are certainly other framings of [1]'s contributions, and we do not claim that our paper is analogous in every way to [1] \u2014 just that [1] is one of many papers with the explicit stated motivation of speeding up training of efficient neural networks, providing evidence from the literature against the assertion that \"...the time to train a network is of much less importance...\" (R2).\n\n### Citations\n\n[1] \"The Lottery Ticket Hypothesis,\" Jonathan Frankle and Michael Carbin.\n\n[8] \"Learning both weights and connections for efficient neural networks,\" Song Han, Jeff Pool, John Tran, William J. Dally.\n\n[9] \"Comparing Rewinding and Fine-tuning in Neural Network Pruning,\" Alex Renda, Jonathan Frankle, Michael Carbin."}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "_nFP5RfY4_n", "original": null, "number": 15, "cdate": 1606189079827, "ddate": null, "tcdate": 1606189079827, "tmdate": 1606189079827, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "lY3NW41KteA", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Thanks for the response! ", "comment": "Thanks a lot for the authors' response, which addresses part of my previous concerns. \n\nFor the theoretical problem, I agree that experiments alone are important and rigorous analysis could come afterwards. As shared by R1, current experiments seem not enough for claiming \"a rigorous empirical evaluation\" due to the lack of universality evaluation.  I am kind of concerned about this point. "}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "SRdC7SfFBUS", "original": null, "number": 14, "cdate": 1606029856832, "ddate": null, "tcdate": 1606029856832, "tmdate": 1606029856832, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "z8HgJhB6eSF", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "A few things to note", "comment": "First, [8] was not cited in [1] and [9] is a later work, so [1] did not treat [8,9] as a baseline to improve upon.\n\n Regarding the first quote, it is misleading as you need to look into the next sentence\n\"However, if a network can be reduced in size, why do we\nnot train this smaller architecture instead in the interest of making training more efficient as well?\nContemporary experience is that the architectures uncovered by pruning are harder to train from the\nstart, reaching lower accuracy than the original networks\"\n\nThe point isn't about speeding up training, it is that the fast method leads to networks that are hard to train.\n\nI still stand by my remark that the main point of the lottery ticket paper is not speeding up training.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "7laaCCGKl5", "original": null, "number": 13, "cdate": 1605903961084, "ddate": null, "tcdate": 1605903961084, "tmdate": 1605903961084, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "opdqNfRjW9A", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Regarding Other splits between full-precision training and low-precision training", "comment": "I re-read the explanation for Fig 3b and and not sure if it answers my question. It's possible we're talking about different things, or that maybe I'm misinterpreting the figure. My understanding fo Fig 3b is that it is a different view of Fig 3a, which basically varies the amount of full-precision vs low-precision training in your proposed algorithm. \n\nWhen I said \"results would be stronger if also compared with full-precision training followed by quantization,\" I'm referring not to your proposed algorithm but to another baseline. This baseline trains in full-precision, saves the model, and then post-hoc quantizes it to a binary network. Basically it's just a baseline I might run if I didn't know about your method, and am just interested in the accuracy difference. Maybe it's just a matter of taking your full-precision models in Figure 2 (in orange) and then quantizing it. You might have these numbers but I couldn't figure it out from the paper. \n\n(I guess this might correspond to the 100% point in Fig 3(b) but there are no points at that region in the plot.)"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "TTuodXc-I16", "original": null, "number": 12, "cdate": 1605903356150, "ddate": null, "tcdate": 1605903356150, "tmdate": 1605903379778, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "h4-wXzhNMG", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Regarding Hyperparameter choices and sweeps", "comment": "Thanks for the author response. Yes, I think the results for multiple datasets in Table 1 are great, and understand that it takes significant time/compute investment to generate these results. \n\nMy main point, which I believe Reviewer4 shares, is that all results are focused on the PACT method. It is natural to wonder whether the results will generalize to other kinds of binarization strategies. I think it is not necessary to rerun all experiments using a different binarization method, but it would be helpful to at least show some preliminary experiments just to give more confidence to the generalizability of the results. \n\nThe other choice would be to state it more explicitly in writing that these results are for PACT and discuss alternative methods in the future work/conclusion section. "}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "F7Cb5Dqje9a", "original": null, "number": 11, "cdate": 1605902985618, "ddate": null, "tcdate": 1605902985618, "tmdate": 1605902985618, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "ldgxUogXDv2", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "thanks", "comment": "Thanks for the various clarifications. Regarding the author's \"general response\" to all reviewers, my replies will be placed separately in that thread. "}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "z8HgJhB6eSF", "original": null, "number": 10, "cdate": 1605835631167, "ddate": null, "tcdate": 1605835631167, "tmdate": 1605835631167, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "0-_gcpVePLR", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Regarding previous work on speedup", "comment": "> the work in [1] is not mainly about speeding up training time... The work is about learning sparse networks with high accuracy...\n\nSparse networks with high accuracy could have been obtained by training the full network for the full duration of training then pruning after training, as is standard practice [8, 9]. [1] shows that at initialization there exist sparse subnetworks that can train to match the accuracy of the full network, with the explicit stated motivation of improving training performance:\n\n> Abstract: \"However, if a network can be reduced in size, why do we not train this smaller architecture instead in the interest of making training more efficient as well?\"\n\n> Introduction: \"However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.\"\n\n> Introduction: \"Improve training performance. Since winning tickets can be trained from the start in isolation, a hope is that we can design training schemes that search for winning tickets and prune as early as possible.\"\n\nThat [1] does not measure training time costs is precisely why we cite it as an example of a paper that proposes speeding up training of efficient neural networks that does not evaluate wall-clock time. Of course, [1] is not a direct parallel to our paper, with claims and contributions that do not overlap with those of our paper; [1] is just one of several ICLR papers that include the motivation of lowering training costs for efficient neural networks, but that do not show explicit wall-clock speedups.\n\n### Citations\n[1] \"The Lottery Ticket Hypothesis,\" Jonathan Frankle and Michael Carbin.\n\n[8] \"Learning both weights and connections for efficient neural networks,\" Song Han, Jeff Pool, John Tran, William J. Dally.\n\n[9] \"Comparing Rewinding and Fine-tuning in Neural Network Pruning,\" Alex Renda, Jonathan Frankle, Michael Carbin."}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "0-_gcpVePLR", "original": null, "number": 9, "cdate": 1605683408483, "ddate": null, "tcdate": 1605683408483, "tmdate": 1605683408483, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "h4-wXzhNMG", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Regarding previous work on speedup", "comment": "I am less familiar with [2,3] but the work in [1] is not mainly about speeding up training time. The only discuss training time costs in the appendix! The work is about learning sparse networks with high accuracy, that was there main claim and what they showed in the experiments. This is completely unrelated to this work where the claim is not that you find a better network, or same performance on a faster network but that you train a fast network faster. "}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "lY3NW41KteA", "original": null, "number": 8, "cdate": 1605591939538, "ddate": null, "tcdate": 1605591939538, "tmdate": 1605591939538, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "XCN2gwRGFVt", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for feedback and commentary! We have provided a general response to all reviewers above ([https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG](https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG)). Here we will provide response to your individual points.\n\n> The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation, whether rigorous or not, why this method works.\n\nPlease see the discussion of theoretical analysis in the general response above.\n\n> There is a lack of explanation of the contradiction result with previous result proposed in Alizadeh et al (2019), which dismissed the approach of pre-training. Is there good explanation of such an opposite result?\n\nThe results compare the accuracies of each technique when training the network from random initialization with a fixed training budget. Compared to standard low-precision training, partial pre-training is therefore allotted half as much time in each phase, meaning the methods are compared at equal training budgets. We will clarify this in the final version of the paper.\n\n> It would be better to show results of different split between full-precision training and low-precision training.\n\nResults for different splits between full-precision training and low-precision training are presented and discussed in Section 6.\n\n> Regarding pre-training for BNN, there are some related works from the Bayesian perspective... [D]oes this imply that partial pre-training provides a better prior than full pre-training?\n\nThe connection to the Bayesian perspective of [8,9] is an interesting connection to explore in future work. To clarify the exact relationship between our claims and the analysis of [9], our results do not imply that partial pre-training provides a better prior than full pre-training. Figure 3a shows that for a fixed amount of low-precision training (i.e., elements on any given row) a larger amount of full-precision training (i.e., farther to the right) results in higher accuracy.\n\n### Citations\n\n[8] \"Learning Discrete Weights Using the Local Reparameterization Trick,\" Oran Shayer, Dan Levi, Ethan Fetaya.\n\n[9] \"Training Binary Neural Networks using the Bayesian Learning Rule,\" Xiangming Meng, Roman Bachmann, Mohammad Emtiyaz Khan.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "i-V21IxK46c", "original": null, "number": 7, "cdate": 1605591881085, "ddate": null, "tcdate": 1605591881085, "tmdate": 1605591881085, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "5KaoPSoBUCv", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for feedback and commentary! We have provided a general response to all reviewers above ([https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG](https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG)). Here we will provide response to your individual points.\n\n> The comparison between the proposed method and quantization-finetuning method is lacked\n\nBased on our understanding of this question (quantizing a pre-trained network then fine-tuning it for a relatively short duration), we do compare against this approach in Section 6, Figure 3. Please see the discussion of other splits between full-precision training and low-precision training in the general response.\n\n> The analysis of the proposed method should be also enhanced.\n\nPlease see the discussion of theoretical and causal results in the general response above.\n\n> The authors claim that the proposed method allows for faster from-scratch training of binarized neural. This seems contradictory to the partial pretraining. The authors may provide more discussion and clarification.\n\nThe results compare the accuracies of each technique when training the network from random initialization with a fixed training budget. Compared to standard low-precision training, partial pre-training is therefore allotted half as much time in each phase, meaning the methods are compared at equal training budgets. We will clarify this in the final version of the paper.\n\n> The improvement of the proposed method is not significant. The proposed algorithm speeds up the training marginally and cannot improve the final test accuracy. This limits the contribution.\n\nPlease see both the discussions of significance of training speedup and also of final accuracy in the general response above."}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "praWbZ5i3db", "original": null, "number": 6, "cdate": 1605591805753, "ddate": null, "tcdate": 1605591805753, "tmdate": 1605591805753, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "V3vuXmHW9Wb", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for feedback and commentary! We have provided a general response to all reviewers above ([https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG](https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG)). Here we will provide response to your individual points.\n\n> The main claim is speeding up training by a factor of 1.2/1.6. While this can help the importance of speeding up training (unless by a much larger factor) is quiet limited. The useful speedup BNN present is at inference time, and the time to train a network is of much less importance (at least for this kind of speedup).\n\nPlease see the discussion of significance of training speedup in the general response above.\n\n> It looks like if you want take the best model on test, then you don't get any speedup...\n\nPlease see the discussion of final accuracy in the general response above.\n\n> Results on cifar-10 seem quiet poor (both full precision and 1-bit)...\n\nThe accuracies reported for CIFAR-10 are standard for the network and hyperparameters used in the evaluation [7; Tables 3,4]. Partial pre-training can be applied with an arbitrary low-precision training setup (including other quantization approaches or other training schemes like that of [8]) in the second phase, though validation of this for other quantization approaches is left to future work. Please also see the discussion of hyperparameter choices and sweeps in the general response above.\n\n### Citations\n[7] \"Binary Neural Networks: A Survey,\" Haotong Qin, Ruihao Gong, Xianglong Liu, Xiao Bai, Jingkuan Song, Nicu Sebe.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "ldgxUogXDv2", "original": null, "number": 5, "cdate": 1605591693807, "ddate": null, "tcdate": 1605591693807, "tmdate": 1605591693807, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "XV0UJQAVnL", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for feedback and commentary! We have provided a general response to all reviewers above ([https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG](https://openreview.net/forum?id=H6ZWlQrPGS2&noteId=h4-wXzhNMG)). Here we will provide response to your individual points.\n\n> The paper mainly compares the proposed method with low-precision training, but the results would be stronger if also compared with full-precision training followed by quantization...\n\nBased on our understanding of this question (quantizing a pre-trained network then fine-tuning it for a relatively short duration), we do compare against this approach in Section 6, Figure 3. Please see the discussion of other splits between full-precision training and low-precision training in the general response.\n\n> Experiment with more quantization methods...\n\nPlease see the discussion of hyperparameter choices and sweeps in the general response above.\n\n> A 1.2x-1.6x speedup on a training process ... seems not so impactful in the grander scheme of things\n\nPlease see the discussion of significance of training speedup in the general response above.\n\n> Is what you call low precision training similar to the mixed-precision training now implemented in TensorCore NVIDIA GPUs?\n\nWhat we define as low-precision training (training with weights constrained to lie in a small fixed set) is possible to accelerate in some cases in NVIDIA Tensor Cores, though not always. Specifically, according to [https://developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/](https://developer.nvidia.com/blog/tensor-cores-mixed-precision-scientific-computing/) NVIDIA Tensor Cores can accelerate low-precision training when the weights are represented by as few as 8 bits. In this paper, we consider low-precision training with 1 bit, which NVIDIA Tensor Cores do not have native support for accelerating. As noted in Section 2, we are not aware of any existing results that show faster wall-clock speeds for training binarized neural networks.\n\n> Table 2: what is it/s. Is it iterations per second?\n\nCorrect, this is iterations per second. We will clarify this in the final version of the paper.\n\n> Table 1 shows learning rate schedule for t up to 60k in CIFAR or 450k in ImageNet, but Figure 1 stops way before those points in the x-axis. This was somewhat confusing.\n\nEach point on the plot shows a network trained for the specified amount of time using a learning rate schedule which is compressed down to that amount of time, as specified in Algorithm 1. We will clarify this in the final version of the paper.\n\n> ...is the full precision training part of the time included in the calculation?\n\nCorrect, full precision training time is included in the calculation. Each point on the plot shows the accuracy when training from random initialization within the given budget."}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Paper518/Reviewers", "ICLR.cc/2021/Conference/Paper518/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "opdqNfRjW9A", "original": null, "number": 4, "cdate": 1605591385218, "ddate": null, "tcdate": 1605591385218, "tmdate": 1605591453317, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "h4-wXzhNMG", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "General response to all reviewers (pt 2)", "comment": "### Other splits between full-precision training and low-precision training (Reviewers 1, 4)\n\n> Reviewer 1: \"The paper mainly compares the proposed method with low-precision training, but the results would be stronger if also compared with full-precision training followed by quantization.\"\n\n> Reviewer 4: \"It would be better to show results of different split between full-precision training and low-precision training.\"\n\nWe do compare against full-precision training followed by quantization in Section 6, Figure 3. Specifically, the right half of Figure 3(b) shows instances of training a network at full-precision for the majority of the budgeted training time, then quantizing and fine-tuning it for a small amount of additional training time. We find that partial pre-training results in equivalent or higher accuracies in general than this technique on a CIFAR-10 ResNet-20. We will include equivalent comparisons on all networks in the final version of the paper.\n\n### Citations\n\n[1] \"The Lottery Ticket Hypothesis,\" Jonathan Frankle and Michael Carbin.\n\n[2] \"SNIP: Single-shot Network Pruning based on Connection Sensitivity,\" Namhoon Lee, Thalaiyasingam Ajanthan, Philip H. S. Torr.\n\n[3] \"An Empirical study of Binary Neural Networks' Optimisation\", Milad Alizadeh, Javier Fern\u00e1ndez-Marqu\u00e9s, Nicholas D. Lane, Yarin Gal.\n\n[4] \"Energy and Policy Considerations for Deep Learning in NLP,\" Emma Strubell, Ananya Ganesh, Andrew McCallum.\n\n[5] \"Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1,\" Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, Yoshua Bengio.\n\n[6] \"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding,\" Song Han, Huizi Mao, William J. Dally.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Paper518/Reviewers", "ICLR.cc/2021/Conference/Paper518/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "h4-wXzhNMG", "original": null, "number": 3, "cdate": 1605591341836, "ddate": null, "tcdate": 1605591341836, "tmdate": 1605591341836, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment", "content": {"title": "General response to all reviewers (pt 1)", "comment": "Thanks to all of the reviewers for their feedback. Here we provide responses to common questions and concerns raised by reviewers. We also provide individualized responses to each reviewer as a comment to each reviewer.\n\n### Significance of training speedup (Reviewers 2, 3)\n\n> Reviewer 2: \"The useful speedup BNN present is at inference time, and the time to train a network is of much less importance...\"\n\n> Reviewer 3: \"The improvement of the proposed method is not significant. The proposed algorithm speeds up the training marginally... This limits the contribution.\"\n\nThe claim that the speedup exhibited by partial pre-training is too small or not important is a subjective claim that does not accord with the interest in such techniques in academia and industry. ICLR has accepted and awarded papers that propose techniques to speed up the training of efficient neural networks, including papers that provide no evaluation of wall-clock time [1, 2, 3]. Though we agree that motivation for acquiring efficient neural networks (e.g., BNNs) is to speed up inference, the cost of training such networks is still a relevant concern when training costs are large [4], especially given that BNNs train less efficiently than full-precision neural networks [3,5]. Our paper identifies a clear problem and makes a tangible step towards solving it, reducing the cost of acquiring binarized neural networks.\n\n### Final accuracy (Reviewers 2, 3, 4)\n\n> Reviewer 2: \"It looks like if you want take the best model on test, then you don't get any speedup\"\n\n> Reviewer 3: \"The improvement of the proposed method is not significant. The proposed algorithm ... cannot improve the final test accuracy...\"\n\n> Reviewer 4: \"it took approximately the same time to reach the final precision even though the proposed method achieves higher accuracy before saturation\"\n\nAs discussed in the introduction and methodology sections, we focus our analysis on the region early in training that exposes the tradeoff between accuracy and training cost, similar to the tradeoffs posed by BNNs themselves. Partial pre-training does not result in a higher final test accuracy, and we do not claim it does in the paper; instead, it results in a more Pareto optimal tradeoff curve between training cost and resultaning accuracy.\n\n### Hyperparameter choices and sweeps (Reviewers 1, 4)\n\n> Reviewer 1: \"Experiment with more quantization methods. Currently we do not know whether the proposed method is uniquely suited to the PACT method used, or is a general technique.\"\n\n> Reviewer 4: \"several dimensions of the hyper-parameters are not explored, other Binary optimizers are not considered, different learning rate schedules, etc.\"\n\nWe have already validated partial pre-training across multiple datasets, networks, optimizers, learning rate schedules, and other training hyperparameters parameters as noted in Table 1. All of our hyperparameter choices, including choices of quantization methods, are standard in literature and practice and do not require additional hyperparameters that must be searched over or validated across. Given the cost of generating the plots in Figure 1, validation or repudiation of the proposed technique in other settings beyond the standard settings presented in the paper is beyond the scope of this paper, and is therefore left for future work.\n\n### Theoretical and causal results (Reviewers 3, 4)\n\n> Reviewer 3: \"The reason why partial pretraining can improve the training of binary neural networks should be investigated more deeply.\"\n\n> Reviewer 4: \"The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation, whether rigorous or not, why this method works.\"\n\nOur contributions include the partial pre-training algorithm along with a rigorous empirical evaluation and analysis of the proposed algorithm. Rigorous theoretical results and causal analysis are left for future work, as is often the case in empirical papers at ICLR [1,6].\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper518/Reviewers", "ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Paper518/Authors"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "H6ZWlQrPGS2", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper518/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper518/Authors|ICLR.cc/2021/Conference/Paper518/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923870104, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Comment"}}}, {"id": "V3vuXmHW9Wb", "original": null, "number": 1, "cdate": 1603359765301, "ddate": null, "tcdate": 1603359765301, "tmdate": 1605024670513, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Review", "content": {"title": "Little novelty and weak results", "review": "The paper suggests a method for training binary neural networks. The proposed method is to partially train with full precision and then continue with binarized training using the straight-through estimator. The method is very simple and there is very limited technical contribution, so in order to be worthy of publication it needs to be supported with compelling experimental results. Unfortunately this is not the case.\n\nThe main claim is speeding up training by a factor of 1.2/1.6. While this can help the importance of speeding up training (unless by a much larger factor) is quiet limited. The useful speedup BNN present is at inference time, and the time to train a network is of much less importance (at least for this kind of speedup).\nFrom Fig.1 I am not convinced that the speedup claims hold. You can see the test accuracy for binary training on 3 of the experiments reaches the Partial Pre-training level but it doesn't look like it completely flattened out yet. It looks like if you want take the best model on test, then you don't get any speedup\nResults on cifar-10 seem quiet poor (both full precision and 1-bit). For example \"Learning discrete weights using the local reparametrization trick\" gets 93.2 1-bit acc on cifar-10 with VGG.", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141344, "tmdate": 1606915810041, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper518/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Review"}}}, {"id": "XCN2gwRGFVt", "original": null, "number": 2, "cdate": 1603701784638, "ddate": null, "tcdate": 1603701784638, "tmdate": 1605024670429, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Review", "content": {"title": "Providing a heuristic method to speed up low-precision training ", "review": "This paper proposed one simple method called partial pre-training to speed up the training of binary neural networks (BNN). The pros and cons are as follows:\n\nPros:\n1. The partial pre-training method is simple and easy to implement;\n2. For standard binary optimizer like the straight-through-estimator (STE), the method improves the training speed to some extent;\n\nCons:\n1. The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation, whether rigorous or not, why this method works. \n\n2. As described in Section 7 by the author themselves, there are several apparent limitations of current evaluation, e.g., several dimensions of the hyper-parameters are not explored, other Binary optimizers are not considered, different learning rate schedules, etc. As a result, it is unconvinced that the partial pre-training could universally improve the speed as a general method. In addition, the improvement of speed-up are not very apparent especially for ResNet-20 and ResNet-34 as shown in Fig. 1 and Fig. 2, i.e., it took approximately the same time to reach the final precision even though the proposed method achieves higher accuracy before saturation. Given the inadequate evaluations and lack of theoretical explanations, this might be due to unfair comparison. \n\n3. There is a lack of explanation of the contradiction result with previous result proposed in Alizadeh et al (2019), which dismissed the approach of pre-training. Is there good explanation of such an opposite result? It would be better to show results of different split between full-precision training and low-precision training. \n\n4. Regarding pre-training for BNN, there are some related works from the Bayesian perspective. In Shayer et al. (2018), they used the result of full-precision training as the prior for the binary training, which improves the final result, as opposed to Alizadeh et al (2019). The Bayesian perspective provides an explanation of the effectiveness of a good prior. In Meng et al. (2020), they showed that STE could be viewed as Bayesian and obtained good result even with a uniform prior. Also, the posterior obtained after full-training (binary) could be used as prior to enable continual learning, which shows effectiveness of the prior. Given the above results, since the authors demonstrate that partial pre-training can increase the speed for STE, does this imply that partial pre-training provides a better prior than full pre-training? If so, why? \n\nShayer, O., Levi, D., and Fetaya, E. Learning discrete weights using the local reparameterization trick. ICLR, 2018.\nMeng, X, Bachmann. R., Khan. E. Training Binary Neural Networks using the Bayesian Learning Rule. ICML, 2020.\n\n\n\n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141344, "tmdate": 1606915810041, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper518/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Review"}}}, {"id": "XV0UJQAVnL", "original": null, "number": 4, "cdate": 1603908066626, "ddate": null, "tcdate": 1603908066626, "tmdate": 1605024670359, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Review", "content": {"title": "More comparisons and longer training runs will make the result more impressive", "review": "\nThis paper addresses the problem of slower training speed with low-precision training of neural nets. It presents a simple solution: first train with full precision on half of the budgeted trainining time, then train with low pecision in the remaining time. This achieves 1.2x - 1.6x speedup compared to low-precision training. \n\nPro:\n- The proposed idea is simple and it is nice to see that it works\n\nCons:\n- I feel there is not enough content (in terms of ideas and experiments) to warrant a full paper. Compared to other ICLR papers, the contributions seem on the low side. See suggestions below. \n- The paper mainly compares the proposed method with low-precision training, but the results would be stronger if also compared with full-precision training followed by quantization. This is especially because all the low-precision accuracies trail behind the full precision ones in the results. \n\nSuggestions:\n- Experiment with more quantization methods. Currently we do not know whether the proposed method is uniquely suited to the PACT method used, or is a general technique. \n- A 1.2x-1.6x speedup on a training process that takes 600 seconds (e.g. CIFG-10 result in Fig 2) seems not so impactful in the grander scheme of things. Even the 12500 second training time is just <4 hours. I understand the results should transfer, but the results would be more impressive if done on larger training runs.\n\nMinor questions/comments:\n- Please comment on the terminology. Is what you call low precision training similar to the mixed-precision training now implemented in TensorCore NVIDIA GPUs?\n- Table 2: what is it/s. Is it iterations per second? \n- Table 1 shows learning rate schedule for t up to 60k in CIFAR or 450k in ImageNet, but Figure 1 stops way before those points in the x-axis. This was somewhat confusing. \n- Another clarification point about Fig 1 and 2: for the proposed method, is the full precision training part of the time included in the calculation? I believe so but just want to double-check. \n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141344, "tmdate": 1606915810041, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper518/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Review"}}}, {"id": "5KaoPSoBUCv", "original": null, "number": 3, "cdate": 1603857106155, "ddate": null, "tcdate": 1603857106155, "tmdate": 1605024670288, "tddate": null, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "invitation": "ICLR.cc/2021/Conference/Paper518/-/Official_Review", "content": {"title": "review", "review": "#### Comments\nSummary:\n\nThe authors propose a fast binarized neural network training algorithm that splits the whole training process into the full precision training stage and the binary training stage. The experimental results show some improvement about training speed in terms of iterations and wall clock time. Generally, the paper is well written.\n\nStrength:\n--The idea is reasonable and the method is presented clearly.\n--The experimental results indicate that the proposed method can accelerate the convergence of the binary networks on image classification and collaborative filtering.\n\nWeakness:\n--The comparison between the proposed method and quantization-finetuning method is lacked, which seems like a closely related work.\n--The analysis of the proposed method should be also enhanced. The reason why partial pretraining can improve the training of binary neural networks should be investigated more deeply.\n\nComments:\n(1) The authors claim that the proposed method allows for faster from-scratch training of binarized neural. This seems contradictory to the partial pretraining. The authors may provide more discussion and clarification. \n(2) The improvement of the proposed method is not significant. The proposed algorithm speeds up the training marginally and cannot improve the final test accuracy. This limits the contribution.\n\nOverall, the reviewer doesn\u2019t recommend accepting this manuscript at its form. The author may demonstrate more differences between the proposed method and the standard quantization-finetuning method.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper518/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper518/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Fast Binarized Neural Network Training with Partial Pre-training", "authorids": ["~Alex_Renda2", "~Joshua_Wolff_Fromm1"], "authors": ["Alex Renda", "Joshua Wolff Fromm"], "keywords": ["binarized neural network", "binary", "quantized", "1-bit", "low precision"], "abstract": "Binarized neural networks, networks with weights and activations constrained to lie in a 2-element set, allow for more time- and resource-efficient inference than standard floating-point networks. However, binarized neural networks typically take more training to plateau in accuracy than their floating-point counterparts, in terms of both iteration count and wall clock time. We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks by first training the network as a standard floating-point network for a short amount of time, then converting the network to a binarized neural network and continuing to train from there. Without tuning any hyperparameters across four networks on three different datasets, partial pre-training is able to train binarized neural networks between $1.26\\times$ and $1.61\\times$ faster than when training a binarized network from scratch using standard low-precision training.\n", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "renda|fast_binarized_neural_network_training_with_partial_pretraining", "one-sentence_summary": "We demonstrate a technique, partial pre-training, that allows for faster from-scratch training of binarized neural networks.", "pdf": "/pdf/2fd73f4b3bcc5d360f0967c0647400b9ef196138.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=4xlKhV7bbA", "_bibtex": "@misc{\nrenda2021fast,\ntitle={Fast Binarized Neural Network Training with Partial Pre-training},\nauthor={Alex Renda and Joshua Wolff Fromm},\nyear={2021},\nurl={https://openreview.net/forum?id=H6ZWlQrPGS2}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "H6ZWlQrPGS2", "replyto": "H6ZWlQrPGS2", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper518/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538141344, "tmdate": 1606915810041, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper518/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper518/-/Official_Review"}}}], "count": 23}