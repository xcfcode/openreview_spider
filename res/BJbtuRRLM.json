{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124481268, "tcdate": 1518426233154, "number": 103, "cdate": 1518426233154, "id": "BJbtuRRLM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "BJbtuRRLM", "signatures": ["~Yen-Chang_Hsu1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The experiments on three different image datasets show the student network gain a performance boost with proposed training strategy.", "paperhash": "xu|training_shallow_and_thin_networks_for_acceleration_via_knowledge_distillation_with_conditional_adversarial_networks", "keywords": [], "_bibtex": "@misc{\n  xu2018training,\n  title={Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},\n  author={Zheng Xu and Yen-Chang Hsu and Jiawei Huang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJbtuRRLM}\n}", "authorids": ["xuzh@cs.umd.edu", "yenchang.hsu@gatech.edu", "jhuang@honda-ri.com"], "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "TL;DR": "Adversarial training for transferring knowledge from teacher network to student network", "pdf": "/pdf/da0041af038cb3f2f944ec4837f543c359600afc.pdf"}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582783390, "tcdate": 1520632248026, "number": 1, "cdate": 1520632248026, "id": "ryxT-tgFM", "invitation": "ICLR.cc/2018/Workshop/-/Paper103/Official_Review", "forum": "BJbtuRRLM", "replyto": "BJbtuRRLM", "signatures": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer2"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "Authors investigate knowledge distillation (KD) in order to train a shallow and thin  student network from a deeper and wider teacher network. Instead of minimizing the KL between the teacher and student, authors propose to match the output logits using adversarial training.  They evaluate their propose approach on ImageNet32, CIFAR10 and CIFAR100 where their approach outperforms KD.\n\nUsing adversarial training in a KD setting appears novel to me. However, authors are to use a combination of adversarial loss with L1 and cross-entropy to ease the optimization of models. In their ablation study it is not clear how important is the GAN loss as the use of cross entropy + L1 leads already to good results.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The experiments on three different image datasets show the student network gain a performance boost with proposed training strategy.", "paperhash": "xu|training_shallow_and_thin_networks_for_acceleration_via_knowledge_distillation_with_conditional_adversarial_networks", "keywords": [], "_bibtex": "@misc{\n  xu2018training,\n  title={Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},\n  author={Zheng Xu and Yen-Chang Hsu and Jiawei Huang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJbtuRRLM}\n}", "authorids": ["xuzh@cs.umd.edu", "yenchang.hsu@gatech.edu", "jhuang@honda-ri.com"], "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "TL;DR": "Adversarial training for transferring knowledge from teacher network to student network", "pdf": "/pdf/da0041af038cb3f2f944ec4837f543c359600afc.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582783194, "id": "ICLR.cc/2018/Workshop/-/Paper103/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper103/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper103/AnonReviewer1"], "reply": {"forum": "BJbtuRRLM", "replyto": "BJbtuRRLM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582783194}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582765940, "tcdate": 1520639884821, "number": 2, "cdate": 1520639884821, "id": "HJrqJjetf", "invitation": "ICLR.cc/2018/Workshop/-/Paper103/Official_Review", "forum": "BJbtuRRLM", "replyto": "BJbtuRRLM", "signatures": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "Summary: This paper describes using GAN loss instead of minimizing KL-divergence between teacher & students to make student network better. The authors show it is a helpful on small datasets, and improves distillation result based on Student + L1 method.\n\nOverall it is an interesting paper. My concern is whether this method still works on larger dataset. Many algorithms works well on small network won't work \nwell on large dataset. So if the author is able to demo result on standard ImageNet level dataset it will be more convincing. \n\nQuestion: Why add adversarial loss only on logit, instead of deep features?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The experiments on three different image datasets show the student network gain a performance boost with proposed training strategy.", "paperhash": "xu|training_shallow_and_thin_networks_for_acceleration_via_knowledge_distillation_with_conditional_adversarial_networks", "keywords": [], "_bibtex": "@misc{\n  xu2018training,\n  title={Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},\n  author={Zheng Xu and Yen-Chang Hsu and Jiawei Huang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJbtuRRLM}\n}", "authorids": ["xuzh@cs.umd.edu", "yenchang.hsu@gatech.edu", "jhuang@honda-ri.com"], "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "TL;DR": "Adversarial training for transferring knowledge from teacher network to student network", "pdf": "/pdf/da0041af038cb3f2f944ec4837f543c359600afc.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582783194, "id": "ICLR.cc/2018/Workshop/-/Paper103/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper103/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer2", "ICLR.cc/2018/Workshop/Paper103/AnonReviewer1"], "reply": {"forum": "BJbtuRRLM", "replyto": "BJbtuRRLM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582783194}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573568501, "tcdate": 1521573568501, "number": 112, "cdate": 1521573568166, "id": "S1OTARCYG", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "BJbtuRRLM", "replyto": "BJbtuRRLM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Congratulations, your paper was accepted to the ICLR workshop."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The experiments on three different image datasets show the student network gain a performance boost with proposed training strategy.", "paperhash": "xu|training_shallow_and_thin_networks_for_acceleration_via_knowledge_distillation_with_conditional_adversarial_networks", "keywords": [], "_bibtex": "@misc{\n  xu2018training,\n  title={Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},\n  author={Zheng Xu and Yen-Chang Hsu and Jiawei Huang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJbtuRRLM}\n}", "authorids": ["xuzh@cs.umd.edu", "yenchang.hsu@gatech.edu", "jhuang@honda-ri.com"], "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "TL;DR": "Adversarial training for transferring knowledge from teacher network to student network", "pdf": "/pdf/da0041af038cb3f2f944ec4837f543c359600afc.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1521242059836, "tcdate": 1521242059836, "number": 2, "cdate": 1521242059836, "id": "HkVR1RFKz", "invitation": "ICLR.cc/2018/Workshop/-/Paper103/Official_Comment", "forum": "BJbtuRRLM", "replyto": "Hkd20q8tM", "signatures": ["ICLR.cc/2018/Workshop/Paper103/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper103/Authors"], "content": {"title": "thanks for comments", "comment": "We thank the reviewer for the insightful comments. We will improve the manuscript based on the suggestions, and we provide response to specific questions below.\n\nQ: It seems that the teacher output t_i should be fixed during the whole process, and is it helpful to train the teacher network simultaneously?\nA: Teacher ouputs are fixed in the proposed method. We are not sure if it is helpful to train the teacher. Wang et al. 2016 observes the benefits of jointly training. However, training together may harm the performance of both teacher and student if they agree on the same bad solution. Moreover, training teacher requires much more computation and memory cost.  \n\nWang, J.; Wei, Z.; Zhang, T.; and Zeng, W. 2016. Deeply-fused nets. arXiv preprint arXiv:1605.07716\n\n\nQ: This learning setting is actually quite similar to the learning using privileged learning (LUPI) setting, and the authors should also discuss some relationships between knowledge distillation and LUPI.\nA: Thanks for the reference. We agree that it is similar to LUPI in a broad sense that they all trained with information that is not used in inference. We will discuss more details in the next version and an extended draft. We feel that in-depth discussion of knowledge distillation and LUPI is slightly beyond the scope of this workshop manuscript. We briefly discuss some relationship here,\n1) LUPI is a general concept on using extra information for training, while knowledge distillation focus on transfering knowledge from teacher to student.\n2) Despite LUPI is very general, in a lot of previous works, the auxiliary information provided in training is often extra (e.g. text description of images for image classification task). The auxiliary information in knowledge distillation is embedded in the network structure of teacher and the pre-trained model, which is discovered from the same set of training samples (e.g. images for image classification task).\n3) Despite LUPI is very general, LUPI is often implemented as variants of SVM+ in previous works, while we focus on knowledge distillation for neural networks."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The experiments on three different image datasets show the student network gain a performance boost with proposed training strategy.", "paperhash": "xu|training_shallow_and_thin_networks_for_acceleration_via_knowledge_distillation_with_conditional_adversarial_networks", "keywords": [], "_bibtex": "@misc{\n  xu2018training,\n  title={Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},\n  author={Zheng Xu and Yen-Chang Hsu and Jiawei Huang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJbtuRRLM}\n}", "authorids": ["xuzh@cs.umd.edu", "yenchang.hsu@gatech.edu", "jhuang@honda-ri.com"], "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "TL;DR": "Adversarial training for transferring knowledge from teacher network to student network", "pdf": "/pdf/da0041af038cb3f2f944ec4837f543c359600afc.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222449178, "id": "ICLR.cc/2018/Workshop/-/Paper103/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJbtuRRLM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper103/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper103/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper103/Reviewers", "ICLR.cc/2018/Workshop/Paper103/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222449178}}, "tauthor": "xuzh@cs.umd.edu"}, {"tddate": null, "ddate": null, "tmdate": 1521032880307, "tcdate": 1521032880307, "number": 1, "cdate": 1521032880307, "id": "Hkd20q8tM", "invitation": "ICLR.cc/2018/Workshop/-/Paper103/Official_Comment", "forum": "BJbtuRRLM", "replyto": "BJbtuRRLM", "signatures": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper103/AnonReviewer3"], "content": {"title": "An interesting method to improve the teacher-student model for the purpose of training deep learning model with simpler structure", "comment": "This paper presents an interesting method to utilize the auxiliary information from a larger and accurate teacher network.\nThe  GAN idea has been adapted to train a discriminator model D(.) to help generalize a network with smaller size (i.e., the student network). \nThe proposed method is verified on 3 datasets and it shows the superior performance when compared with the work from Hinton for the knowledge distillation. \nThe key for the performance improvements comes from both the L1 norm loss as in (4) and the GAN loss as in (5). The L1 norm enforces that the output from the student network should be close to the output from the teachers, while the loss in (5) learns a discriminator to discriminate between the Fake/Real output from the student. \nIt seems that the teacher output t_i should be fixed during the whole process, and is it helpful to train the teacher network simultaneously?\nI think it maybe more clearer if the authors could use a figure to show the structure of the proposed loss structure.\nThis learning setting is actually quite similar to the learning using privileged learning (LUPI) setting, and the authors should also discuss some relationships between knowledge distillation and LUPI.\n\nVladimir Vapnik, Akshay Vashist:\nA new learning paradigm: Learning using privileged information. Neural Networks 22(5-6): 544-557 (2009)\n\nXinxing Xu, Joey Tianyi Zhou, Ivor W. Tsang, Zheng Qin, Rick Siow Mong Goh, Yong Liu:\nSimple and Efficient Learning using Privileged Information. CoRR abs/1604.01518 (2016)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks", "abstract": "There is an increasing interest on accelerating neural networks for real-time applications. We study the student-teacher strategy, in which a small and fast student network is trained with the auxiliary information learned from a large and accurate teacher network. We propose to use conditional adversarial networks to learn the loss function to transfer knowledge from teacher to student. The experiments on three different image datasets show the student network gain a performance boost with proposed training strategy.", "paperhash": "xu|training_shallow_and_thin_networks_for_acceleration_via_knowledge_distillation_with_conditional_adversarial_networks", "keywords": [], "_bibtex": "@misc{\n  xu2018training,\n  title={Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks},\n  author={Zheng Xu and Yen-Chang Hsu and Jiawei Huang},\n  year={2018},\n  url={https://openreview.net/forum?id=BJbtuRRLM}\n}", "authorids": ["xuzh@cs.umd.edu", "yenchang.hsu@gatech.edu", "jhuang@honda-ri.com"], "authors": ["Zheng Xu", "Yen-Chang Hsu", "Jiawei Huang"], "TL;DR": "Adversarial training for transferring knowledge from teacher network to student network", "pdf": "/pdf/da0041af038cb3f2f944ec4837f543c359600afc.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1519222449178, "id": "ICLR.cc/2018/Workshop/-/Paper103/Official_Comment", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "BJbtuRRLM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper103/Authors|ICLR.cc/2018/Workshop/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper103/AnonReviewer[0-9]+|ICLR.cc/2018/Workshop/Paper103/Authors|ICLR.cc/2018/Workshop/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Workshop/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Workshop/Paper103/Reviewers", "ICLR.cc/2018/Workshop/Paper103/Authors", "ICLR.cc/2018/Workshop/Program_Chairs"], "cdate": 1519222449178}}}], "count": 6}