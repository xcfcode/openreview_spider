{"notes": [{"id": "rJliMh09F7", "original": "B1xcGa1qY7", "number": 1296, "cdate": 1538087954886, "ddate": null, "tcdate": 1538087954886, "tmdate": 1557202706296, "tddate": null, "forum": "rJliMh09F7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryxSLIWZgE", "original": null, "number": 1, "cdate": 1544783436783, "ddate": null, "tcdate": 1544783436783, "tmdate": 1545354523902, "tddate": null, "forum": "rJliMh09F7", "replyto": "rJliMh09F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Meta_Review", "content": {"metareview": "The paper proposes a regularization term on the generator's gradient that increases sensitivity of the generator to the input noise variable in conditional and unconditional Generative Adversarial networks, and results in multimodal predictions. All reviewers agree that this is a simple and useful addition to current GANs. Experiments that demonstrate the trade off between diversity and generation quality would be important to include, as well as the experiment on using the proposed method on unconditional GANs, which was conducted during the discussion period. ", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "a simple regularization for preventing mode collapse"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1296/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352890588, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJliMh09F7", "replyto": "rJliMh09F7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1296/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1296/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352890588}}}, {"id": "Hyg2WIvq27", "original": null, "number": 2, "cdate": 1541203459777, "ddate": null, "tcdate": 1541203459777, "tmdate": 1544564885275, "tddate": null, "forum": "rJliMh09F7", "replyto": "rJliMh09F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Review", "content": {"title": "Well written paper with a simple idea for preventing mode-collapse in GANs but with insufficiently experimental validation", "review": "The paper proposes a simple way of addressing the issue of mode-collapse by adding a regularisation to force the outputs to be diverse. Specifically, a loss is added that maximises the l2 loss between the images generated, normalised by the distance between the corresponding latent codes. This method is also used to control the balance between visual quality and diversity.\n\nThe paper is overall well written, introducing and referencing well existing concepts, and respected the 8 pages recommendation.\n\nWhy was the maximum theta (the bound for numerical stability) incorporated in equation 2? What happens if this is omitted in practice? How is this determined?\n\nIn section 4, an increase of the gradient norm of the generator is implied: does this have any effect on the robustness/sensitivity of the model to adversarial attacks?\n\nIn section 5, how is the \u201cappropriate CGAN\u201d determined?\n\nMy main issue is with the experimental setting that is somewhat lacking. The visual quality of the samples illustrated in the paper is inferior to that observed in the state-of-the-art, begging the question of whether this is a tradeoff necessary to obtain better diversity or if it is a consequence of the additional regularisation.. The diversity observed seems to mainly be attributable to colour differences rather than more elaborate differences. Even quantitatively, the proposed method seems only marginally better than other methods.\n\nUpdate post rebuttal\n-----------------------------\nThe experimental setting that is a little lacking. Qualitatively and quantitatively, the improvements seem marginal, with no significant improvement shown. I would have liked a better study of the tradeoff between visual quality and diversity, if necessary at all.\n\nHowever, the authors addressed well the issues. Overall, the idea is interesting and simple and, while the paper could be improved with some more work, it would benefit the ICLR readership in its current form, so I would recommend it as a poster -- I am increasing my score to that effect.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Review", "cdate": 1542234261177, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJliMh09F7", "replyto": "rJliMh09F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335915726, "tmdate": 1552335915726, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkexNsjFAQ", "original": null, "number": 5, "cdate": 1543252775738, "ddate": null, "tcdate": 1543252775738, "tmdate": 1543361104563, "tddate": null, "forum": "rJliMh09F7", "replyto": "Hyg2WIvq27", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "content": {"title": "Response to  Reviewer 1 (part 1)", "comment": "We appreciate your constructive and detailed comments. Due to the character limit of openreview comment system, we provide our responses in two parts. \n\n(1). \u201cWhy was the maximum theta (the bound for numerical stability) incorporated in Equation 2? What happens if this is omitted in practice? How is this determined?\u201d\n\nIn principle, our regularization term (||G(x,z1)-G(x,z2)||/||z1-z2||) is an unbounded operator because 1) its numerator can grow arbitrarily large with the unbounded generator and 2) its denominator can approach zero with the almost identical latent codes. The \\tau in Equation 2 provides a bound to our regularization term thus ensures its numerical stability. However, we found that our regularization term is practically bounded in most conditional GAN implementations because 1) the generator output is usually bounded by non-linear output function (e.g. [0,1] for sigmoid, [-1,1] for the hyperbolic tangent) and 2) it is very unlikely to sample two near-identical latent codes from standard normal distribution. Specifically, we can probably bound the probability of sampling two random codes z and z' from the N-dimensional multivariate standard normal distribution within a distance of \\delta by p(|z-z'|<\\delta) \\leq (\\delta/\\sqrt(2\\pi))^N. For sufficiently small \\delta, we can see that such probability decreases exponentially with the size of the latent code. For example, when \\delta=0.001 and N=10, this bound is about 10^-30, which implies that the probability that such an event happens is practically zero. (We will include the proof in the next revision. We are happy to provide more details upon request.). For these reasons, we omitted the \\tau in Equation 2 in practice as we described in the following sentence of Equation 3. The only hyper-parameter in our formulation (and in all our experiments) is thus \\lambda in Equation 3, which controls the importance of the regularization.\n\n\n(2). \u201cIn section 5, how is the \u2018appropriate CGAN\u2019 determined\u201d?\n\nFor each conditional generation task in the experiment, we chose strong cGAN models from the literature that produces realistic but deterministic outputs, as we described in the 4th line of Section 5. We provided details of cGAN baseline in each task in the second paragraph of each corresponding subsection as follows:\n - Image to image translation (section 5.1): Zhu et al., 2016 \n - Image inpainting (section 5.2): Iizuka et al., 2017 \n - Video prediction (section 5.3): Lee et al., 2018\nThese models are considered as among state-of-the-art methods (if not \u201cthe state-of-the-art\u201d for each task domain). As we described in the paper, we employed the exact same network architectures and hyperparameters provided by the authors for these models, as our formulation requires modification on only objective function. To be self-contained, please note that we also provided detailed settings of these baseline cGANs in the appendix (Section D.1.1, D.2, and D.3.2).\n\n\n(3). \u201cThe visual quality of the samples illustrated in the paper is inferior to that observed in the state-of-the-art \u2026\u201d\n\nBased on our experiment results, we did not observe noticeable quality degradation of our method over its cGAN counterparts. As we discussed in the paper, we conducted a human evaluation study to compare visual realism among cGAN baseline, BicycleGAN, and our method. However, we found that there is no clear winning method over others, which implies that the visual quality of samples is in a similar level for these methods. In terms of FID score, on the other hand, our method consistently achieved substantial improvement over cGAN baseline and BicycleGAN (Table 1 and 3), which shows that the distribution of the generated samples by our method (with improved diversity) matches much better to the true distribution than others. \n\nPlease note that our main contribution is improving diversity in existing cGANs, which is orthogonal (and complementary) to achieving high-level visual realism via sophisticated architectural designs or training strategies, e.g., BicycleGAN, pix2pixHD, SAVP, ProgressiveGAN, BigGAN (unpublished concurrent ICLR submission) to name a few. Practically, We showed that, with a few lines of additional code, the diversity of generated samples dramatically improves upon all strong-performing cGAN models we tried. Although an exhaustive demonstration of our regularization to the latest and the most resource-heavy cGAN models (e.g., ProgressiveGAN or BigGAN) was infeasible due to time/resource limit for our submission, we believe our experiments provide compelling evidence of wide applicability of our regularizer.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604692, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJliMh09F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1296/Authors|ICLR.cc/2019/Conference/Paper1296/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604692}}}, {"id": "S1lcPdiF07", "original": null, "number": 3, "cdate": 1543252066213, "ddate": null, "tcdate": 1543252066213, "tmdate": 1543348048124, "tddate": null, "forum": "rJliMh09F7", "replyto": "HylZvVLo3m", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We appreciate your insightful and supportive comments. We have updated the paper to address your concern with converged discriminator (section C.2). Below we provide our responses to your comments.\n\n(1) \u201cI would expect that upper bounding the generator gradient makes sense if a smooth interpolation in latent space is desired\u201d\n\nOur regularization increases the lower-bound of the generator gradient norm to ensure the sensitivity of the generator with respect to the latent code z. Without such bound, we found that the norm of the generator gradient approaches to zero as training progresses, which makes the conditional generator ignoring z. \n\nWe can still ensure the smoothness of latent space by bounding our regularization term. This can be achieved implicitly by balancing our regularization with the adversarial loss using the hyperparameter \\lambda (Equation 2), or explicitly by introducing an upper-bound to our regularization (\\tau in Equation 2). We found that the first trick alone works practically very well to learn smooth latent manifold. (Empirically we found that \u201cno upper bounding\u201d (i.e., \\tau=\\infty) worked just well; please see our response (1) to Reviewer 1 for more detailed information.) Please see sample interpolation results in Figure D, E, G in appendix and videos on the anonymous website (https://sites.google.com/view/iclr19-dsgan/), where we can observe smooth and continuous transition between samples.  \n\n\n(2) \u201cwill it work if the discriminator is allowed to converge before updating the generator?\u201d\n\nThank you for your insightful comment. We empirically validate the effectiveness of our regularization on vanishing gradient problem and reported the results in Section C.2. As suggested by the reviewer, we simulate the vanishing gradient problem by training cGAN baseline until it converges, and retraining the generator from scratch with our regularization while initializing the discriminator with the pre-trained one. Empirically we observed that the pre-trained discriminator can distinguish the real data and generated samples from the randomly initialized generator almost perfectly, and the generator experiences a severe vanishing gradient problem at the beginning of the training. However, even in such cases, we found that the diversity-sensitive regularization helped overcoming this issue throughout the training.\n\nIn our experiment on label->image dataset, we found that the generator with our regularizer converges to the similar FID/LPIPS scores (FID: 52.31; LPIPS: 0.16) as the ones reported in the paper (FID: 57.20, LPIPS: 0.18). We observed that our regularization term encourages the generator to efficiently explorer the output space in the early training stage when the discriminator gradients are vanishing, which helps the generator to capture useful gradient signals from the discriminator in the later course of training. This trend can be observed more clearly on our experiments on the synthetic dataset (Section C.1), where our diversity-sensitive regularization spreads the generator landscape and captures meaningful modes. Please find the Section C.2. in the revised paper for more detailed experiment settings and discussions.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604692, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJliMh09F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1296/Authors|ICLR.cc/2019/Conference/Paper1296/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604692}}}, {"id": "BJgHwssK0m", "original": null, "number": 6, "cdate": 1543252828868, "ddate": null, "tcdate": 1543252828868, "tmdate": 1543336923652, "tddate": null, "forum": "rJliMh09F7", "replyto": "HkexNsjFAQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "content": {"title": "Response to  Reviewer 1 (part 2)", "comment": "(4). \u201cThe diversity observed seems to mainly be attributable to colour differences rather than more elaborate differences\u201d\n\nWe would like to clarify that the diversity encouraged by our method is not limited to the color differences. In our experiments, we demonstrated more elaborate sample differences on various datasets and tasks as follows:\n\n- In map->photo dataset (Figure 2, the last row), our method generates various landmark textures especially on park areas, such as trees, grass, and playgrounds.\n- In cityscape dataset (Figure 3), our method generates different object textures (buildings, cars, road), lightings and shadows, etc. \n- In face dataset (Figure 5), our method generates various facial attributes such as gender, age, facial expression, makeups, etc.\n- In video datasets (Figure 6), our method generates various object dynamics such as different motion categories and speed. \n\nPlease note that the diversity in the face and video datasets is very subtle in terms of color differences. For instance, we can create different facial expressions or motions by modifying a few pixels around facial or body landmarks. Experiment results show that our method learns semantically more meaningful factors of variations than just color differences. We also remark that our method tends to capture semantically more meaningful diversities than other approaches (i.e. BicycleGAN, SAVP), as shown in Figure B, C, E, and F.   \n\nFinally, we demonstrate in the paper that we can incorporate an additional encoder into our regularization, which allows us to capture more meaningful sample differences (Equation 6). This can be very useful when the semantic distance of the samples is not well captured in the generator output space (e.g. face, sentence).\n\n\n(5). The proposed method is marginally better than other methods.\n\nWe believe that our method achieved meaningful improvement over existing state-of-the-art multimodal cGAN methods. Although its performance is competitive with BicycleGAN on some datasets (Table 1), we showed that our method can achieve substantially better performance over a wide range of latent dimensions (Table 2), and more challenging datasets (Table 3). Compared to SAVP, our method achieved substantially diverse and realistic results especially on challenging KTH datasets, where SAVP ends up generating deterministic outputs (Table 5).\n \nMore importantly, we believe that the main contribution of this paper is proposing a general and principled approach to promote diversity in conditional GANs. Our method achieved consistent improvement over state-of-the-arts on various tasks and frameworks, although such competitors are designed specifically for each task and require non-trivial modifications of baseline cGAN models. As pointed out by other reviewers, alleviating the need to investigate significant changes to model families by focusing instead on a novel optimization objective is an important contribution towards understanding how conditional generative models like cGANs behave.\n\n\n(6). \u201cIn section 4, an increase of the gradient norm of the generator is implied: does this have any effect on the robustness/sensitivity of the model to adversarial attacks?\u201d\n\nWe are sorry, but we could not understand your question clearly. Below we provide a response based on our best guess on your question, but please let us know if it does not address your concern. We are happy to elaborate and discuss further based upon your comments.\n\nWe assume that your concern is on the sensitivity of the generator against the adversarial perturbation on the input condition x, as our regularization increases the norm of the generator gradient. Denoting the very small perturbation as p, your concern can be rephrased as \u201cWill the proposed regularization increase ||G(x+p,z) - G(x,z)||?\u201d. Our answer is \u201cnot necessarily yes\u201d, as our regularization increases the sensitivity of generator over latent code (||G(x,z1) - G(x,z2)||), not an input condition (||G(x+p,z) - G(x,z)||). Also, in another perspective, the cGAN is trained driven with the conditional likelihood (on top of adversarial loss) as a major objective (e.g., \u201cL2 loss\u201d between the generator output and ground-truth output), so a reasonably trained generator model should capture the implicit relationship between input/output pairs in the data and thus would exhibit proper degree of sensitivity to sufficiently different x values (while generating smooth/similar outputs when given very similar x values). Note that we do not need to worry about an adversarial attack on the latent code, as it is always sampled by the model and hidden to users. \n\nTo be more concrete, we will provide some experimental results if the reviewer can elaborate more on the attack method (e.g. reference). To our best knowledge, we are not aware of any existing works on the adversarial attack against cGAN generator.\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604692, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJliMh09F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1296/Authors|ICLR.cc/2019/Conference/Paper1296/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604692}}}, {"id": "BylPIFoFCX", "original": null, "number": 4, "cdate": 1543252303286, "ddate": null, "tcdate": 1543252303286, "tmdate": 1543252303286, "tddate": null, "forum": "rJliMh09F7", "replyto": "rkxiYmDq2m", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We appreciate your insightful and constructive comments. We have updated the paper to include the experiments on unconditional GAN (section C.1). Below we provide our response to your comment.\n\n(1). Is the proposed method applicable to unconditional GANs?\n\nWe believe that our regularization can be applied to unconditional GAN to relax the mode-collapse problem. To demonstrate this idea, we conducted an experiment using the synthetic data and unconditional GAN model employed in Srivastava et al., 2017. Please see Section C.1 of our revised paper for comprehensive descriptions on experiment settings and results. Below we provide a summary of this experiment. \n\nIn this experiment, we used a mixture of eight 2D Gaussian distributions arranged in a ring as a synthetic dataset. We observe that vanilla GAN experiences a severe mode collapse, putting a significant probability mass around a single mode. On the other hand, applying our regularization effectively resolves the mode-collapse problem, enabling the generator to capture all eight modes. Interestingly, our method achieved even higher performance over Srivastava et al., 2017, which also addresses the mode collapse in GAN but for unconditional generation task. It shows that the proposed regularization is also effective in resolving mode collapse problem in unconditional GAN setting. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604692, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJliMh09F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1296/Authors|ICLR.cc/2019/Conference/Paper1296/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604692}}}, {"id": "HylZvVLo3m", "original": null, "number": 3, "cdate": 1541264473019, "ddate": null, "tcdate": 1541264473019, "tmdate": 1541533258458, "tddate": null, "forum": "rJliMh09F7", "replyto": "rJliMh09F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Review", "content": {"title": "An interesting and simple idea.", "review": "The paper proposes a regularization term for the conditional GAN objective in order to promote diverse multimodal generation and prevent mode collapse. The regularization maximizes a lower bound on the average gradient norm of the generator network as a function of the noise variable.\n\nThe regularization is a simple addition to existing conditional GAN models and is certainly simpler than the architectural modifications and optimization tweaks proposed in recent work (BicycleGAN, etc). It is useful to a such a simple solution for preventing mode collapse as well as promoting diversity in generation.\n\nIt is shown to promote the generator landscape to be more spread out by lower bounding the expected average gradient norm under the noise distribution. This is a point to be noted when comparing with other work which focus on the vanishing gradients through the discriminator and try to tweak the discriminator gradients. It is a surprising result that such a penalty on the lower bound can prevent mode collapse while also promoting diversity, since I would expect that upper bounding the generator gradient (i.e. lipschitz continuity which wasserstein GANs and related work rely on but for their discriminator instead) makes sense if a smooth interpolation in latent space is desired. \n\nIt is also not evident how the vanishing discriminator gradient problem is solved using this regularization -- will it work if the discriminator is allowed to converge before updating the generator?\n\nThis simple regularization presented in this paper and its connection to preventing mode collapse feels like an important step towards understanding how conditional generative models like cGANs behave. Alleviating the need to investigate significant changes to model families by focusing instead on a novel optimization objective is an important contribution.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Review", "cdate": 1542234261177, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJliMh09F7", "replyto": "rJliMh09F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335915726, "tmdate": 1552335915726, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxiYmDq2m", "original": null, "number": 1, "cdate": 1541202818704, "ddate": null, "tcdate": 1541202818704, "tmdate": 1541533258044, "tddate": null, "forum": "rJliMh09F7", "replyto": "rJliMh09F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Review", "content": {"title": "Interesting idea with good experimental validation", "review": "The paper proposes a method for generating diverse outputs for various conditional GAN frameworks including image-to-image translation, image-inpainting, and video prediction. The idea is quite simple, simply adding a regularization term so that the output images are sensitive to the input variable that controls the variation of the images. (Note that the variable is not the conditional input to the network.) The paper also shows how the regularization term is related to the gradient penalty term. The most exciting feature about the work is that it can be applied to various conditional synthesis frameworks for various tasks. The paper includes several experiments with comparison to the state-of-the-art. The achieved performance is satisfactory. \n\nTo the authors, wondering if the framework is applicable to unconditional GANs.", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Review", "cdate": 1542234261177, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "rJliMh09F7", "replyto": "rJliMh09F7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335915726, "tmdate": 1552335915726, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJgRk7fChQ", "original": null, "number": 1, "cdate": 1541444326405, "ddate": null, "tcdate": 1541444326405, "tmdate": 1541462277280, "tddate": null, "forum": "rJliMh09F7", "replyto": "BkgYd5Rd3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "content": {"title": "Thank you for your comment!", "comment": "Hi Augustus. \n\nThank you for your comment. We will definitely include your paper in the revision of our paper as two methods are related. As you mentioned, Jacobian clamping and our regularizer optimize similar but different objective functions (i.e. the former clamps the generator Jacobian within a certain range, while the later increases it with some rough upper-bound), which leads to different impacts on the generator in practice. In our initial attempts, we tried Jacobian clamping on Facade dataset with grid hyper-parameter search but could not achieve the similar FID / LPIPS score as our method. We will add more thorough discussion and investigation results in the revised version of our paper. Thank you.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1296/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604692, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJliMh09F7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1296/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1296/Authors|ICLR.cc/2019/Conference/Paper1296/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604692}}}, {"id": "BkgYd5Rd3Q", "original": null, "number": 1, "cdate": 1541102193087, "ddate": null, "tcdate": 1541102193087, "tmdate": 1541102193087, "tddate": null, "forum": "rJliMh09F7", "replyto": "rJliMh09F7", "invitation": "ICLR.cc/2019/Conference/-/Paper1296/Public_Comment", "content": {"comment": "Hey, I think our ICML paper http://proceedings.mlr.press/v80/odena18a.html qualifies as related work in this case.\nIn particular, the Jacobian Clamping algorithm from that paper is pretty similar, though we bound the largest and smallest singular values of the generator jacobian and it looks like you approximately maximize the norm of the whole thing.", "title": "Related work :)"}, "signatures": ["~Augustus_Odena1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1296/Reviewers/Unsubmitted"], "writers": ["~Augustus_Odena1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Diversity-Sensitive Conditional Generative Adversarial Networks", "abstract": "We propose a simple yet highly effective method that addresses the mode-collapse problem in the Conditional Generative  Adversarial  Network (cGAN). Although conditional distributions are multi-modal (i.e., having many modes) in practice, most cGAN approaches tend to learn an overly simplified distribution where an input is always mapped to a single output regardless of variations in latent code. To address such issue, we propose to explicitly regularize the generator to produce diverse outputs depending on latent codes. The proposed regularization is simple, general, and can be easily integrated into most conditional GAN objectives. Additionally, explicit regularization on generator allows our method to control a balance between visual quality and diversity. We demonstrate the effectiveness of our method on three conditional generation tasks: image-to-image translation, image inpainting, and future video prediction. We show that simple addition of our regularization to existing models leads to surprisingly diverse generations, substantially outperforming the previous approaches for multi-modal conditional generation specifically designed in each individual task.", "keywords": ["Conditional Generative Adversarial Network", "mode-collapse", "multi-modal generation", "image-to-image translation", "image in-painting", "video prediction"], "authorids": ["didoyang@umich.edu", "hongseu@umich.edu", "yunseokj@umich.edu", "ericolon@umich.edu", "honglak@eecs.umich.edu"], "authors": ["Dingdong Yang", "Seunghoon Hong", "Yunseok Jang", "Tianchen Zhao", "Honglak Lee"], "TL;DR": "We propose a simple and general approach that avoids a mode collapse problem in various conditional GANs.", "pdf": "/pdf/a945e4dc21b9f4898192a18baa53c618d2f0ea60.pdf", "paperhash": "yang|diversitysensitive_conditional_generative_adversarial_networks", "_bibtex": "@inproceedings{\nyang2018diversitysensitive,\ntitle={Diversity-Sensitive Conditional Generative Adversarial Networks},\nauthor={Dingdong Yang and Seunghoon Hong and Yunseok Jang and Tiangchen Zhao and Honglak Lee},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=rJliMh09F7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1296/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311631524, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "rJliMh09F7", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1296/Authors", "ICLR.cc/2019/Conference/Paper1296/Reviewers", "ICLR.cc/2019/Conference/Paper1296/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311631524}}}], "count": 11}