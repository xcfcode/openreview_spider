{"notes": [{"id": "MFRfzdtj4l", "original": null, "number": 2, "cdate": 1579672096232, "ddate": null, "tcdate": 1579672096232, "tmdate": 1579672096232, "tddate": null, "forum": "HJe-blSYvH", "replyto": "SklFPOGiiS", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Public_Comment", "content": {"title": "wav2vec does benefit from more data", "comment": ">> large-scale pretraining reverse a trend toward worse performance with larger data that was hinted at in [2]\nThis is a misinterpretation of our results. Almost all experiments show that training on more data helps, except for one instance where there was a 0.04 WER degradation on test when adding 8% more data - far from significant. Please fix this in the current draft as it misrepresents our findings."}, "signatures": ["~Michael_Auli1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Michael_Auli1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe-blSYvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504184772, "tmdate": 1576860565273, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Public_Comment"}}}, {"id": "HJe-blSYvH", "original": "HkgcXkgFDH", "number": 2125, "cdate": 1569439737016, "ddate": null, "tcdate": 1569439737016, "tmdate": 1577168289203, "tddate": null, "forum": "HJe-blSYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "OfRwYSTzXM", "original": null, "number": 1, "cdate": 1576798741211, "ddate": null, "tcdate": 1576798741211, "tmdate": 1576800895016, "tddate": null, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Decision", "content": {"decision": "Reject", "comment": "The paper focuses on learning speech representations with contrastive predictive coding (CPC). As noted by reviewers, (i) novelty is too low (mostly making the model bidirectional) for ICLR (ii) comparison with existing work is missing.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725499, "tmdate": 1576800277408, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Decision"}}}, {"id": "rJezAFHRFS", "original": null, "number": 3, "cdate": 1571867082347, "ddate": null, "tcdate": 1571867082347, "tmdate": 1574201180659, "tddate": null, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "This paper investigates an unsupervised learning approach based on bi-directional contrasive predictive coding (CPC) to learning speech representations.  The speech representations learned using 1k and 8k hours unlabeled data based on CPC are shown to be helpful in semi-supervised learning ASR tasks in terms of sample efficiency, WER and cross-domain robustness. The reported work is interesting and may have value to the speech community.  Regarding the paper, I have the following concerns. \n\n1.  In terms of semi-supervised learning ASR, I think any proposed approach should compare with the \"naive\" way of doing it. That is, use a high-performance ASR model to decode the unlabeled data and use the decoded pseudo-truth as the ground truth to train an acoustic model with an appropriate capacity.  In my experience,  many of the \"novel\" approaches can not outperform this \"naive\" method.  I would like to see this as a baseline for the semi-supervised learning experiments. \n\n2. In sec. 3.1 on the setting of unsupervised learning, the authors state that \"all audio signals have a sampling rate of 16KHz\". This is obviously not true for the Switchboard data in Table 6 in Appendix A, which has a sampling rate of 8KHz as they are telephony signals.   The authors should clarify. \n\n3.  It is not clear to me why the authors use two different ASR models (DeepSpeech2 small and TDNN). Why not stick to one architecture but adjust the model capacity?  \n\n4.   I wonder if the latent features learned by CPC can be complementary to the conventional features such as logmel ? How does it perform if  the two are simply concatenated as the input to the acoustic model? \n\nP.S.  rebuttal read. I will stay with my score.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2125/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2125/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682098021, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2125/Reviewers"], "noninvitees": [], "tcdate": 1570237727349, "tmdate": 1575682098034, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Official_Review"}}}, {"id": "r1xr4KzjsS", "original": null, "number": 4, "cdate": 1573755180531, "ddate": null, "tcdate": 1573755180531, "tmdate": 1573755180531, "tddate": null, "forum": "HJe-blSYvH", "replyto": "rkl6lH2for", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment", "content": {"title": "Response to negative sampling description", "comment": "1. We observed sampling negatives from the same audio signal always provide better results. We will remove \u201cand/or mini-batch\u201d from the line.\n\n2. We will be able to release the pre-trained model after publication."}, "signatures": ["ICLR.cc/2020/Conference/Paper2125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe-blSYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2125/Authors|ICLR.cc/2020/Conference/Paper2125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145961, "tmdate": 1576860531636, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment"}}}, {"id": "SyxV5OzjsS", "original": null, "number": 3, "cdate": 1573755020360, "ddate": null, "tcdate": 1573755020360, "tmdate": 1573755020360, "tddate": null, "forum": "HJe-blSYvH", "replyto": "rJlW3Szi_r", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment", "content": {"title": "Response to review #3", "comment": "Thank you for the thorough review.\n\nWe will include more citations to the ZeroSpeech line of work and contextualize our method in terms of it. However, while both our paper and the ZeroSpeech work learn unsupervised acoustic representations, our semi-supervised evaluation is important since it may conceivably require a qualitatively different kind of acoustic features for optimal performance."}, "signatures": ["ICLR.cc/2020/Conference/Paper2125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe-blSYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2125/Authors|ICLR.cc/2020/Conference/Paper2125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145961, "tmdate": 1576860531636, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment"}}}, {"id": "SklFPOGiiS", "original": null, "number": 2, "cdate": 1573754976719, "ddate": null, "tcdate": 1573754976719, "tmdate": 1573754976719, "tddate": null, "forum": "HJe-blSYvH", "replyto": "HJlqbVhpYH", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment", "content": {"title": "Response to review #1", "comment": "Thank you for the review.\n\nWe would like to clarify the novelty of our work and its relation to published work. The representation learning objectives and model architectures are indeed quite similar to [1, 2]; however, we did not intend to imply that either the architecture or training objective is what makes this paper novel, rather we contribute three significant results:\n1. We demonstrate the feasibility and advantages of pre-training on large-scale and noisy data.\n2. We demonstrate robustness on out-of-domain evaluation that large-scale pre-training provides. \n3. We demonstrate that large-scale pre-training results in representations that are universal, as demonstrated by performance on low-resource languages.\nAll three of these points are new compared to results shown in previous papers.\n\nMoreover, these are important aspects of representation learning. And, not only do we explore them systematically for the first time, but we also apparently find that large-scale pretraining reverse a trend toward worse performance with larger data that was hinted at in [2], where they show that increasing the amount of pre-training data did not lead to improved downstream performance. Also, in our replication, the models trained only on Librispeech data did not perform well in out-of-domain evaluations or in low-resource languages, demonstrating the importance of diverse kinds of pre-training data - again, a novel and important result. Those findings are of considerable interest since certainly a major benefit of unsupervised learning is being able to improve the robustness of ASR systems, which is a long standing challenge. The low-resource aspect has been investigated in Zerospeech challenge. We will explain the connections in the final version of the paper.\n\nRegarding the effect of the LM on the WSJ task- again, the point of this paper is the robustness of the acoustic representations across datasets, domains, and languages. The results of our in-domain setup demonstrate that these representations are adequate for this setup, but exploring the in-domain setup in detail distracts from the point of the paper.\n\nRegarding the minor tweaks to the model: The improvements to the model (bidirectional context network and dense residual connections) certainly improved the performance to the level that our representations only needs 10% of the labelled data to achieve the same result as the same model trained on spectrogram features using 100% of the training data.\n\nWe used exactly the same model architecture, learning rate schedule for different features. We fixed the model to DeepSpeech2 and tuned the learning rate and its scheduler for baseline features (not for our features). It is quite likely we could further improve the results with our features if we re-tuned the hyperparameters, but the scientific point we wished to make was already made.\n\n[1] Representation Learning with Contrastive Predictive Coding, van den Oord et al.\n[2] Wav2vec: unsupervised pre-training for speech recognition, Schneider et al."}, "signatures": ["ICLR.cc/2020/Conference/Paper2125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe-blSYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2125/Authors|ICLR.cc/2020/Conference/Paper2125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145961, "tmdate": 1576860531636, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment"}}}, {"id": "H1gpEOzsjH", "original": null, "number": 1, "cdate": 1573754933382, "ddate": null, "tcdate": 1573754933382, "tmdate": 1573754933382, "tddate": null, "forum": "HJe-blSYvH", "replyto": "rJezAFHRFS", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment", "content": {"title": "Response to review #2", "comment": "Thank you for the review.\n\nWe address review concerns here:\n1. Our focus in this paper is on completely unsupervised acoustic representations, as well as the properties they confer on the ASR systems that use them. The self-training approach you suggest can indeed be a good way to improve the performance of a system, but it is a different research question that is beyond the scope of this paper. We will identify this as a related strategy.\n2. The switchboard data was upsampled to 16kHz- we will clarify in the paper.\n3. We wanted to include TDNN because it is the state of the art in supervised ASR. However, we also wanted to be able to work with a simpler model class than is standard, and DeepSpeech2\u2019s recurrent architecture (which is close to SOTA) meant it could be more easily shrunk without changing the receptive field (which would have happened had we removed layers from TDNN). An alternative would have been to reduce the capacity of the TDNN model without changing the receptive field size, but deep low-capacity layers are difficult to train, and we felt would have led to higher variance results.\n4. Thanks for the suggestion. We will run these experiments in ongoing work."}, "signatures": ["ICLR.cc/2020/Conference/Paper2125/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe-blSYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2125/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2125/Authors|ICLR.cc/2020/Conference/Paper2125/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504145961, "tmdate": 1576860531636, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Official_Comment"}}}, {"id": "rkl6lH2for", "original": null, "number": 1, "cdate": 1573205236545, "ddate": null, "tcdate": 1573205236545, "tmdate": 1573205236545, "tddate": null, "forum": "HJe-blSYvH", "replyto": "rJlW3Szi_r", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Public_Comment", "content": {"title": "Negative Sampling Description is vague", "comment": "Thank you for your submission! A couple of questions:\n\n1. The paper says \"the negatives are uniformly sampled from representations in the same audio signal (z) and/or mini-batch\". Could you clarify what this \"and/or\" means? Considering a batch size of 128 and k=12 steps, do you sample uniformly the 10 negatives out of the 128 * 12 representations? Or do you sample from the 128+11 representations that are the union of the current batch representations at a fixed step and the current audio signal at all steps? Does the method used to select negatives change from one experiment to the other? This point is more than just a detail: as the original CPC paper (van den Oord et al. 2018) showed, the method used to select the negative samples (same-speaker vs. uniform on dataset) had a significant impact on the quality of the produced embeddings.\n\n2. Do you plan to open source the code supporting the experiments of this paper?"}, "signatures": ["~Mark_Adams5"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mark_Adams5", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HJe-blSYvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504184772, "tmdate": 1576860565273, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper2125/Authors", "ICLR.cc/2020/Conference/Paper2125/Reviewers", "ICLR.cc/2020/Conference/Paper2125/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Public_Comment"}}}, {"id": "rJlW3Szi_r", "original": null, "number": 1, "cdate": 1570608552644, "ddate": null, "tcdate": 1570608552644, "tmdate": 1572972379910, "tddate": null, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "\nOverview:\n\nThis work uses contrastive predictive coding (CPC) to learn unsupervised speech representations on large amounts of unlabelled speech data and then uses the resulting features in downstream speech recognition systems. Unlabelled data is obtained from several sources (spanning different languages). Supervised systems are then built on top of these features and sample-efficiency and cross-domain robustness is investigated using English data sets. Finally, the approach is applied to four African languages.\n\nStrengths:\n\nFirstly, the paper is very clearly written and motivated. Secondly, a very relevant problem is tackled in a systematic way; compared to transcribed resources, unlabelled resources are much easier to collect and more widely available. This paper shows that these unlabelled resources can be of great benefit in downstream tasks and on languages where few resources are available. Thirdly, the experiments are carried out very systematically to support the claims of the paper: that bidirectional CPC-based feature learning improves same efficiency (they show that much less labelled data is required to achieve the same performance as when using more substantial labelled data with conventional features), and that it improves robustness to out-of-domain data. They perform these experiments on both English and truly low-resource languages.\n\nWeaknesses:\n\nThere are two main weaknesses to the paper. Firstly, as the authors note themselves, unsupervised CPC-based speech feature learning was developed and considered in previous work, and has also been subsequently investigated by others. The main technical contribution is therefore only in changing the unidirectional architecture to bidirectional. Secondly, the paper does a very poor job of linking this work with previous work. The work in [1] is very related. In Section 5, the ZeroSpeech challenges are mentioned briefly (with a single citation), but over the last decade there has been substantial work in this community specifically looking at exactly the main problem addressed in this paper (unsupervised speech representation learning). It would be of great benefit to situate this work within that context, and I would recommend that the paper at least mention [2] to [9].\n\nOverall assessment:\n\nAlthough technical novelty is limited (first weakness), I think there is novelty in the paper's systematic experimental investigation, including ASR experiments on truly low-resource languages. The conclusions of this work also has practical implications for the ASR community. The second weakness can be addressed by amending Section 5. I therefore assign a \"Weak Accept\" to the paper.\n\nQuestions, suggestions, typos, grammar and style:\n\n- In Figure 1, it might be useful to indicate the autoregressive nature of the context vectors by adding arrows in-between the $c$ blocks on the top left. (In the text it says an RNN is used.)\n- p.7: \"... are suitable for driving recognition different languages ...\". A typo or grammatically incorrect sentence.\n- p. 9: \"Tts without t\" -> \"TTS without T\"\n- p. 9: \"african\" -> \"African\" (check all citations for capitalization)\n\nMissing references:\n\n1. https://arxiv.org/abs/1904.03240\n2. A. Jansen et al. A summary of the 2012 JHU CLSP workshop on zero resource speech technologies and models of early language acquisition. ICASSP, 2013.\n3. Badino, L., Canevari, C., Fadiga, L., & Metta, G. (2014). An auto-encoder based approach to unsupervised learning of subword units. in ICASSP.\n4. Versteegh, M., Anguera, X., Jansen, A. & Dupoux, E. (2016). The Zero Resource Speech Challenge 2015: Proposed Approaches and Results. In SLTU-2016 Procedia Computer Science, 81, (pp 67-72).\n5. Renshaw, D et al. (2015). A Comparison of Neural Network Methods for Unsupervised Representation Learning on the Zero Resource Speech Challenge. Interspeech.\n6. R. Thiolliere et al. A  hybrid  dynamic  time  warping-deep  neural  network  architecture  forunsupervised acoustic modeling. Interspeech. 2015\n7. https://arxiv.org/abs/1811.08284\n8. https://arxiv.org/abs/1702.01360\n9. https://arxiv.org/abs/1709.07902\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2125/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2125/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682098021, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2125/Reviewers"], "noninvitees": [], "tcdate": 1570237727349, "tmdate": 1575682098034, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Official_Review"}}}, {"id": "HJlqbVhpYH", "original": null, "number": 2, "cdate": 1571828738089, "ddate": null, "tcdate": 1571828738089, "tmdate": 1572972379873, "tddate": null, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "invitation": "ICLR.cc/2020/Conference/Paper2125/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes an unsupervised method for learning representations of speech signals using contrastive predictive coding. \nThe authors provide results for the speech recognition task, in which they trained their model on up to 8000 hours of speech. The authors provide results on several English benchmark datasets in addition to four low-resource African language datasets. \nThe authors compared their method to the traditional signal processing representations and show that the proposed method is superior. \n\nMy main concern with this submission is its novelty.\nThe proposed method was previously explored in [1] and presented similar results. If I understand it correctly, the main novelty in this work is the usage of bi-directional models together with more data. However, it is not clear what made the improvements. Considering the fact that such an approach was suggested recently by [1], a detailed comparison with uni-directional models is needed.\nFor example, in Table 2, the authors provide results for WSJ dataset, however, with no LM decoding. Can the authors provide experiments of WSJ while using LM similarly to [1]?  Moreover, if the authors wanted to eliminate the effect of LM as they stated in the paper, why not calculating Character Error Rates instead or in addition to Word Error Rates? Again, as done in [1], and in many other papers in the field [2]. \n\nAdditionally, in Table 1 and Table 5, the error rates seem pretty high, especially for the baseline model, did the authors investigated different architectures/stronger ones for these tasks? Different representations such as LogFilterBanks / MFCCs?\n\nI'm willing to increase my score, in case the authors will address my concerns. However, at the moment, I do not see much novelty in this paper comparing to previous work. Additionally, the authors are missing an essential comparison to previous work so we could better understand the contribution of this paper. \n\nMinor comments: \"using a simpler convolutional architecture than is common\" -> should be rephrased.\n\n\n[1] Schneider, Steffen, et al. \"wav2vec: Unsupervised Pre-training for Speech Recognition.\" arXiv preprint arXiv:1904.05862 (2019).\n\n[2] Adi, Yossi, et al. \"To Reverse the Gradient or Not: an Empirical Comparison of Adversarial and Multi-task Learning in Speech Recognition.\" ICASSP, 2019.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2125/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2125/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["kawakamik@google.com", "luyuwang@google.com", "cdyer@google.com", "pblunsom@google.com", "avdnoord@google.com"], "title": "Unsupervised Learning of Efficient and Robust Speech Representations", "authors": ["Kazuya Kawakami", "Luyu Wang", "Chris Dyer", "Phil Blunsom", "Aaron van den Oord"], "pdf": "/pdf/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "abstract": "We present an unsupervised method for learning speech representations based on a bidirectional contrastive predictive coding that implicitly discovers phonetic structure from large-scale corpora of unlabelled raw audio signals. The representations, which we learn from up to 8000 hours of publicly accessible speech data, are evaluated by looking at their impact on the behaviour of supervised speech recognition systems. First, across a variety of datasets, we find that the features learned from the largest and most diverse pretraining dataset result in significant improvements over standard audio features as well as over features learned from smaller amounts of pretraining data. Second, they significantly improve sample efficiency in low-data scenarios. Finally, the features confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets, and the features likewise provide improvements in four different low-resource African language datasets.", "keywords": [], "paperhash": "kawakami|unsupervised_learning_of_efficient_and_robust_speech_representations", "original_pdf": "/attachment/ef7d5e36057351c491522b302edba745f51f0b3c.pdf", "_bibtex": "@misc{\nkawakami2020unsupervised,\ntitle={Unsupervised Learning of Efficient and Robust Speech Representations},\nauthor={Kazuya Kawakami and Luyu Wang and Chris Dyer and Phil Blunsom and Aaron van den Oord},\nyear={2020},\nurl={https://openreview.net/forum?id=HJe-blSYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJe-blSYvH", "replyto": "HJe-blSYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2125/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575682098021, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2125/Reviewers"], "noninvitees": [], "tcdate": 1570237727349, "tmdate": 1575682098034, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2125/-/Official_Review"}}}], "count": 11}