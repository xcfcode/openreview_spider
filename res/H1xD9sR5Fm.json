{"notes": [{"id": "H1xD9sR5Fm", "original": "B1en9AicF7", "number": 544, "cdate": 1538087823412, "ddate": null, "tcdate": 1538087823412, "tmdate": 1550851133171, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BkgydmhxgE", "original": null, "number": 1, "cdate": 1544762214903, "ddate": null, "tcdate": 1544762214903, "tmdate": 1545354475771, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Meta_Review", "content": {"metareview": "The reviewers agree  that the paper is worthy of publication at ICLR, hence I recommend accept.\n\nRegarding section 4.3 of the submission and the claim that this paper presents the first insight for existing work from a divergence minimization perspective, as pointed out by R2, I went and checked the details of RAML and they have similar insights in their equations (5) and (8). Please make this clearer in the paper. Regarding evaluation using greedy search instead of beam search, please consider using beam search for reporting test performance as this is the standard setup in sequence prediction. Please take my comments and the reviews into account an prepare the final version.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Accept"}, "signatures": ["ICLR.cc/2019/Conference/Paper544/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper544/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353178818, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper544/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper544/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper544/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353178818}}}, {"id": "SJevAu1qhX", "original": null, "number": 1, "cdate": 1541171407380, "ddate": null, "tcdate": 1541171407380, "tmdate": 1543865582777, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Official_Review", "content": {"title": "Well written and interesting; experiments could be improved", "review": "In this paper the authors distinguish between two families of training objectives for seq2seq models, namely, divergence minimization objectives and max-margin objectives. They primarily focus on the divergence minimization family, and show that the MRT and RAML objectives can be related to minimizing the KL divergence between the model's distribution over outputs and the \"exponentiated payoff distribution,\" with the two objectives differing in terms of the direction of the KL. In addition, the authors propose an objective using the Hellinger distance rather than the KL divergence, and they conduct experiments on machine translation and summarization comparing all the considered objectives.\n\nThe paper is written extremely clearly, and is a pleasure to read. While the discussion of the relationship between RAML and MRT (and MRT and REINFORCE) is interesting and illuminating, many of these insights appear to have been discussed in earlier papers, and the RAML paper itself notes that it differs from REINFORCE style training in terms of the KL direction.\n\nOn the other hand, the idea of minimizing Hellinger distance is I believe novel (though related to the alpha-divergence work cited by the authors in the related work section), and it's nice that training with this loss improves over the other losses. Since the authors' results, however, appear to be somewhat below the state of the art, I think the main question left open by the experimental section is whether training with the Hellinger loss would further improve state of the art models. Even if it would not, it would still be interesting to understand why, and so I think the paper could be strengthened either by outperforming state of the art results or, perhaps through an ablation analysis, showing what aspects of current state of the art models make minimizing the Hellinger loss unnecessary.\n\nIn summary,\n\nPros:\n- well written and interesting\n- a new loss with potential for improvement over other losses\n- fairly thorough experiments\n\nCons:\n- much of the analysis is not new\n- unclear if the proposed loss will improve the state of the art, and if not why \n\nUpdate after author response: thanks for your response. I think the latest revision of the paper is improved, and even though state of the art BLEU scores on IWSLT appear to be in the mid 33s, I think the improvement over the Convolutional Seq2seq model is encouraging, and so I'm increasing my score to 7. I hope you'll include these newer results in the paper.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper544/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Official_Review", "cdate": 1542234437295, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper544/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748221, "tmdate": 1552335748221, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper544/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJxph-Uo3X", "original": null, "number": 2, "cdate": 1541263797127, "ddate": null, "tcdate": 1541263797127, "tmdate": 1543275397577, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Official_Review", "content": {"title": "some interesting results and connections, but also some technical issues; revisions have improved the paper", "review": "The authors have updated the paper and clarified some things, and now my impression of the paper has improved. It still feels a little incremental to me, but the potential application areas of these sorts of models are quite large and therefore incremental improvements are not insignificant. This paper suggests some natural follow-up work in exploring Hellinger distance and other variations for these models.\n\n----- original review follows: ------\n\nThis paper discusses loss functions for sequence generation tasks that take into account cost functions that reflect the task-specific evaluation metric. They compare RAML and risk (MRT) formally and empirically, and also test a loss based on Hellinger distance. They compare these to some standard max-margin losses. MRT and the Hellinger distance loss perform best in NMT and summarization experiments. \n\nPros:\n\nThere are some interesting aspects of this paper:\n\n- It is interesting to note that RAML and MRT are (similar to) different directions of KL divergences between the same two distributions. (Caveat: the entropy regularizer, which I discuss in \"Cons\" below.)\n- The new Hellinger-distance-based loss seems promising. \n- The empirical comparison among losses for standard NMT/summarization tasks is a potentially valuable contribution.\n\nCons:\n\nA.\nThe focus/story of the paper need some work. It is unclear what the key contributions are. I think the Hellinger distance loss is potentially the most important contribution, but the authors don't spend much time on that.. it seems that they think the comparison of divergence and max-margin losses is more central. However, I think the authors' conclusion (that the divergence losses are better than the max-margin losses) is not the main story, because RAML is not much better than the max-margin losses. Also, I have some concerns about some of the details of the max-margin losses (listed and discussed below), so I'm not sure how reliable the empirical comparison is. \n\nB.\nAs for the connections and comparison between RAML and MRT: \n\nIt does not seem that MRT corresponds to a divergence of the form given at the start of Sec. 4. There is also an entropy regularizer in Eq. (9). Sec. 4.3 states: \"By comparing the above two methods, we find that both RAML and MRT are minimizing the KL divergence between the model output distribution and the exponentiated payoff distribution, but with different directions of D_KL.\" However, this statement ignores the entropy regularizer in Eq. (9). \n\nMaybe I'm being dense, but I didn't understand where Equation (10) comes from. I understand the equations above it for RAML, but I don't understand the MRT case in Eq. (10). Can you provide more details?\n\nI also don't understand the following sentence: \"It turns out that the hyperparameter \\tau in RAML and \\alpha in MRT have the same effect.\" What does this mean mathematically? Also, does this equivalence also require ignoring the entropy regularizer? As formulated, L_{RAML} necessarily contains a \\tau, but L_{MRT} does not necessarily contain an alpha. It is only when moving to the sample approximation does the alpha become introduced. (MRT does not require this sample approximation; Some older work on MRT developed dynamic programming algorithms to exactly compute the gradients for structured output spaces like sequences, so samples were not used in those cases.) So I think the paper needs to clarify what exactly is meant by the connection between tau and alpha, under what conditions there is a connection between the two, and what exactly is the nature of this connection. If more space is needed for this, many of the details in Sec 3 can be cut or moved to appendices because those are standard and not needed for what follows. \n\nIn the experimental results (Sec. 7), MRT outperforms RAML consistently. The authors discuss the impact of the directionality of the KL divergence, but what about the entropy regularizer? It would be interesting to compare MRT both with and without the entropy regularizer. Without the regularizer, MRT would actually correspond to the KL that the authors are describing in the discussion. As it currently stands, two things are changing between MRT and RAML and we don't know which is responsible for the sizable performance gains. \n\nC. \nThere are several technical issues with the writing (and potentially with the claims/conclusions), most of which are potentially flexible with some corrections and more exposition by the authors:\n\nIs L_{RAML} to be maximized or minimized? Looks like maximized, but clearly both L_{MLE} and L_{MRT} are supposed to be minimized, so the use of L for all of these seems confusing. If different conventions are to be used for each one, it should be explicitly mentioned in each case whether the term is to be maximized or minimized.\n\nAt the end of Sec. 4.1, q' is not defined. I can guess what it is, but it is not entirely clear from the context and should be defined. \n\nIn Equation 6, please use different notation for the y in the denominator (e.g., y') to avoid collision with the y in the numerator and that on the left-hand side. \n\nThe discussion of max-margin losses in Sec. 5 has some things that should be fixed. \n\n1. In Sec. 5, it is unclear why \\Delta is defined to be the difference of two r() functions. Why not just make it -r(y, y^*)? Are there some implied conditions on \\Delta that are not stated explicitly? If \\Delta is assumed to be nonnegative, that should be stated. \n\n2. In Eq. (11), F appears to be a function of y and theta, but in the definition of F, it has no functional arguments. But then further down, F appears to be a function of y only. Please make these consistent.\n\n3. Eq. (11) is not only hard for structured settings; it is also hard in the simplest settings (binary classification with 0-1 loss). This is the motivation for surrogate loss functions in empirical risk minimization for classification settings. The discussion in the paper makes it sound as if these challenges only arise in the structured prediction setting. \n\n4. I'm confused by what the paper is attempting to communicate with Equations 12 and 13. In Eq. 12, y on the left-hand side is not bound to anything, so it is unclear what is being stated exactly. Is it for all y? For any y? In Eq. 13, the \\Delta on the right-hand side is outside the max over y -- is that really what was intended? I thought the max (the slack-rescaled loss-augmented inference step) should take into account \\Delta. Otherwise, it is just doing an argmax over the score function. \n\n5. If the authors are directly optimizing the right-hand side of the inequality in Equation 12 (as would be suggested for the formula for the gradient), then there is no global minimum of the loss. It would go to negative infinity. Typically people use a \"max(0, )\" outside the loss so that the global minimum is 0. \n\n\n\nTypos and minor issues follow:\n\nSec. 1:\n\"the SEARN\" --> \"SEARN\"\n\"the DAGGER\" --> \"DAGGER\"\n\nSec. 2:\n\"genrally\" --> \"generally\"\n\"took evaluation metric into training\" --> \"incorporated the evaluation metric into training\"\n\"consistant\" --> \"consistent\"\n\nSec. 3:\nUse \\exp instead of exp.\n\"a detail explanation\" --> \"a detailed explanation\"\n\nSec. 4.1:\n\"predition\" --> \"prediction\"\n\nSec. 4.2:\n\"only single sample\" --> \"only a single sample\"\n\nSec. 6.1:\n\"less significant than\" --> \"less significantly than\"\n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper544/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Official_Review", "cdate": 1542234437295, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper544/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748221, "tmdate": 1552335748221, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper544/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "ByeC4dDphX", "original": null, "number": 3, "cdate": 1541400629749, "ddate": null, "tcdate": 1541400629749, "tmdate": 1543213349356, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Official_Review", "content": {"title": "This paper compares several well known methods and one new method based on Hellinger distance", "review": "This paper compares several well known methods and illustrates some connections among these methods, and proposes one new method based on Hellinger distance for seq2seq models' training, experimental results on machine translation and sentence summarization show that the method based on Hellinger distance gives the best results. \n\nHowever the originality and significance of this work are weak.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper544/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Official_Review", "cdate": 1542234437295, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1xD9sR5Fm", "replyto": "H1xD9sR5Fm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper544/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335748221, "tmdate": 1552335748221, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper544/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkgNtCr_0X", "original": null, "number": 7, "cdate": 1543163515645, "ddate": null, "tcdate": 1543163515645, "tmdate": 1543163515645, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "rJxph-Uo3X", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "content": {"title": "Manuscript Update", "comment": "We\u2019ve uploaded a new manuscript to address the following problems:\n1) We modified the abstract and introduction to highlight our main contributions.\n2) We analyzed the similarity and difference between RAML and MRT using cross entropy now, which is more precise, since it doesn\u2019t need the entropy regularizer.\n3) We changed the writing in max margin section and corrected a lot of math problems.\n4) Other changes included correcting typos, changing wrong notations, etc. And both $L$ need to be minimized now."}, "signatures": ["ICLR.cc/2019/Conference/Paper544/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617173, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xD9sR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper544/Authors|ICLR.cc/2019/Conference/Paper544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617173}}}, {"id": "BJxp2tSERQ", "original": null, "number": 6, "cdate": 1542900148821, "ddate": null, "tcdate": 1542900148821, "tmdate": 1542900148821, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "ByeC4dDphX", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your comments. \nAbout our contributions, we want to make the following clarification,\n\n1. This paper is more than simply comparing several training criteria in seq2seq models. Actually, from an intuitive observation and careful consideration, for the first time we categorize existing work into two categories only in terms of the loss function mathematics, and discover useful connections among those criteria (RAML and MRT are just different directions of the KL divergence/cross entropy, and both minimum divergence and maximum margin try to predict the evaluation score using $\\log p$ during training), which is also the first insight for all these existing work from such a perspective. Much analysis (including the link between MRT and REINFORCE and the similarity and difference between minimum divergence and maximum margin) is original.\n\n2. We propose a new training criterion based on the analysis of existing work, and the new training criterion improves the baseline by a large margin. The use of Hellinger distance is novel for sure."}, "signatures": ["ICLR.cc/2019/Conference/Paper544/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617173, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xD9sR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper544/Authors|ICLR.cc/2019/Conference/Paper544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617173}}}, {"id": "Hylqq_B40Q", "original": null, "number": 5, "cdate": 1542899858112, "ddate": null, "tcdate": 1542899858112, "tmdate": 1542899858112, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "SJevAu1qhX", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your helpful comments. \n\n1. \u201cmuch of the analysis is not new\u201d:\nThe relationship between RAML and reinforcement learning based criteria has been discussed in the RAML paper, while our contributions are:\na) linking MRT to REINFORCE, which is new.\nb) linking the max margin criterion used in [1] to Eq (10), which is new, by deriving from the analysis of RAML and MRT\nc) the training criterion, Hellinger distance, which is new.\n\nWe are sorry if you find something already existing in the analysis part, which is sometimes for better background comprehension. For example, it will be quite difficult for readers to understand if we remove the analysis of RAML and MRT. \n\n\n\n2. comparison with state of the art\nWe made a brief survey on previous state of the art models: \n\n1) Google\u2019s Neural Machine Translation System (no code, no pretrained models)\n2) Convolutional Sequence to Sequence Learning (code + pretrained models)\n3) Transformer (code, no pretrained model uses our dataset)\n4) BERT (code is available, but is still under review now, and extra monolingual dataset is needed)\n\nConsidering our familiarity to existing code and the difficulty to modify it, we chose to re-implement our new training criterion on convolutional seq2seq (since BERT is still under review now, and no suggested hyperparameters are provided for Transformer on the small dataset we use). The results on IWSLT dataset are as follows:\n\ncriterion                     BLEU\nMLE baseline            32.14\nHellinger                   32.30\n\nThe improvement is quite smaller than the results reported in our paper, however, the improvement does exist even on such a strong baseline. A potential reason for the smaller improvement is the batch size. In the standard implementation of MRT, we use the sample size of 100 and the batch size of 1 (due to the limited number of GPUs we have), while in the README files of both conv-seq2seq and Transformer, the authors stressed the importance of a large batch size. \nDue to the limit of computational resources by our hand, we have no way to explore the impact of the batch size for our case. However, we are aware that the recent reported results from a lot of literature have indicated that the larger batch size plays a crucial role for absolute NMT performance improvement. We are optimistic in hope that the relative small improvement is mostly due to such a factor. \n\n[1] Seq2seq learning as beam search optimization, Wiseman and Rush, EMNLP 2016\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper544/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617173, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xD9sR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper544/Authors|ICLR.cc/2019/Conference/Paper544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617173}}}, {"id": "rkegpIsbA7", "original": null, "number": 4, "cdate": 1542727352114, "ddate": null, "tcdate": 1542727352114, "tmdate": 1542727352114, "tddate": null, "forum": "H1xD9sR5Fm", "replyto": "rJxph-Uo3X", "invitation": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "content": {"title": "Response", "comment": "Thanks for your comments. \n\nA. \nabout the focus/story: \nWe\u2019ve changed the writing in abstract and introduction to fit the main story better. Thanks for the helpful comments. \nAbout the paper structure:\nActually we did consider changing the paper structure to give more space to Hellinger distance, however, in order to define $p$ and $q$, and explain why we minimize the Hellinger distance between $p$ and $q$, we do think we need to clearly describe RAML and MRT. Many necessary equations and the comparison of existing work have been introduced before the section of Hellinger distance. If we remove the sections of RAML and MRT, it will be more difficult to read. \n\n\nB. \nIt would be better to understand if we replace KL divergence by cross entropy, in which case neither RAML nor MRT needs the $q\\log q$ term. Now we change the KL divergence to cross entropy. \n\nAbout Eq 10: We made a mistake here before. $p$ in L_MRT should be replaced by $p\u2019$. (since we are talking about Shen et al.\u2019s MRT for NMT paper) We\u2019ve changed the writing of Eq 10. \n\nAbout the connection between alpha and tau: we\u2019ve changed the writing here. Our previous thought is only to point out that alpha and tau are in the same place after taking \\log and they can be simply understood as smoothing techniques. Since MT is more an engineering problem than a theoretical one, we sometimes cannot expect a perfect link among existing work. Our main goal of introducing the link among previous work (as mentioned by reviewer 2, some similar discussions have appeared in a previous paper) is to explain how our idea of using Hellinger distance comes. We\u2019ve also changed the writing here. \n\nAbout the regularizer: as mentioned before, if we replace KL by cross entropy, then the comparison of having or not having the regularizer seems unnecessary. We will do some experiments on this question if we have enough time, while our main focus in the past two weeks was to address Reviewer 2\u2019s concern (trying to rebase MRT and Hellinger distance onto a state of the art model). \n\n\nC.\nBoth $L$ need to be minimized now. Thanks for pointing it out. \n$q\u2019$ is now defined. \nEq 6 has been corrected now. \n\n\nDiscussion of max margin:\n1. \\Delta needs to be nonnegative (otherwise Eq 13 will be wrong). We\u2019ve changed the writing here.\n2. This has been corrected now.\n3. We\u2019ve changed writing here. \n4. Previously we made some mistakes when writing this part. Now we have stated it more clearly.\n5. In the new Eq 12, the rightmost part is an upper bound of \\Delta. Since \\Delta is always nonnegative, the upper bound should also be nonnegative. Thus we don\u2019t think it necessary to add a \u201cmax(0,)\u201d.\n\nThe typos have been corrected now. \n\nAgain, thanks for your helpful comments. "}, "signatures": ["ICLR.cc/2019/Conference/Paper544/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models", "abstract": "Sequence to sequence (seq2seq) models have become a popular framework for neural sequence prediction. While traditional seq2seq models are trained by Maximum Likelihood Estimation (MLE), much recent work has made various attempts to optimize evaluation scores directly to solve the mismatch between training and evaluation, since model predictions are usually evaluated by a task specific evaluation metric like BLEU or ROUGE scores instead of perplexity. This paper puts this existing work into two categories, a) minimum divergence, and b) maximum margin. We introduce a new training criterion based on the analysis of existing work, and empirically compare models in the two categories. Our experimental results show that our new training criterion can usually work better than existing methods, on both the tasks of machine translation and sentence summarization. ", "keywords": ["sequence to sequence", "training criteria"], "authorids": ["zhanghuan0468@gmail.com", "zhaohai@cs.sjtu.edu.cn"], "authors": ["Huan Zhang", "Hai Zhao"], "pdf": "/pdf/4e52410fa0a0badec2fc93d05f33d73d3fb2ae82.pdf", "paperhash": "zhang|minimum_divergence_vs_maximum_margin_an_empirical_comparison_on_seq2seq_models", "_bibtex": "@inproceedings{\nzhang2018minimum,\ntitle={Minimum Divergence vs. Maximum Margin: an Empirical Comparison on Seq2Seq Models},\nauthor={Huan Zhang and Hai Zhao},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=H1xD9sR5Fm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper544/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621617173, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1xD9sR5Fm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper544/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper544/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper544/Authors|ICLR.cc/2019/Conference/Paper544/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper544/Reviewers", "ICLR.cc/2019/Conference/Paper544/Authors", "ICLR.cc/2019/Conference/Paper544/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621617173}}}], "count": 9}