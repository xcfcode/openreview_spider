{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392969780000, "tcdate": 1392969780000, "number": 1, "id": "mmC3yAGS3zZrd", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "0yNguO_G2aycf", "replyto": "0yNguO_G2aycf", "signatures": ["Shinozaki Takashi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "We have uploaded revised version (unfortunately, the replacement requires a little more time).  The network parameters were changed, and the error rate now slightly improved from the previous version. \r\nWe are really apologize for the delayed update."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "decision": "submitted, no decision", "abstract": "We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer and enables robust and simultaneous leaning on multilayer neural network.", "pdf": "https://arxiv.org/abs/1312.5845", "paperhash": "shinozaki|competitive_learning_with_feedforward_supervisory_signal_for_pretrained_multilayered_networks", "keywords": [], "conflicts": [], "authors": ["Takashi Shinozaki", "Yasushi Naruse"], "authorids": ["tshino@nict.go.jp", "y_naruse@nict.go.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392677820000, "tcdate": 1392677820000, "number": 4, "id": "lxPke1W3RDxQ6", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "0yNguO_G2aycf", "replyto": "0yNguO_G2aycf", "signatures": ["Shinozaki Takashi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear reviewer, \r\nThank you for your comments. \r\n\r\nThe \u201cadvance input\u201d x_adv is the feedforward supervisory input, and is processed as same way but just before the target input. The \u201cadvance input\u201d produces required label output, and leave processed values as an aftereffect in the network. The \u201ctarget input\u201d x_target is processed with the decayed after effect. The key part of Eq.2 (Eq.4 in the revised version) is (\beta x_adv + (1 - \beta) x_target),  which exhibits traditional competitive learning algorithm for the sum of two inputs (x_adv and x_target) with a proportion coefficient \beta. Therefore, the proposed method does not use the label directly, but uses a typical input for the label as the supervisory signal. We extensively rewrote the learning method section.\r\n\r\nAs you mentioned, our results has no clear improvement from many previous reports. We use the word \u201cimprovement\u201d for the reduction of the error rate from the pre-training result. We removed the misleading sentence, and rewrote the first paragraph in the 'Conclusion' section. \r\n\r\nMoreover, we added a little bit improved result (up to 6.9% error rate) of the proposed method although the data has just one sample.  We have also got a better result (3.8 % error rate) with different parameters of the network structure.  We will update those results later. \r\n\r\nWe have uploaded a revised version of the manuscript on arXiv."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "decision": "submitted, no decision", "abstract": "We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer and enables robust and simultaneous leaning on multilayer neural network.", "pdf": "https://arxiv.org/abs/1312.5845", "paperhash": "shinozaki|competitive_learning_with_feedforward_supervisory_signal_for_pretrained_multilayered_networks", "keywords": [], "conflicts": [], "authors": ["Takashi Shinozaki", "Yasushi Naruse"], "authorids": ["tshino@nict.go.jp", "y_naruse@nict.go.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392677700000, "tcdate": 1392677700000, "number": 1, "id": "VN79AqkrcIprJ", "invitation": "ICLR.cc/2014/-/submission/workshop/reply", "forum": "0yNguO_G2aycf", "replyto": "Q-z4DUn8LpDam", "signatures": ["Shinozaki Takashi"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear reviewer, \r\nThank you for your comments. \r\n\r\nWe really apologize to the insufficient description of the learning method. The proposed learning method does not use the label data directly, uses an input which generates the required label as the supervisor signal instead.  So, both x_target (as input signal) and x_adv (as supervisory signal) are input vectors (for example, those are 28x28 grayscale image data in the first layer). \r\n\r\nThe implication of the proposed learning rule described by Eqs.1 & 2 is mainly based on SOM & LVQ algorithm, and extended with the advance supervisory signal x_adv.  If x_adv is considered as the input which represents the idealized answer, (x_adv \u2013 x_target) represents the correction of the weight update direction. Thus, the overall weight update direction with a learning coefficient \beta is described as follows: (x_target + \beta (x_adv \u2013 x_target) = \beta x_adv + (1 - \beta) x_target). It is located at the center part of Eq.2 (Eq.4 in the revised version), and corresponds to a gradient of the weight in backpropagation learning rule. The proposed learning rule extracts a gradient-like learning information from the feedforward signal. We extensively rewrote the learning method section in the revised version of the manuscript. \r\n\r\nThe motivation of the proposed learning method is to develop a new supervised learning method which uses more feedforward oriented mechanism. The feedforward network condenses the input information through the feedforward process, meaning less amount of information in later layers. However, the back propagation learning algorithm uses the error information in the last layer for the learning of the whole network. The proposed learning method focuses to use rich input information in the early layer for the supervisory signal at the layer. We speculate that the proposed method could be extended to multiple for multiple advance inputs (for example, use 'red' and 'round shape' to learn 'apple'). We added a description about the motivation in the 'Conclusion' section.\r\n\r\nWe also added the comparison with some of previous reports in the revised version for the clarity. Unfortunately, the proposed method is still in a primitive level, and does not have enough performance to compare with many previous reports with great results. We are currently trying to improve the performance of the proposed learning method. We added a result with more training set iterations in Fig.1(c) with 6.9 % error rate after 20 training set iterations.  Since it is a rough result, we will update it later. \r\n\r\nMoreover, we reordered the structure of the manuscript as you suggested. The sections of 'Network structure' and 'Pre-training' are now located at just after introduction, following 'Learning method' section. \r\n\r\nWe have uploaded the revised version of the manuscript on arXiv."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "decision": "submitted, no decision", "abstract": "We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer and enables robust and simultaneous leaning on multilayer neural network.", "pdf": "https://arxiv.org/abs/1312.5845", "paperhash": "shinozaki|competitive_learning_with_feedforward_supervisory_signal_for_pretrained_multilayered_networks", "keywords": [], "conflicts": [], "authors": ["Takashi Shinozaki", "Yasushi Naruse"], "authorids": ["tshino@nict.go.jp", "y_naruse@nict.go.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391915280000, "tcdate": 1391915280000, "number": 3, "id": "Zufc7LsNO-Z3U", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "0yNguO_G2aycf", "replyto": "0yNguO_G2aycf", "signatures": ["anonymous reviewer fd8d"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "review": "The paper proposes a modified learning rule for competitive learning in multilayer neural networks. The proposed learning algorithm is not clearly explained, in particular the meaning of the variables x_adv and x_target. The variable x_adv is defined to be the 'advance input', but I do not understand what advance input exactly is. Apart from these clarity issues, the method seems relatively elaborated with a pretraining stage and an architecture consisting of a hierarchy of self-organizing maps.\r\n\r\nIn the conclusion, the authors claim that the method improves the accuracy of the classification task. However, it is unclear which improvement, as the reported accuracy of approximately 90% on MNIST is lower than previously published results. No further details are given on the exact setting of the experiment."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "decision": "submitted, no decision", "abstract": "We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer and enables robust and simultaneous leaning on multilayer neural network.", "pdf": "https://arxiv.org/abs/1312.5845", "paperhash": "shinozaki|competitive_learning_with_feedforward_supervisory_signal_for_pretrained_multilayered_networks", "keywords": [], "conflicts": [], "authors": ["Takashi Shinozaki", "Yasushi Naruse"], "authorids": ["tshino@nict.go.jp", "y_naruse@nict.go.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391842740000, "tcdate": 1391842740000, "number": 2, "id": "Q-z4DUn8LpDam", "invitation": "ICLR.cc/2014/-/submission/workshop/review", "forum": "0yNguO_G2aycf", "replyto": "0yNguO_G2aycf", "signatures": ["anonymous reviewer 9d3c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "review": "This paper proposed a new learning algorithm for feedforward neural networks that is motivated by Self Organizing Maps (SOM) and Learning Vector Quantization (LVQ). The paper proposes a way for unsupervised pre-training of network weights followed by a supervised fine-tuning.\r\n\r\nThe paper lacks discussions of clear motivation behind the work, i.e. shortcomings of existing training methods and how the proposed method overcome them. The description of the proposed method still needs a lot of work. More details are needed to clarify the proposed system. For example, in equation (1), d and sigma are not defined. For x_adv and x_target, which one is the input and which one is the label. What are the motivations for the update rule in equation (1) and equation (2)?. \r\nPre-training and SOM are described for the first time in the experiment section while they should appear in earlier sections of the paper. The authors only experiment with MNIST and don\u2019t compare their systems to other good baseline systems on MNIST."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "decision": "submitted, no decision", "abstract": "We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer and enables robust and simultaneous leaning on multilayer neural network.", "pdf": "https://arxiv.org/abs/1312.5845", "paperhash": "shinozaki|competitive_learning_with_feedforward_supervisory_signal_for_pretrained_multilayered_networks", "keywords": [], "conflicts": [], "authors": ["Takashi Shinozaki", "Yasushi Naruse"], "authorids": ["tshino@nict.go.jp", "y_naruse@nict.go.jp"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387795560000, "tcdate": 1387795560000, "number": 8, "id": "0yNguO_G2aycf", "invitation": "ICLR.cc/2014/workshop/-/submission", "forum": "0yNguO_G2aycf", "signatures": ["tshino@nict.go.jp"], "readers": ["everyone"], "content": {"title": "Competitive Learning with Feedforward Supervisory Signal for Pre-trained Multilayered Networks", "decision": "submitted, no decision", "abstract": "We propose a novel learning method for multilayered neural networks which uses feedforward supervisory signal and associates classification of a new input with that of pre-trained input. The proposed method effectively uses rich input information in the earlier layer and enables robust and simultaneous leaning on multilayer neural network.", "pdf": "https://arxiv.org/abs/1312.5845", "paperhash": "shinozaki|competitive_learning_with_feedforward_supervisory_signal_for_pretrained_multilayered_networks", "keywords": [], "conflicts": [], "authors": ["Takashi Shinozaki", "Yasushi Naruse"], "authorids": ["tshino@nict.go.jp", "y_naruse@nict.go.jp"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357014, "id": "ICLR.cc/2014/workshop/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357014}}}], "count": 6}