{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392720120000, "tcdate": 1392720120000, "number": 1, "id": "VVPzN959oZ6un", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "pAi8PkmKuJPvU", "replyto": "07ToXaIwBiYKp", "signatures": ["\u5712\u7530\u7fd4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer a97a\r\nThanks for your constructive comments.\r\nI am really glad for your reading of our paper.\r\n\r\nWe have rearranged the MNIST experiment, and replaced the results with new ones.\r\nIn addition, we supplemented a detailed explanation of sampling procedures in section A.\r\nThe renewed version of the paper will be soon appeared online in a few days.\r\n\r\n> It would be useful to have more information on the order of magnitude by which the method is slower/faster compared to training a classically initialized neural network, and how does the method scale with the number of data points and the dimensions of the input space.\r\n\r\nTheoretical considerations on sampling cost is discussed in the supplemental section A, and the empirical measurement of computation time is listed in the rearranged MNIST experiment section.\r\n\r\nWe have introduced a drastically annealed sampling technique in section A.2, which is as fast as sampling from normal distribution.\r\n\r\nHere is a snippet of time comparison:\r\n- sampling time of SR: 0.0115 [sec.]\r\n- regression time of SR: 2.60 [sec.]\r\n- 45,000 iterations of BP training of SR: 2000 [sec.] (0.05 [sec.] per one itr.)\r\n\r\nTheoretically the annealed sampling scales linearly with the number of required hidden parameters and the dimensionality of the input space respectively.\r\nIn particular it scales constantly with the number of training examples because it conducts sampling with one particular example.\r\n\r\n> I am concerned about the validity of the MNIST experiment where a baseline error of >0.8 (80%?) is obtained with 1000 samples while other papers typically report 10% error for similar amount of data.\r\n\r\nWe have continued further investigations on MNIST dataset in a closer setting to LeCun et al.1998, and improved both error rates and training speeds.\r\n\r\nWhile LeCun et al.1998 achieved 4.7% error rates in a similar setting to ours,\r\nour latest test error rates improved as follows:\r\nSR: 23.0%(right after initialization) -> 9.94%(after BP training)\r\nSBP: 90.0%(right after initialization) -> 8.30%(after BP training)\r\nBP: 90.0%(right after initialization) -> 8.77%(after BP training)\r\nAnd our further experiments showed that SR with 6,000 hidden units marked 3.66% test error rates right after initialization.\r\n\r\nI am looking forward to your reply, thanks.\r\nSonoda"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392718380000, "tcdate": 1392718380000, "number": 1, "id": "W_KH_Oaqtx_iI", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "pAi8PkmKuJPvU", "replyto": "lFiWFX2hzO3DM", "signatures": ["\u5712\u7530\u7fd4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer 2ac8\r\nThanks for your constructive comments.\r\nI am really glad for your reading of our paper.\r\n\r\nWe have rearranged MNIST experiment in a closer setting to LeCun et al.1998,\r\nand improved both error rates and training speeds.\r\nThe paper with this results will be soon appeared in a few days.\r\nIn addition, we supplemented a detailed explanation of sampling procedures in section A.\r\n\r\nWhile LeCun et al.1998 achieved 4.7% error rates, our latest test error rates improved as follows:\r\nSR: 23.0%(right after initialization) -> 9.94%(after BP training)\r\nSBP: 90.0%(right after initialization) -> 8.30%(after BP training)\r\nBP: 90.0%(right after initialization) -> 8.77%(after BP training)\r\nAnd our further experiments showed that SR with 6,000 hidden units marked 3.66% test error rates right after initialization.\r\n\r\n> It would be really helpful to have a notion of how expensive it is compute the approximation of the parameter density and to sample from it. Judging from the formulas this does not seem cheap. \r\n\r\nAs you expected, rigorous calculation/sampling of the oracle distribution is difficult, especially in a high dimensional input space. This sampling difficulty is discussed in a supplemental section A.1.\r\nIn order to draw samples from a high dimensional oracle distribution, therefore, we have developed and used an annealed sampling technique, which is described in a supplemental section A.2.\r\nAlso, in the rearranged MNIST experiment section, the empirical sampling time was listed.\r\n\r\n> The paper studies networks with sigmoid pairs. What can the authors say about sigmoid units? \r\n\r\nAs our method is derived from an analysis of sigmoid pairs networks (SPNs),\r\nless study is done for completely discrete sigmoid units networks (SUNs).\r\nDirect derivation is that an SPN with J sigmoid pairs might have an equivalent representation ability to an SUN with 2J sigmoid units. Our preliminary experiments empirically supports this hypothesis, that is, SR initialized SPN with J sigmoid pairs sometimes scores almost equivalent error rate with BP trained SUN with 2J sigmoid pairs. However the precise comparison is not conducted.\r\nObviously SPN is always SUN, however the converse that an well trained SUN forms SPN, is doubtful.\r\nIn relation to other integral representation study, some authors( Carroll and Dickinson1989; Barron1993; Kurkova2009) published on the integral representation of SUN and they would be help.\r\nIn our authority paper Murata1996, the SPN requirement comes from the integrability of the composing kernel, and the author suggests that the derivative of sigmoid units (which is bell shaped and integrable) is also eligible, in which case a SUN is interpreted as approximating a derivative of target function.\r\n\r\n> In Figure 1 left, the figure does not show that the support is non-convex, as claimed in the caption.\r\n> The axes labels in Figure 1 are too small.\r\n\r\nI am sorry for my lack of attention, I have replaced Figure 1 as correct version.\r\n\r\nI am looking forward to your reply, thanks.\r\nSonoda"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392700980000, "tcdate": 1392700980000, "number": 4, "id": "FNE-gqxSuzFWo", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "pAi8PkmKuJPvU", "replyto": "pAi8PkmKuJPvU", "signatures": ["\u5712\u7530\u7fd4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "In responce to our reviewer's comments, we have supplemented a detailed explanation of sampling procedure, and replaced the MNIST experiment with further investigated version.\r\n\r\nThe sampling algorithm runs as quick as sampling from ordinary distributions such as normal distribution.\r\nThe updated paper is to be online in a few days."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392700920000, "tcdate": 1392700920000, "number": 1, "id": "vf_Vs2hRaxUEP", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "pAi8PkmKuJPvU", "replyto": "fMpXedxjhIfFo", "signatures": ["\u5712\u7530\u7fd4"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Dear Reviewer 0a0f\r\nThanks for your detailed and helpful comments.\r\nI apologize for my ambiguous description,\r\nI am really glad for your reading of our paper.\r\n\r\nWe have supplemented a detailed explanation of sampling procedure,\r\nand replaced the MNIST experiment with further investigated version.\r\nThe sampling algorithm runs as quick as sampling from ordinary distributions such as normal distribution.\r\nThe updated paper is to be online in a few days.\r\n\r\n> 1 - sampling the parameters of the first layer using Importance sampling or an accept-reject MCMC method (both methods are apparently confused by the authors) in a data dependent way.\r\n\r\nCertainly I misused 'importance sampling' without following explanations, they are replaced in the modified paper.\r\nIt is also explained in the supplementary section A that we just used acceptance-rejection sampling method, and not MCMC.\r\nAs the oracle distribution usually has an extremely multimodal shape, MCMC could not perform sufficiently. One of our preliminary experiment showed that they often fail to find some of modes.\r\n\r\n> This may be understood as a 'pre-training scheme', not as an 'initialization'.\r\n\r\nI have taken you meant that:\r\n1) both 'pre-training' and 'initialization' are preceding processes to the real training,\r\n2) 'pre-training' is associated with data and 'initialization' is not,\r\n3) the proposed method contains data dependent sampling and regression, which obviously use the data,\r\n4) therefore, we should call it 'pre-training' instead of 'initialization'.\r\n\r\nI am sorry if I am misunderstanding.\r\nI completely agree to 1), whereas for 2), I think there would be indefiniteness.\r\nI recognize that 'pre-training' has narrow meaning, which typically reminds people 'unsupervised' such as RBMs and Stacking AEs (and their variations). While our oracle distribution contains the information of both input and output vectors (see, for instance, Eq.3 and 8).\r\nOn the other hand I think that 'initialization' has broader meaning, independent of using given data or not. As I surveyed in Section 1, many types of 'initialization's have been proposed and some of them contains data dependent way such as linear regression (Yam and Chow) and prototypes (Denoeux and Lengelle). Therefore I feel less necessity for calling it 'pre-training'.\r\n\r\n>  for instance stating that (Efficient Backprop, Le Cun 1998) proposes to initialize neural networks by sampling from a uniform distribution [-1/sqrt(fan-in);1/sqrt(fan-in)] when it suggests in fact to sample from a normal distribution of mean zero and standard deviation sigma=1/sqrt(fan-in).\r\n\r\nThanks again for your precise correction.\r\nI have corrected the description from range to standard deviation of the distribution.\r\nI am afraid, however, in Efficient Backprop, 'normal' distribution is not necessarily required.\r\n\r\n> does not take the pre-training time into account.\r\n> This is especially worrisome since the pre-training scheme relies on MCMC sampling which is usually very computationally expensive compared to back propagation\r\n\r\nSorry for my lack of attention since we did not use MCMC and the sampling time was enough quicker than BP iterations.\r\nWe added a list of time comparison in the renewed MNIST experimental section.\r\nHere is a snippet of time comparison:\r\n- sampling time of SR: 0.0115 [sec.]\r\n- regression time of SR: 2.60 [sec.]\r\n- 45,000 iterations of BP training of SR: 2000 [sec.] (0.05 [sec.] per one itr.)\r\n\r\n>  inconsistent with previous work when then give a test error rate for back-propagation and 300 hidden units around 90% when it should be around 1.6% (cf. Mnist dataset website).\r\n\r\nThe inconsistency was caused by the difference between experimental settings:\r\n1) the number of hidden units: same (300 units)\r\n2) the number of hidden layers: same (1 layer)\r\n3) scaling of input vectors: NOT same\r\n   - In LeCun et al.1998 (the website setting) the input vectors were scaled, while ours not.\r\n   -> We scaled them in the renewed setting\r\n4) preprocessing of input vectors: NOT same\r\n   - In LeCun et al.1998, 1.6% marking '2-layer NN, 300 HU' used 'deskew'ed image, while we do not.\r\n     Therefore, 4.7% marking '2-layer NN, 300 hidden units, mean square error' should be the closest setting. It still differs in that they used mean square error, while we used cross-entropy loss.\r\n   -> We set our goal around 4.7%\r\n5) the representation of output labels: NOT same\r\n   - In our previous setting we used 'One-of-k' coding, while LeCun et al.1998 not.\r\n   -> We rearranged vectors as 'random coding' it still differs but more standard and efficient setting.\r\n6) the number of training examples: NOT same\r\n   - In LeCun et al.1998, they used 15,000 and more, while we used just 1,000.\r\n   -> In our renewed setting, we used 15,000 examples for training.\r\n\r\nOur latest test error rates improved as follows:\r\nSR: 23.0%(right after initialization) -> 9.94%(after BP training)\r\nSBP: 90.0%(right after initialization) -> 8.30%(after BP training)\r\nBP: 90.0%(right after initialization) -> 8.77%(after BP training)\r\n\r\nAlso, SR with 6,000 hidden units marked 3.66% test error rates right after initialization.\r\n\r\nI am looking forward to your reply, thanks.\r\nSonoda"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391926320000, "tcdate": 1391926320000, "number": 3, "id": "07ToXaIwBiYKp", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "pAi8PkmKuJPvU", "replyto": "pAi8PkmKuJPvU", "signatures": ["anonymous reviewer a97a"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Nonparametric Weight Initialization of Neural Networks via Integral Representation", "review": "This paper introduces a new method for initializing the weights of a neural network. The technique is based on integral transforms. The function to learn f is represented as an infinite combination of basis functions weighted by some distribution. Conversely, this distribution can be obtained by projecting the function f onto another (related) set of basis functions evaluated at every point x in the input space.\r\n\r\nThe powerful analytic framework yields a probability distribution from which initial parameters of the neural network can be sampled. This is done using an acceptance-rejection sampling method. In order to overcome the computational inefficiency of the basic procedure, the authors propose a coordinate transform method that reduces the rejection rate. It would be useful to have more information on the order of magnitude by which the method is slower/faster compared to training a classically initialized neural network, and how does the method scale with the number of data points and the dimensions of the input space.\r\n\r\nThe experimental section consists of three experiments measuring the convergence of learning for various datasets (two low-dimensional toy examples and MNIST). On the low-dimensional toy examples, the proposed initialization is shown to be superior to uniform. However, these two datasets are to a certain extent already well modeled by local methods, for which good initialization heuristics are readily available (e.g. RBF networks + k-means). I am concerned about the validity of the MNIST experiment where a baseline error of >0.8 (80%?) is obtained with 1000 samples while other papers typically report 10% error for similar amount of data."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391811900000, "tcdate": 1391811900000, "number": 2, "id": "lFiWFX2hzO3DM", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "pAi8PkmKuJPvU", "replyto": "pAi8PkmKuJPvU", "signatures": ["anonymous reviewer 2ac8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Nonparametric Weight Initialization of Neural Networks via Integral Representation", "review": "This paper presents a new method for initializing the parameters of a feedforward neural network with a single hidden layer. The idea is to sample the parameters from a data-dependent distribution computed as an approximation of a kernel transformation of the target distribution. \r\n\r\n* The parameter initialization problem is important and the main idea of the paper is interesting. \r\n\r\nNow, computing the transformation of the target distribution is the same as solving an equation for the parameters of the network, analytically, assuming an unlimited number of hidden units. As this is a difficult problem, the method relies on an approximation of the parameter density and sampling therefrom N times when the actual network is assumed to have N hidden units. \r\n\r\n* It would be really helpful to have a notion of how expensive it is compute the approximation of the parameter density and to sample from it. Judging from the formulas this does not seem cheap. \r\n\r\n* The paper studies networks with sigmoid pairs. What can the authors say about sigmoid units? \r\n\r\nIn Figure 1 left, the figure does not show that the support is non-convex, as claimed in the caption. \r\n\r\nThe axes labels in Figure 1 are too small."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391460420000, "tcdate": 1391460420000, "number": 1, "id": "fMpXedxjhIfFo", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "pAi8PkmKuJPvU", "replyto": "pAi8PkmKuJPvU", "signatures": ["anonymous reviewer 0a0f"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Nonparametric Weight Initialization of Neural Networks via Integral Representation", "review": "The paper proposes a new pre-training scheme for neural networks which are to be fine-tuned later with back propagation.\r\n\r\nThis pre-training scheme is done in two steps\r\n 1 - sampling the parameters of the first layer using Importance sampling or an accept-reject MCMC method (both methods  are apparently confused by the authors) in a data dependent way.\r\n 2 - train the parameters of the output layer using linear regression.\r\n\r\nThe experiments compare the test RMSE/error-rate obtained using traditional back propagation and that obtained using the proposed method, on three datasets: a 1D function, a Boolean function and the Mnist dataset.\r\n\r\nThe proposed pre-training scheme is new but the scientific quality of the paper is questionable. First the proposed method is given a misleading name since it proposes to do the initialization in a data dependent way (with a linear regression step). This may be understood as a 'pre-training scheme', not as an 'initialization'. Second, the paper is very misleading in its report of previous work, for instance stating that (Efficient Backprop, Le Cun 1998) proposes to initialize neural networks by sampling from a uniform distribution [-1/sqrt(fan-in);1/sqrt(fan-in)] when it suggests in fact to sample from a normal distribution of mean zero and standard deviation sigma=1/sqrt(fan-in).\r\n\r\nAdditionally, the experiments are again very misleading. First, the main claim of the paper is that using the proposed pre-training scheme, BP will converge faster. However, the time to convergence is reported in terms of the number of BP iterations and does not take the pre-training time into account. This is especially worrisome since the pre-training scheme relies on MCMC sampling which is usually very computationally expensive compared to back propagation. Finally, the results reported on the Mnist dataset are inconsistent with previous work when then give a test error rate for back-propagation and 300 hidden units around 90% when it should be around 1.6% (cf. Mnist dataset website). \r\n\r\npros:\r\ncons:\r\n - Misleading summary of previous work.\r\n - Misleading reference to an initialization strategy which is in fact a data dependent pre-training step.\r\n - Experiments do not report the pre-training time and are therefore strongly biased in favor of the proposed method.\r\n - Results on Mnist are inconsistent with previous work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387870440000, "tcdate": 1387870440000, "number": 45, "id": "pAi8PkmKuJPvU", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "pAi8PkmKuJPvU", "signatures": ["s.sonoda0110@toki.waseda.jp"], "readers": ["everyone"], "content": {"title": "Nonparametric Weight Initialization of Neural Networks via Integral Representation", "decision": "submitted, no decision", "abstract": "A new initialization method for hidden parameters in a neural network is proposed. Derived from the integral representation of the neural network, a nonparametric probability distribution of hidden parameters is introduced. In this proposal, hidden parameters are initialized by samples drawn from this distribution, and output parameters are fitted by ordinary linear regression. Numerical experiments show that backpropagation with proposed initialization converges faster than uniformly random initialization. Also it is shown that the proposed method achieves enough accuracy by itself without backpropagation in some cases.", "pdf": "https://arxiv.org/abs/1312.6461", "paperhash": "sonoda|nonparametric_weight_initialization_of_neural_networks_via_integral_representation", "keywords": [], "conflicts": [], "authors": ["Sho Sonoda", "Noboru Murata"], "authorids": ["s.sonoda0110@toki.waseda.jp", "noboru.murata@eb.waseda.ac.jp"]}, "writers": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 8}