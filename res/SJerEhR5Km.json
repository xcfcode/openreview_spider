{"notes": [{"id": "SJerEhR5Km", "original": "BJeYyEA9tQ", "number": 1446, "cdate": 1538087980789, "ddate": null, "tcdate": 1538087980789, "tmdate": 1545355395837, "tddate": null, "forum": "SJerEhR5Km", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJg_VYOleV", "original": null, "number": 1, "cdate": 1544747312226, "ddate": null, "tcdate": 1544747312226, "tmdate": 1545354515770, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Meta_Review", "content": {"metareview": "This paper extends the transformer model of Vashwani et al. by replacing the sine/cosine positional encodings with information reflecting the tree stucture of appropriately parsed data. According to the reviews, the paper, while interesting, does not make the cut. My concern here is that the quality of the reviews, in particular those of reviewers 2 and 3, is very sub par. They lack detail (or, in the case of R2, did so until 05 Dec(!!)), and the reviewers did not engage much (or at all) in the subsequent discussion period despite repeated reminders. Infuriatingly, this puts a lot of work squarely in the lap of the AC: if the review process fails the authors, I cannot make a decision on the basis of shoddy reviews and inexistent discussion! Clearly, as this is not the fault of the authors, the best I can offer is to properly read through the paper and reviews, and attempt to make a fair assessment.\n\nHaving done so, I conclude that while interesting, I agree with the sentiment expressed in the reviews that the paper is very incremental. In particular, the points of comparison are quite limited and it would have been good to see a more thorough comparison across a wider range of tasks with some more contemporary baselines. Papers like Melis et al. 2017 have shown us that an endemic issue throughout language modelling (and certainly also other evaluation areas) is that complex model improvements are offered without comparison against properly tuned baselines and benchmarks, failing to offer assurances that the baselines would not match performance of the proposed model with proper regularisation. As some of the reviewers, the scope of comparison to prior art in this paper is extremely limited, as is the bibliography, which opens up this concern I've just outlined that it's difficult to take the results with the confidence they require. In short, my assessment, on the basis of reading the paper and reviews, is that the main failing of this paper is the lack of breadth and depth of evaluation, not that it is incremental (as many good ideas are). I'm afraid this paper is not ready for publication at this time, and am sorry the authors will have had a sub-par review process, but I believe it's in the best interest of this work to encourage the authors to further evaluate their approach before publishing it in conference proceedings.", "confidence": "2: The area chair is not sure", "recommendation": "Reject", "title": "Borderline and not ideally reviewed, but not quite ready"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352778536, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352778536}}}, {"id": "SyehfLgq37", "original": null, "number": 2, "cdate": 1541174804078, "ddate": null, "tcdate": 1541174804078, "tmdate": 1544011491999, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Review", "content": {"title": "Interesting but incremental", "review": "The paper describes an interesting idea for using Vashwani's transformer with tree-structured data, where nodes' positions in the tree are encoded using unique affine transformations. They test the idea in several program translation tasks, and find small-to-medium improvements in performance. \n\nOverall the idea is promising, but the work isn't ready for publication. The implementation details weren't easy to follow, the experiments were narrow, and there are key citations missing. I would recommend trying some more diverse tasks, and putting this approach against other graph neural network techniques.\n\n\nREVISED:\nI've revised by review upwards by 1, though I still recommend rejection. The authors improved the scholarship by adding many more citations and related work. They also made the model details and implementation more clear. \n\nThe remaining problem I see is that the results are just not that compelling, and the experiments do not test any other graph neural network architectures.\n\nSpecifically, in Table 1 (synthetic experiments) the key result is that their tree-transformer outperforms seq-transformer on structured input. But seq-transformer is best on raw programs. I'm not sure what to make of this. But I wouldn't use tree-transformer in this problem. I'd use seq-transformer.\n\nIn Table 2 (CoffeeScript-JavaScript experiments), no seq-transformer results are presented. That seems... suspicious. Did the authors try those experiments? What were the results? I'd definitely like to see them, or an explanation of why they're not shown. This paper tests whether tree-transformers are better than seq-transformer and other seq/tree models, but this experiment's results do not address that fully. Of the 8 tasks tested, tree-transformer is best on 5/8 while tree2tree is best on 3/8. \n\nIn Table 3, there's definitely a moderate advantage to using tree-transformer over seq-transformer, but in 5/6 of the tasks tree-transformer is worse than other approaches. The authors write, \"Transformer architectures in general, however, do not yet compete with state-of-the-art results.\". \n\nFinally, no other graph neural network/message-passing/graph attention architectures are tested (eg. Li et al 2016 was cited but not tested, and Gilmer et al 2017 and Veli\u010dkovi\u0107 et al 2017 weren't cited or tested), but there's a reasonable chance they'd outperform the tree-transformer.\n\nSo overall the results are intriguing, and I believe there's something potentially valuable here. But I'm not sure there's sufficient reason presented in the paper to use tree-transformer over seq-transformer or other seq/tree models. Also, while the basic idea is nice, as I understand it is restricted to trees, so other graphical structures wouldn't be handled.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Review", "cdate": 1542234195331, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335981008, "tmdate": 1552335981008, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rygwHC_c0m", "original": null, "number": 8, "cdate": 1543306815145, "ddate": null, "tcdate": 1543306815145, "tmdate": 1543306815145, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SkgZrsYu2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "Hi,\n\nThank you very much for your feedback! We have explicitly clarified potentially confusing notation in our revised draft.\n\nRegarding latency: There is no additional latency during training time as positional  encodings are directly provided by the teacher. During evaluation-time decoding, there is some extra computation needed to compute one more positional encoding per time step, but this pales in comparison to the stack of attention functions and matrix multiplications each time step demands anyway.\n\nRegards!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604962, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1446/Authors|ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604962}}}, {"id": "r1eRq6OcCm", "original": null, "number": 7, "cdate": 1543306646081, "ddate": null, "tcdate": 1543306646081, "tmdate": 1543306646081, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SyehfLgq37", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "Hi,\n\nThank you kindly for your feedback. In our revision we have made an effort to clarify implementation details, add more results from experiments, and expand our citations.\n\nRegards!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604962, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1446/Authors|ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604962}}}, {"id": "HklUr6u50Q", "original": null, "number": 6, "cdate": 1543306558426, "ddate": null, "tcdate": 1543306558426, "tmdate": 1543306558426, "tddate": null, "forum": "SJerEhR5Km", "replyto": "HJgu4gdM6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "content": {"title": "Thank you for your feedback!", "comment": "Hi,\n\nThank you very much for your feedback. \n\nIn our revised draft, we have done our best to address your concerns. We have added a related work section to better ground our contribution, and we have tried to clarify sections 3 and 4 with some additional detail and figures. We appreciate you pointing out the clarity issues and your recommendations on relevant literature to cite.\n\nIn regards to spectral theory approaches to tree node representation, that is a very interesting idea with a lot of promise. It would however be difficult to directly implement within our paradigm. In our system, and in transformers in general, it is assumed that the values of the decoder inputs and positions do not change over time. But as we build up a tree over multiple time steps, its adjacency matrix and associated eigenvectors change. This hinders us from directly using these eigenvectors as positional encodings.\n\nIn regards to binary trees: binary tree representations have been used extensively in NLP literature, and the LCRS representation in particular allows us to directly compare our work with other recent program translation literature. One key benefit to binary tree representations is that they let us work with trees with widely varying degrees among nodes, e.g. abstract syntax trees featuring functions that take arbitrary numbers of arguments. We do agree that binary tree representations have some issues, and are interested in exploring k-ary trees in future work.\n\nRegards!"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604962, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1446/Authors|ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604962}}}, {"id": "SkgZrsYu2X", "original": null, "number": 1, "cdate": 1541081913389, "ddate": null, "tcdate": 1541081913389, "tmdate": 1541990612332, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Review", "content": {"title": "An interesting tree-structured positional embedding", "review": "This work proposes a novel tree structure positional embedding by uniquely representing each path in a tree using a series of transformation, i.e., matmul for going up or down the edges. The tree encoding is used for transformer and shows gains over other strong baselines, e.g., RNNs, in synthetic data and a program translation task.\n\nPros:\n\n- An interesting approach for representing tree structure encoding using a series of transformation. The idea of transformation without learnable parameters is novel.\n\n- Better accuracy both on synthetic tasks and code translation tasks when compared with other strong baselines.\n\nCons:\n\n- Computation seems to be larger given that the encoding has to be recomputed in every decoding step. I'd like to know the latencies incurred by the proposed method.\n\nOther comment:\n\n- I'd like to see experimental results on natural language tasks, e.g., syntax parsing.\n\n- Section 2:  \"we see that is is not at all necessary\" -> that is\n\n- Section 3: Notation is a little bit hard to follow, \":\" for D and U, and \";\" in stacking.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Review", "cdate": 1542234195331, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335981008, "tmdate": 1552335981008, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJgu4gdM6Q", "original": null, "number": 4, "cdate": 1541730352096, "ddate": null, "tcdate": 1541730352096, "tmdate": 1541730352096, "tddate": null, "forum": "SJerEhR5Km", "replyto": "Ske48oug67", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "content": {"title": "More details", "comment": "The current draft has no related work section and does not put the research in context with the existing literature. It contains only a mere 7 references, 3 of which are Transformer (the model used), Adam (the optimizer used) and Chen et al (the only baseline).\nIt ignores the many works that have used position embeddings / encodings before such as [1, 2, 3]. I also suspect that there exist spectral theory approaches to represent nodes in a tree (consider the eigenvectors of the adjacency matrix for example)\n\nRegarding clarity: Some notation is not introduced in section 3. Dimensions are not always obvious and more figures would help with comprehension.\n\nConcerning binary trees: There are a few non-discussed issues about relying on the Left-Child Right-Sibling Representation, such as whether any information is lost (this changes the number of branches between nodes) and how the increase of the tree depth affects downstream performance (since the encodings can only encode information perfectly up to k branches). The trade-off between n and k is also not discussed (for example when does it become useful to represent a n-ary tree into its Left-Child Right-Sibling Representation?).\n\n[1] Self-Attention with Relative Position Representations\n[2] Music Transformer\n[3] Convolutional Sequence to Sequence Learning"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1446/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604962, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1446/Authors|ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604962}}}, {"id": "SJe_mn_x67", "original": null, "number": 3, "cdate": 1541602336283, "ddate": null, "tcdate": 1541602336283, "tmdate": 1541602336283, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SkgZrsYu2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "content": {"title": "Please expand", "comment": "Hello reviewer 3,\n\nYour review, while longer than the others on this paper, is very short. You are the only one supporting acceptance of this paper. Could you give a bit more detail about what its strengths and contributions are, and what areas it could improve on or be more clear about?\n\nBest,\nAC"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604962, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1446/Authors|ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604962}}}, {"id": "B1xJTjdxpX", "original": null, "number": 2, "cdate": 1541602231080, "ddate": null, "tcdate": 1541602231080, "tmdate": 1541602231080, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SyehfLgq37", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "content": {"title": "More detail needed", "comment": "Hello Reviewer 2,\n\nThank you for your review, but I'm afraid a little more detail is needed to justify your score, as your review is quite short.\n\nIn particular, what about the experiments makes them too narrow? What additional experiments would you like to see? What key citations are needed? In particular, what graph neural network approaches would you recommend comparing against?\n\nIt is essential, when recommending rejection, that a coherent argument be made for it so that the authors have something to respond to or rebut, or at least critical feedback they can use when revising the paper.\n\nBest,\nAC"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604962, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1446/Authors|ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604962}}}, {"id": "Ske48oug67", "original": null, "number": 1, "cdate": 1541602124226, "ddate": null, "tcdate": 1541602124226, "tmdate": 1541602124226, "tddate": null, "forum": "SJerEhR5Km", "replyto": "BJeiRhrpn7", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "content": {"title": "More detail needed", "comment": "Hello reviewer 1,\n\nThank you for your review, but I'm afraid a little more detail is needed to justify your score, as your review is quite short.\n\nIn particular, what are some references that you feel are missing? Why is it crucial to show experiments with larger trees, since, for example, any grammar can be binarised by putting it into Chomsky Normal Form? In what areas is the paper unclear, and could this be rectified during the rebuttal period?\n\nThanks.\nAC"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1446/Area_Chair1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621604962, "tddate": null, "super": null, "final": null, "reply": {"forum": "SJerEhR5Km", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1446/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1446/Authors|ICLR.cc/2019/Conference/Paper1446/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers", "ICLR.cc/2019/Conference/Paper1446/Authors", "ICLR.cc/2019/Conference/Paper1446/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621604962}}}, {"id": "BJeiRhrpn7", "original": null, "number": 3, "cdate": 1541393618851, "ddate": null, "tcdate": 1541393618851, "tmdate": 1541533002063, "tddate": null, "forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "invitation": "ICLR.cc/2019/Conference/-/Paper1446/Official_Review", "content": {"title": "Promising approach for enabling transformers to process tree-structured data", "review": "The authors propose to change the positional encodings in the transformer model to allow processing of tree-structured data.\nThe tree positional encodings summarize the path between 2 nodes as a series of steps up or down along tree branches with the constraint that traveling up a branch negates traveling down any branch.\n\nThe experimental results are encouraging and the method notably outperforms the regular transformer as well as the tree2tree LSTM introduced by Chen et al on larger datasets. \n\nThe current draft lacks some clarity and is low on references. It would also be interesting to see experiments with arbitrary trees or at least regular trees with degree > 2 (rather than just binary trees). While the authors only consider binary trees in this paper, it represents a good first step towards generalizing attention-based models to nonlinear structures.\n\nComments:\n* Would it be possible to use the fact that D_kU = I for the correct branch k? (This happens frequently for binary trees)", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1446/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Novel positional encodings to enable tree-structured transformers", "abstract": "With interest in program synthesis and similarly \ufb02avored problems rapidly increasing, neural models optimized for tree-domain problems are of great value. In the sequence domain, transformers can learn relationships across arbitrary pairs of positions with less bias than recurrent models. Under the intuition that a similar property would be beneficial in the tree domain, we propose a method to extend transformers to tree-structured inputs and/or outputs. Our approach abstracts transformer's default sinusoidal positional encodings, allowing us to substitute in a novel custom positional encoding scheme that represents node positions within a tree. We evaluated our model in tree-to-tree program translation and sequence-to-tree semantic parsing settings, achieving superior performance over the vanilla transformer model on several tasks.\n", "paperhash": "shiv|novel_positional_encodings_to_enable_treestructured_transformers", "TL;DR": "We develop novel positional encodings for tree-structured data, enabling transformers to be applied to tree structured problems.", "authorids": ["vishiv@microsoft.com", "chrisq@microsoft.com"], "authors": ["Vighnesh Leonardo Shiv", "Chris Quirk"], "keywords": ["program translation", "tree structures", "transformer"], "pdf": "/pdf/f7b6fef43458c23812481da033d66028fade8b3e.pdf", "_bibtex": "@misc{\nshiv2019novel,\ntitle={Novel positional encodings to enable tree-structured transformers},\nauthor={Vighnesh Leonardo Shiv and Chris Quirk},\nyear={2019},\nurl={https://openreview.net/forum?id=SJerEhR5Km},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1446/Official_Review", "cdate": 1542234195331, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SJerEhR5Km", "replyto": "SJerEhR5Km", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1446/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335981008, "tmdate": 1552335981008, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1446/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 12}