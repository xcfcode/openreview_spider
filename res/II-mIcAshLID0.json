{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392825660000, "tcdate": 1392825660000, "number": 1, "id": "xxEOxtQ0ZytgK", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "II-mIcAshLID0", "replyto": "9ZGs9cekBp9w0", "signatures": ["Jordi Delgado"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "> This paper introduces an early-stopping criteria, to address the \r\npotential degeneracy\r\n > issues in CD-training of Restricted Boltzmann Machine. The method is \r\nclosely related to\r\n > the method of Free-Energy differences (FED), which computes the \r\ndifference of log\r\n > probabilities between the training set X and a held-out validation \r\nset Y. In lieu of a\r\n > validation set however, the authors propose to sample Y to points far \r\nfrom the training\r\n > data (for which the log-prob should monotonically decrease during \r\ntraining). To do this,\r\n > the authors propose sampling Y from the conditional distribution p(x \r\n| h), where h is\r\n > either drawn uniformly over binary states, or clamped to the \r\n'complement' of the infered\r\n > latent states when conditioned on training data, i.e. h = 1-h^(i) \r\nwith $h^(i) ~ p(h | x^(i))$.\r\n >\r\n > Given the similarities to FED, the method is not entirely novel. The \r\nidea of biasing the\r\n > Monte Carlo approximation of log Z to states $v$ sampled from the \r\nconditional p(v|h) is\r\n > interesting and reminiscent of [1,2]: a uniform prior over h could be \r\na quick and dirty way\r\n > to explore the space of visible configurations having some \r\nprobability mass under the\r\n > model. The second option seems less justified in my opinion: why use \r\nthe complement of\r\n > h^(i) instead of h^(i) itself ? The former would allow to quickly \r\nsample (local) perturbations\r\n > of x^(i), which seems more inline with the CD training criteria \r\n(which only raises the\r\n > energy of nearby configurations).\r\nIn general we agree that using part of the examples as a training set \r\nand the rest as a\r\nvalidation set is a good way to proceed. However in the studied data \r\nsets that separation\r\nwould lead to significant information loss that may lead to  wrong results\r\n(for instance in the bars and stripes problem one'd better show the \r\nnetwork all\r\npossible instances).\r\n\r\nIn addition, there is a fundamental difference between FED and the \r\nproposed criterion,\r\nwhich is the fact that in our case we want to compare the probabilities \r\nof data from\r\nthe training set (that should be high) with the probabilities of data in \r\nthe complementary\r\nsubspace (that should be low). In FED, the probabilities of the compared \r\ndata must be\r\nhigh in both cases.\r\n\r\n > All in all the paper is interesting, but much better suited to the \r\nworkshop track of the\r\n > conference. The method is not entirely novel and the experiments are \r\nstill very\r\n > preliminary.\r\n > (1) The datasets are very small and would need to be scaled to more \r\nrealistic datasets,\r\n > using AIS as a proxy to the true likelihood.\r\n\r\nWe agree that it is interesting to explore the scalability with the \r\nsystem size. However\r\nin order to check against exact results (computation of the likelihood) \r\none is restricted\r\nto medium or small spaces. Furthermore we are also aware that AIS may \r\nalso fail\r\nin some cases (we already mention that in our paper and refer to \r\nreference Schult et al. 2010).\r\n\r\n > (2) The evidence in favor of the proposed criteria is rather weak. \r\nThe behavior of the first\r\n > criterion changes completely based on the dataset, while the second \r\ncriterion fails when\r\n > using CD-10 (with no clear explanation why).\r\n\r\nOur justifications are more based on intuitions. We are currently \r\nworking on the mathematical\r\ngrounds behind the ideas, but this is still work in progress. But in any \r\ncase the experimental\r\nresults are promising and seem to support our conclusions, which is the \r\nmain reason why we\r\nsubmitted the paper.\r\n\r\n > Also, the early-stopping point obtained by (ii)\r\n > on the LSE dataset remains rather approximate. Using this criterion \r\nwould have yielded a\r\n > much lower likelihood than possible.\r\n\r\nOur interpretation of these results is quite different. The estimator \r\ngrows very quickly\r\n(as the log-likelihood does) and reaches a maximum very close to the \r\nregion where the\r\nlog-likelihood is maximum also. In addition, the shapes of the curves \r\nare very similar,\r\nshowing that the criterion is gathering the essence of the changes in \r\nthe log-likelihood.\r\n\r\n > (3) The authors seem unaware of the FED method and do not compare \r\nagainst it.\r\n\r\nWe will include references to the FED method in the new revision."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392825540000, "tcdate": 1392825540000, "number": 1, "id": "Y56-5VXmnRYoH", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "II-mIcAshLID0", "replyto": "OmXwbbinULbGy", "signatures": ["Jordi Delgado"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "> On the theoretical side, the proposed criterion (eq. 8) is only \r\n > heuristically motivated. Note\r\n > in particular that a model might be able to reach an arbitrary high \r\nvalue of this criterion if\r\n > P(y_i) -> 0 for some y_i, regardless or how badly it may perform on \r\nthe training x_i's.\r\n\r\nIn our mind we implicitely assume CD_1 is leading to the right place \r\nstarting from\r\nweights initialized at random. if that does not hold, it would lead to \r\ntrouble no matter\r\nwhat the stopping criteria you use.\r\nunder our assumptions, the situation you point out should not happen, \r\nhopefully.\r\nOur experiments seem to back up our line of reasoning, although we agree \r\nthe criteria should\r\nbe contrasted against other problems :)\r\n\r\n > I'm also not sure why one would\r\n > necessarily take N y_i's: why not generalize it to M y_i's, using Pi_i\r\n > P(y_i)^(N/M), so as to be able to choose the sample size based on \r\navailable\r\n > computational resources?\r\n\r\nWe don't say that N_i should be equal to M_i: we just use the proposed \r\nestimator,\r\nbut of course it could happen that enhanced versions of it leads to \r\nbetter results.\r\nStill, we are confident the proposed criteria works well.\r\n\r\n > Finally, I am not convinced by the choice of y_i's:\r\n > first, the sampling rules (random h or '1 - h_i') are not well \r\nmotivated (there\r\n > is no guarantee that we will not sample training points)\r\n\r\nOur belief is that at the very beginning of the training process the \r\nproposed\r\nsampling does lead to states that are far away from the training set, \r\njust because\r\nthe RBM has been initialized to random weights and bias, which are \r\n*very* different\r\nfrom the ones one will get at the optimal point of the learning stage.\r\nof course the criteria gets worse when you surpass that point. But that \r\nis precisely our main message:\r\nthis does not happen while the system is learning and weights are still \r\nnon-optimal.\r\n\r\n > second, since they are defined from the model parameters they lead to \r\na set of\r\n > y_i's that evolves during training (thus the criterion may be unstable)\r\n\r\nWe agree that on general grounds, any procedure that changes with model \r\nparameters could be\r\ndynamically unstable, even standard CD_k could suffer from this problem.\r\nHowever we have not seen anything like that in the problems analyzed.\r\nIn the present case, the size of the spaces studied, despite large, are \r\nsmall enough\r\nto compute the likelihood and to exhaustively explore all possible \r\nstates. While\r\ndoing so, we have not encountered any of the problems mentioned here.\r\n\r\n > third the y_i's are\r\n > expectations and there is no explanation on whether it makes sense \r\nfor binary\r\n > RBMs.\r\n\r\nWe have seen that using average values reduces noise. That makes the \r\nalgorithm more stable,\r\nalthough not using expectations leads to the same statistical solutions.\r\n\r\n > On the empirical side, my first concern is that experiments are performed\r\n > on low-dimensional toy datasets, and there is nothing to tell us that\r\n > behavior observed on such datasets will actually translate into higher\r\n > dimensional tasks. In particular, sampling-based methods tend to \r\nbehave rather\r\n > nicely in low dimension, but may break horribly as the dimension \r\nincreases...\r\n\r\nWe are aware that we have to do a more exhaustive study on the scaling \r\nproperties\r\nof the method, and we are currently working on that. However, we know \r\nstochastic sampling techniques\r\nare best suited for large scale problems. In fcat, when the \r\ndimensionality of the space increases,\r\nstochastic methods are essentially the only ones that provide reliable \r\nresults on a general\r\nground. For instance, in numerical simulations of quantum many-body \r\nsystems of strongly\r\ninteracting partcicles, Monte Carlo methods are known to be the only \r\nones that are able to provide\r\nexact statistical solutions to the Schrodinger equation. We are \r\nconfident the same applies in the\r\npresent case.\r\n\r\n > Thus it would have been good to add experiments in high dimension,\r\n >  for instanceusing AIS to estimate the partition function.\r\n\r\nWe agree that it is interesting to explore the scalability with the \r\nsystem size. However\r\nin order to check against exact results (computation of the likelihood) \r\none is restricted\r\nto medium or small spaces. Furthermore we are also aware that AIS may \r\nalso fail\r\nin some cases (we already mention that in our paper and refer to \r\nreference Schult et al. 2010).\r\n\r\n > My second concern is that only\r\n > training errors are reported: although they are definitely interesting to\r\n > monitor, someone using reconstruction error as a stopping criterion \r\nwill always\r\n > use a validation set for this, and will hope to stop at a point where \r\nvalidation\r\n > log-likelihood is maximized. The comparisons in the paper are thus, \r\nfor the\r\n > most part, uninformative, since they only use the training data.\r\n\r\nIn general we agree that using part of the examples as a training set \r\nand the rest as a\r\nvalidation set is a good way to proceed. However in the studied data \r\nsets that separation\r\nwould lead to significant information loss that may lead to  wrong results\r\n(for instance in the bars and stripes problem one'd better show the \r\nnetwork all\r\npossible instances).\r\n\r\n > A few more minor points:\r\n > - Eq. 5 is missing some characters\r\n > - The number of hidden units is not mentioned in the experiments\r\n > - Plots show 'reconstruction error' as something that is better when \r\nit increases,\r\n > which is counter-intuitive for an error\r\n > - Something potentially worth discussing is that RBMs are often used for\r\n > pre-training purpose in deep networks, and it is not clear that better\r\n > likelihood => better pre-training (if there is work on this topic, it \r\nshould\r\n > be cited, as it is important to motivate this direction of research)\r\n > - Another application worth mentioning to this kind of technique is model\r\n > selection (which RBM is best?) => the proposed criterion may require \r\na bit\r\n > of tweaking to answer this kind of question (common y_i's are needed)\r\n\r\nWe thank for these suggestions, and will try to accomodate them in the \r\nnext revision."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392825420000, "tcdate": 1392825420000, "number": 1, "id": "byzJbRlNaEb8x", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "II-mIcAshLID0", "replyto": "YjyZY1Uuwzj8p", "signatures": ["Jordi Delgado"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "> Dear authors,\r\n >\r\n > Let me reveal my identity before I continue. I'm Kyunghyun Cho who\r\n > wrote the earlier review (by an unexpected coincidence). The previous\r\n > comments reflect what I wanted/want to say as an official reviewer,\r\n > and I will not write another separate review.\r\n >\r\n\r\nThanks for revealing your identity. It's nice to response to the\r\ncomments of a reviewer previous to become a reviewer.\r\n\r\n > Instead, let me answer briefly to the response to my review from the\r\n > authors:\r\n >\r\n > (Authors) 'If you change the value of the hidden units in any way,\r\n > this delicate equilibrium is broken and you expect to get much lower\r\n > probabilities for the visible variables.'\r\n >\r\n > => I cannot agree with this. Intuitively, if we believe that training\r\n > makes latent variables learn potentially lower-dimensional manifold on\r\n > which training samples lie, any small (or even large) change in the\r\n > latent representation shouldn't correspond to a change in the input\r\n > space that moves the point away from the manifold. I'd agree with your\r\n > argument much more, if you were flipping all the bits of the input\r\n > variable.\r\n >\r\n\r\nIn our understanding, the situation you point out happens after the\r\nRBM has successfully learned the data distribution (that is, when the\r\ntraining has finished). During the early stages of the training\r\nprocess, in contrast, we think that this property may not hold, since\r\nthe weights are not good enough yet. We considered the possibility of\r\nflipping the bits of the input data, but it would prevent working with\r\nsymmetric data (like the 'Bars and Stripes' data set, for example).\r\n\r\n > (Authors) 'CD_1 as it is the cheapest way we know'\r\n >\r\n > => Computationally, I don't see why CD-1 should be cheaper than\r\n > PCD. Memory-wise, if you follow the usual practice of maintaining only\r\n > a few persistent samples, it shouldn't matter too much as\r\n > well. Though, I agree that it's easier to train an RBM with CD-1 using\r\n > a much higher learning rate, which may make learning progress faster.\r\n >\r\n\r\nWe agree with you. Comparing CD_1 and PCD, only memory requirements\r\nare different. As previously stated, it would be worth to see if the\r\nproposed criterion works for PCD in a similar way than for CD_1."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391877840000, "tcdate": 1391877840000, "number": 7, "id": "9ZGs9cekBp9w0", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "II-mIcAshLID0", "replyto": "II-mIcAshLID0", "signatures": ["anonymous reviewer 7bb8"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "review": "This paper introduces an early-stopping criteria, to address the potential degeneracy issues in CD-training of Restricted Boltzmann Machine. The method is closely related to the method of Free-Energy differences (FED), which computes the difference of log probabilities between the training set X and a held-out validation set Y. In lieu of a validation set however, the authors propose to sample Y to points far from the training data (for which the log-prob should monotonically decrease during training). To do this, the authors propose sampling Y from the conditional distribution p(x | h), where h is either drawn uniformly over binary states, or clamped to the 'complement' of the infered latent states when conditioned on training data, i.e. h = 1-h^(i) with $h^(i) ~ p(h | x^(i))$.\r\n\r\nGiven the similarities to FED, the method is not entirely novel. The idea of biasing the Monte Carlo approximation of log Z to states $v$ sampled from the conditional p(v|h) is interesting and reminiscent of [1,2]: a uniform prior over h could be a quick and dirty way to explore the space of visible configurations having some probability mass under the model. The second option seems less justified in my opinion: why use the complement of h^(i) instead of h^(i) itself ? The former would allow to quickly sample (local) perturbations of x^(i), which seems more inline with the CD training criteria (which only raises the energy of nearby configurations).\r\n\r\nAll in all the paper is interesting, but much better suited to the workshop track of the conference. The method is not entirely novel and the experiments are still very preliminary.\r\n(1) The datasets are very small and would need to be scaled to more realistic datasets, using AIS as a proxy to the true likelihood.\r\n(2) The evidence in favor of the proposed criteria is rather weak. The behavior of the first criterion changes completely based on the dataset, while the second criterion fails when using CD-10 (with no clear explanation why). Also, the early-stopping point obtained by (ii) on the LSE dataset remains rather approximate. Using this criterion would have yielded a much lower likelihood than possible.\r\n(3) The authors seem unaware of the FED method and do not compare against it.\r\n\r\nAs the proposed criteria are based on heuristics, they require very solid empirical evidence to justify their use. As it stands, the paper simply does not deliver.\r\n\r\nOther:\r\n* paper glosses over another widely used early-stopping heuristic: classification error.\r\n* superfluous citation to (Bengio, 2009) for efficient block Gibbs sampling of RBMs.\r\n* latex bug for visible and hidden unit biases in Eq 5\r\n* 'In doing so, drastic approximations [...] are performed'. Do not understand this sentence.\r\n* 'making zeta when learning is achieved' ?? Learning is an optimization process not an discrete time event.\r\n* 'A second possibility is to suitably compute [...] during data reconstruction'. Very clumsy way to say use 1-h^(i). The reconstruction phase conditions on h, therefore 'the value they should take during [...] reconstruction' is nonsensical.\r\n* 'Anyway, the behavior of the proposed criteria [...] should be further studied.'. Very uninformative statement whose writing style seems *very* inappropriate for a paper submission.\r\n* 'This westimator works well for CD1 but [not] for CD10, which is' - Missing [not]\r\n\r\n[1] Gr\u00e9goire Mesnil, Salah Rifai, Yann Dauphin, Yoshua Bengio and Pascal Vincent, Surfing on the Manifold, Learning Workshop, Snowbird, 2012.\r\n[2] Yoshua Bengio, Gr\u00e9goire Mesnil, Yann Dauphin and Salah Rifai, Better Mixing via Deep Representations, in: Proceedings of the 30th International Conference on Machine Learning (ICML'13), ACM, 2013"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391496480000, "tcdate": 1391496480000, "number": 6, "id": "OmXwbbinULbGy", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "II-mIcAshLID0", "replyto": "II-mIcAshLID0", "signatures": ["anonymous reviewer 0ef5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "review": "This paper investigates stopping criteria for training of RBMs by contrastive\r\ndivergence (CD). Traditional reconstruction error is compared to a ratio of\r\nprobabilities, namely the probabilities of the training data divided by the\r\nprobabilities of an equal number of sampled points (with two variants of the\r\nsampling strategy). By dividing probabilities, the partition function cancels\r\nout, which makes computations tractable. Experiments on two toy datasets show\r\nthat the two proposed variants are overall more useful than reconstruction\r\nerror to identify the point of maximum likelihood on training data, even though\r\nthey do not work on all cases investigated here.\r\n\r\nThe problem of early stopping of RBM training is definitely a relevant one,\r\nsince the intractability of the partition function prevents properly monitoring\r\nthe likelihood. The approach suggested here, however, lacks in both theoretical\r\nmotivation and empirical evaluation, thus in my opinion is not quite ready for\r\npublication.\r\n\r\nOn the theoretical side, the proposed criterion (eq. 8) is only heuristicallymotivated. Note in particular that a model might be able to reach an arbitraryhigh value of this criterion if P(y_i) -> 0 for some y_i, regardless or howbadly it may perform on the training x_i's. I'm also not sure why one would\r\nnecessarily take N y_i's: why not generalize it to M y_i's, using Pi_i\r\nP(y_i)^(N/M), so as to be able to choose the sample size based on available\r\ncomputational resources? Finally, I am not convinced by the choice of y_i's:\r\nfirst, the sampling rules (random h or '1 - h_i') are not well motivated (there\r\nis no guarantee that we will not sample training points), second, since they\r\nare defined from the model parameters they lead to a set of y_i's that evolves\r\nduring training (thus the criterion may be unstable), third the y_i's are\r\nexpectations and there is no explanation on whether it makes sense for binary\r\nRBMs.\r\n\r\nOn the empirical side, my first concern is that experiments are performed\r\non low-dimensional toy datasets, and there is nothing to tell us that \r\nbehavior observed on such datasets will actually translate into higher\r\ndimensional tasks. In particular, sampling-based methods tend to behave rather\r\nnicely in low dimension, but may break horribly as the dimension increases...\r\nThus it would have been good to add experiments in high dimension, for instance\r\nusing AIS to estimate the partition function. My second concern is that only\r\ntraining errors are reported: although they are definitely interesting to\r\nmonitor, someone using reconstruction error as a stopping criterion will always\r\nuse a validation set for this, and will hope to stop at a point where validation\r\nlog-likelihood is maximized. The comparisons in the paper are thus, for the\r\nmost part, uninformative, since they only use the training data.\r\n\r\nA few more minor points:\r\n- Eq. 5 is missing some characters\r\n- The number of hidden units is not mentioned in the experiments\r\n- Plots show 'reconstruction error' as something that is better when it increases,\r\n  which is counter-intuitive for an error\r\n- Something potentially worth discussing is that RBMs are often used for\r\n  pre-training purpose in deep networks, and it is not clear that better\r\n  likelihood => better pre-training (if there is work on this topic, it should\r\n  be cited, as it is important to motivate this direction of research)\r\n- Another application worth mentioning to this kind of technique is model\r\n  selection (which RBM is best?) => the proposed criterion may require a bit\r\n  of tweaking to answer this kind of question (common y_i's are needed)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390885920000, "tcdate": 1390885920000, "number": 5, "id": "YjyZY1Uuwzj8p", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "II-mIcAshLID0", "replyto": "II-mIcAshLID0", "signatures": ["anonymous reviewer 4ea5"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "review": "Dear authors,\r\n\r\nLet me reveal my identity before I continue. I'm Kyunghyun Cho who wrote the earlier review (by an unexpected coincidence). The previous comments reflect what I wanted/want to say as an official reviewer, and I will not write another separate review. \r\n\r\nInstead, let me answer briefly to the response to my review from the authors:\r\n\r\n(Authors) 'If you change the value of the hidden units in any way, this delicate equilibrium is broken and you expect to get much lower probabilities for the visible variables.' \r\n\r\n=> I cannot agree with this. Intuitively, if we believe that training makes latent variables learn potentially lower-dimensional manifold on which training samples lie, any small (or even large) change in the latent representation shouldn't correspond to a change in the input space that moves the point away from the manifold. I'd agree with your argument much more, if you were flipping all the bits of the input variable.\r\n\r\n(Authors) 'CD_1 as it is the cheapest way we know'\r\n\r\n=> Computationally, I don't see why CD-1 should be cheaper than PCD. Memory-wise, if you follow the usual practice of maintaining only a few persistent samples, it shouldn't matter too much as well. Though, I agree that it's easier to train an RBM with CD-1 using a much higher learning rate, which may make learning progress faster.\r\n\r\n- Cho"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390497300000, "tcdate": 1390497300000, "number": 4, "id": "wxTahwWyXKwlI", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "II-mIcAshLID0", "replyto": "II-mIcAshLID0", "signatures": ["David Buchaca Prats"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear Kyunghyun  Cho,\r\n\r\nwe appreciate very much the effort you've made to carefully read our\r\npreprint.  After discussing the topics you've raised we have tried to\r\nwork out a new version of the paper that will (hopefully) clarify some\r\naspects of our work. In the meantime we want to answer (at least some)\r\nof your comments and questions.\r\n\r\n- The proposed stopping criterion itself is not so novel, in my\r\n  opinion. Essentially, it is a general framework in which, for\r\n  instance, CD minimizes, where $y^{(i)}$ is obtained by a single-step\r\n  Gibbs sampling from $x^{(i)}$. In fact, if $y^{(i)}$ is one step of\r\n  persistent MCMC chain, the stopping criterion corresponds to the\r\n  approximate maximum likelihood criterion used by PCD. In short, the\r\n  proposed criterion is, in fact, what a proper MLE should minimize.\r\n\r\n--- We are not sure to understand your comment regarding PCD.  To our\r\n    understanding, PCD may also lead to a decreasing log-likelihood,\r\n    and the reconstruction error is not a good stopping criterion for\r\n    PCD either (see for instance Fischer & Igel, 2010). In PCD, the\r\n    value of $y^{(i)}$ is different from the ones defined in our\r\n    paper, and it is used for other purposes. Still, we agree with you\r\n    that the idea of computing $y^{(i)}$ in a single Gibbs step (in\r\n    whatever the recipe you use) and get a statistical estimator from\r\n    them, is not so novel (actually CD1 does that already).  As you\r\n    mention below, the novelty lies on the way we choose $y^{(i)}$.\r\n    But in any case we agree we can make more emphasis on that aspect\r\n    in a newer version of the paper.\r\n\r\n- The novelty is, however, in the proposed ways to choose\r\n  $y^{(i)}$. The authors proposed two alternatives. Unfortunately (as\r\n  somewhat expected) the first method of selecting random points does\r\n  not work, which is only natural as if it did, why would anyone use\r\n  MCMC to estimate the model statistics of RBM?  The second\r\n  alternative of using the visible samples conditioned on the\r\n  *flipped* hidden states seems to work better, but the authors do not\r\n  provide any good justification for this choice. \r\n\r\n--- Our justifications are more based on intuitions. We are currently\r\n    working on the mathematical grounds behind the ideas, but this is\r\n    still work in progress.  But in any case the experimental results\r\n    are promising and seem to support our conclusions, which is the\r\n    main reason why we submitted the paper.  In our mind, however, the\r\n    idea behind  the work is slightly different from what you point\r\n    out. We believe the new stopping criteria will work better in\r\n    large spaces where the subset of states having a non-negligible\r\n    probability is small. In these systems, only very precise\r\n    combinations of weights and unit values leads to large\r\n    probabilities.  If you change the value of the hidden units in any\r\n    way, this delicate equilibrium is broken and you expect to get\r\n    much lower probabilities for the visible variables. And you can do\r\n    that by either getting the proper value of the hidden units and take\r\n    their complementary (the criterion that works better, apparently),\r\n    or simply by changing them at random. In the learning process, one uses\r\n    MCMC because, in general, one wants to do the opposite. That is, to\r\n    identify the (small) regions of large probability, which is a\r\n    difficult task if the space is large and the set of relevant\r\n    states is small. But that implies that identifying states with low\r\n    probability should be much easier, and our hope is to find any of\r\n    these by generating values of the hidden variables that are very\r\n    different from the right ones. We believe that this is not clearly\r\n    seen on the experiments reported because the size of the spaces\r\n    analyzed is not that large. We plan to extend our numerical\r\n    analysis to other, larger systems where this should be more\r\n    evident.\r\n\r\n- The experiments show that the second choice is superior, but since\r\n  the experiments are rather small-scale, it is difficult to make a\r\n  solid conclusion from them. Also, the proposed method seems to only\r\n  work with CD_1 (see Fig. 4), but the authors' explanation on\r\n  possible reasons for this is not clear. \r\n\r\n--- Actually we are mostly interested in CD_1 as it is the cheapest\r\n    way we know (with respect to time and memory storage) to evaluate\r\n    the correlations required at the learning stage. We must admit,\r\n    however, that the reason why it seems to perform worse in CD_k is\r\n    somewhat unclear to us. As we said above, we are working on the\r\n    mathematical grounds of the method and we hope to have a more\r\n    clear understanding of the fine details of the method in the near\r\n    future. \r\n\r\n- One thing which is clearly missing from the experiments is the\r\n  comparison of the proposed criterion against the actual function CD\r\n  minimizes ($E_D left[ log p (x^{(i)}) - log p(y^{(i)}) \right]$,\r\n  where $y^{(i)}$ is the one-step Gibbs sample starting from\r\n  $x^{(i)}$.) as well as the approximate likelihood (the same thing,\r\n  but use samples from persistent chains as $y^{(i)}$'s). Computing\r\n  these things should be no more expensive than computing the proposed\r\n  criterion, and may give better indication of how log-likelihood\r\n  evolves over time. This will be interesting to see. \r\n\r\n--- We have some of these results, but we decided not to include them\r\n    in the paper to limit its extensions. In any case we have seen\r\n    that the behavior is similar to the reconstruction error, and thus\r\n    show the same problems once compared to the likelihood itself.  In\r\n    the paper we only included 'somewhat positive' results, but we\r\n    agree with you that these results may be interesting to be\r\n    discussed. \r\n\r\n- Of course, one last thing is, why anyone would try to train an RBM\r\n  to be a good generative model (maximizing the log-likelihood) using\r\n  CD in the first place, while PCD (with possibly other MCMC\r\n  algorithms than Gibbs) is available.\r\n\r\n\u00a0--- The paper is not trying to answer this question. We just want to\r\n\u00a0 \u00a0 \u00a0explore new ways to improve the stopping criteria in a learning\r\n\u00a0 \u00a0 \u00a0algorithm where the reconstruction error does not work properly\r\n\u00a0 \u00a0 \u00a0(in general), and we have tested it in CD_1. Since the\r\n\u00a0 \u00a0 \u00a0reconstruction error is not a good stopping criterion for PCD\r\n\u00a0 \u00a0 \u00a0either, PCD may also benefit from other options. It would be\r\n\u00a0 \u00a0 \u00a0interesting to see if the same stopping criteria work properly\r\n\u00a0 \u00a0 \u00a0with PCD, but that we leave for a future work.\r\n\r\n-Despite these seemingly negative comments, I enjoyed reading the\r\n paper, since it is clearly written and well motivated. A few more\r\n experiments showing that the proposed methods of choosing $y^{(i)}$\r\n are better than other possible choices (e.g., using Gibbs sampling) \r\n would make the paper much stronger. \r\n\r\n--- Thanks a lot for the comments and suggestions. We agree with you\r\n    that there is more room to improve, and we are already working on\r\n    that.  But at this point we didn't want to miss the opportunity to\r\n    submit or recent results to this conference in order to discuss\r\n    our ideas with other people with lots of expertise in the\r\n    field. Still we hope to follow this line and to get more\r\n    exhaustive results in the near future.\r\n\r\n- P. S. I wouldn't mind, if you cited my paper (IJCNN 2013) next to\r\n (Desjardins et al., 2010) where I also proposed PT. \r\n\r\n--- Consider it done"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390497240000, "tcdate": 1390497240000, "number": 3, "id": "1qNo1qzRow7I0", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "II-mIcAshLID0", "replyto": "II-mIcAshLID0", "signatures": ["David Buchaca Prats"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear Kyunghyun  Cho,\r\n\r\nwe appreciate very much the effort you've made to carefully read our\r\npreprint.  After discussing the topics you've raised we have tried to\r\nwork out a new version of the paper that will (hopefully) clarify some\r\naspects of our work. In the meantime we want to answer (at least some)\r\nof your comments and questions.\r\n\r\n- The proposed stopping criterion itself is not so novel, in my\r\n  opinion. Essentially, it is a general framework in which, for\r\n  instance, CD minimizes, where $y^{(i)}$ is obtained by a single-step\r\n  Gibbs sampling from $x^{(i)}$. In fact, if $y^{(i)}$ is one step of\r\n  persistent MCMC chain, the stopping criterion corresponds to the\r\n  approximate maximum likelihood criterion used by PCD. In short, the\r\n  proposed criterion is, in fact, what a proper MLE should minimize.\r\n\r\n--- We are not sure to understand your comment regarding PCD.  To our\r\n    understanding, PCD may also lead to a decreasing log-likelihood,\r\n    and the reconstruction error is not a good stopping criterion for\r\n    PCD either (see for instance Fischer & Igel, 2010). In PCD, the\r\n    value of $y^{(i)}$ is different from the ones defined in our\r\n    paper, and it is used for other purposes. Still, we agree with you\r\n    that the idea of computing $y^{(i)}$ in a single Gibbs step (in\r\n    whatever the recipe you use) and get a statistical estimator from\r\n    them, is not so novel (actually CD1 does that already).  As you\r\n    mention below, the novelty lies on the way we choose $y^{(i)}$.\r\n    But in any case we agree we can make more emphasis on that aspect\r\n    in a newer version of the paper.\r\n\r\n- The novelty is, however, in the proposed ways to choose\r\n  $y^{(i)}$. The authors proposed two alternatives. Unfortunately (as\r\n  somewhat expected) the first method of selecting random points does\r\n  not work, which is only natural as if it did, why would anyone use\r\n  MCMC to estimate the model statistics of RBM?  The second\r\n  alternative of using the visible samples conditioned on the\r\n  *flipped* hidden states seems to work better, but the authors do not\r\n  provide any good justification for this choice. \r\n\r\n--- Our justifications are more based on intuitions. We are currently\r\n    working on the mathematical grounds behind the ideas, but this is\r\n    still work in progress.  But in any case the experimental results\r\n    are promising and seem to support our conclusions, which is the\r\n    main reason why we submitted the paper.  In our mind, however, the\r\n    idea behind  the work is slightly different from what you point\r\n    out. We believe the new stopping criteria will work better in\r\n    large spaces where the subset of states having a non-negligible\r\n    probability is small. In these systems, only very precise\r\n    combinations of weights and unit values leads to large\r\n    probabilities.  If you change the value of the hidden units in any\r\n    way, this delicate equilibrium is broken and you expect to get\r\n    much lower probabilities for the visible variables. And you can do\r\n    that by either getting the proper value of the hidden units and take\r\n    their complementary (the criterion that works better, apparently),\r\n    or simply by changing them at random. In the learning process, one uses\r\n    MCMC because, in general, one wants to do the opposite. That is, to\r\n    identify the (small) regions of large probability, which is a\r\n    difficult task if the space is large and the set of relevant\r\n    states is small. But that implies that identifying states with low\r\n    probability should be much easier, and our hope is to find any of\r\n    these by generating values of the hidden variables that are very\r\n    different from the right ones. We believe that this is not clearly\r\n    seen on the experiments reported because the size of the spaces\r\n    analyzed is not that large. We plan to extend our numerical\r\n    analysis to other, larger systems where this should be more\r\n    evident.\r\n\r\n- The experiments show that the second choice is superior, but since\r\n  the experiments are rather small-scale, it is difficult to make a\r\n  solid conclusion from them. Also, the proposed method seems to only\r\n  work with CD_1 (see Fig. 4), but the authors' explanation on\r\n  possible reasons for this is not clear. \r\n\r\n--- Actually we are mostly interested in CD_1 as it is the cheapest\r\n    way we know (with respect to time and memory storage) to evaluate\r\n    the correlations required at the learning stage. We must admit,\r\n    however, that the reason why it seems to perform worse in CD_k is\r\n    somewhat unclear to us. As we said above, we are working on the\r\n    mathematical grounds of the method and we hope to have a more\r\n    clear understanding of the fine details of the method in the near\r\n    future. \r\n\r\n- One thing which is clearly missing from the experiments is the\r\n  comparison of the proposed criterion against the actual function CD\r\n  minimizes ($E_D left[ log p (x^{(i)}) - log p(y^{(i)}) \right]$,\r\n  where $y^{(i)}$ is the one-step Gibbs sample starting from\r\n  $x^{(i)}$.) as well as the approximate likelihood (the same thing,\r\n  but use samples from persistent chains as $y^{(i)}$'s). Computing\r\n  these things should be no more expensive than computing the proposed\r\n  criterion, and may give better indication of how log-likelihood\r\n  evolves over time. This will be interesting to see. \r\n\r\n--- We have some of these results, but we decided not to include them\r\n    in the paper to limit its extensions. In any case we have seen\r\n    that the behavior is similar to the reconstruction error, and thus\r\n    show the same problems once compared to the likelihood itself.  In\r\n    the paper we only included 'somewhat positive' results, but we\r\n    agree with you that these results may be interesting to be\r\n    discussed. \r\n\r\n- Of course, one last thing is, why anyone would try to train an RBM\r\n  to be a good generative model (maximizing the log-likelihood) using\r\n  CD in the first place, while PCD (with possibly other MCMC\r\n  algorithms than Gibbs) is available.\r\n\r\n\u00a0--- The paper is not trying to answer this question. We just want to\r\n\u00a0 \u00a0 \u00a0explore new ways to improve the stopping criteria in a learning\r\n\u00a0 \u00a0 \u00a0algorithm where the reconstruction error does not work properly\r\n\u00a0 \u00a0 \u00a0(in general), and we have tested it in CD_1. Since the\r\n\u00a0 \u00a0 \u00a0reconstruction error is not a good stopping criterion for PCD\r\n\u00a0 \u00a0 \u00a0either, PCD may also benefit from other options. It would be\r\n\u00a0 \u00a0 \u00a0interesting to see if the same stopping criteria work properly\r\n\u00a0 \u00a0 \u00a0with PCD, but that we leave for a future work.\r\n\r\n-Despite these seemingly negative comments, I enjoyed reading the\r\n paper, since it is clearly written and well motivated. A few more\r\n experiments showing that the proposed methods of choosing $y^{(i)}$\r\n are better than other possible choices (e.g., using Gibbs sampling) \r\n would make the paper much stronger. \r\n\r\n--- Thanks a lot for the comments and suggestions. We agree with you\r\n    that there is more room to improve, and we are already working on\r\n    that.  But at this point we didn't want to miss the opportunity to\r\n    submit or recent results to this conference in order to discuss\r\n    our ideas with other people with lots of expertise in the\r\n    field. Still we hope to follow this line and to get more\r\n    exhaustive results in the near future.\r\n\r\n- P. S. I wouldn't mind, if you cited my paper (IJCNN 2013) next to\r\n (Desjardins et al., 2010) where I also proposed PT. \r\n\r\n--- Consider it done"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390497240000, "tcdate": 1390497240000, "number": 2, "id": "mmVw9BfnguBDe", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "II-mIcAshLID0", "replyto": "II-mIcAshLID0", "signatures": ["David Buchaca Prats"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Dear Kyunghyun  Cho,\r\n\r\nwe appreciate very much the effort you've made to carefully read our\r\npreprint.  After discussing the topics you've raised we have tried to\r\nwork out a new version of the paper that will (hopefully) clarify some\r\naspects of our work. In the meantime we want to answer (at least some)\r\nof your comments and questions.\r\n\r\n- The proposed stopping criterion itself is not so novel, in my\r\n  opinion. Essentially, it is a general framework in which, for\r\n  instance, CD minimizes, where $y^{(i)}$ is obtained by a single-step\r\n  Gibbs sampling from $x^{(i)}$. In fact, if $y^{(i)}$ is one step of\r\n  persistent MCMC chain, the stopping criterion corresponds to the\r\n  approximate maximum likelihood criterion used by PCD. In short, the\r\n  proposed criterion is, in fact, what a proper MLE should minimize.\r\n\r\n--- We are not sure to understand your comment regarding PCD.  To our\r\n    understanding, PCD may also lead to a decreasing log-likelihood,\r\n    and the reconstruction error is not a good stopping criterion for\r\n    PCD either (see for instance Fischer & Igel, 2010). In PCD, the\r\n    value of $y^{(i)}$ is different from the ones defined in our\r\n    paper, and it is used for other purposes. Still, we agree with you\r\n    that the idea of computing $y^{(i)}$ in a single Gibbs step (in\r\n    whatever the recipe you use) and get a statistical estimator from\r\n    them, is not so novel (actually CD1 does that already).  As you\r\n    mention below, the novelty lies on the way we choose $y^{(i)}$.\r\n    But in any case we agree we can make more emphasis on that aspect\r\n    in a newer version of the paper.\r\n\r\n- The novelty is, however, in the proposed ways to choose\r\n  $y^{(i)}$. The authors proposed two alternatives. Unfortunately (as\r\n  somewhat expected) the first method of selecting random points does\r\n  not work, which is only natural as if it did, why would anyone use\r\n  MCMC to estimate the model statistics of RBM?  The second\r\n  alternative of using the visible samples conditioned on the\r\n  *flipped* hidden states seems to work better, but the authors do not\r\n  provide any good justification for this choice. \r\n\r\n--- Our justifications are more based on intuitions. We are currently\r\n    working on the mathematical grounds behind the ideas, but this is\r\n    still work in progress.  But in any case the experimental results\r\n    are promising and seem to support our conclusions, which is the\r\n    main reason why we submitted the paper.  In our mind, however, the\r\n    idea behind  the work is slightly different from what you point\r\n    out. We believe the new stopping criteria will work better in\r\n    large spaces where the subset of states having a non-negligible\r\n    probability is small. In these systems, only very precise\r\n    combinations of weights and unit values leads to large\r\n    probabilities.  If you change the value of the hidden units in any\r\n    way, this delicate equilibrium is broken and you expect to get\r\n    much lower probabilities for the visible variables. And you can do\r\n    that by either getting the proper value of the hidden units and take\r\n    their complementary (the criterion that works better, apparently),\r\n    or simply by changing them at random. In the learning process, one uses\r\n    MCMC because, in general, one wants to do the opposite. That is, to\r\n    identify the (small) regions of large probability, which is a\r\n    difficult task if the space is large and the set of relevant\r\n    states is small. But that implies that identifying states with low\r\n    probability should be much easier, and our hope is to find any of\r\n    these by generating values of the hidden variables that are very\r\n    different from the right ones. We believe that this is not clearly\r\n    seen on the experiments reported because the size of the spaces\r\n    analyzed is not that large. We plan to extend our numerical\r\n    analysis to other, larger systems where this should be more\r\n    evident.\r\n\r\n- The experiments show that the second choice is superior, but since\r\n  the experiments are rather small-scale, it is difficult to make a\r\n  solid conclusion from them. Also, the proposed method seems to only\r\n  work with CD_1 (see Fig. 4), but the authors' explanation on\r\n  possible reasons for this is not clear. \r\n\r\n--- Actually we are mostly interested in CD_1 as it is the cheapest\r\n    way we know (with respect to time and memory storage) to evaluate\r\n    the correlations required at the learning stage. We must admit,\r\n    however, that the reason why it seems to perform worse in CD_k is\r\n    somewhat unclear to us. As we said above, we are working on the\r\n    mathematical grounds of the method and we hope to have a more\r\n    clear understanding of the fine details of the method in the near\r\n    future. \r\n\r\n- One thing which is clearly missing from the experiments is the\r\n  comparison of the proposed criterion against the actual function CD\r\n  minimizes ($E_D left[ log p (x^{(i)}) - log p(y^{(i)}) \right]$,\r\n  where $y^{(i)}$ is the one-step Gibbs sample starting from\r\n  $x^{(i)}$.) as well as the approximate likelihood (the same thing,\r\n  but use samples from persistent chains as $y^{(i)}$'s). Computing\r\n  these things should be no more expensive than computing the proposed\r\n  criterion, and may give better indication of how log-likelihood\r\n  evolves over time. This will be interesting to see. \r\n\r\n--- We have some of these results, but we decided not to include them\r\n    in the paper to limit its extensions. In any case we have seen\r\n    that the behavior is similar to the reconstruction error, and thus\r\n    show the same problems once compared to the likelihood itself.  In\r\n    the paper we only included 'somewhat positive' results, but we\r\n    agree with you that these results may be interesting to be\r\n    discussed. \r\n\r\n- Of course, one last thing is, why anyone would try to train an RBM\r\n  to be a good generative model (maximizing the log-likelihood) using\r\n  CD in the first place, while PCD (with possibly other MCMC\r\n  algorithms than Gibbs) is available.\r\n\r\n\u00a0--- The paper is not trying to answer this question. We just want to\r\n\u00a0 \u00a0 \u00a0explore new ways to improve the stopping criteria in a learning\r\n\u00a0 \u00a0 \u00a0algorithm where the reconstruction error does not work properly\r\n\u00a0 \u00a0 \u00a0(in general), and we have tested it in CD_1. Since the\r\n\u00a0 \u00a0 \u00a0reconstruction error is not a good stopping criterion for PCD\r\n\u00a0 \u00a0 \u00a0either, PCD may also benefit from other options. It would be\r\n\u00a0 \u00a0 \u00a0interesting to see if the same stopping criteria work properly\r\n\u00a0 \u00a0 \u00a0with PCD, but that we leave for a future work.\r\n\r\n-Despite these seemingly negative comments, I enjoyed reading the\r\n paper, since it is clearly written and well motivated. A few more\r\n experiments showing that the proposed methods of choosing $y^{(i)}$\r\n are better than other possible choices (e.g., using Gibbs sampling) \r\n would make the paper much stronger. \r\n\r\n--- Thanks a lot for the comments and suggestions. We agree with you\r\n    that there is more room to improve, and we are already working on\r\n    that.  But at this point we didn't want to miss the opportunity to\r\n    submit or recent results to this conference in order to discuss\r\n    our ideas with other people with lots of expertise in the\r\n    field. Still we hope to follow this line and to get more\r\n    exhaustive results in the near future.\r\n\r\n- P. S. I wouldn't mind, if you cited my paper (IJCNN 2013) next to\r\n (Desjardins et al., 2010) where I also proposed PT. \r\n\r\n--- Consider it done"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389665160000, "tcdate": 1389665160000, "number": 1, "id": "ETi6EqZjQ1tFe", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "II-mIcAshLID0", "replyto": "II-mIcAshLID0", "signatures": ["KyungHyun Cho"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "Hi David,\r\n\r\nAfter briefly going through your paper, I have a number of comments on it. \r\n\r\n- The proposed stopping criterion itself is not so novel, in my opinion. Essentially, it is a general framework in which, for instance, CD minimizes, where $y^{(i)}$ is obtained by a single-step Gibbs sampling from $x^{(i)}$. In fact, if $y^{(i)}$ is one step of persistent MCMC chain, the stopping criterion corresponds to the approximate maximum likelihood criterion used by PCD.  In short, the proposed criterion is, in fact, what a proper MLE should minimize.\r\n\r\n- The novelty is, however, in the proposed ways to choose $y^{(i)}$. The authors proposed two alternatives. Unfortunately (as somewhat expected) the first method of selecting random points does not work, which is only natural as if it did, why would anyone use MCMC to estimate the model statistics of RBM? :) The second alternative of using the visible samples conditioned on the *flipped* hidden states seems to work better, but the authors do not provide any good justification for this choice. \r\n\r\n- The experiments show that the second choice is superior, but since the experiments are rather small-scale, it is difficult to make a solid conclusion from them. Also, the proposed method seems to only work with CD_1 (see Fig. 4), but the authors' explanation on possible reasons for this is not clear. \r\n\r\n- One thing which is clearly missing from the experiments is the comparison of the proposed criterion against the actual function CD minimizes ($E_D left[ log p (x^{(i)}) - log p(y^{(i)}) \right]$, where $y^{(i)}$ is the one-step Gibbs sample starting from $x^{(i)}$.) as well as the approximate likelihood (the same thing, but use samples from persistent chains as $y^{(i)}$'s). Computing these things should be no more expensive than computing the proposed criterion, and may give better indication of how log-likelihood evolves over time. This will be  interesting to see. \r\n\r\n- Of course, one last thing is, why anyone would try to train an RBM to be a good generative model (maximizing the log-likelihood) using CD in the first place, while PCD (with possibly other MCMC algorithms than Gibbs) is available.\r\n\r\nDespite these seemingly negative comments, I enjoyed reading the paper, since it is clearly written and well motivated. A few more experiments showing that the proposed methods of choosing $y^{(i)}$ are better than other possible choices (e.g., using Gibbs sampling) would make the paper much stronger.\r\n\r\n- Cho \r\n\r\nP. S. I wouldn't mind, if you cited my paper (IJCNN 2013) next to (Desjardins et al., 2010) where I also proposed PT. :)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387809000000, "tcdate": 1387809000000, "number": 34, "id": "II-mIcAshLID0", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "II-mIcAshLID0", "signatures": ["davidbuchaca@gmail.com"], "readers": ["everyone"], "content": {"title": "Stopping Criteria in Contrastive Divergence: Alternatives to the Reconstruction Error", "decision": "submitted, no decision", "abstract": "Restricted Boltzmann Machines (RBMs) are general unsupervised learning devices to ascertain generative models of data distributions. RBMs are often trained using the Contrastive Divergence learning algorithm (CD), an approximation to the gradient of the data log-likelihood. A simple reconstruction error is often used to decide whether the approximation provided by the CD algorithm is good enough, though several authors (Schulz et al., 2010; Fischer & Igel, 2010) have raised doubts concerning the feasibility of this procedure. However, not many alternatives to the reconstruction error have been used in the literature. In this manuscript we investigate simple alternatives to the reconstruction error in order to detect as soon as possible the decrease in the log-likelihood during learning.", "pdf": "https://arxiv.org/abs/1312.6062", "paperhash": "buchaca|stopping_criteria_in_contrastive_divergence_alternatives_to_the_reconstruction_error", "keywords": [], "conflicts": [], "authors": ["David Buchaca", "Enrique Romero", "Ferran Mazzanti", "Jordi Delgado"], "authorids": ["davidbuchaca@gmail.com", "eromero@lsi.upc.edu", "ferran.mazzanti@upc.edu", "jdelgado.pin@gmail.com"]}, "writers": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 11}