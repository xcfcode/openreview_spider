{"notes": [{"id": "SJe5P6EYvS", "original": "HJx1ZEjPvS", "number": 606, "cdate": 1569439074100, "ddate": null, "tcdate": 1569439074100, "tmdate": 1583912047580, "tddate": null, "forum": "SJe5P6EYvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "HlfRE4e0O6", "original": null, "number": 1, "cdate": 1576798701145, "ddate": null, "tcdate": 1576798701145, "tmdate": 1576800934821, "tddate": null, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "invitation": "ICLR.cc/2020/Conference/Paper606/-/Decision", "content": {"decision": "Accept (Talk)", "comment": "This paper presents a new twist on the typical LSTM that applies several rounds of gating on the history and input, with the end result that the LSTM's transition function is effectively context-dependent. The performance of the model is illustrated on several datasets.\n\nIn general, the reviews were positive, with one score being upgraded during the rebuttal period. One of the reviewers complained that the baselines were not adequate, but in the end conceded that the results were still worthy of publication.\n\nOne reviewer argued very hard for the acceptance of this paper \"Papers that are as clear and informative as this one are few and far between. ... As such, I vehemently argue in favor of this paper being accepted to ICLR.\"", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729110, "tmdate": 1576800281649, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper606/-/Decision"}}}, {"id": "rJe0f83x9B", "original": null, "number": 3, "cdate": 1572025878436, "ddate": null, "tcdate": 1572025878436, "tmdate": 1574357999447, "tddate": null, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "invitation": "ICLR.cc/2020/Conference/Paper606/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "I have read the authors' response. Their points regarding baseline comparisons are sensible in that there isn't a reason to expect the observations to *not* generalization to other datasets. It is odd that mLSTM is outperformed by LSTM in Table 3, but as the authors note in section 4.2 this may be due to instability of mLSTM during training. The results in the paper demonstrate significant improvement over LSTM, and while there are not as many baseline comparison to similar models as I would have liked to see, the quality of this work is sufficiently high that this is not a fatal flaw. In light of the author response and other reviews, I am revising my rating to 6: Weak Accept.\n\n=====\n\nThis paper proposes a modification of LSTM networks in the context of language modeling called Mogrifier LSTM. Ordinary LSTMs are defined as recurrent operations on the current input, previous hidden state, and previous cell state. The proposed Mogrifier LSTM utilizes the same recurrent unit as the LSTM, but the input and previous hidden state are updated with several rounds of mutual gating. In each round, the input  is multiplied elementwise by a gate computed as a function of the hidden state (or vice versa). The authors experiment on word-level and character-level modeling and compare their Mogrifier LSTMs to several state-of-the-art approaches. They also conduct an ablation study to show the effect of various design choices and hyperparameters and experiments on a reverse copy task.\n\nSpecific contributions include:\n* Proposal of a novel approach for modulating inputs to a recurrent unit by mutual gating.\n* Experiments demonstrating strong performance on a number of language modeling tasks.\n  \nThe paper in its current state is borderline, leaning towards weak reject. Points in favor of acceptance include the high clarity of writing, good experiments of the proposed model, and a discussion of possible reasons for why the mogrification operation works well. The main shortcoming of the paper is experimental comparison to baselines.\n\nThe authors were able to train baseline LSTMs to high levels of performance (presumably due to tuning of hyperparameters) and then demonstrate that Mogrifier LSTMs improve upon LSTMs significantly. This is perhaps not entirely surprising, because the hyperparameter range of the Mogrifier LSTM includes zero rounds of updates, which would render it identical to the baseline LSTM. Therefore, if the hyperparameters are tuned sufficiently well, the performance of the Mogrifier LSTM should be at least as good as the LSTM. What the experiments do not show is that the proposed mogrification outperforms other forms of multiplicative interaction and/or gating. The closest that the authors come to this is the single validation perplexity of the Multiplicative LSTM in Table 3. If thorough hyperparameter tuning is applied to the Multiplicative LSTM or the approaches of Wu et al. (2016) and/or Sutskever et al. (2011), does the Mogrifier LSTM still outperform them?\n\nOther than this critical issue of baseline comparison, the experiments are quite informative. The ablation study showing the effect of different design decisions and the hyperparameter visualiztion in Appendix B are particularly useful. The mogrification operation is described precisely enough for other researchers to implement and the arguments made in 4.4 are compelling.\n\nQuestion for the authors:\n* Some qualitative analysis of the learned mogrification operation would be helpful for understanding the nature of the modulation. For example, how do the predictions change depending on the modulation? If x is modulated by different hidden states, is there a noticeable effect on the output?\n* Did you experiment with other forms of modulation before arriving upon the mogrification formulation? There are some naive approaches such as concatenating the hidden state to the input and applying a nonlinear layer, or predicting affine parameters for the input as a function of the hidden state in the style of FiLM [1]. Are there obvious shortcomings in these naive approaches that mogrification handles gracefully?\n\n[1] Perez, E., Strub, F., De Vries, H., Dumoulin, V. and Courville, A., 2018, April. Film: Visual reasoning with a general conditioning layer. In Thirty-Second AAAI Conference on Artificial Intelligence.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper606/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper606/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704390239, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper606/Reviewers"], "noninvitees": [], "tcdate": 1570237749692, "tmdate": 1575704390253, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper606/-/Official_Review"}}}, {"id": "rylMKNabsr", "original": null, "number": 3, "cdate": 1573143673650, "ddate": null, "tcdate": 1573143673650, "tmdate": 1573143673650, "tddate": null, "forum": "SJe5P6EYvS", "replyto": "BylUf0l6YB", "invitation": "ICLR.cc/2020/Conference/Paper606/-/Official_Comment", "content": {"title": "Re: Official Blind Review #2", "comment": "We thank Reviewer #2 for their comments.\n\n> 3. One thing is that since this could take into account more\n>    context, it seems that this model could potentially generate\n>    language / tokens with longer time dependencies. I wonder if the\n>    authors have performed any experiments on this and if they have\n>    seen any improvements on that front.\n>\n> 4. Also, I am curious about the generalization ability of the model.\n>    Could the authors train the model on shorter sequences and test\n>    for generation with longer sequences and see how this compares\n>    with baseline models.\n\nThe only relevant bit in the paper is this: \"Hypothesis: the benefit\nis in handling long-range dependencies better. Experiments in the\nepisodic setting (i.e. sentence-level language modelling) exhibited\nthe same gap as the non-episodic ones.\"\n\nClearly, this does not exactly rule out the possibility of long-range\nvs short-range being a factor, but at the same time it sounds somewhat\nunlikely to observe improvements of the same magnitude in per-sentence\nlanguage modelling.\n\nEvaluating generated text can be rather subjective though, so we\nrefrained from that.\n\n> 5. The model seems to be related to Adaptive Computation Time (ACT)\n>    from Graves et al. it would be nice to compare to the ACT.\n\nWe agree that ACT could be related. The main difference is that the\nmogrifier in its current version performs a fixed number of processing\nsteps.\n\n> 6. Another slight improvement in writing could be to hightlight the\n>    intuition (conclusion in page 8) at the beginning of the paper,\n>    this could help in better understanding the motivation of the\n>    paper.\n\nThank you for the suggestion. We'll consider this.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper606/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJe5P6EYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference/Paper606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper606/Reviewers", "ICLR.cc/2020/Conference/Paper606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper606/Authors|ICLR.cc/2020/Conference/Paper606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168943, "tmdate": 1576860557452, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference/Paper606/Reviewers", "ICLR.cc/2020/Conference/Paper606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper606/-/Official_Comment"}}}, {"id": "B1ehRohZjB", "original": null, "number": 2, "cdate": 1573141459739, "ddate": null, "tcdate": 1573141459739, "tmdate": 1573141459739, "tddate": null, "forum": "SJe5P6EYvS", "replyto": "HklaAK10FS", "invitation": "ICLR.cc/2020/Conference/Paper606/-/Official_Comment", "content": {"title": "Re: Official Blind Review #3", "comment": "We thank Reviewer #3 for their comments.\n\n- \"Mogrification\" as a general preprocessing step: could it also\n  improve performance for transformer models?\n\nPossibly, but it would be quite surprising, as attention might very\nwell be able to express similar transformations.\n\nAs to whether there better ways to preprocess and gate the RNN inputs,\nwe are sure that the answer is yes. More generally, we believe that\nour neural models lack the necessary biases perform well in a\ndata-efficient manner. Large datasets can alleviate the problem, but\nmay not be able to solve it.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper606/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJe5P6EYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference/Paper606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper606/Reviewers", "ICLR.cc/2020/Conference/Paper606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper606/Authors|ICLR.cc/2020/Conference/Paper606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168943, "tmdate": 1576860557452, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference/Paper606/Reviewers", "ICLR.cc/2020/Conference/Paper606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper606/-/Official_Comment"}}}, {"id": "S1efyYn-jH", "original": null, "number": 1, "cdate": 1573140697600, "ddate": null, "tcdate": 1573140697600, "tmdate": 1573140697600, "tddate": null, "forum": "SJe5P6EYvS", "replyto": "rJe0f83x9B", "invitation": "ICLR.cc/2020/Conference/Paper606/-/Official_Comment", "content": {"title": "Re: Official Blind Review #1", "comment": "We thank Reviewer #1 for the critical but thoughtful review. We try to\naddress the issues brought up below.\n\n> The authors were able to train baseline LSTMs to high levels of\n> performance (presumably due to tuning of hyperparameters) and then\n> demonstrate that Mogrifier LSTMs improve upon LSTMs significantly.\n> This is perhaps not entirely surprising, because the hyperparameter\n> range of the Mogrifier LSTM includes zero rounds of updates, which\n> would render it identical to the baseline LSTM. Therefore, if the\n> hyperparameters are tuned sufficiently well, the performance of the\n> Mogrifier LSTM should be at least as good as the LSTM\n\nIt is indeed unsurprising that the Mogrifier is not worse than the\nLSTM since it includes the LSTM as a special case. But in Figure 3\nwhere perplexity is plotted as the function of rounds, the setting\nthat corresponds to the LSTM (rounds=0) is clearly the worst and the\ngap is very significant.\n\n> What the experiments do not show is that the proposed mogrification\n> outperforms other forms of multiplicative interaction and/or gating.\n> The closest that the authors come to this is the single validation\n> perplexity of the Multiplicative LSTM in Table 3. If thorough\n> hyperparameter tuning is applied to the Multiplicative LSTM or the\n> approaches of Wu et al. (2016) and/or Sutskever et al. (2011), does\n> the Mogrifier LSTM still outperform them?\n\nWe agree that having these baselines would strengthen the contribution\nand help position our work more precisely in their context. Due to\ntime and resource constraints, we focussed on the most similar model,\nthe mLSTM, and evaluated it on PTB (which has been predictive of\nperformance on other tasks in our experience) using the same\nhyperparameter tuning methodology as everywhere else in the paper, the\nonly exceptions being a shortened schedule and small BPTT window size.\nThese concessions to practicality make results slightly worse (2-3\nperplexity points), but there is little reason to believe they benefit\none model or the other. And if the mLSTM were more similar to the\nmogrifier than we'd like, we should see that in these experiments. As\nit is, what we found is that the mLSTM does not improve on the\nbaseline LSTM while the Mogrifier does.\n\n> * Some qualitative analysis of the learned mogrification operation\n>   would be helpful for understanding the nature of the modulation. For\n>   example, how do the predictions change depending on the modulation? If\n>   x is modulated by different hidden states, is there a noticeable\n>   effect on the output?\n\nObviously, in terms of perplexity there is a noticable effect. In\nterms of statistics of the modulated vs unmoodulated input vectors, we\ndo not have the data. The closest we have in the paper is that \"the\nmeans of the standard LSTM gates in the Mogrifier were very close\nbetween the two models but their variance was smaller in the\nMogrifier\".\n\n> * Did you experiment with other forms of modulation before arriving\n>   upon the mogrification formulation? There are some naive\n>   approaches such as concatenating the hidden state to the input and\n>   applying a nonlinear layer, or predicting affine parameters for\n>   the input as a function of the hidden state in the style of FiLM\n>   [1]. Are there obvious shortcomings in these naive approaches that\n>   mogrification handles gracefully?\n\nWe tried concatenation of hidden state and input and saw no benefit\ncompared to the Mogrifier to offset the significantly higher number of\nparameters. FiLM sounds similar to one round mogrifier without a\nnon-linearity. As to obvious shortcomings to these methods, we do not\nknow of any. Probably we would need to understand a mogrifier much\nbetter to answer that question.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper606/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SJe5P6EYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference/Paper606/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper606/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper606/Reviewers", "ICLR.cc/2020/Conference/Paper606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper606/Authors|ICLR.cc/2020/Conference/Paper606/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168943, "tmdate": 1576860557452, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper606/Authors", "ICLR.cc/2020/Conference/Paper606/Reviewers", "ICLR.cc/2020/Conference/Paper606/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper606/-/Official_Comment"}}}, {"id": "BylUf0l6YB", "original": null, "number": 1, "cdate": 1571782158090, "ddate": null, "tcdate": 1571782158090, "tmdate": 1572972574432, "tddate": null, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "invitation": "ICLR.cc/2020/Conference/Paper606/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThe paper proposes a novel LSTM architecture that adds several gating mechanism that gates the hidden state and inputs in between the LSTM update. The proposed model shows superior performance on smallish datasets including PTB,  Enwick8 and NWC.\n\nComments on the paper:\n\n1. The paper proposes an interesting architecture and it seems to show significant improvement in terms of performance for some language datasets. \n\n2. The paper is very well written, the motivation and formulation is clear. There are many analysis to understand the model (the strength and weaknesses).\n\n3.. One thing is that since this could take into account more context,  it seems that this model could potentially generate language / tokens with longer time dependencies. I wonder if the authors have performed any experiments on this and if they have seen any improvements on that front.\n\n4. Also, I am curious about the generalization ability of the model. Could the authors train the model on shorter sequences and test for generation with longer sequences and see how this compares with baseline models.\n\n5. The model seems to be related to Adaptive Computation Time (ACT) from Gaves et al. it would be nice to compare to the ACT.\n\n6. Another slight improvement in writing could be to hightlight the intuition (conclusion in page 8) at the beginning of the paper, this could help in better understanding the motivation of the paper.\n\n\nMinor comments on the paper,\n\n1. The link and the self-citations on page 4 are does not seem to be valid links and citations.\n\nOverall, a well-written paper, extensive analysis and good experimental result."}, "signatures": ["ICLR.cc/2020/Conference/Paper606/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper606/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704390239, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper606/Reviewers"], "noninvitees": [], "tcdate": 1570237749692, "tmdate": 1575704390253, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper606/-/Official_Review"}}}, {"id": "HklaAK10FS", "original": null, "number": 2, "cdate": 1571842517461, "ddate": null, "tcdate": 1571842517461, "tmdate": 1572972574262, "tddate": null, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "invitation": "ICLR.cc/2020/Conference/Paper606/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published in this field for several years.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "Summary:\nThis paper tackles the problem of context modelling within recurrent neural networks (RNNs). The authors propose an interdependent gating mechanism that enriches the coupling between inputs and hidden states. For an input x_0 and hidden state h_0; h_0 gates x_0 to create x_1; x_1 then gates h_0 to create h_1; this cyclical gating operation is applied for several rounds and it's output is fed into a recurrent neural network. For the next time-step, this process is repeated, with h_0 as the final h obtained after the final round of gating in the previous time-step. This results in the RNN processing a more contextualized version of the input tokens x.\n\nMain Contributions:\n1. A simple pre-processing step that contextualizes inputs for recurrent neural networks and significantly improves performance.\n2. An extensive evaluation of the proposed technique against previous works and on all relevant datasets.\n\nPros:\nThe paper is very well-written and clear. It motivates and explores the questions and issues surrounding this topic very well.\n\nCons:\nIt would be good to see how this performance translates to other RNN architectures such as GRUs. \n\n\nFinal notes:\nThis paper raises many interesting question:\n- What is the really going on with the gating mechanism? \nThe authors explore this question but the jury is still out on exactly what is going on here.\n- \"Mogrification\" as a general preprocessing step: could it also improve performance for transformer models?\n- Are there better ways to preprocess and gate the RNN inputs?\n\n--------\n\nReview Decision:\nIt is clear, well motivated, well written and represents a concrete contribution to the language modelling literature. Furthermore, most claims made are substantiated via thorough experimentation. Lastly, this work demonstrates that rather than relying on data and model scaling to improve performance; there is alot left to be done in tackling language modelling on smaller scale datasets. "}, "signatures": ["ICLR.cc/2020/Conference/Paper606/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper606/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Mogrifier LSTM", "authors": ["G\u00e1bor Melis", "Tom\u00e1\u0161 Ko\u010disk\u00fd", "Phil Blunsom"], "authorids": ["melisgl@google.com", "tkocisky@google.com", "pblunsom@google.com"], "keywords": ["lstm", "language modelling"], "TL;DR": "An LSTM extension with state-of-the-art language modelling results.", "abstract": "Many advances in Natural Language Processing have been based upon more expressive models for how inputs interact with the context in which they occur. Recurrent networks, which have enjoyed a modicum of success, still lack the generalization and systematicity ultimately required for modelling language. In this work, we propose an extension to the venerable Long Short-Term Memory in the form of mutual gating of the current input and the previous output. This mechanism affords the modelling of a richer space of interactions between inputs and their context. Equivalently, our model can be viewed as making the transition function given by the LSTM context-dependent. Experiments demonstrate markedly improved generalization on language modelling in the range of 3\u20134 perplexity points on Penn Treebank and Wikitext-2, and 0.01\u20130.05 bpc on four character-based datasets. We establish a new state of the art on all datasets with the exception of Enwik8, where we close a large gap between the LSTM and Transformer models.\n", "pdf": "/pdf/dec59d4add1086f58d1c0626c2f4a095c7e455c7.pdf", "paperhash": "melis|mogrifier_lstm", "_bibtex": "@inproceedings{\nMelis2020Mogrifier,\ntitle={Mogrifier LSTM},\nauthor={G\u00e1bor Melis and Tom\u00e1\u0161 Ko\u010disk\u00fd and Phil Blunsom},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SJe5P6EYvS}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f23f0239f57b453d7df6d7c2e128b691c9eb075a.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SJe5P6EYvS", "replyto": "SJe5P6EYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper606/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575704390239, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper606/Reviewers"], "noninvitees": [], "tcdate": 1570237749692, "tmdate": 1575704390253, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper606/-/Official_Review"}}}], "count": 8}