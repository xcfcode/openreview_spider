{"notes": [{"id": "ufZN2-aehFa", "original": "Gvp9u7fsRQn", "number": 637, "cdate": 1601308075904, "ddate": null, "tcdate": 1601308075904, "tmdate": 1616005008378, "tddate": null, "forum": "ufZN2-aehFa", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "H5890rkeS_H", "original": null, "number": 1, "cdate": 1610040404944, "ddate": null, "tcdate": 1610040404944, "tmdate": 1610474001405, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The authors present a Bayesian approach for context aggregation in neural processes based models. The article is well written, and provides a nice and comprehensive framework. The reviewers raised some issues regarding the lack of comparisons to proper baselines. The authors provided additional comparisons in the revised version. The comparisons were found satisfactory by some some reviewers, who increased their scores. Based on the revised version, I recommend acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040404930, "tmdate": 1610474001387, "id": "ICLR.cc/2021/Conference/Paper637/-/Decision"}}}, {"id": "qSFtjdUnyev", "original": null, "number": 3, "cdate": 1605885341471, "ddate": null, "tcdate": 1605885341471, "tmdate": 1606226991394, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Paper Revision", "comment": "Dear reviewers!\n\nThank you very much for your generally positive reviews of our paper and for numerous constructive remarks! We uploaded a revised version of the paper where we highlighted updated passages in blue and added two new paragraphs to the appendix (App. 7.3, 7.4). We further provide detailed comments on each of your reviews below.\n\nKind regards,\nThe authors\n\n___________________________________________\n\n\n***Updates***\n\nNov 24th, 15:08pm (GMT+1): We uploaded a new version of the paper revision, including some further clarifications according to AnonReviewer4's remarks.\n\nNov 23rd, 11:57am (GMT+1): We uploaded a new version of the paper revision, where we put the results of the new baseline employing self-attentive encoders as proposed by AnonReviewer4  into the main part of the text, cf. end of Sec. 5. \n\nNov 22nd, 6:00pm (GMT+1): We uploaded a new version of the paper revision where we improved some passages of the discussion and added further experimental results to App. 7.3."}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "jGUs5g25SWk", "original": null, "number": 11, "cdate": 1606174386603, "ddate": null, "tcdate": 1606174386603, "tmdate": 1606174386603, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "vhguKiCuK6c", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Author answer", "comment": "Dear AnonReviewer4,\n\n1. Thank you for your fair assessment and for raising your score!\n2. It is correct that BA does not generalize MA if r-to-z mappings are included. Our argumentation considers $\\bar{r}$ as the aggregated quantity for MA. In contrast, we motivate BA as a method which aggregates the context data directly in the statistical description of $z$, i.e., here we consider the distribution of z, described by $\\mu_z$ and $\\sigma_z^2$ to be the aggregated quantity. Comparing the aggregated quantities in this sense, our claim is correct. We will update the discussion again in the final revision in order to avoid confusion. Thanks again for insisting on clarity here!\n3. Thank you for pointing this out! We will add a remark to clarify this in the final paper revision.\n4./5. We agree that the results for ANP + MC as well as MA + SA on the image completion task would be informative. Unfortunately, we won't be able to present these results until the rebuttal deadline due to lack of time. Nevertheless, as you suggested, we will consider implementing ANP + MC as well as including the results for the image completion experiments in the camera-ready version.\n\nThank you again for your thorough and constructive review!"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "3PwHp4elcTP", "original": null, "number": 1, "cdate": 1603558061337, "ddate": null, "tcdate": 1603558061337, "tmdate": 1606150526442, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Review", "content": {"title": "A simple and effective idea, but missing an important baseline that can also address the problems of mean-aggregation in NPs", "review": "The authors present the Bayesian Aggregation (BA) mechanism in the context of Neural Processes (NPs) for aggregating the context information into the latent variable z in the form of posterior updates to z. The authors show that this improves predictive performance (in terms of likelihood) compared to mean aggregation MA that it replaces on various regression tasks with varying input-output dimensionality.\n\nStrengths:\n1. The idea is simple and leads to a notable improvement compared to MA in terms of likelihood\n2. The background and method is presented very clearly.\n3. The evaluation is done on a wide variety of tasks, ranging from standard 1D regression of GP samples to pendulum trajectory prediction tasks.\n\nWeaknesses:\n1. The evaluation is missing an important baseline model, which are (A)NP models that have self-attention in the encoder for processing the contexts (c.f. model figure in ANP paper (Kim et al., 2019b)). Contrary to the NP/CNP baselines that are compared against in the paper, the ANP with self-attention in the encoder does not give uniform weights to each context point - the self-attention allows the model to assign varying importance to the different context points (despite using mean-aggregation after the self-attention), which is presented as a key motivation for the BA mechanism introduced in the paper. Hence for the experiments, I strongly suggest comparing against CNP/NP/ANP with self-attention in the deterministic/latent/latent path of the encoder. For completeness, if would be nice to also compare against models that have both deterministic and latent paths, since BA can also be applied to these models. At the same time, I understand that BA would be more interpretable for showing which observations have little/high effect on z compared to the approach of using self-attention in the encoder, but it would still be very informative for the reader to be able to compare the two approaches. Also these two approaches can be combined to have self-attention in the encoder + BA, which might also yield improved performance.\n2. The claim that \u201cBA includes MA as a special case\u201d doesn\u2019t seem to be true. Using a \u201cnon-informative prior and uniform observation variances\u201d leads to constant sigma_z and mu_z being linearly proportional to mean(r_n) (i.e. sum_n r_n / N), which is not quite the same as MA - MA allows sigma_z and mu_z to be non-linear functions of mean(r_n), hence is strictly more expressive than this special case.\n3. In Equation (7), it seems as though the context points (x_n,y_n) only affects r_n via the variance, which seems unnecessarily limiting. Why not have the mean also depend on r_n? e.g. p(r_n|z) = N(r_n| z + mu_{r_n}, diag(sigma_{r_n}^2) where mu_{r_n} is also computed as a function of (x_n,y_n)? This will still give a closed-form posterior p(z|r_{1:N}) since the mean of p(r_n|z) is still linear in z, creating a model that\u2019s strictly more expressive with very similar efficiency. It would be informative to see how this changes the experimental results.\n4. I\u2019m guessing the VI objective was used to train the ANP. Given the clear advantage of training with the MC objective, shouldn\u2019t the ANP also be trained with MC?\n5. The latent variable models were not evaluated on 2D image completion tasks because \u201carchitectures without deterministic paths were not able to solve this task\u201d. Why not then add a deterministic path to these latent variable models to allow them to train?\n\nOther points\n- In the text, it says that the model is also compared against ANP to show that BA can compete with SOTA. This is arguably incorrect since ConvCNP models are SOTA among models of the NP family, showing a significant improvement over ANP. Hence to achieve the goal mentioned in the text, it would make sense to compare with ConvCNP models as part of the evaluation against other deterministic NPs.\n\nOverall the paper is presented very clearly with a simple yet effective idea tested on a wide variety of tasks. However it\u2019s missing an important baseline that uses self-attention in the encoder, along with several other baselines that would be informative to compare against. I am willing to increase my score should these results be included in the revised version of the paper.\n\n=================\n\nScore raised to 6 after inclusion of MA + SA results in rebuttal.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138674, "tmdate": 1606915773974, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper637/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Review"}}}, {"id": "vhguKiCuK6c", "original": null, "number": 10, "cdate": 1606150467203, "ddate": null, "tcdate": 1606150467203, "tmdate": 1606150467203, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "CBwF7ej6Q3", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Reviewer response to author rebuttal", "comment": "1. Thank you for providing the comparison with self-attentive encoders. I see that although BA performs simlarly to MA + SA, it does offer some computational advantages. I'm a little sceptical of how beneficial these advantages are, given that the number of context points used for training is usually quite small. However I'm happy with the inclusion of this important baseline, hence will raise my score to 6.\n2. I see how Eq (8) can be reduced to (6), but if there are non-linear mappings in the r-to-z networks, then won't the resulting q(z|context) distribution in the MA model will be different to Eq (8)? I can't see how Eq (8) can cover this case. If this is correct, then I think it should be pointed out that BA cannot generalise MA when there exist non-linear mappings in the r-to-z network (which is usually the case).\n3. Thank you for the explanation. I think it is worth including this point in the paper for readers who might pose the same question.\n4. Can you not also train ANP with MC loss and add the results to the paper for completeness? It is relevant for this work because one of the things you show is that MC estimation is clearly superior to VI. I imagine the experiments will be simple to run given the current codebase.\n5. If adding results for latent-variable models on the image completion task is a stretch, I think it would be informative to at least add MA + SA results for the camera-ready version.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "Iy9TXfPRAMv", "original": null, "number": 9, "cdate": 1605885973917, "ddate": null, "tcdate": 1605885973917, "tmdate": 1606134858545, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "3PwHp4elcTP", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Author answer (part 1)", "comment": "Thank you for your detailed review, your positive remarks about our Bayesian aggregation (BA) mechanism, \"a simple [idea which] leads to a notable improvement [...] on a wide variety of tasks\", and for many insightful comments. We provide detailed answers for each of your comments below and revise our paper submission accordingly. In particular, we add comparisons with your proposed baseline employing self-attentive encoders, cf. Sec. 5 and App. 7.3.\n\n1.)\tSelf-attentive encoders: \n\nWe agree that an architecture with a self-attention (SA) mechanism in the encoder is an interesting baseline. Indeed, as you pointed out, SA also yields (similar to BA) a weighted sum of the latent observations $r_n$ (with the concrete definition of the weights depending on the type of SA mechanism used). In the initial version of our paper, we did not consider SA, because the reference implementation [2] of Attentive Neural Processes (ANPs) [1] (which first proposed to combine attention mechansims with Neural Processes (NPs)) does not include the option of SA in the encoder network.\n\nInspired by your remarks, we re-implemented SA and evaluated three different versions (\u201cLaplace\u201d, \u201cdot-product\u201d, and \u201cmulithead\u201d self-attention, as proposed in [1,3]) in combination with mean aggregation (MA). For a fair comparison, we used the same extensive hyperparameter optimization procedure as for the other experiments. The results in Tabs. 6,7 in App. 7.3 show that SA can improve the performance of NP-based models drastically in comparison to traditional architectures employing MA without SA in the encoder. Nevertheless, our BA still performs better or at least on-par **without using SA**.\n\nWe further add a detailed discussion of architectural and computational aspects of SA + MA in comparison with BA as well as considerations about how to combine BA with SA to the revised paper version. SA weights each latent observation according to some form of spatial relationship of the corresponding input with all other latent observations in the context set. In contrast, BA\u2019s weight for a given latent observation is based only on features computed from this very latent observation and allows to incorporate an estimation of the amount of information contained in each context tuple into the aggregation. This leads to several computational advantages of BA over SA:\n\n(i) in general, SA scales quadratically in the number $N$ of context tuples, as it has to be evaluated on all $N^2$ pairs of context tuples. In contrast, BA scales linearly with $N$, \n\n(ii) BA allows for efficient incremental updates when context data arrives sequentially, while using SA does not provide this possibility: it requires to store and encode the whole context set at once and to subsequently aggregate the whole set of resulting (SA-weighted) latent observations,\n\n(iii) as you pointed out, combining BA with SA indeed sounds like an interesting and promising avenue of research. Note that BA relies on a second encoder output $\\sigma_{r_n}^2$ (in addition to the latent observation $r_n$) which assesses the information content in each context tuple $(x_n, y_n)$. As each SA-weighted $r_n$ is informed by the other latent observations in the context set, obviously, one would have to also process the set of $\\sigma_{r_n}^2$ with some form of SA mechanism. We are confident that this is indeed possible in a theoretically well-founded manner, but note that it is not immediately obvious how to do this properly. Therefore, we leave a combination of SA and BA for future research.\n\n***Please also consider the second part of our answer below***"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "3j_hwKnyaGw", "original": null, "number": 5, "cdate": 1605885515985, "ddate": null, "tcdate": 1605885515985, "tmdate": 1605994828203, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "rbzGHvGMoiQ", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Author answer (part 2)", "comment": "***Please also consider the first part of our answer above***\n\nAs you pointed out correctly, NPs are motivated in [1-3] as models which combine the computational efficiency of neural networks with well-calibrated uncertainty estimates (like those of GPs). Indeed, NPs scale linearly in the number N of context and M of target data points, i.e., like $\\mathcal O(N+M)$ while GPs scale like $\\mathcal O(N^3 + M^2)$. Furthermore, NPs are shown to exhibit well-calibrated uncertainty estimates. In this sense, NPs can be counted as members of the family of scalable probabilistic regression methods. \n\nHowever, we might not have emphasized clearly enough the following central aspect of NP training which is relevant to clarify your question. We apologize for that and improve our exposition in the revised paper version. NPs are trained in a multi-task fashion (note that this is true not only for our contribution, but holds in general for NP-based model architectures). This means that NPs rely on data from a set of related source tasks from which they automatically learn powerful priors and the ability to adapt quickly to unseen target tasks. This multi-task training procedure of NPs scales linearly in the number $L$ of source tasks, which makes it possible to train these architectures on large amounts of source data. Applying GPs in such a multi-task setting can be challenging, especially for large numbers of source tasks [4-6]. Similarly, Bayesian Neural Networks (BNNs) [7,8] as well as DeepGPs [9] are in their vanilla forms specifically designed for the single-task setting. Therefore, GPs, BNNs, and DeepGPs are not directly applicable in the NP multi-task setting, which is why they are typically not considered as baselines for NP-based models, as discussed in [3]. \n\nThe experiments presented in [1-3] focus mainly on evaluating NPs in the context of few-shot probabilistic regression, i.e., on demonstrating the data-efficiency of NPs on the target task after training on data from a range of source tasks. In contrast, the application of NPs in situations with large ($>1000$) numbers of context/target points per task has to the best of our knowledge not yet been investigated in detail in the literature. Furthermore, it has not been studied how to apply NPs in situations where only a single or very few source tasks are available. The focus of our paper is a clear-cut comparison of the performance of our BA with traditional mean aggregation (MA) in the context of NP-based models. Therefore, we also consider experiments similar to those presented in [1-3] and leave further comparisons with existing methods for (multi-task) probabilistic regressions for future work.\n\nNevertheless, to illustrate this discussion, we add two simple GP-based baseline methods for this rebuttal: \n\n(i) a vanilla GP, i.e., the hyperparameters are optimized on each target task individually and the source data is not used, and \n\n(ii) a naive but easily interpretable example of a multi-task GP which optimizes one set of hyperparameters on all source tasks and uses it for predictions on the target tasks without further adaptation.\n\nThe results in Tab. 8 in App. 7.4 show that those simple GP-based models can only compete with NPs on function classes where either the inductive bias as given by the kernel functions fits the data well, or on function classes which exhibit a relatively low degree of variablity. On more complex function classes NPs produce predictions of much better quality, as they incorporate the source data more efficiently.\n\nWe further add missing literature about DeepGPs [9] and about more elaborate versions of multi-task GP regression methods [4-6] to the revised version of our paper.\n\nWe hope that this discussion together with the illustrative GP-baselines can help to clarify your question.\n\n\n\n[1] Garnelo et al., \"Conditional Neural Processes\", ICML 2018\n\n[2] Garnelo et al., \"Neural Processes\", ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models\n\n[3] Kim et al., \"Attentive Neural Processes\", ICLR 2019\n\n[4] Bardenet et al., \"Collaborative Hyperparameter Tuning\", ICML 2013\n\n[5] Yogatama and Mann, \"Efficient Transfer Learning Method for Automatic Hyperparameter Tuning\", AISTATS 2014\n\n[6] Golovin et al., \"Google Vizier: A Service for Black-Box Optimization\", International Conference on Knowledge Discovery and Data Mining, 2017\n\n[7] MacKay, \"A Practical Bayesian Framework for Backpropagation Networks\", Neural Comput., 1992\n\n[8] Gal and Ghahramani, \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\", ICML, 2016\n\n[9] Damianou and Lawrence, \"Deep Gaussian Processes\", ICML 2013"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "CBwF7ej6Q3", "original": null, "number": 8, "cdate": 1605885836859, "ddate": null, "tcdate": 1605885836859, "tmdate": 1605886606122, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "3PwHp4elcTP", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Author answer (part 2)", "comment": "***Please also consider the first part of our answer above***\n\n2.)\tMA as a special case of BA: \nAs you pointed out correctly, in the discussion on p. 5 of our initial paper version, we claim that Eq. (8) reduces to the mean-aggregated latent observation $\\bar{r}$ as given by Eq. (6) for non-informative priors and uniform observation variances. We believe that this claim is correct. Note that we explicitly refer to the mean-aggregated latent observation $\\bar{r}$ **before** non-linear mappings as given by the $r$-to-$z$ networks. Thank you for pointing out that our discussion was potentially misleading in this respect. We clarify this in the revised version.\n\n3.)\tExtending the BA-observation model: \nWe agree that your proposed observation model $p(r_n|z) = N(r_n | z + \\mu_{r_n}, \\mathrm{diag}(\\sigma_{r_n}^2))$ with $\\mu_{r_n}$ being a third encoder output (in addition to $r_n$ and $\\sigma_{r_n}^2$ as proposed in our paper) would still allow for a closed-form posterior $p(z | r_{1:N})$. Indeed, in comparison to Eq. 8, $\\sigma_z^2$ stays unchanged and $\\mu_z$ now reads: $\\mu_z = \\mu_{z,0} + \\sigma_z^2 \\odot \\sum_{n=1}^N (r_n - \\mu_{r_n} - \\mu_{z,0}) \\oslash \\sigma_{r_n}^2$. Note that $\\mu_{r_n}$ enters this equation only subtracted from $r_n$. Therefore, this observation model does **not** represent a more expressive model than our proposed version without $\\mu_{r_n}$, as we would just add up two distinct encoder outputs computed from the same inputs, which does **not** increase expressivity. This is why we do not consider this possibility in our paper. \n\n4.)\tTraining loss for ANPs: \nYou are right in that ANPs are defined in [1] to be trained with VI. Also, the reference implementation [2] we used considers only VI, which is why we only provide results for ANP+VI. As we mention in Sec. 6, our experiments show that the MC loss is a promising candidate to replace VI in future work on NPs.  \n\n5.)\tNPs with parallel latent and deterministic paths: \nWe agree that using BA in NP-based models with parallel latent and deterministic paths is an interesting direction for future work. However, it is outside of the scope of this paper as the primary focus of our work is to provide a clear and concise comparison of our proposed BA with traditional MA. Therefore, we designed our experiments with the goal to extract and assess the influence of the aggregation mechanism itself, without the influence of any other confounding factors. This is why we made sure to consistently compare architectures of the same type and perform extensive hyperparameter optimization, individually for each architecture and individually for each experiment to ensure a fair comparison. We believe that considering more complex architectures (such as architectures containing parallel deterministic and latent paths) would not add too much informative value to this intended focus of our experiments, in particular because there are numerous different design choices to consider here (e.g.: does the encoder attached to the deterministic path receive mean-aggregated quantities $\\bar{r}$ or Bayesian-aggregated quantities $\\mu_z$, $\\sigma_z$, or both? How to properly combine cross-attention with BA? ...). Nevertheless, as stated in Sec. 6, we agree that all these novel architectural options are very interesting and promising approaches which should definitely be investigated in future work.\n\n6.)\tConvCNPs as SOTA: \nThank you for pointing this out. We add a remark about ConvCNPs in the revised paper version.\n\nWe hope that the addition of your proposed SA-baseline together with the extended discussion will convince you that our paper is a valuable addition to the NP literature and ready for publication at ICLR. If so, we would be grateful if you reconsidered your score.\n\n[1] Kim et al., \"Attentive Neural Processes\", ICLR 2019\n\n[2] https://github.com/deepmind/neural-processes/blob/master/attentive_neural_process.ipynb\n\n[3] Vaswani et al., \"Attention is all you need\", NeurIPS 2017\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "bLWk2ejveeB", "original": null, "number": 6, "cdate": 1605885546594, "ddate": null, "tcdate": 1605885546594, "tmdate": 1605886019686, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "rbzGHvGMoiQ", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Author answer (part 1)", "comment": "Thank you very much for your detailed comments and the positive review of our paper, which rates our novel Bayesian aggregation (BA) mechanism and our evaluation of MC-based likelihood approximations as a \"solid improvement for neural process inference\" which \"make the neural process model significantly cleaner from a Bayesian perspective\".\n\nWe gladly address your remaining question about where Neural Process (NP)-based models are located on the map of (scalable) probabilistic regression methods by adding a detailed discussion together with an illustrative comparison with GP-based baselines to the revised version of our paper, cf. App. 7.4.\n\n***Please also consider the second part of our answer below***"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "1bdxrVZj-Sq", "original": null, "number": 7, "cdate": 1605885687147, "ddate": null, "tcdate": 1605885687147, "tmdate": 1605885687147, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "8Zf4uJ1IKqb", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Author answer", "comment": "Thank you very much for your review and your positive remarks, judging our paper to be \"relevant for the conference\", as it proposes Bayesian context aggregation, a \"sound solution\" for a \"difficult problem\"."}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "HjGNkcDz06I", "original": null, "number": 4, "cdate": 1605885413100, "ddate": null, "tcdate": 1605885413100, "tmdate": 1605885431282, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "Z5i8d9My61", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment", "content": {"title": "Author answer", "comment": "Thank you very much for your positive review of our paper and for your comments, which we address in the revised paper version!\n\nWe would further like to comment on your remark about originality and significance of our work. Neural Processes (NPs) are a well-established model family for probabilistic few-shot regression, proposed in [1-3]. A central component of such models is a context data aggregation which constitutes \"a difficult problem\" (AnonReviewer1). Traditionally, NP-based models use mean-aggregation (MA), a solution which \"look[s] limited\" (AnonReviewer1). Our contribution is Bayesian aggregation (BA), a novel, \"much cleaner and more natural (from a Bayesian perspective)\" (AnonReviewer3), and \"simple and effective\" (AnonReviewer4) aggregation mechanism. We can show that it is compatible with existing NP-based architectures and \"leads to notable improvement compared to MA in terms of likelihood\" (AnonReviewer4). Therefore, we are convinced that our paper constitutes an original and significant contribution to the literature. \n\n[1] Garnelo et al., \"Conditional Neural Processes\", ICML 2018\n\n[2] Garnelo et al., \"Neural Processes\", ICML 2018 workshop on Theoretical Foundations and Applications of Deep Generative Models\n\n[3] Kim et al., \"Attentive Neural Processes\", ICLR 2019"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "ufZN2-aehFa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper637/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper637/Authors|ICLR.cc/2021/Conference/Paper637/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923868826, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Comment"}}}, {"id": "8Zf4uJ1IKqb", "original": null, "number": 2, "cdate": 1603718101456, "ddate": null, "tcdate": 1603718101456, "tmdate": 1605024641862, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Review", "content": {"title": "An interesting Paper Proposing a Sound solution", "review": "Summary of the Paper:\n  \n        This paper describes Bayesian context aggregation for neural processes. These models are useful to address regression problems in which a set of related tasks are available for inference with associated context information in the form of extra data. These models assume that there is a task-specific global latent variable and task-independent latent variable. They are learned via approximate maximum posterior likelihood, in which the latent variables specific for each tasks are marginalzied out. For this, an approximation to the posterior distribution of these variables is need. This requires conditioning to the context dataset which is challenging. In the past, a latent representation is used and the context data set is aggregated as the mean of the latent representation. In this paper a Bayesian way of aggregating context information is proposed. This is based on using Bayes rule and a Gaussian generative model for the latent representations. The proposed method also leads to a new way of training CLV models which is based on moment matching. The method is validated on several synthetic an real-world experiments showing improvements over mean aggregation.\n\nDetailed Comments:\n\nI believe that this is a relevant paper. Context aggregation is a difficult problem that is required to address the learning tasks described in the paper. Previous solution look limited and the proposed method seems natural and a more effective method of aggregating this information. The paper is well written and the proposed method is sound. The experiments are also convincing and exhaustive. I believe that this is a relevant paper for the conference.\n\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138674, "tmdate": 1606915773974, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper637/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Review"}}}, {"id": "rbzGHvGMoiQ", "original": null, "number": 3, "cdate": 1603927320843, "ddate": null, "tcdate": 1603927320843, "tmdate": 1605024641801, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Review", "content": {"title": "A solid improvement for neural process inference, but I'd like to see actual regression tasks", "review": "In this paper, the authors make two contributions to neural process-like CLV models. First, they replace the somewhat adhoc variational-like approach to learning the amortized latent variable distribution with a monte carlo based approximation. Second, they replace the step of context aggregation with direct latent variable inference over z. \n\nOverall, in my opinion these modifications make the neural process model significantly cleaner from a Bayesian perspective and is quite nice. In the context of neural processes, I have very little criticism for the authors' methods. Everything makes sense, and the MC approach in equation (3) seems cleaner to me than the somewhat ad hoc \"VI like\" approach in equation (2).\n\nThe biggest difficulty I have is determining how to evaluate the authors' clear improvements to neural processes in the broader context of scalable probabilistic regression, an area to which the authors claim membership. To start with, the authors clearly demonstrate the value of both Bayesian context aggregation and a MC based likelihood approximation scheme on precisely the same types of problems that existing neural processes papers (e.g., Garnelo et al., 2018) have considered (with the notable exception that the 2D image completion task considers only MNIST as a target dataset). In this respect, it's difficult to fault the experimental evaluation.\n\nHowever, this paper and many neural process papers are written in the context of \"Formulating scalable probabilistic regression models with reliable uncertainty estimates.\" Surely, at some point, this should involve a comparison of these approaches to existing techniques for probabilistic regression, whether that be deep Gaussian processes, dropout based approaches, Bayesian neural networks, or other approaches. I don't mean to imply here that the authors are unaware of this large body of literature -- indeed, the authors have a decent if incomplete overview of techniques in this area (notably missing work on deep GPs).\n\nRather, it just seems surprising to me that the discussion of the relevant probabilistic regression literature ends at \"well, it exists.\" What I would like to see is a discussion of where the authors' impressive improvements to neural processes leave the model family in this broader context. How close or far off is the family on performance for standard benchmark regression tasks? Are there settings in which we can leverage the fully NN based nature of neural processes to achieve probabilistic regression in settings where the inductive biases of kernel methods are poor, like in computer vision or natural language processing? The relatively toy nature and limited dimensionality of the problems considered suggests that there is still significant progress to be made before such a comparison would be reasonable or even possible.\n\nTo summarize, in the context of neural processes I feel the paper makes good methodological contributions in presenting a much cleaner and more natural (from a Bayesian perspective) version of the model that has more of the flavor of standard amortized inference for latent variable models. Within the very narrow context of neural process papers, I therefore have very little to complain about. However, from a broader scientific perspective I would feel that the paper would be significantly strengthened by a fair evaluation to the rest of this literature, whether empirical or simply in discussion, regardless of how the authors' approach fares in comparison.", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138674, "tmdate": 1606915773974, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper637/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Review"}}}, {"id": "Z5i8d9My61", "original": null, "number": 4, "cdate": 1604270162145, "ddate": null, "tcdate": 1604270162145, "tmdate": 1605024641741, "tddate": null, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "invitation": "ICLR.cc/2021/Conference/Paper637/-/Official_Review", "content": {"title": "A new Bayesian regression model as a multi-task learning problem", "review": "The paper builds upon previous lines of research on multi-task learning problem, such as conditional latent variable models including the Neural Process. As shown by the extensive Related Work section, this seems to be an active research direction. This makes it difficult for me to judge originality and significance, but it is well-written and clear.\n\nSpecific comments\n- the approximate posterior distribution q_\\phi is often referred to as \"the posterior distribution\". I would keep \"approximate\" here. \n- p2: \"correspondence of GPs with infinite Bayesian NNs (BNNs)\", what is meant by \"infinite BNN\"? is it infinite-width BNN? please specify.\n- p2: \"adaptive BLR\" please describe the acronym.\n- p6: the Gaussian approximation of the posterior predictive likelihood (10) is said to be \"inspired by GPs which also define a Gaussian likelihood\". This is also essentially what is done by Synthetic Likelihood (a Nature paper by SN Wood, 2010) which is I think more closely related to the proposed approach than GPs.\n- p6, one line below: define PB acronym the first time it is used (not three lines after).\n\nTypos\n- top p2: \"does not introducing noticeable computational overhead\".\n- p6: \"conditional model with an decoder operating on\".\n- The references list could be tidied: some first names are abbreviated, some not.", "rating": "6: Marginally above acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2021/Conference/Paper637/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper637/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Bayesian Context Aggregation for Neural Processes", "authorids": ["~Michael_Volpp1", "~Fabian_Fl\u00fcrenbrock1", "~Lukas_Grossberger1", "~Christian_Daniel1", "~Gerhard_Neumann1"], "authors": ["Michael Volpp", "Fabian Fl\u00fcrenbrock", "Lukas Grossberger", "Christian Daniel", "Gerhard Neumann"], "keywords": ["Aggregation Methods", "Neural Processes", "Latent Variable Models", "Meta Learning", "Multi-task Learning", "Deep Sets"], "abstract": "Formulating scalable probabilistic regression models with reliable uncertainty estimates has been a long-standing challenge in machine learning research.  Recently, casting probabilistic regression as a multi-task learning problem in terms of conditional latent variable (CLV) models such as the Neural Process (NP) has shown promising results. In this paper, we focus on context aggregation, a central component of such architectures, which fuses information from multiple context data points. So far, this aggregation operation has been treated separately from the inference of a latent representation of the target function in CLV models. Our key contribution is to combine these steps into one holistic mechanism by phrasing context aggregation as a Bayesian inference problem. The resulting Bayesian Aggregation (BA) mechanism enables principled handling of task ambiguity, which is key for efficiently processing context information. We demonstrate on a range of challenging experiments that BA consistently improves upon the performance of traditional mean aggregation while remaining computationally efficient and fully compatible with existing NP-based models.", "one-sentence_summary": "We propose a Bayesian Aggregation mechanism for Neural Process-based models which improves upon traditional mean aggregation.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "volpp|bayesian_context_aggregation_for_neural_processes", "pdf": "/pdf/e30c747da671c808211da380e77feb9735c74530.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nvolpp2021bayesian,\ntitle={Bayesian Context Aggregation for Neural Processes},\nauthor={Michael Volpp and Fabian Fl{\\\"u}renbrock and Lukas Grossberger and Christian Daniel and Gerhard Neumann},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=ufZN2-aehFa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "ufZN2-aehFa", "replyto": "ufZN2-aehFa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper637/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538138674, "tmdate": 1606915773974, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper637/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper637/-/Official_Review"}}}], "count": 15}