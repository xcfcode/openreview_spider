{"notes": [{"id": "H1gz_nNYDS", "original": "r1e6gF9gIH", "number": 34, "cdate": 1569438826505, "ddate": null, "tcdate": 1569438826505, "tmdate": 1577168259589, "tddate": null, "forum": "H1gz_nNYDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "3RxM_EGaLz", "original": null, "number": 1, "cdate": 1576798685542, "ddate": null, "tcdate": 1576798685542, "tmdate": 1576800949393, "tddate": null, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "invitation": "ICLR.cc/2020/Conference/Paper34/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a simple one-shot approach on searching the number of channels for deep convolutional neural networks. It trains a single slimmable network and then iteratively slim and evaluate the model to ensure a minimal accuracy drop. The method is simple and the results are promising. \n\nThe main concern for this paper is the limited novelty. This work is based on slimmable network and the iterative slimming process is new, but in some sense similar to DropPath. The rebuttal that PathNet \"has not demonstrated results on searching number of channels, and we are among the first few one-shot approaches on architectural search for number of channels\" seem weak.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795730643, "tmdate": 1576800283476, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper34/-/Decision"}}}, {"id": "BJghFTlYjr", "original": null, "number": 5, "cdate": 1573617028294, "ddate": null, "tcdate": 1573617028294, "tmdate": 1573617028294, "tddate": null, "forum": "H1gz_nNYDS", "replyto": "H1xkjO_ptS", "invitation": "ICLR.cc/2020/Conference/Paper34/-/Official_Comment", "content": {"title": "Authors' Reply to Review", "comment": "Thanks for your review efforts! We have addressed all questions below:\n\nQ1: Thanks for delving deep into our discussion in experiments. In our view, the difference of channel number distribution may come from several reasons. First, VGGNet, which many previous pruning methods targeted to optimize, has much more channels than MobileNets designed for efficiency. For example, the last stage of VGG has 4096 channels, ResNet and Inception have 2048 channels, but MobileNet v1 and v2 only have 1024 channels. Different network backbones may lead to the hallucinations like \"deeper layers have more redundancy\". Second, the channel distribution also depends on the datasets. For example, in Section 4.3 we stated that \"we observed that the optimized architecture for CIFAR10 has much fewer channels in deep layers, which we guess may lead to better generalization on test set for small datasets like CIFAR10\".\n\nQ2: We trained our final optimized models from scratch as a slimmable network using both \u201csandwich rule\u201d and \u201cin-place distillation\u201d, as  slimmable models are more flexible to deploy adaptively. Our results are better than the all baseline slimmable models with \u201csandwich rule\u201d and \u201cin-place distillation\u201d on MobileNet v1, v2 and ResNets [1][2].\n\nQ3: \"ResNet-50\u201d means standard ResNets,  \u201cHe-ResNet-50\" means pruned ResNet-50 [3] by Yihui He et al. Both \"He-ResNet-50\" and \"ThiNet-ResNet-50\" did not report their params, memory and CPU Latency, so we omit them. We will make it more clear by inserting references in the table.\n\nQ4: We directly transfer the networks from ImageNet to CIFAR-10 without changing the ratio of the number of channels. We only change the final predictions from 1000 to 10 classes. A similar architecture can be found in Figure 4 (left). Also to transfer, the input resolution is directly changed from 224 (ImageNet) to 32 (CIFAR-10) without modifying other architecture-related configurations.\n\nQ5: Our search cost is mainly on training a slimmable network. In our settings, training a slimmable network for 50 epochs takes roughly the same cost as training a normal network for 200 epochs. Compared with previous architecture search baseline MNasNet, we have more than 100x saving on the search cost (MNasNet samples 8000 models with each model trained with 5 epochs on ImageNet). The strict walltime of the search depends on different platforms and hardware (GPU vs. TPU,  memory and disk, etc.) thus are omitted in our submission.\n\nQ6: Our proposed AutoSlim is a general framework and can indeed be used to search for the number of neurons in fully connected layers. In fact, the final layer in convolutional networks for image classification is a fully-connected layer, and its input number of channels is optimized by AutoSlim.\n\n\n[1] Slimmable neural networks. Yu et al. ICLR 2019.\n[2] Universally Slimmable Networks and Improved Training Techniques. Yu et al. ICCV 2019.\n[3] Channel pruning for accelerating very deep neural networks. Yihui He, Xiangyu Zhang, and Jian Sun. ICCV 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper34/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gz_nNYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference/Paper34/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper34/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper34/Reviewers", "ICLR.cc/2020/Conference/Paper34/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper34/Authors|ICLR.cc/2020/Conference/Paper34/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177348, "tmdate": 1576860543509, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference/Paper34/Reviewers", "ICLR.cc/2020/Conference/Paper34/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper34/-/Official_Comment"}}}, {"id": "r1l2e3eKjr", "original": null, "number": 3, "cdate": 1573616628337, "ddate": null, "tcdate": 1573616628337, "tmdate": 1573616754179, "tddate": null, "forum": "H1gz_nNYDS", "replyto": "Bkx0hBuM9B", "invitation": "ICLR.cc/2020/Conference/Paper34/-/Official_Comment", "content": {"title": "Authors' Reply to Review", "comment": "Thanks for your review efforts! We have addressed all questions below:\n\n1. We hope to reiterate our contribution and novelty in this work. The goal of slimmable networks is to provide adaptive accuracy-efficiency trade-offs and their results are just close to non-slimmable models. Our goal is architecture search on number of channels and our results are much better than baseline models. In this work, we present a one-shot approach for searching number of channels, with extensive results on ImageNet. We proposed the AutoSlim pipeline, designed the search space,  addressed implementation issues and showed better results. Our proposed AutoSlim automates the design of channel configurations for resource constrained devices with much lower search cost.\n\n2. Thanks for point out the reference. We will discuss this work in our revised version. To the best of our knowledge, the referenced approach has not demonstrated results on searching number of channels, and we are among the first few one-shot approaches on architectural search for number of channels.\n\n3. Thanks for the suggestion. In Table 1, we think only listing pairs like MobileNet vs AutoSlim-MobileNet is not enough, because the network pruning methods and network architecture search methods are all our baselines and related work. Methods like AMC, NetAdapt and MNasNet all tried to optimize the number of channels of a backbone architecture. We will try to simplify our Table 1 by removing some NAS baselines like NASNet and PNASNet.\n\n4. We included the detail of AutoSlim in Sec 3.2 The Search Space: \"We train a slimmable model that can execute between 0.15x and 1.5x\", and Sec 3.3 Greedy Slimming: \"We start with the largest model (e.g., 1.5x)\". In our search space, N is 1.5. \n\n5. We start from a larger model (e.g., 1.5x) because we simultaneously compare our discovered models at 1.3x, 1.0x, 0.75x, etc, through a single greedy slimming process. While we agree that pruning baseline methods (AMC and ThiNet) should also start from the same larger models for strictly fair comparison, their search code is not publicly available. Moreover, our discovered small models (e.g., 0.75x) has channels less than 1.0x model in most layers. We believe the performance improvement is mainly from our proposed methods, instead of the search space.\n\n6. For ResNet-50, the training settings are exactly the same. For mobile networks, we use same settings as ShuffleNets. Official mobile models from google (MobileNets and MNasNets) used a slightly different setting (~380 epochs with exponentially decaying learning rates and RMSProp Optimizer) on TensorFlow. To verify the fairness, we re-implemented and trained MobileNet v2 and MNasNet with strictly same training setting, and the performance is very close. On MobileNet v2 our result is 0.1 better and on MNasNet our result is 0.2 worse.\n\n7. As suggested in [1], \"the pruned architecture itself, rather than a set of inherited important weights, is more crucial to the efficiency in the final model\". In our work, the goal is to directly find the best \"pruned\" architectures. In this context, \"the importance of channel numbers\" is an explicit and direct optimization objective, while \"the importance of trained weights\" is just an implicit and surrogate objective. Our final goal of finding efficient network is to find the best architecture, which aligns better with \"the importance of channel numbers\".\n\n\n[1] Rethinking the Value of Network Pruning. Liu et al. ICLR 2019\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper34/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gz_nNYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference/Paper34/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper34/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper34/Reviewers", "ICLR.cc/2020/Conference/Paper34/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper34/Authors|ICLR.cc/2020/Conference/Paper34/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177348, "tmdate": 1576860543509, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference/Paper34/Reviewers", "ICLR.cc/2020/Conference/Paper34/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper34/-/Official_Comment"}}}, {"id": "B1xldnxtir", "original": null, "number": 4, "cdate": 1573616743597, "ddate": null, "tcdate": 1573616743597, "tmdate": 1573616743597, "tddate": null, "forum": "H1gz_nNYDS", "replyto": "Hye2e3RpFB", "invitation": "ICLR.cc/2020/Conference/Paper34/-/Official_Comment", "content": {"title": "Authors' Reply to Review", "comment": "Thanks for your review efforts! We have addressed all questions below:\n\n1. Our search cost is mainly on training a slimmable network. In our settings, training a slimmable network for 50 epochs takes roughly the same cost as training a normal network for 200 epochs. Compared with previous architecture search baseline MNasNet, we have more than 100x saving on the search cost (MNasNet samples 8000 models with each model trained with 5 epochs on ImageNet). The strict walltime of the search depends on different platforms and hardware (GPU vs. TPU,  memory and disk, etc.) thus are omitted in our submission.\n\n2. As suggested in [1], \"the pruned architecture itself, rather than a set of inherited important weights, is more crucial to the efficiency in the final model\". In our work, the goal is to directly find the best \"pruned\" architectures. In this context, \"the importance of channel numbers\" is an explicit and direct optimization objective, while \"the importance of trained weights\" is just an implicit and surrogate objective. Our final goal of finding efficient network is to find the best architecture, which aligns better with \"the importance of channel numbers\".\n\n\n[1] Rethinking the Value of Network Pruning. Liu et al. ICLR 2019"}, "signatures": ["ICLR.cc/2020/Conference/Paper34/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gz_nNYDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference/Paper34/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper34/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper34/Reviewers", "ICLR.cc/2020/Conference/Paper34/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper34/Authors|ICLR.cc/2020/Conference/Paper34/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504177348, "tmdate": 1576860543509, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper34/Authors", "ICLR.cc/2020/Conference/Paper34/Reviewers", "ICLR.cc/2020/Conference/Paper34/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper34/-/Official_Comment"}}}, {"id": "H1xkjO_ptS", "original": null, "number": 1, "cdate": 1571813526600, "ddate": null, "tcdate": 1571813526600, "tmdate": 1572972647130, "tddate": null, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "invitation": "ICLR.cc/2020/Conference/Paper34/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "In this paper, the authors propose a method to perform architecture search on the number of channels in convolutional layers. The proposed method, called AutoSlim, is a one-shot approach based on previous work of Slimmable Networks [2,3]. The authors have tested the proposed methods on a variety of architectures on ImageNet dataset. \n\nThe paper is well-written and easy to follow. I really appreciate the authors for structuring this paper so well. I have the following questions:\n\nQ1: In figure 4, the authors find that \u201cCompared with default MobileNet v2, our optimized configuration has fewer channels in shallow layers and more channels in deep ones.\u201d This is interesting. Because in network pruning methods, it is found that usually later stages get pruned more [1] (e.g. VGG), indicating that there is more redundancy for deep layers. However, in this case, actually deep layers get more channels than standard models. Is there any justification for this? Is it that more channels in deep layers benefit the accuracy?\n\nQ2: In \u201cTraining Optimized Networks\u201d, the authors mentioned that \u201cBy default we search for the network FLOPs at approximately 200M, 300M and 500M, and train a slimmable model.\u201d Does this mean that the authors train the final optimized models from scratch as a slimmable network using \u201csandwich rule\u201d and \u201cin-place distillation\u201d rule? Or are the authors just training the final model with standard training schedule? If it is the first case, can the authors justify why?\n\nQ3: In Table 1, \u201cHeavy Models\u201d, what is the difference between \u201cResNet-50\u201d and \u201cHe-ResNet-50\u201d? Also, why the params, memory and CPU Latency of some networks are omitted?\n\nQ4: In the last paragraph of section 4, the authors tried the transferability of networks learned from ImageNet to CIFAR-10 dataset. I am not sure how the authors transfer the networks from Imagenet to CIFAR-10? Is it the ratio of the number of channels? Can the authors provide the architecture details of MobileNet v2 on CIFAR-10 dataset?\n\nQ5: What is the estimated time for a typical run of AutoSlim? How does it compare to network pruning methods or neural architecture search methods?\n\nQ6: Can the methods be used to search for the number of neurons in fully connected layers? Are there any results?\n\n[1] Rethinking the Value of Network Pruning. Zhuang et al. ICLR 2019\n[2] Slimmable neural networks. Yu et al. ICLR 2019.\n[3] Universally Slimmable Networks and Improved Training Techniques. Yu et al. Arxiv.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper34/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper34/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574840457285, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper34/Reviewers"], "noninvitees": [], "tcdate": 1570237758111, "tmdate": 1574840457297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper34/-/Official_Review"}}}, {"id": "Hye2e3RpFB", "original": null, "number": 2, "cdate": 1571838963697, "ddate": null, "tcdate": 1571838963697, "tmdate": 1572972647087, "tddate": null, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "invitation": "ICLR.cc/2020/Conference/Paper34/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "This paper proposes a simple and one-shot approach on neural architecture search for the number of channels to achieve better accuracy. Rather than training a lot of network samples, the proposed method trains a single slimmable network to approximate the network accuracy of different channel configurations. The experimental results show that the proposed method achieves better performance than the existing baseline methods.\n\n- It would be better to provide the search cost of the proposed method and the other baseline methods because that is the important metric for neural architecture search methods. As this paper points out that NAS methods are computationally expensive, it would be better to make the efficiency of the proposed method clear.\n\n- According to this paper, the notable difference between the proposed method and the existing pruning methods is that the pruning methods are grounded on the importance of trained weights, but the proposed method focuses more on the importance of channel numbers. It is unclear to me why such a difference is caused by the proposed method, that is, which part of the proposed method causes the difference? And how does the difference affect the final performance?"}, "signatures": ["ICLR.cc/2020/Conference/Paper34/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper34/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574840457285, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper34/Reviewers"], "noninvitees": [], "tcdate": 1570237758111, "tmdate": 1574840457297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper34/-/Official_Review"}}}, {"id": "Bkx0hBuM9B", "original": null, "number": 3, "cdate": 1572140469898, "ddate": null, "tcdate": 1572140469898, "tmdate": 1572972647042, "tddate": null, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "invitation": "ICLR.cc/2020/Conference/Paper34/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "The paper targets on learning the number of channels across all layers, under computation/model size/memory constraints. The method is simple and the results seems promising. \n\nHowever, the following issues need to be resolved:\n1. The main method is based on published \"slimmable networks,\" such that the novelty is limited;\n2. The method is very simpler to DropPath in [1], which uses DropPath to learn important branches while this paper uses it to learn channels. They are similar.\n3. Better ablation studies are required in Table 1. This table should be simplified. As the method cannot learn architectures but channel numbers, the only useful pairs of comparisons are those having the same architecture, such as  a pair of MobileNet vs AutoSlim-MobileNet.\n4. an important detail is missing: where does the AutoSlim start from? Does it start from a larger model than the baseline? In the set of \"500M FLOPs\" experiments, I see the size of \"AutoSlim-MobileNet v1\" (4.6M) is larger than \"MobileNet v1 1.0x\" (4.2M), this implies that AutoSlim start from a \"MobileNet v1 Nx\" and N > 1.0. What is exactly N?\n5. If AutoSlim starts from a larger baseline model with N times (N > 1.0) width, then the pruning baseline methods (AMC and ThiNet) should also start from the same larger models for fair comparison. In general, starting from a larger model and pruning it down can achieve a better accuracy vs. size trade-off.\n6. \"300 epochs with linearly decaying learning rate for mobile networks, 100 epochs with step learning rate schedule for ResNet-50 based models\", are baselines trained in the same way?\n\nMinor: \n1. missing captions in a couple of figures, e.g., Figure 5.\n2. \"the importance of trained weights\" vs \"the importance of channel numbers\" is trivial\n\n\n[1] Bender, Gabriel, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc Le. \"Understanding and simplifying one-shot architecture search.\" In International Conference on Machine Learning, pp. 549-558. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper34/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper34/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "\nWe study how to set the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size). A simple and one-shot approach, named AutoSlim, is presented. Instead of training many network samples and searching with reinforcement learning, we train a single slimmable network to approximate the network accuracy of different channel configurations. We then iteratively evaluate the trained slimmable model and greedily slim the layer with minimal accuracy drop. By this single pass, we can obtain the optimized channel configurations under different resource constraints. We present experiments with MobileNet v1, MobileNet v2, ResNet-50 and RL-searched MNasNet on ImageNet classification. We show significant improvements over their default channel configurations. We also achieve better accuracy than recent channel pruning methods and neural architecture search methods with 100X lower search cost.\n\nNotably, by setting optimized channel numbers, our AutoSlim-MobileNet-v2 at 305M FLOPs achieves 74.2% top-1 accuracy, 2.4% better than default MobileNet-v2 (301M FLOPs), and even 0.2% better than RL-searched MNasNet (317M FLOPs). Our AutoSlim-ResNet-50 at 570M FLOPs, without depthwise convolutions, achieves 1.3% better accuracy than MobileNet-v1 (569M FLOPs).\n", "title": "AutoSlim: Towards One-Shot Architecture Search for Channel Numbers", "keywords": ["AutoSlim", "Neural Architecture Search", "Efficient Networks", "Network Pruning"], "pdf": "/pdf/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "authors": ["Jiahui Yu", "Thomas Huang"], "TL;DR": "We present an automated approach to search the number of channels in a neural network to achieve better accuracy under constrained resources (e.g., FLOPs, latency, memory footprint or model size).", "authorids": ["jyu79@illinois.edu", "t-huang1@illinois.edu"], "paperhash": "yu|autoslim_towards_oneshot_architecture_search_for_channel_numbers", "original_pdf": "/attachment/efbd615da73f83d215fe415d8b172f87d0b69d10.pdf", "_bibtex": "@misc{\nyu2020autoslim,\ntitle={AutoSlim: Towards One-Shot Architecture Search for Channel Numbers},\nauthor={Jiahui Yu and Thomas Huang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gz_nNYDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gz_nNYDS", "replyto": "H1gz_nNYDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574840457285, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper34/Reviewers"], "noninvitees": [], "tcdate": 1570237758111, "tmdate": 1574840457297, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper34/-/Official_Review"}}}], "count": 8}