{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028613161, "tcdate": 1490028613161, "number": 1, "id": "B1aHOK6ig", "invitation": "ICLR.cc/2017/workshop/-/paper123/acceptance", "forum": "S1L-hCNtl", "replyto": "S1L-hCNtl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Adversarial Learning of Markov Chains", "abstract": "We investigate generative adversarial training methods to learn a transition operator for a Markov chain, where the goal is to match its stationary distribution to a target data distribution. We propose a novel training procedure that avoids sampling directly from the stationary distribution, while still capable of reaching the target distribution asymptotically. The model can start from random noise, is likelihood free, and is able to generate multiple distinct samples during a single run. Preliminary experiment results show the chain can generate high quality samples when it approaches its stationary, even with smaller architectures traditionally considered for Generative Adversarial Nets.", "pdf": "/pdf/ce96eec69a3b6a7c15c6c700f3abcfbe526dd3ab.pdf", "TL;DR": "We can train Markov Chains with an adversarial network.", "paperhash": "song|generative_adversarial_learning_of_markov_chains", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["stanford.edu", "tsinghua.edu.cn", "duke.edu"], "authors": ["Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["tsong@cs.stanford.edu", "zhaosj12@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028613688, "id": "ICLR.cc/2017/workshop/-/paper123/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1L-hCNtl", "replyto": "S1L-hCNtl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028613688}}}, {"tddate": null, "tmdate": 1489644889494, "tcdate": 1489644889494, "number": 2, "id": "HkWDTivsg", "invitation": "ICLR.cc/2017/workshop/-/paper123/public/comment", "forum": "S1L-hCNtl", "replyto": "rkfqGpgie", "signatures": ["~Jiaming_Song1"], "readers": ["everyone"], "writers": ["~Jiaming_Song1"], "content": {"title": "Responses over some concerns", "comment": "Thanks for your comments! To answer some of your concerns:\n\nQ: No objective way to measure model quality.\n\nA: As far as we know, there are at least two ways to measure the model quality from samples only. One is to approximately measure \"likelihood\" by using KDE, such as in (Wu et al. 2017); another is to use scores from pretrained networks, such as in (Salimans et al. 2016). Both methods are far from perfect. We will include such measures in a longer version of the work.\n\nIn the appendix, we consider another metric unique to iterative sampling methods, which is the transition of labels. If the model suffers from the \"missing mode\" problem, then the transition would be highly unbalanced. Figure 4 shows that each label has roughly the same chance of appearing in the stationary distribution; hence our model does not suffer from \"missing mode\" (of labels, at least).\n\n\nQ: It seems that there is no significant improvement of quality of samples over long steps.\n\nA: MNIST is a relatively easy dataset for generative models. Therefore, it is not too hard to find a transition that mixes fast with a relatively simple neural network. One feasible solution is for a transition to jump between two sharp samples, not necessarily sharpening the same sample. From the empirical results, it seems that using 2 steps seems to generate high-quality samples in MNIST.\n\n\nQ: Even applying the chain once, changes the style and digit identity completely.\n\nA: While the next digit looks completely different, we argue that applying the chain once may preserve style and/or digit identity.\n\nWe provide a figure that generates steps 1-50 using the MLP architecture. Here: https://drive.google.com/file/d/0B0LzoDno7qkJalI4dkNndjBVbXM/view?usp=sharing\nEach row is obtained by running a sample from step 1 to step 50. \n\nNote that although the model does not \u201crefine samples\u201d, the next sample is still dependent on the previous sample, in terms of higher level features, such as width, rotation, and digit identity. \n\nThe transition probabilities for labels in Figure 4 also supports this argument (in terms of digit identity).\n\n\nQ: The chain mixes too fast.\n\nA: Additionally, we have some preliminary results (on CelebA) where we can alter the architecture for slower mixing, producing GSN-like results. Here:\nhttps://drive.google.com/file/d/0B0LzoDno7qkJenBWZzlaWkNxdUU/view?usp=sharing\n\nThis suggests that it is possible to slow down mixing if we consider other architectures for the transitions.\n\n\nQ: What is the advantage over (ancestral sampling) GANs?\nOur advantage over \"ancestral sampling\" GANs is that we could use a simpler architecture (such as using MLP) to iteratively approach a sample (in the MNIST case, 2 steps seems to suffice), where realistic samples cannot be obtained through ancestral sampling. \n\n\n\n=== References ===\nWu et al. \"On the Quantitative Analysis of Decoder-Based Generative Models.\" ICLR 2017.\nSalimans et al. \"Improved techniques for training gans.\" NIPS 2016.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Adversarial Learning of Markov Chains", "abstract": "We investigate generative adversarial training methods to learn a transition operator for a Markov chain, where the goal is to match its stationary distribution to a target data distribution. We propose a novel training procedure that avoids sampling directly from the stationary distribution, while still capable of reaching the target distribution asymptotically. The model can start from random noise, is likelihood free, and is able to generate multiple distinct samples during a single run. Preliminary experiment results show the chain can generate high quality samples when it approaches its stationary, even with smaller architectures traditionally considered for Generative Adversarial Nets.", "pdf": "/pdf/ce96eec69a3b6a7c15c6c700f3abcfbe526dd3ab.pdf", "TL;DR": "We can train Markov Chains with an adversarial network.", "paperhash": "song|generative_adversarial_learning_of_markov_chains", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["stanford.edu", "tsinghua.edu.cn", "duke.edu"], "authors": ["Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["tsong@cs.stanford.edu", "zhaosj12@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487363070422, "tcdate": 1487363070422, "id": "ICLR.cc/2017/workshop/-/paper123/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper123/reviewers"], "reply": {"forum": "S1L-hCNtl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487363070422}}}, {"tddate": null, "tmdate": 1489644764000, "tcdate": 1489644764000, "number": 1, "id": "rkEypjDix", "invitation": "ICLR.cc/2017/workshop/-/paper123/public/comment", "forum": "S1L-hCNtl", "replyto": "SJShbwXie", "signatures": ["~Jiaming_Song1"], "readers": ["everyone"], "writers": ["~Jiaming_Song1"], "content": {"title": "Responses to some concerns", "comment": "Thanks for your comments! To answer some of your concerns:\n\nQ: Architecture:\n\nA: There is only one path through the network (x -> z -> x -> z -> ...); hence it is not an HMM or a typical RNN with latent variables. \n\n\nQ: Visualize the chain from the first step to 50.\n\nA: Here: https://drive.google.com/file/d/0B0LzoDno7qkJalI4dkNndjBVbXM/view?usp=sharing\n\nEach row is from step 1 to step 50. From the results it seems that in most cases the model is able to converge to a visible sample in 2-3 steps, and then \"jumping\" through samples, although the next sample seems to be dependent on the previous one (for example, if the previous sample is wide/narrow, then the next sample will also be wide/narrow).\n\n\nQ: Why doesn't the chain seems to refine the samples?\n\nA: The model is only required to come up with one solution for the transition. Although moving slowly and seemingly refining the samples (as in GSN) is a legitimate solution, other reasonable transitions could be jumping from a sharp sample from another sharp sample. From the results in our paper, it seems that our model prefers the latter.\n\nThe network does not have to refine the same digit, as is performed in Infusion Training (Bordes et al. 2017). Moreover, it seems unnecessary to run a long chain to get only one sharp sample, when you can actually run a similar model to obtain multiple high-quality samples.\n\nNevertheless, preliminary results on CelebA show that we can reduce the mixing speed (and generate GSN-style samples) by introducing shortcut connections. Here:\nhttps://drive.google.com/file/d/0B0LzoDno7qkJenBWZzlaWkNxdUU/view?usp=sharing\n\n=== References ===\nBordes et al. \u201cLearning to Generate Samples from Noise through Infusion Training\u201d. ICLR 2017.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Adversarial Learning of Markov Chains", "abstract": "We investigate generative adversarial training methods to learn a transition operator for a Markov chain, where the goal is to match its stationary distribution to a target data distribution. We propose a novel training procedure that avoids sampling directly from the stationary distribution, while still capable of reaching the target distribution asymptotically. The model can start from random noise, is likelihood free, and is able to generate multiple distinct samples during a single run. Preliminary experiment results show the chain can generate high quality samples when it approaches its stationary, even with smaller architectures traditionally considered for Generative Adversarial Nets.", "pdf": "/pdf/ce96eec69a3b6a7c15c6c700f3abcfbe526dd3ab.pdf", "TL;DR": "We can train Markov Chains with an adversarial network.", "paperhash": "song|generative_adversarial_learning_of_markov_chains", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["stanford.edu", "tsinghua.edu.cn", "duke.edu"], "authors": ["Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["tsong@cs.stanford.edu", "zhaosj12@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487363070422, "tcdate": 1487363070422, "id": "ICLR.cc/2017/workshop/-/paper123/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper123/reviewers"], "reply": {"forum": "S1L-hCNtl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487363070422}}}, {"tddate": null, "tmdate": 1489363372728, "tcdate": 1489363372728, "number": 2, "id": "SJShbwXie", "invitation": "ICLR.cc/2017/workshop/-/paper123/official/review", "forum": "S1L-hCNtl", "replyto": "S1L-hCNtl", "signatures": ["ICLR.cc/2017/workshop/paper123/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper123/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "Summary: This paper proposed a novel method for training Markov chains using the indistinguishability framework popularized by generative adversarial networks. Experiments on MNIST are promising. The experiments show a clear improvement of quality as the chain size increases.\n\nNovelty: The idea has been featured in prior work but only amongst other ideas mixed in, but this is the first paper which describes and validates this idea alone.\n\nClarity: The paper is very clearly written, and the experiments are appropriate for a workshop paper.\n\nQuality: Neat execution of idea. However, it's not clear whether the Markov chain is actually refining the same digit or whether the improvement is just due to more computation that a large chain can perform. Is the improvement due to the new objective, or is this just a recurrent neural network? The authors need to show a chain of the sample size from 0 to 50 to prove the refinement.\n\nPros: Independent atomic description and evaluation of a significant idea.\nCons: It's not 100% clear whether it's the MCMC that works, or just recurrent computation.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Adversarial Learning of Markov Chains", "abstract": "We investigate generative adversarial training methods to learn a transition operator for a Markov chain, where the goal is to match its stationary distribution to a target data distribution. We propose a novel training procedure that avoids sampling directly from the stationary distribution, while still capable of reaching the target distribution asymptotically. The model can start from random noise, is likelihood free, and is able to generate multiple distinct samples during a single run. Preliminary experiment results show the chain can generate high quality samples when it approaches its stationary, even with smaller architectures traditionally considered for Generative Adversarial Nets.", "pdf": "/pdf/ce96eec69a3b6a7c15c6c700f3abcfbe526dd3ab.pdf", "TL;DR": "We can train Markov Chains with an adversarial network.", "paperhash": "song|generative_adversarial_learning_of_markov_chains", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["stanford.edu", "tsinghua.edu.cn", "duke.edu"], "authors": ["Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["tsong@cs.stanford.edu", "zhaosj12@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489363373580, "id": "ICLR.cc/2017/workshop/-/paper123/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper123/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper123/AnonReviewer1", "ICLR.cc/2017/workshop/paper123/AnonReviewer2"], "reply": {"forum": "S1L-hCNtl", "replyto": "S1L-hCNtl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper123/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489363373580}}}, {"tddate": null, "tmdate": 1489191561903, "tcdate": 1489191561903, "number": 1, "id": "rkfqGpgie", "invitation": "ICLR.cc/2017/workshop/-/paper123/official/review", "forum": "S1L-hCNtl", "replyto": "S1L-hCNtl", "signatures": ["ICLR.cc/2017/workshop/paper123/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper123/AnonReviewer1"], "content": {"title": "Simple, appealing idea but no clear improvement over std. GANs", "rating": "6: Marginally above acceptance threshold", "review": "The authors propose to use an adversarial objective to train a transition operator for a Markov chain such that the stationary distribution is indistinguishable from the training data. Samples are either generated by starting from some fixed distribution \\pi^0 and applying the operator multiple times, or by starting from a training set sample. The idea is simple, intuitively appealing and seems to be  mathematically correct.\n\nIn the experimental section the authors apply their approach to MNIST and show results for three differently parameterized transition operators: a DCGAN based architecture, a convolutional- and a fully connected neural neural network. The corresponding samples in Figs. 1-3 look decent, although not obviously better than those of other GAN based models.\nA surprising (and potentially disappointing?) property apparent from Figs. 1-4 is, that even applying the transition operator only once, typically changes the digit-class and style completely. It therefore seems, the model uses the current state of the chain merely as a source randomness. It has not learned to \u201crefine\u201d the current state. Phrased more positively: the learned MC mixes extraordinary fast. \n  \nPositive: \nSimple idea; straightforward implementation.\nCombines ideas from GSNs (generative stochastic networks) and adversarial training.\n\nNegative:\nNo objective way to compare model quality; no clear improvement over std. GANs.\nMight use the current state merely as source of randomness: taking 5 or 10 steps does not provide obvious improvement over taking 2 steps.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative Adversarial Learning of Markov Chains", "abstract": "We investigate generative adversarial training methods to learn a transition operator for a Markov chain, where the goal is to match its stationary distribution to a target data distribution. We propose a novel training procedure that avoids sampling directly from the stationary distribution, while still capable of reaching the target distribution asymptotically. The model can start from random noise, is likelihood free, and is able to generate multiple distinct samples during a single run. Preliminary experiment results show the chain can generate high quality samples when it approaches its stationary, even with smaller architectures traditionally considered for Generative Adversarial Nets.", "pdf": "/pdf/ce96eec69a3b6a7c15c6c700f3abcfbe526dd3ab.pdf", "TL;DR": "We can train Markov Chains with an adversarial network.", "paperhash": "song|generative_adversarial_learning_of_markov_chains", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["stanford.edu", "tsinghua.edu.cn", "duke.edu"], "authors": ["Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["tsong@cs.stanford.edu", "zhaosj12@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489363373580, "id": "ICLR.cc/2017/workshop/-/paper123/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper123/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper123/AnonReviewer1", "ICLR.cc/2017/workshop/paper123/AnonReviewer2"], "reply": {"forum": "S1L-hCNtl", "replyto": "S1L-hCNtl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper123/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper123/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489363373580}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487462313251, "tcdate": 1487363069836, "number": 123, "id": "S1L-hCNtl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1L-hCNtl", "signatures": ["~Jiaming_Song1"], "readers": ["everyone"], "content": {"title": "Generative Adversarial Learning of Markov Chains", "abstract": "We investigate generative adversarial training methods to learn a transition operator for a Markov chain, where the goal is to match its stationary distribution to a target data distribution. We propose a novel training procedure that avoids sampling directly from the stationary distribution, while still capable of reaching the target distribution asymptotically. The model can start from random noise, is likelihood free, and is able to generate multiple distinct samples during a single run. Preliminary experiment results show the chain can generate high quality samples when it approaches its stationary, even with smaller architectures traditionally considered for Generative Adversarial Nets.", "pdf": "/pdf/ce96eec69a3b6a7c15c6c700f3abcfbe526dd3ab.pdf", "TL;DR": "We can train Markov Chains with an adversarial network.", "paperhash": "song|generative_adversarial_learning_of_markov_chains", "keywords": ["Deep learning", "Unsupervised Learning"], "conflicts": ["stanford.edu", "tsinghua.edu.cn", "duke.edu"], "authors": ["Jiaming Song", "Shengjia Zhao", "Stefano Ermon"], "authorids": ["tsong@cs.stanford.edu", "zhaosj12@cs.stanford.edu", "ermon@cs.stanford.edu"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 6}