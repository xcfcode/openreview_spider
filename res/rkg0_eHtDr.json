{"notes": [{"id": "rkg0_eHtDr", "original": "H1gYD6eKwH", "number": 2417, "cdate": 1569439861852, "ddate": null, "tcdate": 1569439861852, "tmdate": 1577168261766, "tddate": null, "forum": "rkg0_eHtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "beqc2bOBH", "original": null, "number": 1, "cdate": 1576798748627, "ddate": null, "tcdate": 1576798748627, "tmdate": 1576800887385, "tddate": null, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "invitation": "ICLR.cc/2020/Conference/Paper2417/-/Decision", "content": {"decision": "Reject", "comment": "This paper studies over-parameterization for unsupervised learning. The paper does a series of empirical studies on this topic. Among other things the authors observe that larger models can increase the number latent variables recovered when fitting larger variational inference models. The reviewers raised some concern about the simplicity of the models studied and also lack of some theoretical justification. One reviewer also suggests that more experiments and ablation studies on more general models will further help clarify the role over-parameterized model for latent generative models. I agree with the reviewers that this paper is \"compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods\". I disagree with the reviewers that theoretical study is required as I think a good empirical paper with clear conjectures is as important. I do agree with the reviewers however that for empirical paper I think the empirical studies would have to be a bit more thorough with more clear conjectures. In summary, I think the paper is nice and raises a lot of interesting questions but can be improved with more through studies and conjectures. I would have liked to have the paper accepted but based on the reviewer scores and other papers in my batch I can not recommend acceptance at this time. I strongly recommend the authors to revise and resubmit. I really think this is a nice paper and has a lot of potential and can have impact with appropriate revision.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795706533, "tmdate": 1576800254610, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2417/-/Decision"}}}, {"id": "r1g54dkzqH", "original": null, "number": 3, "cdate": 1572104241965, "ddate": null, "tcdate": 1572104241965, "tmdate": 1574289132549, "tddate": null, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "invitation": "ICLR.cc/2020/Conference/Paper2417/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper performs empirical study on the influence of overparameterization to generalization performance of noisy-or networks and sparse coding, and points out overparameterization is indeed beneficial. I find the paper has some drawbacks.\n\n1. Overparameterization is better than underparamterization and exact parameterization is not surprising. The question is how much do we need to overparameterize. As the number of parameters goes to infinity, the model can eventually remember all the training data, and has poor generalization. The real interesting question to ask is how to use an excessive amount of parameters, yet still avoid overfitting.\n\n2. The discussed models are too simple. I am expecting some theoretical analysis for tasks simple as noisy-or and sparse coding, or some experiments for more complicated (deep) models need to be done, to make the paper more solid.\n\nUpdate\n=====\n\nThank the authors for the response. The authors do address my comment #1. I agree that overparameterization improves recovery is a new finding. However, I still think the \"information gain\" of this paper is somewhat thin. There could be at least some intuitions on why overparameterization helps noisy-or models. I think the analysis can be more in-depth to make this paper more interesting. \n\nI would like to raise my score a bit to a \"neutral\" score, but given the current scoring system I'll just keep my score. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2417/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2417/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575658954556, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2417/Reviewers"], "noninvitees": [], "tcdate": 1570237723117, "tmdate": 1575658954570, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2417/-/Official_Review"}}}, {"id": "B1gZcbv3sS", "original": null, "number": 3, "cdate": 1573839241449, "ddate": null, "tcdate": 1573839241449, "tmdate": 1573839548663, "tddate": null, "forum": "rkg0_eHtDr", "replyto": "r1g54dkzqH", "invitation": "ICLR.cc/2020/Conference/Paper2417/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for your comments!\n\nRegarding (1), the rationale of the reviewer applies to the *likelihood* of the model we fit: the training-set likelihood should improve, and there is a potential that the test likelihood will drop (i.e. memorization happens). Our paper *does not* focus on this -- it focuses on *parameter recovery* -- here, it is entirely unclear why overparametrization should help, as there can be potentially many overparametrized models with equally good likelihood, but no relation to the ground truth parameters of the model whatsoever. This isn\u2019t an issue of memorization -- rather in the presence of overparametrization, the model could in principle be un-identifiable (i.e. multiple sets of parameters give rise to the same distribution), so there is no a-priori reason why the optimization should prefer the ground truth parameters. \n\nMoreover, in our noisy-OR experiments, 128 latent variables are already 16 times more than the true number of latent variables, and at this level of overparameterization the performance is still much better than without overparameterization. Hence, we show that if there is a \u201ccritical\u201d amount of overparametrization at which performance starts to suffer, it may be quite large. \n\nRegarding (2): while it would be great to have theory accompanying our empirical observations, we note that theoretical analysis for our settings is likely to be very non-trivial given our current understanding of the optimization for these latent-variable models. For noisy-OR networks, the currently known algorithms with provable guarantees use tensor-based techniques and are very different from the gradient-descent algorithm used by us (e.g. see [1] and [2]). For sparse coding, the currently known results about gradient-descent like algorithms assume incoherence of the ground truth matrix, as well as the *iterates* of the algorithm (e.g. see [3] and [4]). It is clear that the iterates will not be incoherent in our setup due to the existence of near-duplicates -- so such techniques seem difficult to generalize.\n\nWe agree that a study of deep generative models would be very interesting, but we see this work as a necessary prerequisite. By focusing on the simplest linear (sparse coding) and non-linear (noisy-OR) models in which the beneficial effect of overparameterization manifests, it allowed us to determine precisely how variations in the parameters of the ground-truth model and algorithms affect it.\n\nThere are several key challenges in moving to deep generative models: \n1. Even basic questions of identifiability are not well understood. Specifically, for parameter recovery to even make sense, the underlying generative process (i.e., the parameters for the p(z,x) distribution) has to be identifiable from the marginal distribution p(x). Results in this vein are known for sparse coding and noisy-OR networks, but not for deep generative models; note that the recent papers on learning disentangled representations do not include synthetic experiments where data is drawn from a deep generative model and the resulting model is shown to be \u201crecovered\u201d.\n2. Depending on the architecture, there are many different ways to overparametrize a deeper model. (One could overparametrize in terms of depth, width, in some structurally constrained way, etc.) \n3. Designing filtering/variable extraction steps to recover the ground truth variables is entirely unclear. It\u2019s likely that the outcome of the experiment will vary significantly depending on the implementation of this step, and the set of potential choices is vast.\n\n[1] Jernite, Yacine, Yonatan Halpern, and David Sontag. \"Discovering hidden variables in noisy-or networks using quartet tests.\" In Advances in Neural Information Processing Systems, pp. 2355-2363. 2013.\n\n[2] Arora, Sanjeev, Rong Ge, Tengyu Ma, and Andrej Risteski. \"Provable learning of noisy-or networks.\" In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1057-1066. ACM, 2017.\n\n[3] Arora, Sanjeev, Rong Ge, Tengyu Ma, and Ankur Moitra. \"Simple, efficient, and neural algorithms for sparse coding.\" (2015).\n\n[4] Chatterji, Niladri, and Peter L. Bartlett. \"Alternating minimization for dictionary learning: Local Convergence Guarantees.\"  arXiv:1711.03634\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg0_eHtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference/Paper2417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2417/Reviewers", "ICLR.cc/2020/Conference/Paper2417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2417/Authors|ICLR.cc/2020/Conference/Paper2417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141675, "tmdate": 1576860542543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference/Paper2417/Reviewers", "ICLR.cc/2020/Conference/Paper2417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2417/-/Official_Comment"}}}, {"id": "HygbeZDhsS", "original": null, "number": 2, "cdate": 1573839080918, "ddate": null, "tcdate": 1573839080918, "tmdate": 1573839080918, "tddate": null, "forum": "rkg0_eHtDr", "replyto": "HJeivC_0YH", "invitation": "ICLR.cc/2020/Conference/Paper2417/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your comments. We reworded the \u201cmaking precise\u201d sentence to now read, \u201ca controlled empirical study that measures and disentangles the benefits of overparameterization in unsupervised learning settings.\u201d\n\nAbout the lack of precise mathematical statements: while it would be great to have theory accompanying our empirical observations, we note that theoretical analysis for our settings is likely to be very non-trivial given our current understanding of the optimization for these latent-variable models. For noisy-OR networks, the currently known algorithms with provable guarantees use tensor-based techniques and are very different from the gradient-descent algorithm used by us (e.g. see [1] and [2]). For sparse coding, the currently known results about gradient-descent like algorithms assume incoherence of the ground truth matrix, as well as the *iterates* of the algorithm (e.g. see [3] and [4]). It is clear that the iterates will not be incoherent in our setup due to the existence of near-duplicates -- so such techniques seem difficult to generalize.\n\n[1] Jernite, Yacine, Yonatan Halpern, and David Sontag. \"Discovering hidden variables in noisy-or networks using quartet tests.\" In Advances in Neural Information Processing Systems, pp. 2355-2363. 2013.\n\n[2] Arora, Sanjeev, Rong Ge, Tengyu Ma, and Andrej Risteski. \"Provable learning of noisy-or networks.\" In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1057-1066. ACM, 2017.\n\n[3] Arora, Sanjeev, Rong Ge, Tengyu Ma, and Ankur Moitra. \"Simple, efficient, and neural algorithms for sparse coding.\" (2015).\n\n[4] Chatterji, Niladri, and Peter L. Bartlett. \"Alternating minimization for dictionary learning: Local Convergence Guarantees.\"  arXiv:1711.03634"}, "signatures": ["ICLR.cc/2020/Conference/Paper2417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg0_eHtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference/Paper2417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2417/Reviewers", "ICLR.cc/2020/Conference/Paper2417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2417/Authors|ICLR.cc/2020/Conference/Paper2417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141675, "tmdate": 1576860542543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference/Paper2417/Reviewers", "ICLR.cc/2020/Conference/Paper2417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2417/-/Official_Comment"}}}, {"id": "B1e0qeD2sS", "original": null, "number": 1, "cdate": 1573838998113, "ddate": null, "tcdate": 1573838998113, "tmdate": 1573838998113, "tddate": null, "forum": "rkg0_eHtDr", "replyto": "HJledmXCtr", "invitation": "ICLR.cc/2020/Conference/Paper2417/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your comments!\n\nWe agree that a study of deep generative models would be very interesting, but we see this work as a necessary prerequisite. By focusing on the simplest linear (sparse coding) and non-linear (noisy-OR) models in which the beneficial effect of overparameterization manifests, it allowed us to determine precisely how variations in the parameters of the ground-truth model and algorithms affect it.\n\nThere are several key challenges in moving to deep generative models: \n1. Even basic questions of identifiability are not well understood. Specifically, for parameter recovery to even make sense, the underlying generative process (i.e., the parameters for the p(z,x) distribution) has to be identifiable from the marginal distribution p(x). Results in this vein are known for sparse coding and noisy-OR networks, but not for deep generative models; note that the recent papers on learning disentangled representations do not include synthetic experiments where data is drawn from a deep generative model and the resulting model is shown to be \u201crecovered\u201d.\n2. Depending on the architecture, there are many different ways to overparametrize a deeper model. (One could overparametrize in terms of depth, width, in some structurally constrained way, etc.) \n3. Designing filtering/variable extraction steps to recover the ground truth variables is entirely unclear. It\u2019s likely that the outcome of the experiment will vary significantly depending on the implementation of this step, and the set of potential choices is vast.\n\nOn theory: while it would be great to have theory accompanying our empirical observations, we note that theoretical analysis for our settings is likely to be very non-trivial given our current understanding of the optimization for these latent-variable models. For noisy-OR networks, the currently known algorithms with provable guarantees use tensor-based techniques and are very different from the gradient-descent algorithm used by us (e.g. see [1] and [2]). For sparse coding, the currently known results about gradient-descent like algorithms assume incoherence of the ground truth matrix, as well as the *iterates* of the algorithm (e.g. see [3] and [4]). It is clear that the iterates will not be incoherent in our setup due to the existence of near-duplicates -- so such techniques seem difficult to generalize.\n\n[1] Jernite, Yacine, Yonatan Halpern, and David Sontag. \"Discovering hidden variables in noisy-or networks using quartet tests.\" In Advances in Neural Information Processing Systems, pp. 2355-2363. 2013.\n\n[2] Arora, Sanjeev, Rong Ge, Tengyu Ma, and Andrej Risteski. \"Provable learning of noisy-or networks.\" In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 1057-1066. ACM, 2017.\n\n[3] Arora, Sanjeev, Rong Ge, Tengyu Ma, and Ankur Moitra. \"Simple, efficient, and neural algorithms for sparse coding.\" (2015).\n\n[4] Chatterji, Niladri, and Peter L. Bartlett. \"Alternating minimization for dictionary learning: Local Convergence Guarantees.\"  arXiv:1711.03634"}, "signatures": ["ICLR.cc/2020/Conference/Paper2417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkg0_eHtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference/Paper2417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2417/Reviewers", "ICLR.cc/2020/Conference/Paper2417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2417/Authors|ICLR.cc/2020/Conference/Paper2417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504141675, "tmdate": 1576860542543, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2417/Authors", "ICLR.cc/2020/Conference/Paper2417/Reviewers", "ICLR.cc/2020/Conference/Paper2417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2417/-/Official_Comment"}}}, {"id": "HJledmXCtr", "original": null, "number": 1, "cdate": 1571857256155, "ddate": null, "tcdate": 1571857256155, "tmdate": 1572972340903, "tddate": null, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "invitation": "ICLR.cc/2020/Conference/Paper2417/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates benefit of over-parameterization for latent variable generative model while existing researches typically focus on supervised learning settings. It is experimentally shown that the over-parameterization helps to obtain better optimization, but too much over-parameterization gives performance deterioration. In the numerical experiments, the effect of over-parameterization is investigated from several aspects.\n\nThe motivation of this paper is interesting. The writing of the paper is clear, and I could follow the contents easily.\n\nOn the other hand, I have the following concerns on the significance of the paper.\n- All datasets investigated in this paper are rather small. If there were thorough investigations on more modern deep generative models, then the paper would be stronger. For example, the latent variable model is recently well discussed in the context of disentanglement representation. The generative models to obtain disentanglement representation could be investigated in the frame-work of this paper.\n- This is an empirical study, but if there was theory to support the empirical observations, then the paper was more convincing. The problem itself is just a sparse coding problem. Hence, I think what investigated in this paper can be discussed by relating sparse coding theories. However, there is no theoretical justification on the experimental results.\n- Summarizing the above arguments, the insight obtained in this paper is a bit weak. More ablation study and more experiments on general models will clarify what is going on in the over-parameterized model for latent generative models."}, "signatures": ["ICLR.cc/2020/Conference/Paper2417/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2417/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575658954556, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2417/Reviewers"], "noninvitees": [], "tcdate": 1570237723117, "tmdate": 1575658954570, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2417/-/Official_Review"}}}, {"id": "HJeivC_0YH", "original": null, "number": 2, "cdate": 1571880547443, "ddate": null, "tcdate": 1571880547443, "tmdate": 1572972340858, "tddate": null, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "invitation": "ICLR.cc/2020/Conference/Paper2417/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The paper \u201caims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. \u201d The author\u2019s empirical study is comprehensive, and to my knowledge the most detailed published work on this to date. Specifically, the authors empirically study \n- the ability of networks to recover latent variables\n- the effects of extreme overparameterization\n- the effects the training method (e.g. batch size)\n- latent variable stability over the course of training\n\nIn line with the findings for supervised settings, the authors find that overparameterization is often beneficial, and that overfitting is a surprisingly small issue. This is an interesting and useful observation, particularly since it at first sight appears to be in disagreement with some earlier work (the authors suggest explanations for the differing observations). \n\nAs the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. \n\nThe authors perform studies on a range of different real-world and synthetic datasets. \n\nThe paper is well-written, well-structured, and easy to follow. Relevant literature has been cited. The appendices contain a wealth of details that will make this work reproducible. \n\nDecision: weak accept. The paper contains some new insights, but its contributions are not quite as substantial (e.g. lack of precise mathematical statements) or surprising as those in stronger ICLR papers. \n\nA small gripe: the authors promise \u201c a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings\u201d. I would argue that \u201cmaking precise\u201d is too strong for what the paper actually delivers. I suggest rewording this."}, "signatures": ["ICLR.cc/2020/Conference/Paper2417/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2417/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rbuhai@mit.edu", "aristesk@andrew.cmu.edu", "yhalpern@google.com", "dsontag@csail.mit.edu"], "title": "Benefits of Overparameterization in Single-Layer Latent Variable Generative Models", "authors": ["Rares-Darius Buhai", "Andrej Risteski", "Yoni Halpern", "David Sontag"], "pdf": "/pdf/5a9b3f9bba59f819e208f1c030b90ee105134505.pdf", "TL;DR": "Overparameterization aids parameter recovery in unsupervised settings.", "abstract": "One of the most surprising and exciting discoveries in supervising learning was the benefit of overparameterization (i.e. training a very large model) to improving the optimization landscape of a problem, with minimal effect on statistical performance (i.e. generalization). In contrast, unsupervised settings have been under-explored, despite the fact that it has been observed that overparameterization can be helpful as early as Dasgupta & Schulman (2007). In this paper, we perform an exhaustive study of different aspects of overparameterization in unsupervised learning via synthetic and semi-synthetic experiments. We discuss benefits to different metrics of success (recovering the parameters of the ground-truth model, held-out log-likelihood), sensitivity to variations of the training algorithm, and behavior as the amount of overparameterization increases. We find that, when learning using  methods such as variational inference,  larger models can significantly increase the number of ground truth latent variables recovered.", "code": "https://drive.google.com/file/d/1bKia5vceblhQuggssScteiUR_QfoVzmu/view?usp=sharing", "keywords": ["overparameterization", "unsupervised", "parameter recovery", "rigorous experiments"], "paperhash": "buhai|benefits_of_overparameterization_in_singlelayer_latent_variable_generative_models", "original_pdf": "/attachment/87dd82aaa2efcbac886127e25409c98d5c316021.pdf", "_bibtex": "@misc{\nbuhai2020benefits,\ntitle={Benefits of Overparameterization in Single-Layer Latent Variable Generative Models},\nauthor={Rares-Darius Buhai and Andrej Risteski and Yoni Halpern and David Sontag},\nyear={2020},\nurl={https://openreview.net/forum?id=rkg0_eHtDr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkg0_eHtDr", "replyto": "rkg0_eHtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575658954556, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2417/Reviewers"], "noninvitees": [], "tcdate": 1570237723117, "tmdate": 1575658954570, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2417/-/Official_Review"}}}], "count": 8}