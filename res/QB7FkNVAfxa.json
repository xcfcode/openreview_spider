{"notes": [{"id": "QB7FkNVAfxa", "original": "dVgzX7M0MhF", "number": 3558, "cdate": 1601308395310, "ddate": null, "tcdate": 1601308395310, "tmdate": 1614985773596, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SonXgBfOmfL", "original": null, "number": 1, "cdate": 1610040357853, "ddate": null, "tcdate": 1610040357853, "tmdate": 1610473947706, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "The authors provide a new analysis of learning of two-layer linear networks with gradient flow, leading to some novel optimization and generalization guarantees incorporating a notion of the imbalance in the weights.  While there was some diversity of opinion, the prevailing view was that the results were not sufficiently significant for publication in ICLR."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040357839, "tmdate": 1610473947686, "id": "ICLR.cc/2021/Conference/Paper3558/-/Decision"}}}, {"id": "1WATnh7UIOg", "original": null, "number": 3, "cdate": 1603867348563, "ddate": null, "tcdate": 1603867348563, "tmdate": 1607196810234, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review", "content": {"title": "concerns about significance", "review": "This paper studies the optimization and generalization properties of a two-layer linear network. The considered setting is over-parameterized linear regression where the input dimension is D, number of samples is n<D, and the target dimension is m. The hidden width is h. The paper has two main results. The first result is exponential convergence of gradient flow to global minimum, where the convergence rate depends on the (m+n-1)-th singular value of an \"imbalance\" matrix. The second result shows that the solution found is close to the minimum L2 norm solution if certain orthogonality assumption is approximately satisfied at initially; then it was shown that if the width h is sufficiently large, then under a random initialization scheme, the solution found is close to the minimum L2 norm solution with a distance $1/\\sqrt{h}$.\n\npros:\nThe results are not previously known to my knowledge. The proofs appear to be correct as far as I can tell.\n\ncons:\nMy overall concern is the significance of the results. The results, while correct, do not contribute much to our understandings of optimization and generalization in deep learning. The ways in which the authors interpret the results are unsatisfactory or even misleading.\n\n1) Thm 1 shows a convergence rate of $e^{-ct}$, where $c$ is the (m+n-1)-th singular value of an imbalance matrix. On the appearance this result seems to suggest that a larger $c$ is beneficial for convergence. However I believe this suggestion is incorrect and can be very misleading. Indeed, previous work (e.g. Arora et al. 2018a) has shown linear convergence under zero imbalance ($c=0$), as cited in the paper, but Thm 1 fails to capture that. I think in general this $e^{-ct}$ is a very loose bound that does not capture the real convergence rate (unless the authors can provide convincing evidence that suggests otherwise).\n\nThat said, I do think Thm 1 is an interesting theoretical result and the proof is clever. I'm concerned about the practical relevance and the possibly misleading message it sends.\n\nAnother weakness is that Thm 1 only considers gradient flow but not gradient descent. \n\n2) Thm 2 and its interpretations are unsatisfactory in a number of ways.\n\nFirst, we know that just doing a normal linear regression using gradient descent (starting from 0) leads to the minimum L2 norm solution. So now we go through all the trouble in the 2-layer net and finally show we can find a solution that's almost as good as linear regression -- what's the point of doing that?\nOf course, one may argue that we are studying a toy model in order to better understand deep learning. However, the main message from this result can be also conveyed in linear regression -- as shown in Sec 4.1, the main step is to find an invariant manifold for gradient flow such that the minimizer in that manifold must be the min-norm solution; for linear regression, such manifold also exists, which is just the span of the data points. \n\nSecond, the initialization used ($1/h$ variance in both layers) is unconventional. It's different from the standard 1/fan_in initialization or the NTK parameterization. What happens if we use those more standard initializations? And what happens if we make the initialization smaller, e.g. $1/h^2$, or $1/h^{100}$? Would those change the result? The scale of the initialization is very important in this line of work (such as NTK), so this should be addressed clearly.\n(The authors actually claim that as $h\\to\\infty$ we would get the NTK solution, following Jacot et al (on page 7). I actually don't think Jacot et al.'s work directly implies this, because this paper uses a different initialization scale.)\n\nThird, the authors try to differ this result from all the NTK results, but the theorem is exactly showing that the final solution is close to the NTK solution. Isn't this a bit ironic?\n\nFourth, the authors claim \"this is the first non-asymptotic bound regarding the generalization of linear networks in the global sense.\" Maybe check out these papers:\nImplicit Bias of Gradient Descent on Linear Convolutional Networks,\nImplicit Regularization in Matrix Factorization.\nAlso, many NTK papers also have non-asymptotic bounds. For 2-layer linear networks, one should be able to easily get a bound on the distance of the learned model and the min-norm solution -- might be better than Thm 2.\n\n\n-------- after rebuttal --------\n\nThanks to the authors for the response and the updated manuscript. My assessment stays the same, and below are my additional comments.\n\n1. About Appendix E\n\nThanks for the clarification about the initialization scaling. However, this raises more concern about the significance of the result. In Appendex E, it is shown that the scaling considered in the paper and the NTK scaling lead to the **same** gradient flow dynamics. This suggests that we are actually still in the kernel regime, in contrary to the main motivation and the claims in the paper. (As for the time rescaling issue, it doesn't matter in gradient flow since the difference can be absorbed by rescaling the learning rates.)\n\nAppendix E also mentions that several previous papers used a small multiplier $\\kappa$ to make the initial network small. The authors claim that this makes the convergence rate slower. I don't think this affects the convergence rate, but it only affects the width requirement (see e.g. [1]). In fact, in stead of using this multiplier, there is another way to make the output zero without changing the NTK and without requiring a larger width, that is to use an anti-symmetric initialization -- see e.g. [2][3][4][5].\n\n[1] Arora et al. Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks\n\n[2] Chizat et al. On lazy training in differentiable programming\n\n[3] Hu et al.  Simple and effective regularization methods for training on noisily labeled data with generalization guarantee\n\n[4] Bai and Lee. Beyond linearization: On quadratic and higher-order approximation of wide neural networks\n\n[5] Zhang et al. A type of generalization error induced by initialization in deep neural networks\n\n2. About the motivating questions\n\nThis paper proposes to answer two questions in the introduction. The first question is \"Is the kernel regime, which requires impractical bounds on the network width, necessary to achieve good generalization?\" First, I don't think this paper answers this question since the considered regime is still basically the same as the kernel regime. Second, even if it does, this question itself is not valid, since there are numerous previous theoretical works that study generalization outside the kernel regime, in more interesting settings, e.g. [6][7][8][9] and many more (none of which are mentioned in the paper).\n\n[6] Allen-Zhu and Li. Backward Feature Correction: How Deep Learning Performs Deep Learning\n\n[7] Allen-Zhu and Li. What Can ResNet Learn Efficiently, Going Beyond Kernels?\n\n[8] Wei et al. Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel\n\n[9] Woodworth et al. Kernel and Rich Regimes in Overparametrized Models.\n\nThe second main question in the introduction is \"Does generalization depends explicitly on acceleration? Or is acceleration required only due to the choosing an initialization outside the good generalization manifold?\" I genuinely cannot understand this question.\n\n3. In the updated manuscript the authors state \"To the best of our knowledge, this is the first non-asymptotic bound regarding the generalization property of wide linear networks under random initialization in the global sense.\" This is still false (and insignificant) since the stated result is a direct consequence of previous NTK work.\n\n4. I certainly understand that understanding deep learning is very challenging so it's a natural step to start with simple models. However I think this paper in its current form has limited significance and has major issues in how it discusses previous work, main motivations and contributions, etc., for reasons described in the review.", "rating": "3: Clear rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073770, "tmdate": 1606915760605, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3558/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review"}}}, {"id": "d-J1FoJHzVf", "original": null, "number": 1, "cdate": 1603669244619, "ddate": null, "tcdate": 1603669244619, "tmdate": 1606406004069, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review", "content": {"title": "Good result", "review": "This paper analyzes the convergence of gradient descent optimizing overparametrized linear nn, and proves a exponential convergence rate. Moreover, the paper proposes the distance of the optimizer to the smallest norm solution, which is justified in other papers such as Montanari, etc. as the generalizable solution. Thus the solution that SGD outputs has good generalization as well.\nI believe overall the result is good, and the points are stated clearly. I have the following suggestions:\n\n1. If the space is allowed in appendix, the algebra proving that (9) is time invariant can be provided, rather than \"one can easily check\". This intermediate step is critical for the full proof so I'd like to check it. (with this I can raise score to 6)\n2. A sketch of Montanari paper about the property of $\\hat \\Theta$ can be discussed in the appendix. \n3. Regarding the thm, it would be definitely sufficient for the conference if anything can be suggested with RELU activation. In NTK work that's just another kernel so it's easy to extend, but it might be hard here, I'm not sure.\n4. More literature review. I think Rong Ge has some papers about the landscape of matrix factorization problem so it's great to compare with them in detail, even if in appendix.\n5. In appendix, it's also great to prove that under a certain initialization, what is the expectation/value of high probability of the imbalance singular value.\n6. How does amount of data affect generalization bound? I think it's 1/\\sqrt{n} in NTK work, any a similar behavior here?", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073770, "tmdate": 1606915760605, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3558/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review"}}}, {"id": "oNV9B6-aseS", "original": null, "number": 8, "cdate": 1606302100217, "ddate": null, "tcdate": 1606302100217, "tmdate": 1606306752024, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "d-J1FoJHzVf", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for your comments on our paper and suggestions. Below are our response to your comments:\n\n1) **Imbalance matrix and its singular values**:\n\nWe greatly appreciate your suggestions and we have added the proof right after we state that the imbalance matrix is time-invariant in Section 3.2.\n\nRegarding the singular values of the imbalance matrix, we have added the result in Appendix F. We show that when the entries are initialized with $\\mathcal N(0,h^{-1})$ (setting $\\alpha=1/2$ in Claim F.1) all non-zero singular values of the imbalance matrix concentrate to 1 as $h$ increases.\n\n2) **Property of min-norm solution and generalization bound**:\n\nThank you for your comment. In our paper, we focus on the implicit regularization of overparametrized networks as the generalization property of our interest. We are interested in how random initialization and overparametrization leads to a regularized solution, close to the minimum norm,  rather than asking how good such a regularized solution is. Making statements on the generalization properties of the minimum norm solution $\\hat{\\Theta}$ further requires making additional assumptions on the data, that go beyond qualitative conditions such as rank. \nWe thus think that a sketch of other papers on the property of min-norm solution $\\hat{\\Theta}$, which will require an additional set of assumptions, may not be beneficial to the readers. \n\nFor the same reason, our analysis on implicit regularization does not directly suggest a generalization bound. But we also agree that it would be interesting to derive generalization bounds for wide networks given additional assumptions on the data.\n\n3) **Extension to nonlinear networks**: \n\nIndeed, understanding the convergence and generalization properties of networks with nonlinear activation will be of most practical value. However, the case of linear networks has not been fully understood yet. For example, the fact that the imbalance contributes to the exponential convergence has not been shown previously. We believe that we need a deep understanding of simple models, that allow for tighter analyses and counterfactual thinking, in order to thoroughly understand more general cases, with non-linear activations, that are used in deep learning. \n\nReviewer 1 also asked about the challenges in extending our analysis to ReLU networks. We reiterate next for convenience. \nFor the convergence analysis, if the activation is a ReLU, the diagonal terms of the imbalance matrix will be preserved (Du et al. 2018) under differential inclusion. There is still invariance in the imbalance matrix but it is unclear how it contributes to the convergence of the learning dynamics. For the generalization property/implicit regularization, we believe the key challenge would be to identifying the good manifold in terms of generalization, given certain data distribution assumptions.\n\n4) **Comparison with other literature**:\n\nThank you for the comment. We carefully read several of Rong's papers on matrix/tensor factorization which we think could be related. But we could not see a direct relationship that merited a detailed comparison. Those papers have settings that are substantially different from ours, for example, they either do not consider a growing overparametrization via width $h$, or they consider different training procedures than gradient flow/descent, thus we don't think a meaningful comparison is possible.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QB7FkNVAfxa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3558/Authors|ICLR.cc/2021/Conference/Paper3558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836317, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment"}}}, {"id": "LyVbn7-fAd", "original": null, "number": 7, "cdate": 1606301863628, "ddate": null, "tcdate": 1606301863628, "tmdate": 1606306719126, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "GtYYaSorf9", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for your comments on our paper. We are glad you find our results interesting and novel. Below are our responses to your comments:\n\n1) **Dependence on the input dimension**:\n\nSince we use the basic random matrix theory to prove our concentration result, the input dimension naturally arises as we study the singular values of matrix $\\begin{bmatrix}V^T &U^T\\end{bmatrix}$. It would be interesting to see whether such dependence is necessary, and we think the answer will be clear once we understand the concentration of the imbalance matrix and other matrices of interest in our analysis, such as $U_1U_2^T,VU_2^T$. \n\nAs for the comparison with previous works, we have not seen previously a non-asymptotic bound between the wide linear network to the min-norm solution, to our best knowledge. Therefore there might not be a direct comparison. \n\n2) **Noisy Gradient Descent**:\n\nThank you for your question. We believe a similar analysis would work for gradient descent and stochastic gradient descent will sufficiently small step size. While there are additional challenges when moving to discrete-time analysis, we are working in that direction. However, our intuition in this regard suggests that noise will require a faster rate of convergence to counteract/limit the drifting that would move the trajectories away from the \"good\" manifold.\n\n3) **Extension to nonlinear networks**:\n\nFor the convergence analysis, if the activation is a ReLU, the diagonal terms of the imbalance matrix will be preserved (Du et al. 2018) under differential inclusion. There is still invariance in the imbalance matrix but it is unclear how it contributes to the convergence of the learning dynamics.\n\nFor the generalization property/implicit regularization, we believe the key challenge would be to identifying the good manifold in terms of generalization, given certain data distribution assumptions.\n\n\n**References**:\n\nSimon S Du, Wei Hu, and Jason D Lee. \"Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced\". In Advances in Neural Information Processing Systems, 2018."}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QB7FkNVAfxa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3558/Authors|ICLR.cc/2021/Conference/Paper3558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836317, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment"}}}, {"id": "2n9gZpZeRwt", "original": null, "number": 6, "cdate": 1606301648019, "ddate": null, "tcdate": 1606301648019, "tmdate": 1606306679388, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "gWg7rTnG3KF", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (Part 2)", "comment": "3) **Comparison with NTK and the scale of initialization**:\n\nThis comment has led to new results and a deeper understanding. We greatly thank the reviewer for this. \n\nWe believe there is nothing wrong with being unconventional. Our initialization is indeed different from the one used in NTK, but it is arguably better as it achieves the same limiting end-to-end function, but at a faster convergence rate, as our analysis shows. To clarify this issue, we added a detailed comparison of our problem setting with previous works on NTK analysis (See Appendix E.). In particular,  we show that one can relate our model assumptions to the NTK ones by rescaling the parameters and time. This time scaling leads to a slower convergence rate. We also note that our result does not rely on studying the tangent kernel of the network, hence there is a significant difference between our approach and the NTK one.\n\nMoreover, we now prove thm 2 in a more general setting where the variance for the entries of $U,V$ is $h^{-2\\alpha}$, where $1/4<\\alpha\\leq 1/2$. The case $\\alpha=1/2$, i.e. the variance is $h^{-1}$, is a particular case we consider in the main paper. Finally, we note that our analysis can not make the variance smaller, i.e. $\\alpha>1/2$, because the imbalance singular value is vanishingly small as $h$ increases. Please see Appendix F. for the general result for different initialization scale.\n\n4) **Wrong claim in the contribution**:\n\nWe apologize for our misleading phrase. What we intended to say is \"To the best of our knowledge, this is the first non-asymptotic bound regarding the generalization property of wide linear networks under random initialization in the global sense.\" We have modified the text accordingly. Regarding the non-asymptotic bound from NTK papers, we believe we properly cited related papers (Arora et al., 2019b; Buchanan et al., 2020). \n\n**References**:\n\nSanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. \"A convergence analysis of gradient descent for deep linear neural networks\". In International Conference on Learning Representations, 2018a.\n\nSanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.\n\"On exact computation with an infinitely wide neural net\". In Advances in Neural Information\nProcessing Systems, pp. 8141\u20138150, 2019b.\n\nSam Buchanan, Dar Gilboa, and John Wright. \"Deep networks and the multiple manifold problem\".\narXiv preprint arXiv:2008.11245, 2020.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QB7FkNVAfxa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3558/Authors|ICLR.cc/2021/Conference/Paper3558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836317, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment"}}}, {"id": "gWg7rTnG3KF", "original": null, "number": 5, "cdate": 1606301523059, "ddate": null, "tcdate": 1606301523059, "tmdate": 1606306639577, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "1WATnh7UIOg", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (Part 1)", "comment": "Thank you for your comments on our paper. Below are our response to your comments:\n\n0) **Significance of our results**:\n\nWe respectfully disagree with the reviewer's comment about the significance of our work and we have provided a detailed response in the general comments. That being said, we have taken the reviewer's comment very seriously and modified the paper to better articulate the significance of our results. In addition, the reviewer's comments inspired extensions to our analysis that are now also included in appendices E and F. We thank the reviewer for this. Also, we are sorry the reviewer feels the interpretation of our results is misleading. We have thoroughly looked at our paper to identify statements that may have been construed as misleading, and we edited the paper accordingly. We hope the reviewer finds our modified statements concise and accurate.\n\n1) **Regarding the Convergence Result**:\n\nBoth our results and Arora's are sufficient conditions that are valid in complementary regimes. Therefore, one should not expect our results to capture Arora's and vice versa. Specifically, Our result is not showing that $e^{-ct}$ is a tight characterization of the convergence rate of overparametrized linear networks, we never stated this, and we don't think it is true. Rather, we show that the imbalance is a factor that contributes to the exponential convergence of such networks. In the paper, we do not suggest one should artificially make $c$ large for fast convergence, but rather, we show that random initialization together with overparametrization naturally satisfies the rank condition of the imbalance. And this is sufficient for us to further guarantee that the gradient flow stays close to the good generalization manifold.\n\nPrevious works (Arora et al. 2018a etc.) have shown very insightful results on the convergence rate when the imbalance is zero. We see our result as a complement to those works because we consider the case where the imbalance is non-zero. Arguably, as most bounds, including those obtained in the related literature, while sufficient, it is not necessary for exponential convergence. We point out, however, that most existing conditions for linear-networks, that are based on requiring an approximately balanced initialization are not satisfied with high probability under random initialization without having the variance of all the entries sufficiently small, which may lead to a poor rate of convergence. We have modified the comments after Theorem 1 to better reflect on our response above.\n\nWe also believe similar results can be derived for gradient descent with a sufficiently small step size (In this case the imbalance is not invariant but we should be able to bound its changes), and we are working in that direction.\n\n2) **Invariant manifold in the overparametrized setting**:\n\nWe respectfully disagree with the reviewer's comment questioning the necessity of studying the invariant manifold in the overparametrized setting. As we stated in the global response, fully understanding the simple overparametrized model could shed light on how to analyze more complex models. \nWe certainly agree that for the standard linear regression the \"good\" manifold reduces to the span of the data points. However, unlike the linear regression case,  there is not a clear data-agnostic way to initialize so as to guarantee fast convergence and proximity to the manifold in the overparamterized setting. This is because the zero initialization is in fact a stationary (saddle) point of the gradient flow. As a result, any initialization with small $||U||$ and $||V||$, will be slow to converge, even if it is within the manifold of interest. Our analysis explicitly provides the sufficient condition $V(0)U_2^T(0)=0,U_1(0)U_2^T(0)=0$ to ensure the proximity to the manifold during training. Moreover, we show for wide linear networks with the random initialization, this condition is approximately satisfied for the entire trajectory.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QB7FkNVAfxa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3558/Authors|ICLR.cc/2021/Conference/Paper3558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836317, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment"}}}, {"id": "aKEiI83kB_j", "original": null, "number": 4, "cdate": 1606301230668, "ddate": null, "tcdate": 1606301230668, "tmdate": 1606306601918, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "G4Sfk3Vvbgo", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for your comments on our paper. We are glad that you find our result interesting. Below are our response to your comments:\n\n1) **On the width requirement $h>n+m-1$**:\n\nThe reviewer raises an interesting point that we had discussed in the appendix, where we show that the width requirement can be relaxed depending on the rank of the data matrix X, which consistent with previous literature. Specifically, in Section 2 we show that when the data matrix $X$ has full rank and $n<D$, the width requirement is $h>n+m-1$. However, in appendix B we show that for rank deficient data matrix $X$ with $rank(X)=d\\leq n<D$, the width requirement becomes $h>d+m-1$. We apologize for not presenting the result in full generality in the main paper. For the works listed by the reviewer, the width requirement scales up as the rank of data matrix $X$ increases, but we note that there is hardly a direct comparison because of the different settings. We have refined the remarks after Theorem 1 to address your concern.\n\n2) **Over-determined linear regression case**\n\nCorrect. When the data matrix $X$ satisfies $n>D$, the regression problem is over-determined. Theorem 1 implies exponential convergence to a global minimum of the loss function, whose end-to-end function corresponds to the unique solution of the regression problem. However, this is not the regime we are interested in since, as the reviewer points out, in that case, the analysis is straightforward. \n\n3) **Numerical Verification and Conservativeness of our Imbalance Bound**:\n\nThank you for your suggestion. We added numerical validations for thm 1 and thm 2 in Appendix A. Indeed, the bound on the convergence rate based on the Imbalance is only an upper bound. The convergence rate does depend on several aspects of the initialization as the existing literature points out. Our imbalance bound is neither better nor worse than the previous bound. Rather it complements existing results by showing a different regime that ensures exponential convergence. Interestingly, this regime is in fact more aligned with standard practices in Deep Learning, as the alternative analysis requires approximate balancedness, which is not satisfied with high probability under random initialization. \n\n4) **Extension to multi-linear cases**:\n\nWe believe imbalance also plays a role in training multi-layer networks, but rigorously showing so is the subject of future research. In particular, the notion of imbalance for multi-layer networks does exist, but it is unclear under which conditions on the imbalance the exponential convergence is guaranteed. Moreover, similar to the single-hidden-layer case, we would like to find conditions that are naturally satisfied by random initialization along with overparametrization. Even if we had an answer to all these questions, we do not think we could concisely fit them with the existing results.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "QB7FkNVAfxa", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3558/Authors|ICLR.cc/2021/Conference/Paper3558/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923836317, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Comment"}}}, {"id": "G4Sfk3Vvbgo", "original": null, "number": 4, "cdate": 1603927782070, "ddate": null, "tcdate": 1603927782070, "tmdate": 1605023979045, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review", "content": {"title": "Interesting observation and theory, more detailed comparsions and some experiments are needed.", "review": "This paper proves the convergence rate of gradient flow for training two-layer linear networks. In particular, this paper discusses the connection between initialization, optimization, generalization, and overparameterization. The results show that gradient flow can converge to the global minimum at a rate depending on the level of imbalance of the initialization. Moreover, the authors show that random initialization and overparameterization can implicitly constrain the gradient flow trajectory to converge to a point lying in a low-dimensional manifold, thus guarantees good generalization ability.\n\nThis paper is well organized. It is interesting that sufficient imbalance can guarantee global convergence of two-layer linear networks while other papers may require nearly zero initialization or wide enough networks. Besides, my detailed comments are as follows.\n\nOne drawback is that this paper still requires that the width be greater than n+m-1 (Theorem 1), while the network width condition proved in some existing works (listed as follows) does not depend on the number of training examples n (although they require random or orthogonal initialization), the authors may need to comment their network width conditions after Theorem 1 (currently the authors only say that \u201cour results is not limited to extremely wide networks with random initialization\u2019\u2019).\n\n[1] Du, S. S., & Hu, W., Width provably matters in optimization for deep linear neural networks. arXiv preprint arXiv:1901.08572.\n\n[2] Hu, W., Xiao, L., & Pennington, J., Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks. In International Conference on Learning Representations.\n\n[3] Zou, D., Long, P. M., & Gu, Q., On the Global Convergence of Training Deep Linear ResNets. In International Conference on Learning Representations.\n\nThe authors prove that the limit of gradient flow can be sufficiently close to the minimum-norm solution if the neural network is sufficiently wide. This conclusion is good and of certain importance to understand the optimization path of training linear networks. However, if the data matrix X is of full rank and D<n, the training objective is strongly convex. In this case, there is only one minimum, thus the convergence result Theorem 1 can directly imply the parameter convergence results in Theorem 2. \n\nSome experiments may be needed to verify the theory. In particular, theorem 1 only provides an upper bound result, thus cannot fully characterize the effect of the imbalance on the convergence. The authors may try initializations with different imbalances and plot the convergence curves to demonstrate the results in Theorem 1. Additionally, results in Theorem 2 may also need to be verified in experiments.\n\nSo far I can only see that the imbalance plays an important role in training two-layer linear networks, can you extend this to multi-layer cases? Will the imbalance at the initialization together with sufficient overparameterization still guarantee the convergence of gradient flow?\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073770, "tmdate": 1606915760605, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3558/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review"}}}, {"id": "GtYYaSorf9", "original": null, "number": 2, "cdate": 1603796734111, "ddate": null, "tcdate": 1603796734111, "tmdate": 1605023978973, "tddate": null, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "invitation": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review", "content": {"title": "Comments to \"On the Explicit Role of Initialization on the Convergence  and Generalization Properties of Overparametrized Linear Networks\"", "review": "#### General Comments\nA proper initialization plays an important role in the success of over-parameterized models such as deep neural networks and high dimensional models.  However, the explicit role of initialization in theoretical results of an algorithm has not been stated well to my knowledge.  The main task of this paper is to  present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. Specially,  it is shown that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization.\nWith respect to linear networks, the paper makes the following three main contributions:\n\uff081\uff09 The role of initialization of the gradient flow on the convergence is characterized explicitly.  \n  (2) The stationary point of the gradient flow is sufficiently close to the min-norm solution in the linear case.\n\uff083\uff09 Random initialization for large wide  linear networks ensures that the dynamics of the network parameters \n      are constrained to a low-dimensional manifold. \nOverall, this is a written- well paper with significant novelty.  The results seem interesting in the deep learning theory literature. \n#### Specific Comments\n(1) For Theorem 2,  the network width is required to be a polynomial of the input dimension D, which may be loose in some practical network structures.  I wonder whether such constrain can be relaxed further? it will be better that some quantitative comparison with those related work is made. \n(2) When noisy gradient descent is considered,   is the current analysis  still applicable to the case and similar results can be derived?  \n(3) If an activation function is added such that the hypothesis class is nonlinear,  is the adopted analysis still valid? if not, what is the additional challenges? ", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3558/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3558/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks", "authorids": ["~Hancheng_Min1", "~Salma_Tarmoun1", "~Rene_Vidal1", "~Enrique_Mallada1"], "authors": ["Hancheng Min", "Salma Tarmoun", "Rene Vidal", "Enrique Mallada"], "keywords": [], "abstract": "Neural networks trained via gradient descent with random initialization and without any regularization enjoy good generalization performance in practice despite being highly overparametrized. A promising direction to explain this phenomenon is the \\emph{Neural Tangent Kernel} (NTK), which characterizes the implicit regularization effect of gradient flow/descent on infinitely wide neural networks with random initialization. However, a non-asymptotic analysis that connects generalization performance, initialization, and optimization for finite width networks remains elusive. In this paper, we present a novel analysis of overparametrized single-hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. We exploit the fact that gradient flow preserves a certain matrix that characterizes the \\emph{imbalance} of the network weights, to show that the squared loss converges exponentially at a rate that depends on the level of imbalance of the initialization. Such guarantees on the convergence rate allow us to show that large hidden layer width, together with (properly scaled) random initialization, implicitly constrains the dynamics of the network parameters to be close to a low-dimensional manifold. In turn,  minimizing the loss over this manifold leads to solutions with good generalization, which correspond to the min-norm solution in the linear case. Finally, we derive a novel $\\mathcal{O}( h^{-1/2})$ upper-bound on the operator norm distance between the trained network and the min-norm solution, where $h$ is the hidden layer width. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "min|on_the_explicit_role_of_initialization_on_the_convergence_and_generalization_properties_of_overparametrized_linear_networks", "pdf": "/pdf/d7f8acba87c54523c97e217baefbf15354be807d.pdf", "one-sentence_summary": "This paper studies the convergence and generalization property of single-hidden-layer linear networks", "supplementary_material": "/attachment/eaba17ffe2c1c428f7160fcc00dc7847449aef4e.zip", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=sB8TwRKBf4", "_bibtex": "@misc{\nmin2021on,\ntitle={On the Explicit Role of Initialization on the Convergence and Generalization Properties of Overparametrized Linear Networks},\nauthor={Hancheng Min and Salma Tarmoun and Rene Vidal and Enrique Mallada},\nyear={2021},\nurl={https://openreview.net/forum?id=QB7FkNVAfxa}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "QB7FkNVAfxa", "replyto": "QB7FkNVAfxa", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3558/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538073770, "tmdate": 1606915760605, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3558/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3558/-/Official_Review"}}}], "count": 11}