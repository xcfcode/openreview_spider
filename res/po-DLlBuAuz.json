{"notes": [{"id": "po-DLlBuAuz", "original": "Sr4T93jMd4v", "number": 216, "cdate": 1601308032651, "ddate": null, "tcdate": 1601308032651, "tmdate": 1616068261877, "tddate": null, "forum": "po-DLlBuAuz", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "fm9lufeU0m-", "original": null, "number": 1, "cdate": 1610040373680, "ddate": null, "tcdate": 1610040373680, "tmdate": 1610473965505, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper got a quite high disagreement in the scores from the reviewers. R2 voted for rejecting the paper as he did not see the connection of the algorithm to the continuation method and also that the continuation method does not address the distributional shift, which is one of the main problems for offlline RL. Yet, these concerns have been properly answered in the rebuttal of the authors and the distributional shift is also addressed by the continuation method by reducing the error in policy evaluation. Further concerns from the reviewers were raised in terms of related work to a similar algorithm (BRAC), which is also addressed in the revision of the paper. \n\nThe reviewers also identified the following strong points of the paper:\n- The algorithm is a simple and very effective adaptation to SAC \n- The presented results are exhaustive and convincing\n- The paper provides strong theoretical results for the presented algorithm\n- The authors did a very good job with their revision, adding more comparisons and ablation studies.\n\nI agree that this paper very interesting and recommend acceptance."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040373666, "tmdate": 1610473965488, "id": "ICLR.cc/2021/Conference/Paper216/-/Decision"}}}, {"id": "jiMEpu9fbd", "original": null, "number": 1, "cdate": 1603854645732, "ddate": null, "tcdate": 1603854645732, "tmdate": 1606757329675, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Review", "content": {"title": "Solid paper, but important novelty concerns", "review": "The authors propose a KL regularized approach for batch RL, where the importance of the KL term is reduced during learning. Theoretical guarantees are provided in the tabular domain. The algorithm is tested in several domains (MuJoCo, Atari, and a recommender task). \n\nStrengths:\n- Although the theoretical results are mostly straightforward extensions of existing results, they provide a solid backing to the method. In particular, Theorem 1 is an interesting result which motivates KL regularization in this setting. \n- The experimental section is very thorough. I would prefer more seeds but the coverage over many domains and behavior policies is convincing evidence for the empirical success of the method.\n- The appendix is very comprehensive. Although code is missing, the method and experiments are reproducible with the provided descriptions.\n- The writing was clear, and the method was well-motivated. Overall, the paper feels fairly complete and well polished. \n\nWeaknesses:\n- There is one very glaring weakness to this paper- the proposed KL regularized approach for offline RL already exists (Wu et al., 2019) & related (Jacques, et al., 2019). This is additionally problematic as both methods are not cited or discussed in the paper. To the best of my knowledge, the continuation aspect is still novel as well as the theoretical contributions. However, a discussion on (Wu et al., 2019) is necessary.\n- My first impression was that the variance solution to checkpointing $\\tau$ in Section 3.3 was somewhat \"hacky\". On second thought, however, as suggested by the authors in the introduction, gradually reducing $\\tau$ provides a mechanism for searching for the \"optimal\" value which trades between the constraint and learning on top of the proposed benefits of continuation for optimization. I think this is an interesting component of the method. On the plus side I think the checkpointing solves an important problem for batch RL, but on the downside, I think measuring the variance of the ensemble is not as well motivated as the rest of the method. I think the paper could benefit from additional discussion, or experiments which examine this aspect further. \n- While the method is the \"best\" over a wide range of domains, the performance benefit seems fairly incremental. Consequently, for most users it's unclear if the benefits are sufficiently significant to warrant the additional complexity.\n\nRecommendation:\n\nSo firstly, the novelty concerns absolutely need to be addressed and the mentioned papers cited/discussed in the paper. Regardless, I do feel like there is a meaningful contribution that builds on this prior work at both a theoretical and empirical level. As a result, I'm leaning on the side of accept.\n\nReferences\n- Wu, Yifan, et al. \"Behavior regularized offline reinforcement learning.\" 2019.\n- Jaques, Natasha, et al. \"Way off-policy batch deep reinforcement learning of implicit human preferences in dialog.\" 2019.\n\n**Post-Rebuttal\n\nThe authors have addressed most of my concerns. I have increased my score. Although the additional experiments on the variance/checkpointing are helpful I would still like to see more discussion in the paper itself. \n", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147933, "tmdate": 1606915765111, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper216/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Review"}}}, {"id": "ZE6Eo5GljNY", "original": null, "number": 6, "cdate": 1606123349275, "ddate": null, "tcdate": 1606123349275, "tmdate": 1606125509622, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "Wg1INsIFxPy", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment", "content": {"title": "Author Response to Reviewer 1", "comment": "We thank R1 for all the comments. Please also refer to the common response above for the answers about the prior work BRAC.\n\n$\\textbf{Q:}$ Theorem 1 seems to be a bit disconnected from the main contribution of the paper and it's hard to understand its role .... Could the convergence rate improvement be shown in the policy iteration case? \u2026 Does that mean theorem 1 consider the on-policy setting? In such a case what does \u03b2 mean?\n\n$\\textbf{A:}$ We would like to first acknowledge the gap between Theorem 1 and the Algorithm 1. Theorem 1 assumes tabular setting and access to exact/true gradient, while Algorithm 1 has to adapt to function approximation and offline setting with noisy gradient estimates. To the best of our knowledge, little has been proved on the convergence rate of policy gradient methods until very recently [1], which made the same assumption as ours. We prove that the soft policy iteration in Algorithm 1 optimizes the soft objective in Theorem 2 and 3, and leave the proof of its convergence rate as future work.  Several related works do suggest the possibility: [2] proves a precise equivalence between KL-regularized Q-learning and policy gradient; [3] utilizes visualization tools to show that in policy iteration, the loss landscape for the entropy regularized objective is promoting the global minimum while the vanilla objective function exerts a chaotic loss landscape\n\nTheorem 1 assumes access to the exact gradient with respect to the policy parameters. As a result the theorem does not necessarily imply an on-policy or off-policy setting.  $\\beta$ can be interpreted as \u201cprior policy\u201d in on-policy settings [2][4] or \u201cbehavior policy\u201d in off-policy settings. \n\n\n$\\textbf{Q:}$ In general it will be better to compare with these more recent baselines (especially BRAC since it's very related.)\n\n$\\textbf{A:}$ The original submission included many recent baselines, including ABM, CRR (which outperforms BEAR and BCQ on Mujoco), and discrete BCQ (which outperforms REM on Atari). Upon the reviewer\u2019s suggestion, we add the comparison with BRAC (including both fixed regularization weight and adaptive regularization weight) and CQL[5] in experiment section 4.1.\n\nAs observed in [5], CQL performs better than or comparable with BRAC and BEAR on Mujoco datasets. On datasets with behavior policy of reasonable quality (i.e. $\\alpha=0.2, 0.4, 0.6$), ours performs comparable or better than the baselines. With $\\alpha=0.2$, i.e., close to optimal behavior policy, all the methods perform similarly and one can achieve a good return by simply cloning the behavior policy. With $\\alpha=0.8$, i.e., low-quality behavior policy, there are few good trajectories in the dataset for any methods to learn. The advantage of our method is most obvious when $\\alpha=0.6$, as the dataset contains trajectories of both high and low cumulative rewards. Our method can learn from the good trajectories while at the same time deviate from the behavior policy to avoid those bad trajectories and achieve higher return.\n\n| env | Hopper | Hopper | Hopper |  Hopper | Cheetah| Cheetah | Cheetah|  Cheetah | Walker| Walker | Walker|  Walker |\n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------| -------- | --------| --------| --------|\n| $\\alpha$|0.2|0.4|0.6|0.8|0.2 |0.4 |0.6|0.8 |0.2|0.4|0.6|0.8|                             \n|BRAC |523 | 2 | 460|185|**2249**|**1922**|1410|582| 573| 1079| 70 | 6| \n|CQL  |1963 |1262 | 609|**296**|2106|1812|1157 |**606**|1385|980|219|3|          \n|Ours |**2097**|**1569**|**1648**|226|2145|1824|**1487**|547|**1441**|**1223**|**853**|**7**|   \n\nIn experiment section 4.2, we add the baseline CQL. As presented in [5], on the Atari dataset, CQL performs better than REM, and our method outperforms CQL in 7 out of 8 datasets. \n\n| env      |    Amidar   |    Asterix   |    Breakout   |    Enduro   |    MsPacman   |    Qbert   |    Seaquest   |    SpaceInvaders   |\n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------|\n|CQL   |145 | 2619 |**254**|207|2235|4095 | 4653| 494 |\n|Ours   |**175**|**3477**|199|**923**|**2494**|**4733**|**9935**|**1070**|\n\n\n[1] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. arXiv preprint arXiv:2005.06392, 2020.\n\n[2] Schulman, J., Chen, X., & Abbeel, P. (2017). Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440.\n\n[3] Bekci, R. Y., & G\u00fcm\u00fc\u015f, M. (2020). Visualizing the Loss Landscape of Actor Critic Methods with Applications in Inventory Optimization. arXiv preprint arXiv:2009.02391.\n\n[4] Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., & Schuurmans, D. (2019). Algaedice: Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074.\n\n[5] Kumar, A., Zhou, A., Tucker, G., & Levine, S. (2020). Conservative Q-Learning for Offline Reinforcement Learning. arXiv preprint arXiv:2006.04779.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "po-DLlBuAuz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper216/Authors|ICLR.cc/2021/Conference/Paper216/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment"}}}, {"id": "VY6StbsWgT", "original": null, "number": 5, "cdate": 1606122731937, "ddate": null, "tcdate": 1606122731937, "tmdate": 1606125481897, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment", "content": {"title": "Common Response and Summary of the Updates in the Paper (2/2)", "comment": "**Variance of value estimation in ensemble Q networks(R3, R4):**\n\nHigh Q estimation errors can mislead the policy updates. In our algorithm, we use the variance in the ensemble Q networks to detect high Q estimation error, preventing further annealing $\\tau$. The Q estimation error can be decomposed into two parts: the bias of the Q network and the variance of Q estimations. In our submission, we use variance in the Q ensemble to approximate the Q estimation variance. We conducted a separate set of experiments by training the critic networks with jackknife resampling to obtain the estimation variance and showed these two setups lead to very similar results.   \n\n- **train different critic networks with the same data**\n\nAs in Algorithm 1 line 8, the critic networks in the ensemble are updated with the same batch of data, the discrepancy in the value estimation comes from the different initializations. On state-action pairs frequently visited by the behavior policy, the impact of initialization diminishes as training goes on, resulting in small variance. On state-action pairs less frequently, the large variance in the Q estimation persists. Similar to [3], the discrepancy among different networks is a proxy for the inverse visitation count of the state-action pair.\n\nWe empirically validate that the variance in the Q ensemble is a good proxy for the Q estimation error in Appendix D to motivate its usage as the stopping criterion. For each state-action pair in the grid world environment, we visualize the variance in the Q ensemble $var(Q_{\\phi^{(1)}}(s,a), Q_{\\phi^{(2)}}(s,a), \\cdots, Q_{\\phi^{(K)}}(s,a))$ and the error in Q estimation $\\frac{1}{K}\\sum_{k=1}^K Q_{\\phi^{(k)}}(s,a) - \\tilde{Q}^{\\pi, \\tau}(s,a)$, and show these two quantities are highly correlated. If the learned policy $\\pi$ tends to select the state-action pairs with high variance in Q estimation, the error in the Q estimation on these state-action pairs will propagate to the other state-action pairs and mislead the policy update. We thus measure $E_{a\\sim\\pi} [var(Q_{\\phi^{(1)}}(s,a), Q_{\\phi^{(2)}}(s,a), \\cdots, Q_{\\phi^{(K)}}(s,a))]$ as the criteria to stop further annealing $\\tau$. \n\n- **train different critic networks with different data**\n\nA more standard approach to estimate the variance is through data perturbation. We added experiments with training the critic network with delete-d jackknife resampling. To train each of four critic networks, we leave out $\\frac{1}{4}$ data samples in the dataset (Note that the target value in the Q update is still averaged across the four critics. We found it to be important to the performance of the learned policy). \nAs shown below, the empirical results with critics trained on different data are similar to the ones we reported in the submission for Mujoco and Atari. The variances measured in these two approaches are quite similar as well. \n\n | env | Hopper | Hopper | Hopper |  Hopper | Cheetah| Cheetah | Cheetah|  Cheetah | Walker| Walker | Walker|  Walker |\n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------| -------- | --------| --------| --------|\n| $\\alpha$ |  0.2 | 0.4 | 0.6 | 0.8 | 0.2 | 0.4 | 0.6 | 0.8 | 0.2 | 0.4 | 0.6 | 0.8 |\n| Same |  2097 | 1569 | 1648 | 226 | 2145 | 1824 | 1487 | 547 | 1441 | 1223 | 853 | 7|\n| Jackknife| 2072  | 1718 | 1404 | 230 | 2206  | 1840 | 1529 | 589 | 1462 | 1078 | 920 | 8 |\n\n| env |    Amidar   |    Asterix   |    Breakout   |    Enduro   |    MsPacman   |    Qbert   |    Seaquest   |    SpaceInvaders   |\n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------|\n|Same        |      175      |     3477     |       199         |      923      |         2494        |     4733   |        9935      |            1070            |\n|Jackknife   |      171      |     3890     |       217        |       908      |         2305        |     5389   |       9636      |           914              |   \n\n \n[1] Wu, Y., Tucker, G., & Nachum, O. (2019). Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361.\n\n[2] Jaques, N., Ghandeharioun, A., Shen, J. H., Ferguson, C., Lapedriza, A., Jones, N., ... & Picard, R. (2019). Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456.\n\n[3] Burda, Y., Edwards, H., Storkey, A., & Klimov, O. (2018). Exploration by random network distillation. arXiv preprint arXiv:1810.12894."}, "signatures": ["ICLR.cc/2021/Conference/Paper216/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "po-DLlBuAuz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper216/Authors|ICLR.cc/2021/Conference/Paper216/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment"}}}, {"id": "n-FT-u8dLN4", "original": null, "number": 3, "cdate": 1605604728403, "ddate": null, "tcdate": 1605604728403, "tmdate": 1606125456109, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "3BH34ALeFD-", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment", "content": {"title": "Author Response to Reviewer 2 (2/2)", "comment": "$\\textbf{Q:}$ The remaining theoretical contributions in the paper do not pertain to \"continuation\" per se.\n\n$\\textbf{A:}$ Our theoretical results are all aiming at supporting the use of continuation methods, or \u201cnatural parameter continuation\u201d as commented by the reviewer for policy optimization, i.e., set up an easy problem, and \u201cgradually transform back into the original problem and follow the solution as it moves from the solution of the easy problems to the solution of original problem\u201d [3]. Theorem 1 proves that optimizing the KL regularized expected return is the easy problem, which can be solved more efficiently than the original problem. Theorem 2 and Theorem 3 proves that if we trace the solution of the easy problems, we will reach the solution of the original problem as the temperature $\\tau$ goes to 0. We thank the reviewer for pointing us to the related literature. We will include them in the draft.\n\n\n$\\textbf{Q:}$ Theorem 1 provides a bound on the policy gradient methods with a softmax policy, but this is different from the \"soft\" optimality equations. \n\n$\\textbf{A:}$ We would like to first acknowledge the gap between Theorem 1 and Algorithm 1. Theorem 1 assumes tabular setting and access to exact/true gradient, while Algorithm 1 has to adapt to function approximation and offline setting with noisy gradient estimates. To the best of our knowledge, little has been proved on the convergence rate of policy gradient methods until very recently [2], which made the same assumption as ours. We prove that the soft policy iteration in Algorithm 1 optimizes the soft objective in Theorem 2 and 3, and leave the proof of its convergence rate as future work. Despite the gap, we would like to argue that Theorem 1 motivates the use of continuation and sheds light on the underlying mechanism of Algorithm 1, and our empirical results validate the efficacy of Algorithm 1. As the success of many algorithms beyond applications supported by theoretical proofs, for example, stochastic gradient descent (SGD) applied to deep neural networks, upper confidence bound (UCB) or Thompson Sampling beyond bandits settings, we hope our work can motivate future theoretical results in closing the gap.  \n\nThanks for reading! Please let us know if there are other comments we miss to address here. \n\n[1] Minmin Chen, Ramki Gummadi, Chris Harris, and Dale Schuurmans. Surrogate objectives for batch policy optimization in one-step decision making. In Advances in Neural Information Processing Systems, pp. 8825\u20138835, 2019.\n\n[2] Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence rates of softmax policy gradient methods. arXiv preprint arXiv:2005.06392, 2020.\n\n[3] Nocedal, J., & Wright, S. (2006). Numerical optimization. Springer Science & Business Media.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "po-DLlBuAuz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper216/Authors|ICLR.cc/2021/Conference/Paper216/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment"}}}, {"id": "JAjTz41WtIK", "original": null, "number": 8, "cdate": 1606123879620, "ddate": null, "tcdate": 1606123879620, "tmdate": 1606123879620, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "jiMEpu9fbd", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment", "content": {"title": "Author Response to Reviewer 3", "comment": "We thank R3 for all the comments. Please also refer to the common response above for the answers about BRAC and the measurement of variance.\n\n$\\textbf{Q:}$  the performance benefit seems fairly incremental ... it's unclear if the benefits are sufficiently significant to warrant the additional complexity.\n\n$\\textbf{A:}$ We respectfully disagree on this comment that our performance improvement is \u201cfairly incremental\u201d. On Atari datasets, the relative improvement over the best baseline (BCQ) is 37.6% on average, while the prior published work REM only achieves relative improvement 4.9% over the best baseline (calculated from Table 1 in [1]). \n\n|env                            |  Amidar   |   Asterix   |   Breakout  |  Enduro   |  MsPacman  |   Qbert   |  Seaquest   | SpaceInvaders |\n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------|\n|BCQ                            |  154.4     |   2466.7   |     203.8     |    604.7    |   2299.7        |  4088.3   |   4420.0     |      726.5           |\n|Ours                            |  174.5     |   3476.7   |     199.0     |    922.9    |   2494.0        |  4732.5   |   9935.0     |      1070.3         |\n|relative improvement  | 13.0%     |    40.9%   |     -2.4%    |    52.6%   |    8.4%          |   15.8%   |   124.8%    |      47.3%             |\n\nWe apply the paired sample t-test to determine whether the true mean difference between our score and each baseline score on various datasets is greater than 0 for Mujoco and movie dataset.  All tests return p-value less than 0.05. \n\n|dataset | Mujoco | Mujoco | Mujoco | Mujoco | Mujoco | Mujoco |       Movie      |     Movie    |   \n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------|              \n|baseline |    BEAR   |    ABM    |    CRR    |    BCQ    |    CQL   |   BRAC   | Cross-Entropy |   IPS    |\n|p-value  |    0.0103  |  0.0279   |   0.0359 |    0.0026 |  0.0190  |  0.0112   |      0.0002        | 0.0003 |\n\nWe would argue the added \u201ccomplexity\u201d of our method is minimal. The implementation is almost identical to SAC by simply replacing the entropy regularization with KL regularization and an additional step to decay the weight of the regularization. Our method can be implemented on top of SAC with less than 20 lines of code changes. The baselines, e.g., BEAR, CRR, ABM are essentially actor-critic with various regularization terms (e.g. MMD distance, KL divergence). Our method follows this general framework.  Regarding hyper-parameter search, while the baselines tune the weight of the regularization, we tune the initial KL regularization weight which is much less sensitive across different datasets. In conclusion, the complexity of our method in implementation and hyper-parameter search is similar to the baselines while our method performs favorably to the baselines. \n\n$\\textbf{Q:}$ more seeds & missing code\n\n$\\textbf{A:}$ We thank R3\u2019s recognition that our experiment is convincing with coverage on various domains and behavior policies. We will add runs with more seeds later when the computational resources are available. We will also release the code with the camera-ready version.\n\n[1] Agarwal, R., Schuurmans, D., & Norouzi, M. (2020). An Optimistic Perspective on Offline Reinforcement Learning.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "po-DLlBuAuz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper216/Authors|ICLR.cc/2021/Conference/Paper216/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment"}}}, {"id": "a1OeA3KOlLW", "original": null, "number": 7, "cdate": 1606123680742, "ddate": null, "tcdate": 1606123680742, "tmdate": 1606123680742, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "0STSDnEhEAb", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment", "content": {"title": "Author Response to Reviewer 4", "comment": "We thank R4 for all the comments. Please also refer to the common response above for the answers about the measurement of variance.\n\n$\\textbf{Q:}$ Can you relate your work to [1]?\n\n$\\textbf{A:}$ Yes, the optimization problem of maximizing the KL regularized objective function $\\tilde{V}^{\\pi, \\tau}(\\rho)$ can be converted to its Lagrangian duality form. Solving the dual optimization problem is connected to our policy evaluation and policy improvement steps. Therefore, the soft policy iteration algorithm can also be unified in the duality perspective proposed in [1]. The details of the derivation are provided in appendix E.\n\n$\\textbf{Q:}$ Does your method work when the behavior policy is not known but only the dataset is available?\n\n$\\textbf{A:}$ Yes, in all the experiments reported in the paper, the behavior policy is approximated via behavior cloning, as we stated in line 6 in Algorithm 1. \n\n$\\textbf{Q:}$ Can you quantify how far the optimal policy is allowed to be from the behavioral policy? Can one give some criteria when the method is expected to work well?\n\n$\\textbf{A:}$ We study the performance of our method for different behavior policy on the Mujoco dataset. The details are included in Appendix F. For each environment, we interpolate behavior policies between a well-trained policy learned on-policy ($\\alpha=0$) and a random policy ($\\alpha=1$). We find our method outperforms the behavior policy and various baselines in most cases, and the improvement is most significant with a mediocre behavior policy, that is when the data contains both trajectories of high and low cumulative rewards (i.e. $\\alpha \\in [0.4, 0.6]$). We plot the histogram of the trajectory cumulative reward in the behavior data and show the mean and the standard deviation of trajectory reward in each dataset in appendix F. We find the dispersion of the trajectory rewards, in particular the percentage of trajectories with reward better than one standard deviation of the mean (i.e. %1$\\sigma$ trajectory), highly correlates with the improvement (i.e. the average trajectory reward we achieved minus the average trajectory reward of the behavior dataset). \n\n| env | Hopper | Hopper | Hopper |  Hopper | Cheetah| Cheetah | Cheetah|  Cheetah | Walker| Walker | Walker|  Walker |\n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------| -------- | --------| --------| --------|\n|$\\alpha$       |     0.2    |     0.4    |     0.6    |     0.8    |     0.2    |     0.4    |     0.6    |     0.8    |     0.2    |     0.4    |     0.6    |     0.8    |                 \n|%$1\\sigma$ trajectory |  10.2% |  16.7%  |    17%   |     8%   |   9.9%  |  12.5%  |  17.1%  |  14.4%  |     3%    |    24%   |    14 %  |    9%    | \n|behavior  | 2001 | 1167 |  470 |  246 |1724| 1327|  512 |  174 | 1382 |  917  |  210  |  0  | \n|ours         |    2097   |  1569 |   1648   |   226  | 2145 |  1824  | 1487  |   547  | 1441  |  1223 |   853  |   7    |    \n|improvement |     96   |    402    |   1178   |   -20    |   421    |    497    |    975    |    373    |     59     |    306    |    643    |    7      |  \n\n\n[1] Nachum, O., & Dai, B. (2020). Reinforcement learning via fenchel-rockafellar duality. arXiv preprint arXiv:2001.01866.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "po-DLlBuAuz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper216/Authors|ICLR.cc/2021/Conference/Paper216/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment"}}}, {"id": "5D7X17-iEw", "original": null, "number": 4, "cdate": 1606122239011, "ddate": null, "tcdate": 1606122239011, "tmdate": 1606122239011, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment", "content": {"title": "Common Response and Summary of the Updates in the Paper (1/2)", "comment": "We thank all the reviewers for the constructive feedback. We are encouraged that the reviewers find our proposed method \u201cwell motivated\u201d with \u201cthorough experiments\u201d, and the \u201ctheoretical results provide solid backing to the method\u201d. We address the common questions here and will reply to each reviewer separately for their specific comments.\n\n**Updates in the paper:**\n\n- Section 2: Discussion of related work BRAC [1] and [2].\n- Section 4.1: Comparison with baseline BRAC and CQL\n- Section 4.2: Comparison with baseline CQL\n- Appendix C: Experimental details of the baseline BRAC and CQL\n- Appendix D: Study of variance of ensemble critic networks\n- Appendix E: Study to relate our method with duality in RL algorithm\n- Appendix F: Study of performance on datasets of different quality\n\n**Discussion of related work BRAC (R1, R3):**\n\nWe thank the reviewers for pointing out the prior works. We added the discussion of BRAC [1] and [2] in the related work section in the revision. These prior works constrained the KL divergence between the target policy and the behavior policy through a fixed regularization weight or a fixed threshold. The fixed weight variant of BRAC is equivalent to our baseline method denoted as \u201cConstant\u201d in Figure 2&3, with some minor differences in implementation details (e.g., regularization in network parameters, fixing behavior policy after 30K updates).  Our main contribution on the other hand lies in introducing continuation to anneal the weight of the KL regularization for 1) solving the original problem of maximizing expected return to the best extent possible while addressing the distribution shift under batch RL; 2) more efficient optimization. We provided theoretical justification for introducing KL regularization and continuation for batch RL, a practical algorithm (Algorithm 1), and extensive experiments to showcase its efficacy. We would therefore argue the existence of these prior works does not weaken the novelty or contribution of our work. \n\nWe added the experimental comparison with BRAC using the implementation from [1]  in section 4.1.  As explained in section 1, the strength of the fixed constraint is a critical hyper-parameter that is hard to tune. We conduct the hyper-parameter search for BRAC as done in [1] (more details can be found in Appendix C), and find its performance to vary significantly over different environments. BRAC performs significantly worse on Hopper and Walker (as shown in the table below), similar to what we observed for the \u201cConstant\u201d baseline. The \u201cbest\u201d hyper-parameters identified for these datasets often lead to erroneous updates with degenerating performance. This comparison further validates the merits of continuation method that gradually relaxing the constraint vs keeping a fixed constraint. \n\n| env | Hopper | Hopper | Hopper |  Hopper | Cheetah| Cheetah | Cheetah|  Cheetah | Walker| Walker | Walker|  Walker |\n| -------- | -------- | --------| --------| --------| -------- | --------| --------| --------| -------- | --------| --------| --------|\n| $\\alpha$ |  0.2 | 0.4 | 0.6 | 0.8 | 0.2 | 0.4 | 0.6 | 0.8 | 0.2 | 0.4 | 0.6 | 0.8 |                       \n| BRAC |   523     |      2      |     460   |    185    |**2249**|**1922**|   1410   |**582**|    573    |  1079    |    70      |      6      |     \n| Ours   |**2097**|**1569**|**1648**|**226**|   2145   |   1824   |**1487**|    547    |**1441**|**1223**|**853**|**7**|\n\n(to be continued)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "po-DLlBuAuz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper216/Authors|ICLR.cc/2021/Conference/Paper216/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment"}}}, {"id": "otfXryprp8", "original": null, "number": 2, "cdate": 1605604543549, "ddate": null, "tcdate": 1605604543549, "tmdate": 1605604543549, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "3BH34ALeFD-", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment", "content": {"title": "Author Response to Reviewer 2 (1/2)", "comment": "We thank R2 for all the comments. Here we reply to the questions raised in the review.\n\n$\\textbf{Q:}$ From the perspective of addressing the challenging optimization landscape ..., ... \"continuation\" + smooth approximation ... makes sense. However, the narrative doesn't hold when the authors motivate their method in the context of offline batch policy gradient methods. ... the distributional shift should be addressed\u2026 this is the main challenge in the off-policy setting and the proposed continuation-based solution does not address this issue.\n\n$\\textbf{A:}$ We respectfully disagree on this comment and address it in two stages. First, continuation helps the challenging optimization landscape of policy gradient methods even when exact/true gradient is available; second, continuation combined with the specific soft objective introduced addresses the distribution shift pertaining to offline batch policy gradient. \n\nAs empirically constructed in [1] and proved in [2], the vanilla softmax policy gradient objective is extremely difficult to optimize even with the exact policy gradient. The expected return objective may exhibit suboptimal plateaus and exponentially many local optima in the worst case, and policy gradient on this objective converges to the global optimal policy at a rate of $\\mathcal{O}(1/t)$. The entropy-regularized policy gradient on the other hand enjoys a significantly faster linear convergence rate $\\mathcal{O}(e^{\u2212t} )$. As explained in [2], introducing the entropy regularization bears similarity to adding a strongly convex regularizer in convex optimization, which is known to smooth the landscape and significantly improve the rate of convergence of first-order methods. A similar argument can be made in our case as we set a large temperature $\\tau$ in equation (1) initially to smooth the optimization landscape. We prove the linear convergence rate of the soft objective defined in equation (1) in Theorem 1. The soft objective in (1) however converges to a different point than the optimal softmax policy optimizing the expected return, which is what we are interested in, we thus employ continuation to gradually decrease the temperature to solve the original problem. \n\nTo address the distribution shift caused by offline data as pointed out by R2, we adapted the plain entropy regularization to the KL divergence to regularize toward the behavior policy. As a result, when we set a large temperature $\\tau$ initially, we not only ease the optimization challenge but also limit the distribution shift by forcing the learned policy to be close to the behavior policy. As we gradually decrease the temperature, the learned policy is allowed to deviate from the behavior policy, and extrapolation errors can occur. The continuation, i.e., solving the series of approximated objectives as defined in equation (1) with warm-starting, however, helps us avoid roaming into regions with high extrapolation errors, as shown in Figure 2 and empirically demonstrated in the experiments. In summary, our proposed continuation method not only helps address \u201cthe challenging optimization landscape\u201d but also alleviates the problem of extrapolation error induced by \u201cthe distributional shift\u201d in offline RL.\n\n(to be continued)\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "po-DLlBuAuz", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper216/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper216/Authors|ICLR.cc/2021/Conference/Paper216/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923873404, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Comment"}}}, {"id": "0STSDnEhEAb", "original": null, "number": 2, "cdate": 1603875525668, "ddate": null, "tcdate": 1603875525668, "tmdate": 1605024738371, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Review", "content": {"title": "Good paper, accept", "review": "Summary\n-------------\nThe paper extends soft actor-critic (SAC) to the batch RL setting, replacing the policy entropy in the objective function with the KL divergence from the behavioral policy. The temperature parameter tau weighting the reward agains the KL term is annealed towards zero during the optimization process, which corresponds to starting with behavioral cloning for high values of tau and ending up with the standard reward maximization RL objective for tau=0. Theoretical analysis and experiments confirm the advantages of the proposed method.\n\nDecision\n-----------\nI vote for accepting the paper. The idea of annealing the KL constraint is simple and elegant. Although it is very similar to other constrained policy update methods discussed in the Related Work section, the evaluation in the batch RL setting and demonstration of the improved convergence properties is novel. The execution is of high quality, with evaluations on tabular problems, MuJoCo, Atari, and a contextual bandit problem for movie recommendation.\n\nQuestions\n--------------\n1. As pointed out in Sec. 3.3, when the policy deviates too much from the behavioral policy, the value estimate becomes erroneous. Therefore, a criterion based on the ensemble variance of the Q-function estimates is proposed. Is there a way to derive such a criterion from first principles?\n2. Can you relate your work to [1]?\n3. Does your method work when the behavior policy is not known but only the dataset is available?\n4. Can you quantify how far the optimal policy is allowed to be from the behavioral policy? For example, on a pendulum swing-up task, if the behavioral policy is taking random actions and the pendulum always jitters at the bottom, inferring the optimal policy from this data appears quite challenging. Can one give some criteria when the method is expected to work well?\n\nReferences\n---------------\n[1] Nachum, O., & Dai, B. (2020). Reinforcement learning via fenchel-rockafellar duality. arXiv preprint arXiv:2001.01866.", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147933, "tmdate": 1606915765111, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper216/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Review"}}}, {"id": "Wg1INsIFxPy", "original": null, "number": 3, "cdate": 1603930915981, "ddate": null, "tcdate": 1603930915981, "tmdate": 1605024738303, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Review", "content": {"title": "New batch RL algorithm with natural intuition and solid empirical study", "review": "Summary of the paper:\nThis paper proposed a new batch RL algorithm based on the continuation method in numerical optimization. The proposed method makes use of KL regularization in the offline policy optimization process, with a decreasing temperature parameter -- which comes from the continuation method of optimization. The paper shows that the policy learning with the KL regularized value yields a faster convergence rate, as the motivation of using KL regularization, and a decreasing temperature will ensure the final convergence to the optimal policy. Experiments on Mojoco, atari, and a recommender data-set shows that the proposed algorithm is effective.\n\nJustification for the score:\nThe proposed algorithm is a natural improvement over the current behavior regularized policy optimization methods. The intuition from the continuation method provides a justification for using a decreasing temperature. I think the main merit of this paper is the solid experiment, in simulation tasks with discrete actions and continuous actions, and in a real data-set. This contribution seems solid, but I have some concerns about Theorem 1 and important related work that is missed.\n\nDetailed comments:\nPro:\n1. The intuition behind the algorithmic change is clear enough. Section 3.1 gives the motivation of using a KL regularization and section 3.3 gives an illustrative example of why the continuation method can be better than a constant threshold.\n2. The experiment section covers the standard RL benchmark in both continuous and discrete action settings. It additionally studied the performance of the proposed algorithm in a real dataset. Since while most batch RL work only run experiments in simulation tasks, this is a good step to bring the algorithm close to the motivation of doing batch RL.\n\nCons:\n1. Theorem 1 seems to be a bit disconnected from the main contribution of the paper and it's hard to understand its role for me. Later theoretical analysis and practical approximation are all based on the policy iteration algorithm, but Theorem 1 seems to be based on policy gradient. Could the convergence rate improvement be shown in the policy iteration case? Additionally, Theorem 1 says \"maximizing $\\widetilde{V}^{\\pi,\\tau}$\", does that mean theorem 1 consider the on-policy setting? In such a case what does $\\beta$ mean?\n2. The proposed algorithm seems to be very related to the BRAC framework and the algorithm BRAC with the KL value penalty. Why is it not mentioned at all? The BRAC paper may not directly use such a decreasing temperature or link it to the continuation method, but as prior work in batch RL, it also considered KL regularization and even studied using an adaptive regularization coefficient. \n3. More recent batch RL algorithms like BRAC, CQL, etc also reported their result in mujoco and atari (CQL did) domains and seems to be better than BCQ, BEAR, and REM. An open dataset D4RL has the reported performance of those more recent baselines. I think in general it will be better to compare with these more recent baselines (especially BRAC since it's very related.).\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147933, "tmdate": 1606915765111, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper216/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Review"}}}, {"id": "3BH34ALeFD-", "original": null, "number": 4, "cdate": 1603936536851, "ddate": null, "tcdate": 1603936536851, "tmdate": 1605024738241, "tddate": null, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "invitation": "ICLR.cc/2021/Conference/Paper216/-/Official_Review", "content": {"title": "A continuation-like method", "review": "While the title of the paper suggests that it leverages techniques from the vast literature on numerical continuation, the proposed approach is much more specific. The main idea consists in annealing the temperature parameter of the soft Bellman operator, and to warm start the the corresponding series of problems across time. In the language of numerical continuation, this approach fits under the umbrella of \"natural parameter continuation\", which I would describe succintly as \"warm starting\". \n\nFrom the perspective of addressing the challenging optimization landscape presupposed in many problems, the combination of \"continuation\" + smooth approximation to the optimal equations makes sense. However, for me the narrative doesn't hold when the authors motivate their method in the context of offline batch policy gradient methods. As the authors point out, the main challenge associated with offline data is the inability to sample new data. This is problematic in the policy gradient setting because our derivative estimator only holds under the distribution under which the samples have been collected. As soon as the policy parameters are updated, the distributional shift should be addressed via an appropriate change of measure (or via a model). To me, this is the main challenge in the off-policy setting and the proposed continuation-based solution does not address this issue.  I view this as a Monte Carlo estimation problem first; not one pertaining to the optimization landscape. \n\nPerhaps the paper should have been named differently because the remaining theoretical contributions in the paper do not pertain to \"continuation\" per se. Theorem 1 provides a bound on the policy gradient methods with a softmax policy (this is different from the \"soft\" optimality equations). Theorem 2 and 3 follow from the results in Rust (1994, 1996) and in econometrics where the smooth (soft) Bellman operator has been widely used. Theorem 2 follows from the perspective of policy iteration as an application Newton-Kantorovich to the smooth Bellman optimality equations. Theorem 3 follows from Dini's theorem where $\\lim_{\\tau \\to 0} T^\\star_\\tau v = T^\\star v$ where $T_\\tau$ would be the smooth (soft) Bellman operator and $T^\\star$ the usual Bellman operator. See Rust 1996 \"Numerical Dynamic Programming in Economics\", section 4, more specifically equations 4.2 and 4.4\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper216/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper216/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Batch Reinforcement Learning Through Continuation Method", "authorids": ["~Yijie_Guo1", "~Shengyu_Feng1", "~Nicolas_Le_Roux2", "~Ed_Chi1", "~Honglak_Lee2", "~Minmin_Chen1"], "authors": ["Yijie Guo", "Shengyu Feng", "Nicolas Le Roux", "Ed Chi", "Honglak Lee", "Minmin Chen"], "keywords": ["batch reinforcement learning", "continuation method", "relaxed regularization"], "abstract": "Many real-world applications of reinforcement learning (RL) require the agent to learn from a fixed set of trajectories, without collecting new interactions.  Policy optimization under this setting is extremely challenging as: 1) the geometry of the objective function is hard to optimize efficiently; 2) the shift of data distributions causes high noise in the value estimation. In this work, we propose a simple yet effective policy iteration approach to batch RL using global optimization techniques known as continuation.  By constraining the difference between the learned policy and the behavior policy that generates the fixed trajectories, and continuously relaxing the constraint, our method 1) helps the agent escape local optima; 2) reduces the error in policy evaluation in the optimization procedure.   We present results on a variety of control tasks, game environments, and a recommendation task to empirically demonstrate the efficacy of our proposed method.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "guo|batch_reinforcement_learning_through_continuation_method", "supplementary_material": "/attachment/cd6b4e8e5375df38f85f6cb9cb9ce88ad149d9e8.zip", "pdf": "/pdf/84a7a35d996f84ab9fbbbcabccbdc21f44f2ba68.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nguo2021batch,\ntitle={Batch Reinforcement Learning Through Continuation Method},\nauthor={Yijie Guo and Shengyu Feng and Nicolas Le Roux and Ed Chi and Honglak Lee and Minmin Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=po-DLlBuAuz}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "po-DLlBuAuz", "replyto": "po-DLlBuAuz", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper216/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538147933, "tmdate": 1606915765111, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper216/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper216/-/Official_Review"}}}], "count": 13}