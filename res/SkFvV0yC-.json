{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730185948, "tcdate": 1509053537113, "number": 190, "cdate": 1518730185937, "id": "SkFvV0yC-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "SkFvV0yC-", "original": "SyOw4CkAb", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Network Iterative Learning for Dynamic Deep Neural Networks via Morphism", "abstract": "In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.", "pdf": "/pdf/3d944aba38adc392d386d7d7b8bee8d1516a7017.pdf", "paperhash": "wei|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism", "_bibtex": "@misc{\nwei2018network,\ntitle={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\nauthor={Tao Wei and Changhu Wang and Chang Wen Chen},\nyear={2018},\nurl={https://openreview.net/forum?id=SkFvV0yC-},\n}", "keywords": ["Network Iterative Learning", "Morphism"], "authors": ["Tao Wei", "Changhu Wang", "Chang Wen Chen"], "authorids": ["taowei@buffalo.edu", "wangchanghu@toutiao.com", "chencw@buffalo.edu"]}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260088340, "tcdate": 1517249751439, "number": 473, "cdate": 1517249751420, "id": "SykJHyaSM", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "SkFvV0yC-", "replyto": "SkFvV0yC-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "The paper presents a variant of network morphism (Wei et al., 2016) for dynamically growing deep neural networks. There are some novel contributions (such as OptGD for finding a morphism given the parent network layer). However, in the current form, the experiments mostly focus on comparisons against fixed network structure (but this doesn't seem like a strong baseline, given Wei et al.'s work), so the paper should provide more comparisons against Wei et al. (2016) to highlight the contribution of this work. In addition, the results will be more convincing if the state-of-the-art performance can be demonstrated for large-scale problems (such as ImageNet classification). "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Iterative Learning for Dynamic Deep Neural Networks via Morphism", "abstract": "In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.", "pdf": "/pdf/3d944aba38adc392d386d7d7b8bee8d1516a7017.pdf", "paperhash": "wei|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism", "_bibtex": "@misc{\nwei2018network,\ntitle={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\nauthor={Tao Wei and Changhu Wang and Chang Wen Chen},\nyear={2018},\nurl={https://openreview.net/forum?id=SkFvV0yC-},\n}", "keywords": ["Network Iterative Learning", "Morphism"], "authors": ["Tao Wei", "Changhu Wang", "Chang Wen Chen"], "authorids": ["taowei@buffalo.edu", "wangchanghu@toutiao.com", "chencw@buffalo.edu"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642406528, "tcdate": 1511800609284, "number": 1, "cdate": 1511800609284, "id": "S1YX1ptgz", "invitation": "ICLR.cc/2018/Conference/-/Paper190/Official_Review", "forum": "SkFvV0yC-", "replyto": "SkFvV0yC-", "signatures": ["ICLR.cc/2018/Conference/Paper190/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "The submission looks interesting and is well-written", "rating": "7: Good paper, accept", "review": "This submission develops a learning scheme for training deep neural networks with adoption of network morphism (Wei et al., 2016), which optimizes a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized, instead of directly optimizing a static objective function. Overall, the idea looks interesting and the manuscript is well-written. The shown experimental results should be able to validate the effectiveness of the learning scheme to some extent.\n\nIt would be more convincing to include the performance evaluation of the learning scheme in some representative applications, since the optimality of the training objective function is not necessarily the same as that of the trained network in the application of interest.\n\nBelow are two minor issues:\n\n- In page 2, it is stated that Fig. 2(e) illustrates the idea of the proposed network iterative learning scheme for deep neural networks based on network morphism. However, the idea seems not clear from Fig. 2(e).\n\n- In page 4, \u201csuch network iterative learning process\u201d should be \u201csuch a network iterative learning process\u201d.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Iterative Learning for Dynamic Deep Neural Networks via Morphism", "abstract": "In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.", "pdf": "/pdf/3d944aba38adc392d386d7d7b8bee8d1516a7017.pdf", "paperhash": "wei|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism", "_bibtex": "@misc{\nwei2018network,\ntitle={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\nauthor={Tao Wei and Changhu Wang and Chang Wen Chen},\nyear={2018},\nurl={https://openreview.net/forum?id=SkFvV0yC-},\n}", "keywords": ["Network Iterative Learning", "Morphism"], "authors": ["Tao Wei", "Changhu Wang", "Chang Wen Chen"], "authorids": ["taowei@buffalo.edu", "wangchanghu@toutiao.com", "chencw@buffalo.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642406421, "id": "ICLR.cc/2018/Conference/-/Paper190/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper190/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper190/AnonReviewer2", "ICLR.cc/2018/Conference/Paper190/AnonReviewer1", "ICLR.cc/2018/Conference/Paper190/AnonReviewer4"], "reply": {"forum": "SkFvV0yC-", "replyto": "SkFvV0yC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper190/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642406421}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642406485, "tcdate": 1511823183973, "number": 2, "cdate": 1511823183973, "id": "BkdUwGqgz", "invitation": "ICLR.cc/2018/Conference/-/Paper190/Official_Review", "forum": "SkFvV0yC-", "replyto": "SkFvV0yC-", "signatures": ["ICLR.cc/2018/Conference/Paper190/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Interesting discussion, but not novel enough", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes an iterative approach to train deep neural networks based on morphism of the network structure into more complex ones. The ideas are rather simple, but could be potentially important for improving the performance of the networks. On the other hand, it seems that an important part of the work has already been done before (in particular Wei et al. 2016), and that the differences from there are very ad-hoc and intuition for why they work is not present. Instead, the paper justifies its approach by arguing that the experimental results are good. Personally, I am skeptical with that, because interesting ideas with great added value usually have some cool intuition behind them. The paper is easy to read, and there does not seem to exist major errors. Because I am not an active researcher in the topic, I cannot judge if the benefits that are shown in the experiments are enough for publication (the theoretical part is not the strongest of the paper).", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Iterative Learning for Dynamic Deep Neural Networks via Morphism", "abstract": "In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.", "pdf": "/pdf/3d944aba38adc392d386d7d7b8bee8d1516a7017.pdf", "paperhash": "wei|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism", "_bibtex": "@misc{\nwei2018network,\ntitle={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\nauthor={Tao Wei and Changhu Wang and Chang Wen Chen},\nyear={2018},\nurl={https://openreview.net/forum?id=SkFvV0yC-},\n}", "keywords": ["Network Iterative Learning", "Morphism"], "authors": ["Tao Wei", "Changhu Wang", "Chang Wen Chen"], "authorids": ["taowei@buffalo.edu", "wangchanghu@toutiao.com", "chencw@buffalo.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642406421, "id": "ICLR.cc/2018/Conference/-/Paper190/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper190/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper190/AnonReviewer2", "ICLR.cc/2018/Conference/Paper190/AnonReviewer1", "ICLR.cc/2018/Conference/Paper190/AnonReviewer4"], "reply": {"forum": "SkFvV0yC-", "replyto": "SkFvV0yC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper190/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642406421}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642406439, "tcdate": 1512428428364, "number": 3, "cdate": 1512428428364, "id": "By4qQIQ-f", "invitation": "ICLR.cc/2018/Conference/-/Paper190/Official_Review", "forum": "SkFvV0yC-", "replyto": "SkFvV0yC-", "signatures": ["ICLR.cc/2018/Conference/Paper190/AnonReviewer4"], "readers": ["everyone"], "content": {"title": "Interesting paper, but requires more clarifications on novelty and experiment results", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposed an iterative learning scheme to train a very deep convolutional neural network. Instead of learning a deep network from scratch, the authors proposed to gradually increase the depth of the network while transferring the knowledge obtained from the shallower network by applying network morphism. \n\nOverall, the paper is clearly written and the proposed ideas are interesting. However, many parts of the ideas discussed in the paper (Section 3.3) are already investigated in Wei et al., 2016, which limits the novel contribution of the paper. Besides, the best performances obtained by the proposed method are generally much lower than the ones reported by the existing methods (e.g. He et al., 2016) except cifar-10 experiment, which makes it hard for the readers to convince that the proposed method is superior than the existing ones. More thorough discussions are required.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Network Iterative Learning for Dynamic Deep Neural Networks via Morphism", "abstract": "In this research, we present a novel learning scheme called network iterative learning for deep neural networks. Different from traditional optimization algorithms that usually optimize directly on a static objective function, we propose in this work to optimize a dynamic objective function in an iterative fashion capable of adapting its function form when being optimized. The optimization is implemented as a series of intermediate neural net functions that is able to dynamically grow into the targeted neural net objective function. This is done via network morphism so that the network knowledge is fully preserved with each network growth. Experimental results demonstrate that the proposed network iterative learning scheme is able to significantly alleviate the degradation problem. Its effectiveness is verified on diverse benchmark datasets.", "pdf": "/pdf/3d944aba38adc392d386d7d7b8bee8d1516a7017.pdf", "paperhash": "wei|network_iterative_learning_for_dynamic_deep_neural_networks_via_morphism", "_bibtex": "@misc{\nwei2018network,\ntitle={Network Iterative Learning for Dynamic Deep Neural Networks via Morphism},\nauthor={Tao Wei and Changhu Wang and Chang Wen Chen},\nyear={2018},\nurl={https://openreview.net/forum?id=SkFvV0yC-},\n}", "keywords": ["Network Iterative Learning", "Morphism"], "authors": ["Tao Wei", "Changhu Wang", "Chang Wen Chen"], "authorids": ["taowei@buffalo.edu", "wangchanghu@toutiao.com", "chencw@buffalo.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642406421, "id": "ICLR.cc/2018/Conference/-/Paper190/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper190/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper190/AnonReviewer2", "ICLR.cc/2018/Conference/Paper190/AnonReviewer1", "ICLR.cc/2018/Conference/Paper190/AnonReviewer4"], "reply": {"forum": "SkFvV0yC-", "replyto": "SkFvV0yC-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper190/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642406421}}}], "count": 5}