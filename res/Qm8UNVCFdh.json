{"notes": [{"id": "Qm8UNVCFdh", "original": "tzx1kGuk61", "number": 894, "cdate": 1601308102454, "ddate": null, "tcdate": 1601308102454, "tmdate": 1615058707168, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "kB811jI1RWc", "original": null, "number": 1, "cdate": 1610040506913, "ddate": null, "tcdate": 1610040506913, "tmdate": 1610474114165, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Poster)", "comment": "The paper presents an attempt to learn interaction-based representations by taking advantage of body part movements and gaze attention. Video representations are learned by benefiting from additional supervisory signals, which are not the ones commonly used, making the paper more interesting.\n\nR3 expresses a concern that the supervisory signal does not come \"for free\" and that the paper is misleading. The ACs do agree with R3 that the paper benefits from additional signals and is not a pure self-supervised learning paper, strictly speaking. The authors also agreed to this in their response to the R3\u2019s comment. R1 also mentioned (after the rebuttal phase) that the proposed approach is not a practical self-supervised learning solution and that it does not perform as effectively as conventional self-supervised learning methods like InfoNCE on Moco.\n\nSimultaneously, the AC and the majority of the reviewers believe that the paper itself has a value as a multi-modal learning paper. We strongly suggest the authors revise the paper to remove the 'self-supervision' claim. As mentioned above, the paper is not a self-supervised learning paper and the authors are asked to correct the details of the paper to reflect this. We also recommend adding analysis on each body signal qualitatively in the final manuscript, as suggested by R4.\n\nIt will be great if the authors can consider this as a \"conditional accept\". In particular, the 'self-supervision' claim in the current version of the paper is misleading and this must be corrected in the final version. Note that this was also pointed out by the Program Chairs."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040506900, "tmdate": 1610474114150, "id": "ICLR.cc/2021/Conference/Paper894/-/Decision"}}}, {"id": "U_W5QKLhvhB", "original": null, "number": 3, "cdate": 1603880751278, "ddate": null, "tcdate": 1603880751278, "tmdate": 1607404767711, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Review", "content": {"title": "Novel idea, new dataset and improvements over SOTA approach", "review": "This paper proposes to improve upon unsupervised representation learning for various downstream vision tasks by leveraging human motion and attention (gaze) information. The authors collect a large spatio-temporal dataset with gaze and body motion labels for this task. They train a network to jointly predict the visual focus of attention in scenes and body motion besides visual instance recognition via an NCE loss to learn good visual representations. They show large improvements in accuracy of many different visual recognition downstream tasks with their approach versus the SOTA MOCO approach, which uses visual information only.\n\nPros:\nThe work is novel and considers a new dimension to solving the problem of representation learning, which hasn't been explored before. It explores supervising neural networks to predict humans' motion and visual focus of attention. This is biological motivated by human beings' similar learning strategies. The novel datatset containing both gaze and body motion labels can be useful to the research community for other tasks beyond visual representation learning. The authors show consistent improvements with their proposed method of incorporating knowledge of gaze and body motion versus using visual information only for many downstream tasks they consider. The experimental section is fairly thorough (except for an important missing experiment as explained below). \n\nCons:\nThe authors argue that one of the disadvantages of the current approaches for unsupervised representation learning is that they use ImageNet-type datasets that require significant effort for curation and cleanup. In contrast to this, the authors' proposed approach of requiring large amounts of data with expensive TOBII eye-tracking glasses and body-IMUs from multiple subjects and with special calibration and syntonization procedures seems even more cumbersome and less accessible to ordinary practitioners of AI in the real world. How do the authors justify this? To clearly show the superiority of their dataset versus ImageNet, the authors should also include the results of MOCO trained on ImageNet for each of downstream tasks shown in their paper.\n------\n\nPost Rebuttal:\nI thank the authors for their response and additional experiments to show the performance of MOCO trained on ImageNet for the various downstream tasks considered in this paper. It is evident from the results that the authors presented in Table 5 that their best method (using their multimodal data) performs worse than MOCO trained only on ImageNet with InfoNCE. Hence, while this current work has some interesting novel insights of theoretical value, I don't think the complete proposed method of data collection and training is very practical or broadly scalable. It is likely to be of limited practical applicability.  In contrast to the authors' proposed cumbersome method of collecting annotated data using expensive gaze and motion sensors, cell phones and cameras are nowadays ubiquitous and image and video data is routinely uploaded to the internet by users all over the world. Using such abundantly available existing data on the web, which can often times simply be downloaded for free without any annotations, is what I believe is likely to be a much more practical and broadly applicable approach to solving the problem of representation learning via self-supervision. This concern is also shared by Reviewer 3. \n\nOn weighing the various pros and cons of the proposed approach, I will maintain my previous rating.", "rating": "6: Marginally above acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper894/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132337, "tmdate": 1606915802024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper894/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Review"}}}, {"id": "pdeulpjH3I", "original": null, "number": 4, "cdate": 1605380708494, "ddate": null, "tcdate": 1605380708494, "tmdate": 1605386784790, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "3yD8e2U9rcU", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment", "content": {"title": "Response to R2", "comment": "We thank R2 for the insightful and detailed review. We appreciate the positive comments on the importance and relevance of this work, the clarity of descriptions, and diverse results and ablations. We address the questions and comments below:\n\n -**R2: There is a lot of redundant information between gaze and motion sensors which is surprising.**\n\nA correlation between gaze and motion sensors is somewhat expected in some specific scenarios. For example, if the gaze is on a specific object, it is likely that we interact with that object. Therefore, the arm sensors will probably move as well. \n\n-**R2: What inference can we draw about the results in Table 2? I guess the main point is that the addition of gaze and motion also improves Lae results.**\n\nThat\u2019s right. Our point was to show results on a different type of visual encoder.  \n\n-**R2: Why would there be better complementarity between gaze, motion and NCE compared to AE?**\n\nWe conjecture that the AE provides a weaker visual signal compared to NCE, and a lot of visual information is lost. Therefore, making connections between the AE visual encoding and gaze and motion is probably much more difficult. \n\n-**R2: Section 5.3.2 It seems surprising that gaze, a 2 dimensional quality needs a 512 D embedding. Is this some sort of one-hot encoding over a matrix of locations?**\n\nThe main reason for this was that the gaze embedding has a comparable size to the other features in the network. Concatenating a 2 dimensional vector with a large image embedding would probably diminish the effect of the gaze information.\n\n-**R2: How are alpha, beta and gamma set? Were any other forms of regularization used such as dropout or weight decay?**\n\nOur losses have different norms, we set these values to normalize the norm difference. We use a dropout of 0.5 and weight decay of 0.1. We have added these to the revised supplementary material.\n\n-**Feedback**\n\nThanks for the feedback. We have applied them to the revision we submitted.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper894/Authors"], "readers": ["everyone", "ICLR.cc/2021/Conference/Paper894/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm8UNVCFdh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper894/Authors|ICLR.cc/2021/Conference/Paper894/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866017, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment"}}}, {"id": "gQZu1WQaEJF", "original": null, "number": 5, "cdate": 1605380972977, "ddate": null, "tcdate": 1605380972977, "tmdate": 1605382895077, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "U_W5QKLhvhB", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment", "content": {"title": "Response to R1", "comment": "We thank R1 for the valuable feedback. We appreciate the positive comments about the novelty of our work, the usefulness of our dataset and the thoroughness of our experiments.\n\n-**R1: How do the authors justify superiority over ImageNet?**\n\nBy no means we claim superiority over ImageNet. ImageNet is an excellent, well-designed, dataset that has led to breakthroughs in supervised and unsupervised representation learning. Our point is that our dataset does not have any costly annotation compared to ImageNet which required a significant amount of money and resources for months or years of annotation, image search engines for image retrieval, etc. The cost of our sensors is negligible compared to those. Moreover our dataset is closer to the data humans use for learning (interaction with objects and scenes vs large-scale annotated datasets). However, as requested by R1, we provide the result of pre-training MoCo on ImageNet in the table below. We have added these results to Table 5 in the revised supplementary as well.\n\n|                  | Scene  Classification | Action Recognition | Dynamics Prediction | Walkable Estimation | Depth Estimation | \n| :---             |       :----:         |        :----:      |     :----:          |   :----:            |   :----:         |\n| MoCo (ImageNet)  |        35.18         |       30.28        |       16.37         |        63.95        |       0.135      |\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper894/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm8UNVCFdh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper894/Authors|ICLR.cc/2021/Conference/Paper894/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866017, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment"}}}, {"id": "cKP6KdDusmQ", "original": null, "number": 7, "cdate": 1605382004902, "ddate": null, "tcdate": 1605382004902, "tmdate": 1605382004902, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "nKbMstJujAI", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment", "content": {"title": "Response to R3", "comment": "We thank R3 for the great feedback. We are glad that R3 found the dataset interesting and the supervision signal novel and useful. \n\n-**R3: Even though the signal is useful, it does not come for free like SpeedNet, Rotationnet or infoNCE.**\n\nWe agree with R3 that our method is not as \"free\" as the visual-only methods such as SpeedNet, RotationNet and the others mentioned in the paper since our approach is multi-modal, and we require sensors in addition to the camera to capture data for other modalities. Our point in this paper is just to show there are signals other than visual information (such as gaze and body part movements) that can be used for improving visual representation. However, these signals might not be obtained for free. We revised our conclusion to reflect this. "}, "signatures": ["ICLR.cc/2021/Conference/Paper894/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm8UNVCFdh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper894/Authors|ICLR.cc/2021/Conference/Paper894/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866017, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment"}}}, {"id": "JmPevHd8CTq", "original": null, "number": 6, "cdate": 1605381266459, "ddate": null, "tcdate": 1605381266459, "tmdate": 1605381527839, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "HfT4A1qCpH9", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment", "content": {"title": "Response to R4", "comment": "We thank R4 for the valuable feedback and we appreciate the positive comments about the novelty of the idea, clarity of the paper, strength of the results, novelty of the dataset, and justification of the claims. \n\n-**R4: The exclusion of the torso results in a lower error for the depth estimation, but an explanation for why exactly would that be the case may be beneficial.**\n\nThis is an excellent question. We **conjecture** that information captured by torso movements for depth prediction is not as reliable as gaze or neck movements for example since torso movements intuitively do not provide much information about far regions in an image. So removing torso, makes the training easier leading to better results. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper894/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Qm8UNVCFdh", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper894/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper894/Authors|ICLR.cc/2021/Conference/Paper894/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923866017, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Comment"}}}, {"id": "HfT4A1qCpH9", "original": null, "number": 2, "cdate": 1603866936596, "ddate": null, "tcdate": 1603866936596, "tmdate": 1605024580240, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Review", "content": {"title": "The paper provides a novel intuition to learn visual representations from human interactions and motion cues. The authors have well documented their research and have provided ample validation for their results, which makes this paper a notable contribution", "review": "The main aim of the paper is to make use of human interaction/motion to learn a visual  representation that can be re-used for classic visual tasks such as depth estimation. The authors claim that by encoding interaction and attention cues in the self-supervised representation, the method can outperform visual-only state-of-the-art methods. To study the interaction element, the authors attach sensors like Inertial Movement Units (IMUs) to the limbs of subjects and monitor their reaction to visual events in daily life. The paper also introduces a new dataset of 4260 minutes of human interactions by 35 participants which include synchronized streams of images, body part movements, and gaze information.\n\nPaper Strengths:\nThis is a simply awesome paper. Idea is novel, well-validated, and well-written. The result is strong.\n+ Novel intuition: The idea of the paper is intuitive, where it proposes to incorporate body part movements and gaze information in learning visual representations. Attention does play an impact in many tasks like action recognition and scene classification, which might benefit from the proposed representation learning. Also, in case of tasks like depth estimation and future prediction of dynamics, it is insightful to use body movement since it encodes temporal changes.\n+ Experimental setup and ablation studies: The intuition of the authors to incorporate body part movements and gaze information in their representation has been well justified by the experimental setups and ablation studies. The importance of using each objective in the representation learning has been effectively demonstrated by showing its impact on various target tasks.\n+ Performance: The authors have shown that the representation, trained using movement and attention supervision, outperforms the visual-only representations in all the tasks mentioned in the paper by a range of 1.3% to 7%\n+ Dataset: The paper also introduces a new dataset of 4260 minutes of human interactions by 35 participants. This is a novel dataset with synchronized streams of images, body part movements and gaze information.\n\nPaper Weaknesses:\n- Objective function selection: In the ablation studies of body parts, we see how the removal of a body part can affect the performance on the target tasks. However, it is still not clear how each individual part may fare since the performances do not vary greatly from each other. The exclusion of the torso results in a lower error for the depth estimation, but an explanation for why exactly would that be the case may be beneficial, since intuitively it may seem that the complete movement of the body should result in better performance.\n", "rating": "9: Top 15% of accepted papers, strong accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper894/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132337, "tmdate": 1606915802024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper894/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Review"}}}, {"id": "3yD8e2U9rcU", "original": null, "number": 4, "cdate": 1603936652678, "ddate": null, "tcdate": 1603936652678, "tmdate": 1605024580116, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Review", "content": {"title": "Predicting eye gaze and motion of humans improves downstream performance on visual tasks compared to visual self-supervision alone ", "review": "\n# Paper Summary \n\nThe paper uses a combination of visual, human gaze and human motion sensors to build representations that perform better on downstream tasks such as action recognition, physics prediction and depth estimation than representations extracted from solely visual input. The paper announces the release a new data set of aligned visual images, eye gaze fixations and IMU motion readings from test subjects walking around an environment. Representations are computed using three different forms of information simultaneously. Given a visual input, the system tries to predict the location of eye gaze in image frame coordinates, whether each of 6 groups of motion detectors are active or not (head, torso, legs, etc.) and the result of a more traditional auxiliary visual pretext task. In this work, the paper uses \u201cinstance discrimination\u201d where representations of augmented versions of a specific image are pushed close together in latent space and far away from augmentations of other images. Tests on diverse benchmarks show that the gaze and motion prediction improve over visual pretext tasks alone and that there is a small benefit to using both together, but it is not additive. The paper also shows the benefit of gaze and motion is present for two different visual auxiliary tasks.\n\n\n# Pros and Cons \n\nThe paper defines a new way of obtaining self-supervised representations for a variety of core computer vision tasks which is important and highly relevant to the ICML community. \n\nThe paper clearly describes prior work and situates its contributions well within this space. \n\nThe paper evaluates the proposed augmented loss function on a diverse collection of standard benchmarks to test their hypothesis including scene classification, action recognition, future motion, walkable surfaces and depth estimation. \n\nTable 1 provides a clear ablation study showing that predicting gaze and motion improve downstream performance on diverse benchmarks by non-trivial amounts:  ~7% , 3.5%, 1%, 1% and reduce RMSE  on depth estimation by 2% or so.  \n\nIn the first benchmark, adding either attention or movement increases results by 6%, but adding both only improves results by 7%, not 14%! This suggests that there is a lot of redundant information between gaze and motion sensors which is a surprising as superficially they seem like very different modalities. \n\nIt is interesting and surprising that eye-gaze, where a person is looking in an image, is a useful feature for tasks such as depth prediction. This suggests there are features that are easy to compute (compared to interactions between robots and physical world)  that significantly improve network intuitions about what is important in images.  \n\nFrom table 4, it seems like Torso movement is more informative for scene classification, neck more informative for action recognition, arms for dynamics and arms for walkability. It is interesting that different parts are more or less informative for different tasks. \n \nI am a little unsure what inference to draw about the results in table 2 (experiments on Lnce vs. Lae). I guess the main point is that the addition of gaze and motion also improves Lae results, so the effect is not specific to Lnce? It is interesting that gaze and motion are not as helpful with Lae ( 3% gain vs 7% with Lnce) but it is not clear why. Why would there be better complementarity between gaze, motion and NCE? Some thoughts on this would be interesting. \n\n\n# Recommendation  \n\nAccept - it is an interesting and surprising result that adding self-supervised tasks for other modalities significantly improves self-supervised representations and that gaze and motion are redundant with each other.  \n\n \n# Questions  \n\nHow are alpha, beta and gamma set? Hmm, see from appendix A.4.1 these are 0.09, 0.01 and 0.9 respectively indicating that visual loss is the primary driver here and the others are acting more like regularizations on the representation. It is a bit surprising that the other modalities have so large an effect given their small weights in the loss function. Were any other forms of regularization used such as dropout or weight decay?  Didn\u2019t see anything about this in the appendix. \n\nSection 5.3.2 It seems surprising that gaze, a 2 dimensional quality needs a 512 D embedding. Is this some sort of one-hot encoding over a matrix of locations? \n\n \n# Feedback  \n\nSection 3: \u201cfeature extractor to embed a more detailed representation\u201d \u2026 I don\u2019t know if it is more detailed, but hopefully, it is more invariant, at least to the augmentations applied, ideally to all non-semantic attributes of the image. \n\nI guess it is implied that the baseline \u2018vis\u2019 approach in the paper  is identical to the \u2018vis\u2019 in He et al 2020, but it would be nice if this was explicitly in the caption for Table 1.  \n\n ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper894/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132337, "tmdate": 1606915802024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper894/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Review"}}}, {"id": "nKbMstJujAI", "original": null, "number": 1, "cdate": 1603654964039, "ddate": null, "tcdate": 1603654964039, "tmdate": 1605024580056, "tddate": null, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "invitation": "ICLR.cc/2021/Conference/Paper894/-/Official_Review", "content": {"title": "Where is the self-supervision signal (free) coming from?", "review": "The paper proposes an interesting video dataset with body-part movement signal, gaze signal. And they also treat these signals together with infoNCE/or AE as self-supervised signals. With random initialization on ResNet18, their model is better than the model only with infoNCE/AE. \n\nI agree with the author that their supervision is working and working together with infoNCE. However, this supervision does not come for free like Rotation/Speed/Contrastive learning. For example, how do you apply your signal on the same 'egocentric' video like 'Epic-kitchen' dataset?\n\nPros: a new dataset contributing to the 'affordance' society, a novel and useful supervision signal.\n\nCons: Even though the signal is useful, it does not come for free like SpeedNet, Rotationnet or infoNCE, they still need to capture the information when recording the data.\n\nIf the author can make the supervision more free?(self-supervision), I can change my rating.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper894/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper894/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions", "authorids": ["~Kiana_Ehsani1", "~Daniel_Gordon1", "~Thomas_Hai_Dang_Nguyen1", "~Roozbeh_Mottaghi1", "~Ali_Farhadi3"], "authors": ["Kiana Ehsani", "Daniel Gordon", "Thomas Hai Dang Nguyen", "Roozbeh Mottaghi", "Ali Farhadi"], "keywords": ["representation learning", "computer vision"], "abstract": "Learning effective representations of visual data that generalize to a variety of downstream tasks has been a long quest for computer vision. Most representation learning approaches rely solely on visual data such as images or videos. In this paper, we explore a novel approach, where we use human interaction and attention cues to investigate whether we can learn better representations compared to visual-only representations. For this study, we collect a dataset of human interactions capturing body part movements and gaze in their daily lives. Our experiments show that our ``\"muscly-supervised\" representation that encodes interaction and attention cues outperforms a visual-only state-of-the-art method MoCo (He et al.,2020), on a variety of target tasks: scene classification (semantic), action recognition (temporal), depth estimation (geometric), dynamics prediction (physics) and walkable surface estimation (affordance). Our code and dataset are available at: https://github.com/ehsanik/muscleTorch.", "one-sentence_summary": "We learn a muscly-supervised visual representation from human's interactions with the visual world.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ehsani|what_can_you_learn_from_your_muscles_learning_visual_representation_from_human_interactions", "supplementary_material": "/attachment/b6d8ee701764c517d880dbf83799ae046525b62a.zip", "pdf": "/pdf/68e63865a158882a87332ea2d2b8f196693b3f62.pdf", "venue": "ICLR 2021 Poster", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nehsani2021what,\ntitle={What Can You Learn From Your Muscles? Learning Visual Representation from Human Interactions},\nauthor={Kiana Ehsani and Daniel Gordon and Thomas Hai Dang Nguyen and Roozbeh Mottaghi and Ali Farhadi},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=Qm8UNVCFdh}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Qm8UNVCFdh", "replyto": "Qm8UNVCFdh", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper894/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538132337, "tmdate": 1606915802024, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper894/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper894/-/Official_Review"}}}], "count": 10}