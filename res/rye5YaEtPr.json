{"notes": [{"id": "rye5YaEtPr", "original": "rJl0km6PPB", "number": 679, "cdate": 1569439105721, "ddate": null, "tcdate": 1569439105721, "tmdate": 1583912041230, "tddate": null, "forum": "rye5YaEtPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "23W1iqagLq", "original": null, "number": 1, "cdate": 1576798703085, "ddate": null, "tcdate": 1576798703085, "tmdate": 1576800932929, "tddate": null, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "The reviewers all appreciated the results. They expressed doubts regarding the discrepancy between the assumptions made and the reality of the loss of deep networks.\n\nI share these concerns with the reviewers but also believe that, due to the popularity of Adam, a careful analysis of a variant is worthy of publication.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728850, "tmdate": 1576800281340, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper679/-/Decision"}}}, {"id": "ryghUaohjB", "original": null, "number": 8, "cdate": 1573858644015, "ddate": null, "tcdate": 1573858644015, "tmdate": 1573858644015, "tddate": null, "forum": "rye5YaEtPr", "replyto": "ByZyAKc2jS", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "We thank the reviewer for the response.\n\nQ1: However, given that ICLR is a specialized conference (compared to ICML or Neurips), assuming a very strong structure, which does not hold in many applications that ICLR community is interested in, causes me to justify the suitability of the paper for the conference.\n\nA1: We understand that ICLR is more related to representation learning/deep learning, and thus performed experiments on deep neural networks (including a 4-layer CNN and ResNet-18) to examine the empirical performance of SAdam. We note that Mukkamala & Hein (2017) also applied SC-RMSprop to neural networks, and obtain promising results. So, the idea of using a faster decaying step size, although originally designed for strongly convex functions, could lead to superior practical performance even in some highly non-convex cases such as deep learning tasks.\n\n\nA2: We appreciate the insightful suggestion, and will investigate the performance of SAdam in non-convex optimization in future work."}, "signatures": ["ICLR.cc/2020/Conference/Paper679/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "ByZyAKc2jS", "original": null, "number": 7, "cdate": 1573853639504, "ddate": null, "tcdate": 1573853639504, "tmdate": 1573853951209, "tddate": null, "forum": "rye5YaEtPr", "replyto": "HkepewY3oB", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "Response to authors", "comment": "I thank the authors for their response. I provide further remarks:\n\nA1: I agree that analysis of Adam is more involved compared to RMSProp and I appreciate the authors' efforts in the paper and repeat that I think it is solid. However, given that ICLR is a specialized conference (compared to ICML or Neurips), assuming a very strong structure, which does not hold in many applications that ICLR community is interested in, causes me to justify the suitability of the paper for the conference.\n\nA2: I agree with the authors that their proposed ideas are interesting and worth studying. However, it is not easy to argue that these ideas are obtainable from the results of this paper. For example, in Ge et al. (2015), assumption of strong convexity is on the function $f(x) = E[\\phi(x)]$, whereas this paper's assumption, translated to the language of Ge et al. (2015) is on the component functions $\\phi(x)$ which is a much stronger assumption than strong convexity of $f(x)$. But as the authors pointed out earlier, it might be possible to relax the assumption, when specialized to stochastic optimization, but this is a future study. In addition, in Remark 7 in Ge et al. (2015), the authors use the step size $1/t$ to obtain faster convergence. I agree that a similar analysis can be carried out for SAdam (or a noised version), but this is also a future study.\n\nTo be clear on my previous comments on nonconvex optimization, the current research trend that is also cited by the authors is to show convergence of adaptive methods for nonconvex optimization. And what I was questioning is that, for the goal of showing convergence in the nonconvex setting, the specialized techniques for improving regret for strongly convex optimization, might not be very insightful."}, "signatures": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "HkepewY3oB", "original": null, "number": 6, "cdate": 1573848821186, "ddate": null, "tcdate": 1573848821186, "tmdate": 1573849117077, "tddate": null, "forum": "rye5YaEtPr", "replyto": "HJg5icVniH", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thanks for your response!\n\nQ1: Square root is removed from the term to have this faster decrease. Therefore, it is not surprising that the same modification improves the regret of Adam for strongly convex functions, after Adagrad and RMSProp.\n\nA1: We agree that the idea of removing the square root from $V$ is inspired by previous work. However, we note that the analysis of SAdam is more complicated since it involves the first-order momentum as well as a more general $\\beta_{2t}$ (compared to SC-RMSprop). Moreover, the regret bound for SC-RMSprop provided by Mukkamala & Hein (2017) is data-independent, while we derived the first data-dependent regret bound for SC-RMSprop.\n\nQ2: in the *strongly convex* case, the manipulation needed for improving regret for these methods is very specific to structure (i.e. strong convexity) and well-known. One can exploit the additional quadratic term coming from strong convexity to be able to use a faster decreasing step size, leading to smaller regret. But I am very doubtful that this idea will be useful for showing convergence for nonconvex optimization, when such a structure is not present.\n\nA2: We agree that global strong convexity does not hold in the non-convex setting. However, many real-world applications, including tensor decomposition (Ge et al., 2015), matrix sensing (Bhojanapalli et al., 2016), and over-parameterized neural networks (Du et al., 2019), exhibit strong local geometric properties similar to strong convexity in the global setting, and exploiting these properties may lead to much faster convergence to local (or global) minima. Therefore, we believe that our work is meaningful and will inspire the analysis of Adam-type algorithms under many non-convex settings that enjoy such strong convexity-like properties. \nTo give a simple example, we consider the strict saddle condition defined in Ge et al. (2015), which assumes that the loss function is strongly convex in a region close to local minimum. In their paper (Remark 7), the authors firstly use a noised version of SGD to output a point that is close to a local minimum, then employ standard SGD with step size 1/t to ensure that the algorithm can converge. The analysis of both procedures depends on the local strong convexity. A na\u00efve replacement of the latter algorithm with SAdam may lead to faster data-dependent theoretical guarantees, and it is also interesting to investigate whether a noised version of SAdam can find a point close to a local minimum faster under this condition.\n\n\n\nR. Ge, F. Huang, C. Jin, & Y. Yuan. Escaping from saddle points\u2014online stochastic gradient for tensor decomposition. In COLT, 2015.\nS. Bhojanapalli, B. Neyshabur, & N. Srebro. Global optimality of local search for low rank matrix recovery. In NIPS, 2016.\nS. Du, X. Zhai, B. Poczos, & A. Singh. Gradient Descent Provably Optimizes Over-parameterized Neural Networks. In ICLR, 2019. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper679/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "HJg5icVniH", "original": null, "number": 5, "cdate": 1573829281908, "ddate": null, "tcdate": 1573829281908, "tmdate": 1573829364841, "tddate": null, "forum": "rye5YaEtPr", "replyto": "rkezMo5rjr", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "Response to authors", "comment": "I thank the authors for their response and I provide further remarks below.\n\nA1: \n\nThank you for the explanation. I see that global strong convexity of $f_t$ is a common assumption for online convex optimization, used for OGD, Adagrad and RMSProp in the literature. But indeed, it would be nice to have a weaker assumption for the stochastic optimization setting.\n\nA2:\n\nIt is true that it might be possible to combine the guarantees in this paper with Metagrad-type framework, and the results of this paper would be important in this case. Authors can use such an approach instead of tuning the algorithms etc, but I see that line of work to be independent for evaluating this paper.\n\nA3: \n\nThank you for the experimental clarification.\n\nA4: \n\nI disagree on this point. It is true that OGD, Adagrad and RMSProp are analyzed under convex and strongly convex settings. However, the idea to improve the regret bound under strong convexity is essentially the same in all of them. We want to have a faster decreasing step size when there is strong convexity. It is clear how to achieve this in the non-adaptive case of OGD, one takes a step size $\\alpha_0/k$ where $\\alpha_0$ depends on strong convexity constant, instead of standard $\\alpha_0/\\sqrt{k}$. For the adaptive case, the idea goes back to a technical report of Duchi, Hazan, Singer from 2010 (please see Section 5 and discussions therein):\n\nJ. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. Technical Report 2010-24, UC Berkeley Electrical Engineering and Computer Science\nhttps://www2.eecs.berkeley.edu/Pubs/TechRpts/2010/EECS-2010-24.pdf\n\nThe idea for getting a smaller regret bound is therefore common for Adagrad, RMSProp and in this paper, for Adam. Square root is removed from the term $\\hat{V}$ to have this faster decrease. Therefore, it is not surprising that the same modification improves the regret of Adam for strongly convex functions, after Adagrad and RMSProp.\n\nSecond, I do not agree that the analysis of this paper will help on showing guarantees for nonconvex optimization with Adam. It is true that analyses for convex case, might give insights on the nonconvex studies. However, in the *strongly convex* case, the manipulation needed for improving regret for these methods is very specific to structure (i.e. strong convexity) and well-known. One can exploit the additional quadratic term coming from strong convexity to be able to use a faster decreasing step size, leading to smaller regret. But I am very doubtful that this idea will be useful for showing convergence for nonconvex optimization, when such a structure is not present.\n\nLastly, I see that this method can be useful in practice for neural networks, so it can be a nice heuristic that people might try when training neural networks. However, I think that this part is orthogonal to the theoretical contribution of the paper, which is to have smaller regret for strongly convex functions.\n\nFinally, I think that this is indeed a solid work, however I do not think that the theoretical results of this paper are very insightful for ICLR community on neural network training or nonconvex optimization. Practical results might be of independent interest as a heuristic, but I do not find those enough for accepting the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "H1lCDDQtsS", "original": null, "number": 4, "cdate": 1573627749586, "ddate": null, "tcdate": 1573627749586, "tmdate": 1573627749586, "tddate": null, "forum": "rye5YaEtPr", "replyto": "r1eacscHoB", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "saw the resp", "comment": " I saw the additional experiments on ResNet18 in Appendix H. So, the paper performs experiments on training one strongly convex function ($\\ell_2$-regularized softmax regression), and two neural networks (4-layer CNN and ResNet18). I am satisfied with the experiments now and feel they are sufficient for a conference paper.\n \nRegarding the theoretical analysis of the first-order momentum, I understand this is a challenging problem, and encourage the authors to investigate it."}, "signatures": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "Hyxyk3cSsB", "original": null, "number": 3, "cdate": 1573395415150, "ddate": null, "tcdate": 1573395415150, "tmdate": 1573395415150, "tddate": null, "forum": "rye5YaEtPr", "replyto": "SJxbm19sKB", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thanks for your comments!\n\nQ1: It would be interesting to report (for example in Appendix) a sensitivity analysis of SAdam, using different choices of \\beta_1 and \\beta_{2t}.\nA1: Thanks for your constructive suggestion and we will provide experimental results about the sensitivity with respect to \\beta_1 and \\beta_{2t} in the revised version. From our experience, SAdam performs well in a wide range of hyper-parameter choices.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper679/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "r1eacscHoB", "original": null, "number": 2, "cdate": 1573395348598, "ddate": null, "tcdate": 1573395348598, "tmdate": 1573395348598, "tddate": null, "forum": "rye5YaEtPr", "replyto": "HJgSWr8htH", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thanks for the comments!\n\nQ1: \u201cThe role of the first-order momentum is unclear\u2026is there some contribution on this aspect?\u201d\nA1: While our paper is the first to show that algorithms equipped with first-order momentum can achieve logarithmic regret bound for strongly convex functions, it is still an open problem to explicitly analysis the influence of this procedure. We note that all the regret bounds of Adam-like algorithms (e.g., Reddi et al., 2018; Chen et al., 2018a) suffer this limitation, and the advantage of first-order momentum is mainly proved by empirical studies. The difficulty is caused by the fact that the regret bound is data-dependent. Specifically, our regret bounds depend on the cumulation of all gradients g_t, and each g_t would be affected by the first-order momentum. We would like to analyze the influence of first-order momentum theoretically in the future.\n\n\nQ2: The 4-layer CNN in Section 4.2 is a bit small. It would be better if the authors test their algorithm on larger and more popular neural networks.\nA2: Thanks for the suggestion. We have applied our algorithm to the training of ResNet18 (He et al., 2016) in Appendix H. We are sorry that we forget to mention this clearly in the main paper, and will provide more experiments in the full version.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper679/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "rkezMo5rjr", "original": null, "number": 1, "cdate": 1573395210154, "ddate": null, "tcdate": 1573395210154, "tmdate": 1573395210154, "tddate": null, "forum": "rye5YaEtPr", "replyto": "Syg4z4lAFr", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thanks for the comments!\n\nQ1: \u201cIn the proof of Theorem 1, the authors use strong convexity with the x_*, so they can maybe replace global strong convexity to strong convexity restricted to the path towards the solution.\u201d\nA1: Thanks for the suggestion. We agree that it could be possible to replace the global strong convexity to restricted strong convexity. Then, we probably need to study the stochastic setting, and bound the excess risk instead of the regret. It is difficult to exploit restricted strong convexity in the analysis of regret of OCO. Because in the online setting, the optimal solution of each f_t is different and may not be x_*. Therefore, if we replace global strong convexity of each f_t by restricted strong convexity (with respect to its own optimal solution), the inequality about x_* (eq.(13)) can not be obtained, unless all f_t share the same optimal solution. We will study the restricted strong convexity as a future work.\n\nQ2: \u201cGiven that one might not know if the problem has strong convexity, \u2026 it is not clear if one should apply Adam or SAdam\u2026I would expect SAdam's step sizes to be not very suitable when there is no strong convexity\u2026the step size of SAdam depends on the global strong convexity parameter lambda.\u201d\nA2: First, we would like to emphasize that optimization under lambda-strong convexity is a classic problem which has been widely studied in both OCO and stochastic optimization (e.g., Hazan et al., 2007, Hazan & Kyle, 2014, Mukkamala & Hein, 2017, Chen et al. 2018). The problem is important by its own right.\nSecond, if the type of loss functions or the value of lambda is unknown to the learner, it is possible to combine the theoretical guarantees of Adam and SAdam by applying the universal algorithm framework (van Erven et al., 2016, Wang et al., 2019). The key idea is to simultaneously run multiple copies of each algorithm with different learning rates in every round, and adaptively learn the best one on the fly. In this way, the algorithm can handle both convex and strongly convex functions, and does not need to know any prior knowledge of lambda. It is an interesting problem and will be investigated in the future. \n\nQ3: How do the authors pick step sizes now since it depends on strong convexity constant as in eq. (7).\nA3: Following previous work (Mukkamala & Hein, 2017), for all optimization algorithms, we pick the step sizes in the set {10^{-1},10^{-2},10^{-3} , 10^{-4}} and report the best results.\n\nQ4: \u201cGiven that neural networks are certainly non-strongly convex\u2026I would suspect that much worse effects can be seen for non-convex optimization.\u201d\nA4: We agree that currently there still exists a gap between the theoretical analysis of SAdam and its applications to training networks. However, we note that, initially, most of the popular algorithms such as Adagrad, Adam, AMSgrad and SC-RMSprop, are analyzed under the convex assumption or strongly convex assumption. Although these assumptions are violated in training networks, these algorithms have exhibited outstanding results in the experiments. Moreover, the analysis in convex setting lays the foundations of many follow-up works that investigate the non-convex problems (e.g., Basu et al., 2018, Chen et al., 2019, Staib et al., 2019). In this paper, we prove that our proposed SAdam is able to attain tighter regret bounds under strongly convex condition, and empirically show that it achieves better performance for training some networks. We believe our results are meaningful and could inspire the analysis of Adam-type algorithms under non-convex settings.\n\n\nT. van Erven, and W. M. Koolen. Metagrad: Multiple learning rates in online learning. In NIPS, pages 3666\u20133674, 2016. \nG. Wang, S. Lu, and L. Zhang. Adaptivity and optimality: A universal algorithm for online convex optimization. In UAI, 2019.\nM. Staib, S. J. Reddi, S. Kale, S. Kumar, & S. Sra. Escaping saddle points with adaptive gradient methods. arXiv preprint arXiv:1901.09149, 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper679/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rye5YaEtPr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper679/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper679/Authors|ICLR.cc/2020/Conference/Paper679/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504167873, "tmdate": 1576860552337, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper679/Authors", "ICLR.cc/2020/Conference/Paper679/Reviewers", "ICLR.cc/2020/Conference/Paper679/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Comment"}}}, {"id": "SJxbm19sKB", "original": null, "number": 1, "cdate": 1571688217382, "ddate": null, "tcdate": 1571688217382, "tmdate": 1572972565648, "tddate": null, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "In the setting of online convex optimization, this paper investigates the question of whether adaptive gradient methods can achieve \u201cdata dependent\u201d logarithmic regret bounds when the class of loss functions is strongly convex. To this end, the authors propose a variant of Adam - called SAdam - which indeed satisfies such a desired bound. Importantly, SAdam is an extension of SC-RMSprop (a variant of RMSprop) for which a \u201cdata independent\u201d logarithmic bound was found. Experiments on optimizing strongly convex functions and training deep networks show that SAdam outperforms other adaptive gradient methods (and SGD).  \n\nThe paper is very well-written, well-motivated and well-positioned with respect to related work. The regret analysis of SAdam is conceptually simple and elegant. The experimental protocol is well-detailed, and the results look promising. In a nutshell, this is an excellent piece of work.\n\nI have just a minor comment. In the experiments, SAdam was tested using $\\beta_1 = 0.9$ and $\\beta_{2t} = 1 - \\frac{0.9}{t}$. Since Corollary 2 covers a wide range of admissible values for these parameters, it would be interesting to report (for example in Appendix) a sensitivity analysis of SAdam, using different choices of $\\beta_1$ and $\\beta_{2t}$. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575578987602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper679/Reviewers"], "noninvitees": [], "tcdate": 1570237748659, "tmdate": 1575578987623, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Review"}}}, {"id": "HJgSWr8htH", "original": null, "number": 2, "cdate": 1571738877292, "ddate": null, "tcdate": 1571738877292, "tmdate": 1572972565608, "tddate": null, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this paper, the authors propose a variant of Adam, named as SAdam, and establish a data-dependent O(log T) regret bound. The key idea is using a faster decaying yet under controlled step size to exploit strong convexity. Some experiments are carried out to demonstrate the effectiveness of the proposed algorithm. The idea seems interesting, the writing is well-written, and the analysis seems correct (I did not fully check all steps, but the key steps seems ok to me). \n\nProbs:\n1. The proposed SAdam is an effective variant of Adam designed for strongly convex functions. The algorithm is a natural extension of Adam, and SC-RMSprop could be regarded as a special case.\n2. The authors establish a data-dependent O(log T) regret bound for SAdam, and as a byproduct, they present the first data-dependent logarithmic regret for SC-RMSprop. The authors also fix a small bug in the analysis of AMSgrad. The theoretical result is the key technical contribution of this paper.\n3. The experimental results shows that Aadam can be used to minimize strongly convex functions, as well as neural networks, which is believed to be non-convex.\n\nCons:\n1. As the authors mentioned in Remark 2, the main limitation of their analysis is that the role of the first-order momentum is unclear. Although the first-order momentum can accelerate the convergence in practice, proving this in theory remains an open problem. Is there some contribution on this aspect? \n\n2. The 4-layer CNN in Section 4.2 is a bit small. It would be better if the authors test their algorithm on larger and more popular neural networks.\n\nIn summary, this paper contributes the theoretical studies of ADAM-type algorithm, although the algorithm is somehow incremental. To me, a bit surprising result is that the step size originally designed for strongly convex functions also works well for training CNN.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575578987602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper679/Reviewers"], "noninvitees": [], "tcdate": 1570237748659, "tmdate": 1575578987623, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Review"}}}, {"id": "Syg4z4lAFr", "original": null, "number": 3, "cdate": 1571845132362, "ddate": null, "tcdate": 1571845132362, "tmdate": 1572972565571, "tddate": null, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "invitation": "ICLR.cc/2020/Conference/Paper679/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper studies Adam and proves that under strong convexity assumption, it obtains the improved regret bound $O(log(T))$. The regret bound is data-dependent, thus as a side-effect it also improves previous known result for strongly convex RMSProp (SC-RMSProp).\n\nThe paper is clear and well-written and I also think that theoretical results are correct and new. However, I have some concerns on the possible impacts of the results especially in the context of ICLR:\n\n- First of all, the assumption to show improved regret is strong convexity of all functions $f_t$. However, this is very restrictive and much stronger than the assumption that the sum of functions is strongly convex. In addition, from what I see, in the proof of Theorem 1, the authors use strong convexity with the $x^\\star$, so they can maybe replace global strong convexity to strong convexity restricted to the path towards the solution. A reference where these restricted strong convexity type assumptions are studied:\n\nNecoara, Nesterov, Glineur, \u201cLinear convergence of first order methods for non-strongly convex optimization\u201d, Math. Prog. 2019.\n\n- To show improved regret, consistent with previous work SC-Adagrad and SC-RMSProp, the authors modify the algorithm to use $V_t^{-1}$ in page 3 in the step size, instead of $V_t^{-1/2}$ of regular Adam. It is easy to see that this is to make sure step size has a faster decrease, which is needed also to show standard SGD gets $1/k$ rate for strongly convex problems. However, given that one might not now if the problem has strong convexity (there might exist cases where this property only exists locally), it is not clear if one should apply Adam or SAdam. \n\n- Another remark related to the previous one is the following. Standard SGD uses step size $\\alpha_0/\\sqrt{k}$ for convex optimization without strong convexity and $\\alpha_0/k$ for strongly convex optimization. If one uses $alpha_0/k $for convex optimization without strong convexity, one gets a very bad rate $1/log(k)$ and very bad practical performance. So, given SAdam gives step sizes suited for strongly convex optimization (similar to SGD for strongly convex optimization), I would expect SAdam's step sizes to be not very suitable when there is no strong convexity.\n\n- An additional point is that the step size of SAdam depends on the global strong convexity parameter $\\lambda$ which further restricts the applicability of the method. For the theoretical results to hold, the step size should be set according to $\\lambda$, and when the step size is not selected that way, one loses the fast convergence rate.\n\n- In the experiments, the authors show the performance of SAdam for neural network training and related to my previous remarks, I have the following concerns. First of all, how do the authors pick step sizes now since it depends on strong convexity constant as in eq. (7). In addition, given that neural networks are certainly non-strongly convex, I would expect that the fast decreasing step size caused by using $V_t^{-1}$ might also hurt the performance considerably, which happens as I discussed above even for convex but non-strongly convex losses. I would suspect that much worse effects can be seen for non-convex optimization. Of course, the authors can argue that if the loss landscape of neural network has some local strong convexity parameters, SAdam would adapt and get faster convergence. But unfortunately, I would not agree with such a statement, because the analysis is not made to adapt to local strong convexity and a dependence to strong convexity constant is present due to eq. (7), so if one does not know the constant, the theoretical guarantees would not apply. In addition, the provided experiments for neural network training is not extensive enough to convince practitioners to use SAdam instead of Adam which has been used for years.\n\nOverall, I think that it is interesting to see that a variant of Adam can be shown to obtain improved regret under strong convexity, I find the assumptions strong and the impact for neural network training, therefore for ICLR, quite questionable."}, "signatures": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper679/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "SAdam: A Variant of Adam for Strongly Convex Functions", "authors": ["Guanghui Wang", "Shiyin Lu", "Quan Cheng", "Wei-wei Tu", "Lijun Zhang"], "authorids": ["guhuwang@gmail.com", "lsy1116@qq.com", "chengquangm@gmail.com", "tuwwcn@gmail.com", "zljzju@gmail.com"], "keywords": ["Online convex optimization", "Adaptive online learning", "Adam"], "TL;DR": "A variant of Adam for strongly convex functions", "abstract": "The Adam algorithm has become extremely popular for large-scale machine learning. Under convexity condition, it has been proved to enjoy a data-dependent $O(\\sqrt{T})$ regret bound where $T$ is the time horizon. However, whether strong convexity can be utilized to further improve the performance remains an open problem. In this paper, we give an affirmative answer by developing a variant of Adam (referred to as SAdam) which achieves a data-dependent $O(\\log T)$ regret bound for strongly convex functions. The essential idea is to maintain a faster decaying yet under controlled step size for exploiting strong convexity. In addition, under a special configuration of hyperparameters, our SAdam reduces to SC-RMSprop, a recently proposed variant of RMSprop for strongly convex functions, for which we provide the first data-dependent logarithmic regret bound. Empirical results on optimizing strongly convex functions and training deep networks demonstrate the effectiveness of our method.", "pdf": "/pdf/f883ae3febb0e6041c7439f676d7551915f421e6.pdf", "code": "https://github.com/SAdam-ICLR2020/codes", "paperhash": "wang|sadam_a_variant_of_adam_for_strongly_convex_functions", "_bibtex": "@inproceedings{\nWang2020SAdam:,\ntitle={SAdam: A Variant of Adam for Strongly Convex Functions},\nauthor={Guanghui Wang and Shiyin Lu and Quan Cheng and Wei-wei Tu and Lijun Zhang},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=rye5YaEtPr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/c938788e2f3564e1e2aec9f689bc28fdf1ef2b90.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rye5YaEtPr", "replyto": "rye5YaEtPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper679/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575578987602, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper679/Reviewers"], "noninvitees": [], "tcdate": 1570237748659, "tmdate": 1575578987623, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper679/-/Official_Review"}}}], "count": 13}