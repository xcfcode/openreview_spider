{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396398861, "tcdate": 1486396398861, "number": 1, "id": "SkvlnG8ug", "invitation": "ICLR.cc/2017/conference/-/paper162/acceptance", "forum": "BkXMikqxx", "replyto": "BkXMikqxx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "There is consistent agreement towards the originality of this work and that the topic here is \"interesting\". Additionally there is consensus that the work is \"clearly written\", and (excepting questions of the word \"cortical\") all would be primed to accept this style of work. \n \n However there is a shared concern about the quality and potential impact of the work, in particularly in terms of the validity of empirical evaluations. Reviewers are generally not inclined to believe that the current empirical evidence validates the conclusions of the word. Suggestions are to: make greater use of a language model, compare to external baselines, or remove the handwriting aspects."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396401210, "id": "ICLR.cc/2017/conference/-/paper162/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkXMikqxx", "replyto": "BkXMikqxx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396401210}}}, {"tddate": null, "tmdate": 1485168461033, "tcdate": 1482237546452, "number": 3, "id": "SkG_8jUVe", "invitation": "ICLR.cc/2017/conference/-/paper162/official/review", "forum": "BkXMikqxx", "replyto": "BkXMikqxx", "signatures": ["ICLR.cc/2017/conference/paper162/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper162/AnonReviewer1"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "This paper explores the use of Open Bigrams as a target representation of words, for application to handwriting image recognition. \n\nPros:\n- The use of OBs is novel and interesting.\n- Clearly written and explained.\n\nCons:\n- No comparison to previous state of the art, only with author-generated results. \n- More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.\n- While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. https://arxiv.org/abs/1406.2227 [1] and https://arxiv.org/abs/1412.5903 [2]. Both use bag of ngrams models and achieve state of the art results, so it would be interesting to see whether open bigrams in the same experimental setup as [1] would yield better results. \n- Why not use a graph-based decoder like in Fig 2 b?\n\nOverall an interesting paper but the lack of comparisons and benchmarks makes it difficult to assess the reality of the contributions.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512678758, "id": "ICLR.cc/2017/conference/-/paper162/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper162/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper162/AnonReviewer2", "ICLR.cc/2017/conference/paper162/AnonReviewer3", "ICLR.cc/2017/conference/paper162/AnonReviewer1"], "reply": {"forum": "BkXMikqxx", "replyto": "BkXMikqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512678758}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1483429820812, "tcdate": 1478257418941, "number": 162, "id": "BkXMikqxx", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkXMikqxx", "signatures": ["~Th\u00e9odore_Bluche1"], "readers": ["everyone"], "content": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1483349206345, "tcdate": 1483349206345, "number": 5, "id": "rJRRncvSg", "invitation": "ICLR.cc/2017/conference/-/paper162/public/comment", "forum": "BkXMikqxx", "replyto": "BJk3aarEg", "signatures": ["~Th\u00e9odore_Bluche1"], "readers": ["everyone"], "writers": ["~Th\u00e9odore_Bluche1"], "content": {"title": "Answers to AnonReviewer3", "comment": "Thank you for the detailed review and comments\n\n> - I find the \"cortical inspired\" claim troublesome. If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. \n\nOur title is: \"Cortical inspired open-bigram representation for handwritten word recognition\".  \"Cortical inspired\" refers to the hypothesis that there exists a map (or layer) in the cortex that is devoted to code for bigrams.\nIt is true that we know about this hypothesis thanks to cognitive psychology experiments and models, but it seems more logical to refer to the subject's study (bigrams' map) instead of the research domain (cognitive prychology). As for the recurrent question about the legitimacy of a cortical map (or layer) devoted to code for bigrams, recent results (Glasser 2015) establish that the cortex is made of about 360 maps (180 per hemisphere), and that indeed there is a igram representation (Vinckier 2011). It is enough to get the inspiration that one of this 360 maps codes for bigrams...\n\n- Glasser, M. F., Coalson, T., Robinson, E., Hacker, C., Harwell, J., Yacoub, E., ... & Smith, S. M. (2015). A Multi-modal parcellation of human cerebral cortex. Nature.\n- Vinckier, Fabien, Emilie Qiao, Christophe Pallier, Stanislas Dehaene & Laurent Cohen (2011). The impact of letter spacing on reading: a test of the bigram coding hypothesis. Journal of Vision 11: 1-21.\n\n> Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more \"findings in cognitive neurosciences [sic] research on reading\" (p. 8) to substantiate those claims. \n\nYes, there are direct evidences (Vinckier 2011, Grainger 2014).\n\n- Vinckier, Fabien, Emilie Qiao, Christophe Pallier, Stanislas Dehaene & Laurent Cohen (2011). The impact of letter spacing on reading: a test of the bigram coding hypothesis. Journal of Vision 11: 1-21.\n- J Grainger, T Hannagan (2014). What is special about orthographic processing? Written Language & Literacy 17 (2), 225-252.\n\n\n> I am further worried by the fact that the authors seem to think that \"deep neural networks are based on a series of about five pairs of neurons [sic] layers\". \n\nWe will change the formulation. We don't claim that all deep neural nets need five hidden layers, but rather that the number of layers is larger than one or two. I agree that this sentence is misleading, we will reformulate the analogy.\n\n> Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? \n\nNo, not specifically. LeNet-5 is another example, the RNNs for handwriting recognition by Graves (2008) too.\n\n> I hope you don't mean to imply that all deep neural nets need five layers. \n\nWe don't. \n\n> It is also not true that ten is \"quite close to the number of layers of an efficient deep NN\" -- what network? what task? etc.\n\nWell, most deep nets in the litterature (for computer vision, speech recognition, ...) have between 5 and 15 layers. \nOf course, it depends on the task.\nWe will reformulate the analogy between the cortex and deep (artificial) neural networks.\n\n\n> - The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, \n\nWe will extend A.3 to provide more details\n\n> but this does not include e.g. the objective function,\n\nSect. 4.2 : \"We trained one RNN for each order-d bigram, with the Connectionist Temporal Classification (CTC (Graves et al.,2006)) criterion. [...] The CTC training criterion optimizes the Negative Log-Likelihood~(NLL) of the correct label sequence. We set the learning rate to 0.001, [...]\"\n\n> or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). \n\nI guess you refer to \"The first LSTM layers have 100 hidden LSTM units. The outputs of two consecutive timesteps of both directions are fed to a feed-forward layer with 100 nodes,\". We will reformulate : the \"outputs\" there are the outputs of the first BLSTM ; the \"two consecutive timesteps\" allow a subsampling, but both timesteps are used as inputs of the next layer. We will clarify this in a more detailed version of A.3.\n\n\n> This is probably because the paper argues that it \"is focused on the decoder\" (p. 6), rather than on the whole problem. \n\nWe present a system for the whole problem of recognizing words from a visual input. The novelty only lies in the decoder so we decided to focus the paper on the decoder. We will add more details on the neural networks in appendix A.3. but this part won't contain any scientific contribution.\n\n> I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams\n\nThe motivation of this approach lies in theories of reading mechanisms in the brain (i.e. reconstruct a word from a visual input). \nOpen bigrams are a way of representing the words (alternative to character sequences).  \nWe will add the results of the decoder only, i.e. assume we know already the open-bigrams to measure how easy it is to reconstruct a word from its open bigrams \n\n> which has very little to do with handwriting recognition \n\nOffline handwriting recognition consists in recognizing words from a visual input. \n\n> (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?\n\nThey were chosen because they are \"reading\" tasks. Indeed, a text corpus could be used to evaluate the decoder, but in that case the open-bigrams will be known perfectly, and I fear that the results would be too high to have an interest, just like evaluating a prefix tree decoder on a text corpus would have a limited interest.\n\n\n> - The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. \n\nThe Viterbi decoder also has access to all character positions and ordering. The OB decoder only knows which bigrams are present, but not the order, count, or position.\n\n> Did the Viterbi model have access to the word boundary information (at one point rather confusingly called \"extremities\") that pushed the open bigram model over the edge in terms of performance? \n\nOf course. The Viterbi decoder sees a sequence of prediction, so the first character prediction is also the first boundary. \n\n> Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? \nWe added the comparisons with different orders (Table 7).\n\n\n> The dataset also appears to be biased in favor of the proposed approach (longer words, only ). \nThat is not true. See added figures 5 and 7.\n\n> I am not convinced that this paper really shows that open bigrams help. I very much like the idea of the paper, but I am simply not convinced by its claims.\n\nNote that we don't claim that the OB decoder is a better choice than the sequential decoder. Even though we sometimes got better empirical results, we conclude by saying that the approach is viable, and that it is possible to recognize words with a decoder operating on the OB representation.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287705751, "id": "ICLR.cc/2017/conference/-/paper162/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkXMikqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper162/reviewers", "ICLR.cc/2017/conference/paper162/areachairs"], "cdate": 1485287705751}}}, {"tddate": null, "tmdate": 1483345908715, "tcdate": 1483345908715, "number": 4, "id": "SkTgl9vHe", "invitation": "ICLR.cc/2017/conference/-/paper162/public/comment", "forum": "BkXMikqxx", "replyto": "SkG_8jUVe", "signatures": ["~Th\u00e9odore_Bluche1"], "readers": ["everyone"], "writers": ["~Th\u00e9odore_Bluche1"], "content": {"title": "Anwsers to AnonReviewer1", "comment": "Thanks for the feedback and relevant comments.\n\n> - No comparison to previous state of the art, only with author-generated results. \n\nNo fair comparison is possible since we limited ourselves to ([a-z]+) words. \nMoreover, we don't claim to improve handwriting recognition with this method, but rather to learn a representation of words inspired from neuroscience, and to build a decoder operative on that representation.\n\n\n> - More ablation studies needed -- i.e. fill in Table3 with rnn0,1 rnn0,1,2 rnn0,1' etc etc. It is not clear where the performance is coming from, as it seems that it is single character modelling (0) and word endings (') that are actually beneficial.\n\nWe will add those experiments and results.\n\n> - While the use of Open bigrams is novel, there are works which use bag of bigrams and ngrams as models which are not really compared to or explored. E.g. https://arxiv.org/abs/1406.2227 [1] and https://arxiv.org/abs/1412.5903 [2]. Both use bag of ngrams models and achieve state of the art results, so it would be interesting to see whether open bigrams in the same experimental setup as [1] would yield better results.\n\nI agree, it would be interesting, although the goal of the COGNILEGO project was to \"[develop] novel systems for handwriting recognition based on the latest advances in cognitive perception research.\" (http://cognilego.univ-tln.fr/doku.php).\nIt would be also interesting to see how [1] and [2] perform in our setup.\n \n> - Why not use a graph-based decoder like in Fig 2 b?\n\nThe assumption in the approach is that we only have a set of bigrams. I don't see a simple and efficient way (compared to the cosine similarity) to build a decoder from the hypothetical graph of Fig. 2b. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287705751, "id": "ICLR.cc/2017/conference/-/paper162/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkXMikqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper162/reviewers", "ICLR.cc/2017/conference/paper162/areachairs"], "cdate": 1485287705751}}}, {"tddate": null, "tmdate": 1482226847976, "tcdate": 1482226847976, "number": 3, "id": "B1_s2dIVx", "invitation": "ICLR.cc/2017/conference/-/paper162/public/comment", "forum": "BkXMikqxx", "replyto": "ByaIT9WEg", "signatures": ["~Th\u00e9odore_Bluche1"], "readers": ["everyone"], "writers": ["~Th\u00e9odore_Bluche1"], "content": {"title": "Answers to AnonReviewer2", "comment": "Thank you for the comments and feedback, and for pointing out the typos.\n\n> It would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.\n\nWe added the number of parameters of the RNNs used for the experiments in Appendix A (Table 5., about 2M for each of the RNNs). \n\n\n> For your comparative experiments you use only 70% of the data by choosing longer words only. \n\nThat is not quite correct. We chose the words written with lowercase letters and single-letter words. It indeed filters out the frequent \"a\" in English, and punctuation marks, which are of length 1, but also many words with capital letters or accent. The distribution of word lengths of filtered words will be added in the appendices.\n\n\n> I do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.\n\nThe goal of this research paper was to evaluate a decoder based on the findings in cognitive research. We wanted to focus on the ability to recognize a word from a set of unordered bigram predictions. Thus the order is not expected to be present or available at any point. At the implementation level, we use RNNs with a notion of order, but this is not the focus of the paper. In the ideal case (in the hypothesis that reading is achieved via bigram coding and not explicit ordering), the recognition system should output a set of bigram and not model the order. \n\nHowever, in Table 6 (prev. Table 5), we report the edit distance (in character/bigram sequences) and sequence error rate (~= word error rates) when only the order is varied.\n\n\n> End of page 1: \"whole language method\" - please explain what is meant by this.\n\nThe \"whole language\" method is a teaching method for learning how to read (https://en.wikipedia.org/wiki/Whole_language), often opposed to the phonics method.\nTo simplify, in the former method, children are taught to focus on the word globally, whereas in the latter, they learn to associate phonemes with graphemes, or spelling.\n\n\n> Page 6: define your notation for rnn_d(x,t).\nAdded.\n\n> The number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.  Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this.\n\nThanks for the suggestion, the column 0 is now separated."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287705751, "id": "ICLR.cc/2017/conference/-/paper162/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkXMikqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper162/reviewers", "ICLR.cc/2017/conference/paper162/areachairs"], "cdate": 1485287705751}}}, {"tddate": null, "tmdate": 1482182054780, "tcdate": 1482182054780, "number": 2, "id": "BJk3aarEg", "invitation": "ICLR.cc/2017/conference/-/paper162/official/review", "forum": "BkXMikqxx", "replyto": "BkXMikqxx", "signatures": ["ICLR.cc/2017/conference/paper162/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper162/AnonReviewer3"], "content": {"title": "Review", "rating": "4: Ok but not good enough - rejection", "review": "This paper uses an LSTM model to predict what it calls \"open bigrams\" (bigrams of characters that may or may not have letters inbetween) from handwriting data. These open bigrams are subsequently used to predict the written word in a decoding step. The experiments indicate that the system does slightly better than a baseline model that uses Viterbi decoding. I have some major concerns about this paper:\n\n- I find the \"cortical inspired\" claim troublesome. If anything, it is psychology/cognitive science inspired, in the sense that open bigrams appear to help for word recognition (Touzet et al. 2014). But the implied cortical characteristics, implicitly referred to e.g. by pointing to analogies between deep neural nets for object recognition and in that case the visual cortex, is unfounded. Is there any direct evidence from neuroscience that open-bigrams constitute a wholly separate layer in the cortex for a handwriting recognition task? Dehaene's work is a proposal, so you'll need to describe more \"findings in cognitive neurosciences [sic] research on reading\" (p. 8) to substantiate those claims. I am further worried by the fact that the authors seem to think that \"deep neural networks are based on a series of about five pairs of neurons [sic] layers\". Unless I misunderstand something, you are specifically referring to Krizhevsky's AlexNet here (which you should probably have cited there)? I hope you don't mean to imply that all deep neural nets need five layers. It is also not true that ten is \"quite close to the number of layers of an efficient deep NN\" -- what network? what task? etc.\n\n- The model is not clearly explained. There is a short paragraph in Appendix A.3. that roughly describes the setup, but this does not include e.g. the objective function, or answer why the network output is only considered each two consecutive time steps, rather than at each time step (or so it seems?). This is probably because the paper argues that it \"is focused on the decoder\" (p. 6), rather than on the whole problem. I find this problematic, because in that case we're effectively measuring how easy it is to reconstruct a word from its open bigrams, which has very little to do with handwriting recognition (it could have been evaluated on any text corpus). In fact, as the example on page 4 shows, handwriting is not necessary to illustrate the open bigram hypothesis. Which leads me to wonder why these particular tasks were chosen, if we are only interested in the decoding mechanism?\n\n- The comparison is not really fair. The Viterbi decoder only has access to unigrams, as far as I can tell. The only model that does better than that baseline has access to a lot more information, and does not do that much better. Did the Viterbi model have access to the word boundary information (at one point rather confusingly called \"extremities\") that pushed the open bigram model over the edge in terms of performance? Why is there no comparison to e.g. rnn_0,1' (unigram+bigram+boundary markers)? The dataset also appears to be biased in favor of the proposed approach (longer words, only ). I am not convinced that this paper really shows that open bigrams help.\n\nI very much like the idea of the paper, but I am simply not convinced by its claims.\n\nMinor points:\n- There are quite a few typos. Just a sample: \"independant\" (Fig.1), \"we evaluate an handwritten\", \", hand written words [..], an the results\", \"their approach include\", \"the letter bigrams of a word w is\", \"for the two considered database\"\n- Wouldn't it be easy to add how many times a bigram occurs, which would improve the decoding process? You can just normalize over the full counts instead of the binary occurrence counts.\n- The results in Table 5 are the same (but different precision) as the results in Table 2, except that edit distance and SER are added, this is confusing.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512678758, "id": "ICLR.cc/2017/conference/-/paper162/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper162/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper162/AnonReviewer2", "ICLR.cc/2017/conference/paper162/AnonReviewer3", "ICLR.cc/2017/conference/paper162/AnonReviewer1"], "reply": {"forum": "BkXMikqxx", "replyto": "BkXMikqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512678758}}}, {"tddate": null, "tmdate": 1481907541242, "tcdate": 1481907541242, "number": 1, "id": "ByaIT9WEg", "invitation": "ICLR.cc/2017/conference/-/paper162/official/review", "forum": "BkXMikqxx", "replyto": "BkXMikqxx", "signatures": ["ICLR.cc/2017/conference/paper162/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper162/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "This submission investigates the usability of cortical-inspired distant bigram representations for handwritten word recognition. Instead of generating neural network based posterior features for character (optionally in local context), sets posterior for character bigrams of different length are used to represent words.  The aim here is to investigate the viability of this approach and to compare to the standard approach.\n\nOverall, the submission is well written, although information is missing w.r.t. to the comparison between the proposed approach and the standard approach, see below.\n\nIt would be desirable to see the model complexity of all the different models used here, i.e. the number of parameters used.\n\nLanguage models are not used here. Since the different models utilize different levels of context, language models can be expected to have a different effect on the different approaches. Therefore I suggest to include the use of language models into the evaluation.\n\nFor your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question remains, if this choice is advantageous for one of the tasks, or not - corresponding quantitative results should be provided to be able to better evaluate the effect of using this constrained corpus. Without clarification of this I would not readily agree that the error rates are competitive or better than the standard approach, as stated at the end of Sec. 5.\n\nI do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. It would be interesting to see an experiment, where only the use of the order is varied, to differentiate the effect of the order from the effect of other aspects of the approach.\n\nEnd of page 1: \"whole language method\" - please explain what is meant by this.\n\nPage 6: define your notation for rnn_d(x,t).\n\nThe number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different.  Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. At least, the column for order 0 should be visually separated to highlight this.\n\n\nMinor comments: a spell check is recommended\np. 2: state-of-art -> state-of-the-art\np. 2: predict character sequence -> predict a character sequence\np. 3, top: Their approach include -> Their approach includes\np. 3, top: an handwritten -> a handwritten\np. 3, bottom: consituent -> constituent\np. 4, top: in classical approach -> in the classical approach\np. 4, top: transformed in a vector -> transformed into a vector\np. 5: were build -> were built\nReferences: first authors name written wrongly: Thodore Bluche -> Theodore Bluche\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512678758, "id": "ICLR.cc/2017/conference/-/paper162/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper162/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper162/AnonReviewer2", "ICLR.cc/2017/conference/paper162/AnonReviewer3", "ICLR.cc/2017/conference/paper162/AnonReviewer1"], "reply": {"forum": "BkXMikqxx", "replyto": "BkXMikqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512678758}}}, {"tddate": null, "tmdate": 1480328829109, "tcdate": 1480328829104, "number": 2, "id": "BJHFUKYGl", "invitation": "ICLR.cc/2017/conference/-/paper162/public/comment", "forum": "BkXMikqxx", "replyto": "BJ4x8tKze", "signatures": ["~Th\u00e9odore_Bluche1"], "readers": ["everyone"], "writers": ["~Th\u00e9odore_Bluche1"], "content": {"title": "RE: Various questions (2/2)", "comment": "> At the beginning of Sec. 5.1 you state that no language models were used. As far as I understand this applies to both baseline/standard and open-bigram approach, right? \n\nThat is correct\n\n> What would be the performance of the baseline model when using a language model?\n\nBoth models would benefit from a language model, although the benefits would depend on the corpus on which they are estimated. In (Bluche et al. 2013) we observed about 15% reduction of the error rate with a language model. On text lines (Bluche, 2015) we get about 25% improvement.\n\n> For your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question is, if this choice is advantageous for one of the tasks, or not. Can you comment on this?\n\nIn both cases, there are more errors for short words.\nHowever, I should point out that we did not choose the 70% longer words. We discarded words of length 1 (<< 30% of data) and words containing capital letters, punctuation/apostrophes, and accents.\n\n> I do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. Ignoring the sequential order, words being anagram of each other would only be distinguishable via their probability of occurrence. Even if including word boundary symbols into the open-bigrams, anagrams indistinguishable from unordered open-bigrams would remain, e.g. in English: \"parental\" vs. \"paternal\". Did you check the performance of a combination of your bigram features with an ordered generation/search process?\n\nOur goal was to evaluate a \u201cpure\u201d open bigrams approach. Therefore, we didn't try to improve the performance of our unordered bigrams model by introducing additional ordered information (apart from the boundary symbols). \nIt is true that anagrams are then indistinguishable. Getting into the details of biological vision, cognitive psychology told us that the quality of image depends on the distance to the eye focus point (viewing position effect - usually a little on the left of the middle of the word). Long words may require 2 eye saccades, i.e., two different focus points. By taking into account this parameter (letter distance to the focus point), it is then possible to code bigrams with continuous values ([0,1]) instead of \u201cAbsent/Present\u201d ({0,1}). Anagrams would then become distinguishable. \n\nMoreover, the RNNs are trained with CTC to output the bigrams of a given order sequentially. In Appendix B.2, the edit distance between predicted and true sequences of bigrams, and the sequence error are reported on Table 5 and correspond to an ordered search.\n\n> The number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different. Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. Could you comment on this?\n\nIndeed, they are not comparable. However, from the perspective of the decoder, they indicate the difficulty to some extend (more errors to be corrected by the open-bigram decoder). Taken indivudually, they give an idea of the quality of the RNNs predicting open-bigrams."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287705751, "id": "ICLR.cc/2017/conference/-/paper162/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkXMikqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper162/reviewers", "ICLR.cc/2017/conference/paper162/areachairs"], "cdate": 1485287705751}}}, {"tddate": null, "tmdate": 1480328732677, "tcdate": 1480328683825, "number": 1, "id": "BJ4x8tKze", "invitation": "ICLR.cc/2017/conference/-/paper162/public/comment", "forum": "BkXMikqxx", "replyto": "H10_h5VGe", "signatures": ["~Th\u00e9odore_Bluche1"], "readers": ["everyone"], "writers": ["~Th\u00e9odore_Bluche1"], "content": {"title": "RE: Various questions", "comment": "Thank you for the interesting and relevant comments and feedback.\n\n> The performance numbers provided for IAM and RIMES in Table 1 do not seem to be optimal. The best results known to me were published in [Poznansky & Wolf, CVPR 2016], with 3.9% WER for RIMES, and 6.44% WER for IAM.\n\nAs mentioned in the introduction of Sections 4 & 5, we only used a subset of these databases and could not compare with published results directly.\nRegarding [Poznansky & Wolf, CVPR 2016] the vocabulary size is not mentioned (from the contents of Rimes and IAM, I guess it is around 10-15k words) whereas our vocabulary contains 50k words. If we are to compare the results although they are not computed on the same datasets, those we report are close or better than the older results reported in Table I of [Poznansky & Wolf, CVPR 2016] (which were without data augmentation, as our system. And I should point out that most of these results are obtained at line -not word- level with out-of-vocabulary words in the test set). Finally, Poznansky & Wolf state that the current state of the art at the time of writing was Bluche et al. [10]. In Table 1, we report the results of the systems of that paper, trained on our subset, and with the same vocabulary, in order to have a fair comparison.\n\n\n> On the other hand, the model complexity chosen here, as described in Appendix A.3, seems to be limited, with only 100 hidden units per layer. Did you also check larger models, and does this influence the comparison between standard and proposed open-bigram approach?\n\nFor the standard approach we tried larger models for text-line recognition : the were slightly better on that task. For this work we selected the network architecture offering a good compromise between performance and complexity. We can probably get better results with larger models and more data, but we decided to focus the paper on validating the decoding approach.\n\n> Also, when comparing the standard approach with the open-bigram approach: what is the number of parameters in each case? It seems, that in case of the open-bigram, not only multiple RNNs are used, but also, the actual bigram RNNs (order >=1) do have a much larger output layer. \n\nThat is correct. However, we did not focus on the \"optical model\" but rather on the decoder in this paper. In that perspective, the open-bigram decodeur gets predictions from the RNNs that are not as good as those in the standard approach.\n\n> On the other hand, the standard approach would benfit from using context-dependent targets and might lead to a different result of the comparison. Did you check this?\n\nOn Rimes and IAM we do not get much better results with context dependent models. See for example:\n* Bluche, T., Ney, H., & Kermorvant, C. (2013). Tandem HMM with convolutional neural network for handwritten word recognition. In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 2390-2394). IEEE.\n* Bluche, T. (2015). Deep Neural Networks for Large Vocabulary Handwritten Text Recognition (Doctoral dissertation, Universit\u00e9 Paris Sud-Paris XI). (Table 4.3)\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287705751, "id": "ICLR.cc/2017/conference/-/paper162/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkXMikqxx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper162/reviewers", "ICLR.cc/2017/conference/paper162/areachairs"], "cdate": 1485287705751}}}, {"tddate": null, "tmdate": 1480009193134, "tcdate": 1480006773946, "number": 1, "id": "H10_h5VGe", "invitation": "ICLR.cc/2017/conference/-/paper162/pre-review/question", "forum": "BkXMikqxx", "replyto": "BkXMikqxx", "signatures": ["ICLR.cc/2017/conference/paper162/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper162/AnonReviewer2"], "content": {"title": "Various questions concerning modeling and evaluation", "question": "The performance numbers provided for IAM and RIMES in Table 1 do not seem to be optimal. The best results known to me were published in [Poznansky & Wolf, CVPR 2016], with 3.9% WER for RIMES, and 6.44% WER for IAM. On the other hand, the model complexity chosen here, as described in Appendix A.3, seems to be limited, with only 100 hidden units per layer. Did you also check larger models, and does this influence the comparison between standard and proposed open-bigram approach?\n\nAlso, when comparing the standard approach with the open-bigram approach: what is the number of parameters in each case? It seems, that in case of the open-bigram, not only multiple RNNs are used, but also, the actual bigram RNNs (order >=1) do have a much larger output layer. On the other hand, the standard approach would benfit from using context-dependent targets and might lead to a different result of the comparison. Did you check this?\n\nAt the beginning of Sec. 5.1 you state that no language models were used. As far as I understand this applies to both baseline/standard and open-bigram approach, right? What would be the performance of the baseline model when using a language model?\n\nFor your comparative experiments you use only 70% of the data by choosing longer words only. On the other hand, it is well known that the shorter words are more prone to result in misrecognitions. The question is, if this choice is advantageous for one of the tasks, or not. Can you comment on this?\n\nI do see the motivation for introducing open-bigrams in an unordered way due to the corresponding evidence from cognitive research. However, decision theoretically I wonder, why the order should be given up, if the underlying sequential classification problem clearly is of a monotonous nature. Ignoring the sequential order, words being anagram of each other would only be distinguishable via their probability of occurrence. Even if including word boundary symbols into the open-bigrams, anagrams indistinguishable from unordered open-bigrams would remain, e.g. in English: \"parental\" vs. \"paternal\". Did you check the performance of a combination of your bigram features with an ordered generation/search process?\n\nThe number of target for the RNNs modeling order 0 (unigrams effectively) and the RNNs modeling order 1 and larger are very much different. Therefore the precision and recall numbers in Table 2 do not seem to be readily comparable between order 0 and orders >=1. Could you comment on this?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Cortical-Inspired Open-Bigram Representation for Handwritten Word Recognition", "abstract": "Recent research in the cognitive process of reading hypothesized that we do\nnot read words by sequentially recognizing letters, but rather by identifing\nopen-bigrams, i.e. couple of letters that are not necessarily next\nto each other. \nIn this paper, we evaluate an handwritten word recognition method based on original\nopen-bigrams representation. We trained Long Short-Term Memory Recurrent Neural Networks\n(LSTM-RNNs) to predict open-bigrams rather than characters, and we show that\nsuch models are able to learn the long-range, complicated and intertwined dependencies\nin the input signal, necessary to the prediction. \nFor decoding, we decomposed each word of a large vocabulary into the set of\nconstituent bigrams, and apply a simple cosine similarity measure between this \nrepresentation and the bagged RNN prediction to retrieve the vocabulary word. \nWe compare this method to standard word recognition techniques based on \nsequential character recognition.\nExperiments are carried out on two public databases of handwritten words\n(Rimes and IAM), an the results with our bigram decoder are comparable \nto more conventional decoding methods based on sequences of letters.", "pdf": "/pdf/56cb1c6e5391871943a5b771bea4c66f54ca31a0.pdf", "TL;DR": "We propose an handwritten word recognition method based on an open-bigram representation of words, inspired from the research in cognitive psychology", "paperhash": "bluche|corticalinspired_openbigram_representation_for_handwritten_word_recognition", "keywords": [], "conflicts": ["a2ia.com", "univ-amu.fr", "univ-tln.fr"], "authors": ["Th\u00e9odore Bluche", "Christopher Kermorvant", "Claude Touzet", "Herv\u00e9 Glotin"], "authorids": ["tb@a2ia.com", "kermorvant@teklia.com", "claude.touzet@univ-amu.fr", "glotin@univ-tln.fr"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959431276, "id": "ICLR.cc/2017/conference/-/paper162/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper162/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper162/AnonReviewer2"], "reply": {"forum": "BkXMikqxx", "replyto": "BkXMikqxx", "writers": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper162/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959431276}}}], "count": 11}