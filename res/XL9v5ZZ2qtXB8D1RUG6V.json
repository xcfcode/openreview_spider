{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457651511141, "tcdate": 1457651511141, "id": "WL9xG0mgkf5zMX2Kf2m5", "invitation": "ICLR.cc/2016/workshop/-/paper/70/comment", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "r8lj136nnF8wknpYt5jv", "signatures": ["~Erich_Elsen1"], "readers": ["everyone"], "writers": ["~Erich_Elsen1"], "content": {"title": "Response to reviewer 12", "comment": "The main difficulty in implementing GRU and LSTM layers using this technique is larger parameter count (3 or 4x) which requires the layer size be proportionally smaller, which would mean very narrow layers.  We think the other technical challenges related to pipelining, barriers and how to map the parameters amongst the SMs for load balancing could be solved. Newer GPUs like Pascal will increase the available amount of registers to store parameters which combined with fp16 storage could increase the GRU and LSTM layer size to something reasonable.\n\nThe sentence you are referring to is saying that the performance of traditional gemm libraries (like CUBLAS or Nervana) is much better at layer sizes of 2048 or 2560 compared with 1152, so the advantage of persistent kernels is larger for deep and narrow networks.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455744971923, "ddate": null, "super": null, "final": null, "tcdate": 1455744971923, "id": "ICLR.cc/2016/workshop/-/paper/70/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/70/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457648546224, "tcdate": 1457648546224, "id": "r8lj136nnF8wknpYt5jv", "invitation": "ICLR.cc/2016/workshop/-/paper/70/review/12", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": ["ICLR.cc/2016/workshop/paper/70/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/70/reviewer/12"], "content": {"title": "Efficient pipelining and GPU synchronization for RNN training for 30X recurrent layer speed up and 10x system level speed up ", "rating": "6: Marginally above acceptance threshold", "review": "Important work in dealing with bottlenecks in training of the recurrent layers of an RNN, which allows for high GPU usage down to small mini-batch sizes of 4. I'm not familiar with other hand written assembler software pipelines and optimized implementations of a global barrier, making it difficult to gauge baseline comparisons to Nervana and CUBLAS (being the one small con), but the gains are impressive and should be reported. 30X on recurrent layers, 10x at system level and 45% of peak theoretical throughput for NVIDIA TitanX GPU.\n\nOne question, the implementation is focused on the recurrent computation of a vanilla RNN, but more and more we see GRUs or LSTMs in end to end system, which have more complicated recurrent dependencies. For something like a GRU where reset gates computed from one recurrent computation is applied as a pointwise multiplication on another recurrent computation, do you forsee any problems in your current pipeline strategy and optimized global barrier?\n\nOn page 3 you mention performance is much better at layer sizes 2048 or 2560, suggesting advantages of persistent implement as model become deeper and thinner (do you mean wider? as the comparison is to 1152?). ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579930099, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579930099, "id": "ICLR.cc/2016/workshop/-/paper/70/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "XL9v5ZZ2qtXB8D1RUG6V", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/70/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457630306631, "tcdate": 1457630306631, "id": "ZY9AKAGZ0I5Pk8ELfEy5", "invitation": "ICLR.cc/2016/workshop/-/paper/70/review/10", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": ["ICLR.cc/2016/workshop/paper/70/reviewer/10"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/70/reviewer/10"], "content": {"title": "Implementation paper on RNNs that greatly speed-up small mini-batches", "rating": "7: Good paper, accept", "review": "This is a good paper, adequate for the ICLR workshop track. Some comments:\n\n-Decreasing the batch size whilst maintaining a decent throughput in terms of examples / s is a crucial component of minibatch-style optimization methods. Thus, this work is important and should motivate the community towards that direction.\n-Do speed ups hold when decreasing the number of RNNs? Or decreasing their size?\n-In the data parallelism experiments with multiple GPUs, do the authors use synchronous SGD? If so, the effective batch size increases. Do they observe a degradation of training speed / epoch of data seen?\n-Are the authors going to release the source code? Given the amount of code sharing and deep learning platforms, it seems like the major contribution to the community would be to do so.\n-Please define SM", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579931649, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579931649, "id": "ICLR.cc/2016/workshop/-/paper/70/review/10", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "XL9v5ZZ2qtXB8D1RUG6V", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/70/reviewer/10", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457567231516, "tcdate": 1457567231516, "id": "81DGX2k1pU6O2Pl0UVMK", "invitation": "ICLR.cc/2016/workshop/-/paper/70/comment", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "lx9ryX6Zvt2OVPy8Cvy7", "signatures": ["~Erich_Elsen1"], "readers": ["everyone"], "writers": ["~Erich_Elsen1"], "content": {"title": "Response to Reviewer 11", "comment": "(Response broken over multiple comments due to length limits)\n\n[Response to Comment on very deep residual RNNs] : Note that we did include results on \"shallower\" RNNs with only 8 and 24 layers, showing a clear trend of improved performance with depth.  We did not include the results for smaller numbers of layers to save space (since this result has been demonstrated in prior work), but the trend continues down to 1 layer.  In our previous work, we showed that batch normalization improved performance with more than 3-4 layers (up to 7-10 layers), and in this work, we provide preliminary results that the combination of residual connections and batch normalization enables better performance up to 40-50 layers.  We agree that it would be interesting to study GRUs or LSTMs in future work, as this idea could also be applied to those architectures.  We plan to explore this next.\n \nNote that the higher WER rates compared to DS2 are a result of the use of a model with only 800ms of future context (as opposed bidirectional models with unlimited context), and a smaller training dataset (500 hours vs 11,000 hours).  The focus on models with less future context is done to make our models most relevant for deployment scenarios that are sensitive to latency.  The use of a smaller dataset was primarily due to time constraints with the submission.  We hoped that this would be acceptable given the emphasis on preliminary results in the CFP for this workshop.\n\n [Response to side impl details] : We also wish that we could share more implementation details about this work, but it was not possible to fit everything into the 3-page workshop format. We tried to include the most essential aspects (caching RNN weights in the register file and performing inter-SM communication with a global barrier), but it should be clear that including all of the details about the assembly level optimizations that went into this kernel would not have fit in the 3 page format.  We would be open to suggestions about additional details that could be added, but given the page limit, anything that we add will involve removing something else.  We expect to follow on this work with a longer paper and blog post explaining the kernel implementation in enough detail for others to replicate this work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455744971923, "ddate": null, "super": null, "final": null, "tcdate": 1455744971923, "id": "ICLR.cc/2016/workshop/-/paper/70/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/70/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457567182836, "tcdate": 1457567182836, "id": "GvV1w6WPkF1WDOmRiMQX", "invitation": "ICLR.cc/2016/workshop/-/paper/70/comment", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "lx9ryX6Zvt2OVPy8Cvy7", "signatures": ["~Erich_Elsen1"], "readers": ["everyone"], "writers": ["~Erich_Elsen1"], "content": {"title": "Response to Reviewer 11", "comment": "(Response broken over multiple comments due to length limits)\n\n[Response to 3] :\n  One of the main reasons to move to a smaller mini batch size is that it enables greater levels of parallelism, a point which the reviewer seems to have missed.  If convergence starts to slow down after a batch size of 1024, then with a mini-batch size of 64 per GPU, this limits data parallel runs to 16 GPUs.  With a mini-batch size of 4, it increases the limit to 128 GPUs.\n \n   We did not have space in the 3-page format to include empirical results showing slower convergence with very large mini-batch sizes.  Clearly in the extreme case it must be true that large minibatch sizes converge more slowly than smaller sizes.  For example, our models with a mini-batch size of 512 converge in about 20 epochs, and one would not expect a model with a mini-batch size of the entire training set (approximately 6 million samples) to converge in 20 iterations.  In practice we have found no difference in epochs to reach the same training accuracy for mini batch sizes between 1 and 1024, but beyond that, we have seen significantly slower convergence.  See the following empirical results showing that convergence is slower (in terms of epochs to the same level of performance).  Note that we searched over the other Nesterov SGD hyperparameters after changing the batch size.\n \n    Dev set cost at 10 epochs:\n    32 mini-batch : 48.2\n64 mini-batch : 48.1\n128 mini-batch : 48.2\n256 mini-batch : 48.2\n512 mini-batch : 48.2\n1024 mini-batch : 51.5\n2048 mini-batch : 75.3"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455744971923, "ddate": null, "super": null, "final": null, "tcdate": 1455744971923, "id": "ICLR.cc/2016/workshop/-/paper/70/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/70/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457567094379, "tcdate": 1457567094379, "id": "E8VYKO1GKI31v0m2iDP4", "invitation": "ICLR.cc/2016/workshop/-/paper/70/comment", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "lx9ryX6Zvt2OVPy8Cvy7", "signatures": ["~Erich_Elsen1"], "readers": ["everyone"], "writers": ["~Erich_Elsen1"], "content": {"title": "Response to Reviewer 11", "comment": "(Responses broken over multiple comments due length limits)\n\n[Response to 2] : While we do appreciate the importance of exploring alternative techniques like  activation caching in CPU memory and considering other processors types (like CPUs) for performing the RNN computation, we respectfully disagree with both of these points in the context of training RNNs on commodity GPU platforms for the following reasons.\n \n    Dense GPU systems like ours don't have enough PCIe and CPU memory bandwidth to stream the activations back to the CPU while running at full speed.  Specifically, running at approximately 3 TFLOP/s, a single 1152 recurrent layer finishes processing activations with 700 timesteps and mini-batch of 4 (per GPU) in approximately 2.4 milliseconds.  We use 8 GPUs in one system that share memory provided by 2 CPUs.  Using careful assignment of CPU memory to local NUMA nodes for individual GPUs, we sustain about 2.5 GB/s of CPU DRAM bandwidth per GPU when all 8 GPUs are copying data simultaneously. This is lower than the peak PCIe bandwidth of a single GPU connected to a single CPU because multiple GPUs are sharing PCIe routers, as well as the CPU memory controllers.  So transferring the activations to the CPU would take approximately 5.1 milliseconds, causing about a 2x slowdown for the complete system even if the copies and compute were perfectly overlapped.  Additionally, we already use the majority of available PCIe and CPU interconnect bandwidth to perform the all-reduce for data-parallel training, which reduces our available PCIe and CPU memory bandwidth for streaming activations even further. \n \n    Regarding CPU vs GPU performance, Intel's fastest commercially available Xeon processor is currently the E-5 2699v3 processor, which has a peak single precision floating point throughput of approximately 1.3 TFLOP/s.  This is approximately 5x lower than a single TitanX GPU's 6.1 TFLOP/s (using base clocks for both the CPU and the GPU) and a 20x difference between the 8 GPUs and 2 CPUs in one server.  We would like to note that CPU GEMM implementations are also sensitive to small batch sizes, where the more important metric becomes memory bandwidth.  The maximum memory bandwidth of the two Intel processors would be 126 GB/sec vs. 2692 GB/sec for the 8 GPUs. Although the CPUs are less memory capacity constrained, requiring a large batch size per CPU could still limit the maximum amount of data-parallel scaling.  So a CPU could also benefit from this technique.\n \n   Systems that use fewer GPUs or slower RNN implementations may not have these problems, but the motivation of our work is improving the performance of training RNNs, so we do care about factors of 2x and 5x.\n \n    Finally, this technique reduces the memory required to train an RNN in an absolute sense that is complementary to increasing the memory capacity.  For any memory capacity (12GB on a GPU, or 512 GB on a CPU), this technique increases the maximum utterance length or network size that can be trained."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "tmdate": null, "cdate": 1455744971923, "ddate": null, "super": null, "final": null, "tcdate": 1455744971923, "id": "ICLR.cc/2016/workshop/-/paper/70/comment", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "replyto": null, "writers": {"values-regex": "~.*"}, "forum": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "invitees": ["~", "ICLR.cc/2016/workshop/paper/70/reviewer/10"], "nonreaders": [], "noninvitees": []}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457133427650, "tcdate": 1457133427650, "id": "lx9ryX6Zvt2OVPy8Cvy7", "invitation": "ICLR.cc/2016/workshop/-/paper/70/review/11", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": ["ICLR.cc/2016/workshop/paper/70/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/70/reviewer/11"], "content": {"title": "GPU kernel trick to speed up computation by caching weights", "rating": "5: Marginally below acceptance threshold", "review": "The authors presented a GPU kernel trick to speed up computations for RNNs, key points:\n1] w/ minibatch size of 4, they claim 30x speedup compared to standard GEMM kernels. This is very impressive, such a simple engineering trick can yield 30x speedup!\n\n2] One of the arguments to use smaller minibatch sizes is due to memory (i.e., only 12G/24G in modern GPUs). For example, the authors demonstrated very deep RNNs in their paper. The memory argument isn't very strong since you can always cache the activations (quite easily, cheaply and transparently) to CPU memory using DMA. Additionally, a lot of groups also run CPU models for RNNs which have can easily have >128G of RAM per CPU (CPUs are not that much slower than GPUs in the RNN GEMM type ops esp w/ AVX512 + FMA).\n\n3] If the memory argument doesn't hold, the other reason to use a smaller minibatch is if it gives better convergence properties (whether theoretical or empirical). In theory, smaller minibatches should give better convergence, but in practice that may not necessarily be true due to the high variance of the gradients in RNNs. The authors didn't show whether the smaller minibatches did indeed give better empirical performance, if large minibatches are required to get state-of-the-art performance, then this would invalidate a lot of motivations to use smaller minibatches.\n\nComment on the very deep residual RNNs:\nThe authors also claimed that residual RNNs (i.e., skip connections) make possible optimizations of deep RNNs (i.e., see Table 1); compared w/o the skip connections, the deep (48 layers) of RNNs are difficult to converge. However, the authors did not compare their models to GRUs or LSTMs which are generally regarded as much better than RNNs, and stacking such deep RNNs may not be necessary. For example, many acoustic model and end-to-end speech models use only 3-4 layers for RNNs because deeper networks don't seem to help (see Sak et. al 2014; Bahdanau et. al 2015; and Chan et. al, 2015 for their acoustic models / end-to-end attention speech papers). The authors also didn't show that the deep RNNs out perform a \"shallow RNN\" (i.e., assuming the same dev/train sets are used, the Deep Speech 2 paper showed much lower WERs, suggesting that the deep residual RNNs are not necessary). However, part of their motivation is that w/ this kernel trick, we can explore much deeper RNNs.\n\nSide impl details:\nThe authors left out much detail on the implementation of their kernel. This would make it hard for open source groups to implement and/or replicate their work. Would be really nice/cool if the authors were willing to open source their kernel to Theano / Torch / TensorFlow etc...\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579930209, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579930209, "id": "ICLR.cc/2016/workshop/-/paper/70/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "XL9v5ZZ2qtXB8D1RUG6V", "replyto": "XL9v5ZZ2qtXB8D1RUG6V", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/70/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455744970342, "tcdate": 1455744970342, "id": "XL9v5ZZ2qtXB8D1RUG6V", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "XL9v5ZZ2qtXB8D1RUG6V", "signatures": ["~Erich_Elsen1"], "readers": ["everyone"], "writers": ["~Erich_Elsen1"], "content": {"CMT_id": "", "title": "Persistent RNNs: Stashing Weights on Chip", "abstract": "This paper introduces a framework for mapping Recurrent Neural Network (RNN) architectures efficiently onto parallel processors such as GPUs. Key to our ap- proach is the use of persistent computational kernels that exploit the processor\u2019s memory hierarchy to reuse network weights over multiple timesteps. Using our framework, we show how it is possible to achieve substantially higher computa- tional throughput at lower mini-batch sizes than direct implementations of RNNs based on matrix multiplications. Our initial implementation achieves 2.8 TFLOP/s at a mini-batch size of 4 on an NVIDIA TitanX GPU, which is about 45% of the- oretical peak throughput, and is 30X faster than a standard RNN implementation based on optimized GEMM kernels at this batch size. Reducing the batch size from 64 to 4 per processor provides a 16x reduction in activation memory foot- print, enables strong scaling to 16x more GPUs using data-parallelism, and allows us to efficiently explore end-to-end speech recognition models with up to 108 residual RNN layers.", "pdf": "/pdf/XL9v5ZZ2qtXB8D1RUG6V.pdf", "paperhash": "diamos|persistent_rnns_stashing_weights_on_chip", "conflicts": ["baidu.com"], "authors": ["Greg Diamos", "Shubho Sengupta", "Bryan Catanzaro", "Mike Chrzanowski", "Adam Coates", "Erich Elsen", "Jesse Engel", "Awni Hannun", "Sanjeev Satheesh"], "authorids": ["gregdiamos@baidu.com", "ssengupta@baidu.com", "bryan.catanzaro@baidu.com", "mikechrzanowski@baidu.com", "adamcoates@baidu.com", "erichelsen@baidu.com", "jengel@baidu.com", "awnihannun@baidu.com", "sanjeevsatheesh@baidu.com"]}, "nonreaders": [], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 8}