{"notes": [{"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1458138602048, "tcdate": 1458138602048, "id": "K1VMqRGvJu28XMlNCVoV", "invitation": "ICLR.cc/2016/workshop/-/paper/15/review/12", "forum": "jZ9WrEWPmsnlBG2XfGLl", "replyto": "jZ9WrEWPmsnlBG2XfGLl", "signatures": ["ICLR.cc/2016/workshop/paper/15/reviewer/12"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/15/reviewer/12"], "content": {"title": "In principle promising idea to add coverage information to an attention-based neural MT system, but paper is uncomprehensible as it is", "rating": "5: Marginally below acceptance threshold", "review": "This paper extends attention-based neural machine translation (NNMT) with a coverage model.  In the standard attention model, for each target word a subset of relevant source words are \"selected\" as context vector.  In principle, it can happen that source words are used several times or not all.  Therefore, the introduction of the notion of source word coverage in an NNMT attention model is an interesting idea (a coverage model is used in standard PB SMT).\n\nI don't agree with the statement that learning the coverage information by back-prop is potentially weak and one should add \"linguistic information\". In that case, one could question the whole idea to do NNMT - in such a model every \"decision\" is purely statistical without any linguistic information.\n\nThe description of the used coverage model itself is very complicated to understand. Given the space constraints, it seems a bad idea to first present a general model (Eqn 1) and than to use a much simpler one, which is insufficiently explained. I wasn't able to understand how the coverage model was calculated, how the fertility probabilities were obtained (Eqn 2+3), etc.\n\nFinally, the results are not analyzed - the authors just provide two figures and a table.  There is no information on what data the system was trained on nor the actual language pair !!  Also, I'm surprised that the BLEU score of the NNMT system decreases substantially with the length of the sentences (Figure 1 left).  This is in contrast to results by Bahdanau et al. who show that the attention model does prevent this decrease (plot RNN search-50 in Figure 2 in their paper) !  This raises some doubts on the experimental results ...\n\nwhy the attention coverage vector beta is uniformly initialized ? I expect it to be zero (nothing covered)\n - you use notation without defining it, e.g.\n    - what is d in \".. is a vector (d>1) ..\"\n    - s_i and h_j ; a small figure would be very helpful !\n\nSeveral sentences are difficult to understand and I spotted a couple of stupid errors (e.g. \"predefined constantto denoting\"). Please proof-read the paper !", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Coverage-based Neural Machine Translation", "abstract": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both translation and alignment qualities over NMT without coverage.", "pdf": "/pdf/jZ9WrEWPmsnlBG2XfGLl.pdf", "paperhash": "tu|coveragebased_neural_machine_translation", "conflicts": ["huawei.com", "ict.ac.cn"], "authors": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "authorids": ["tu.zhaopeng@huawei.com", "lu.zhengdong@huawei.com", "liuyang2011@tsinghua.edu.cn", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579933174, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579933174, "id": "ICLR.cc/2016/workshop/-/paper/15/review/12", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "jZ9WrEWPmsnlBG2XfGLl", "replyto": "jZ9WrEWPmsnlBG2XfGLl", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/15/reviewer/12", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "ddate": null, "cdate": null, "tmdate": 1457651522538, "tcdate": 1457651522538, "id": "ROVpzJnpYivnM0J1Ipq5", "invitation": "ICLR.cc/2016/workshop/-/paper/15/review/11", "forum": "jZ9WrEWPmsnlBG2XfGLl", "replyto": "jZ9WrEWPmsnlBG2XfGLl", "signatures": ["ICLR.cc/2016/workshop/paper/15/reviewer/11"], "readers": ["everyone"], "writers": ["ICLR.cc/2016/workshop/paper/15/reviewer/11"], "content": {"title": "interesting ideas and results for neural MT, but very difficult to understand and follow. I suspect this is due to hasty and overly-aggressive compression of the original paper to this 3-page format.", "rating": "5: Marginally below acceptance threshold", "review": "The paper is about introducing a notion of (soft) source-side coverage into neural MT models. The idea makes sense and is shown to produce reasonable gains in BLEU. \n\nThis version of the paper is extremely difficult to understand. I had to google for the uncompressed arxiv version to get any kind of confidence that I understood the paper. I would recommend that people read the original paper -- it is quite interesting.  I think the authors should have tried to actually write an extended abstract that conveys the key points, rather than trying to fit the entire formal description of their approach into the 3-page format. \n\nBelow are just a few of the notational issues in this version:\n\n$auxs$ --> maybe $\\psi$?\n\n\\alpha_{i,j} in Eq. (1) is not defined.\n\n\\phi(h_j) is used in the equation, but then \\phi_i(h_j) is defined immediately thereafter.  Which should it be?\n\nWhat is the \"decoding state\" s_i? This is not defined in the paper.\n\nThe equation in Section 2.1 uses \\alpha_{i,j} but below that the authors write \"Here we only employ \\alpha_{i-1}...\" -- this seems to be a mismatch. Or if it's not a mismatch, I don't understand what it means.\n\n\nThere is also no description of the experimental setup -- only some tables and plots are shown.  I think the work is interesting and compelling but I am hesitant to recommend acceptance of this paper as an ICLR workshop paper.  I would prefer that the authors submit this as a conference paper to another venue, like ACL, CoNLL, EMNLP, or COLING.  This paper would be a good fit for one of these NLP conferences. \n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"CMT_id": "", "title": "Coverage-based Neural Machine Translation", "abstract": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both translation and alignment qualities over NMT without coverage.", "pdf": "/pdf/jZ9WrEWPmsnlBG2XfGLl.pdf", "paperhash": "tu|coveragebased_neural_machine_translation", "conflicts": ["huawei.com", "ict.ac.cn"], "authors": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "authorids": ["tu.zhaopeng@huawei.com", "lu.zhengdong@huawei.com", "liuyang2011@tsinghua.edu.cn", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1456579932869, "ddate": null, "super": null, "final": null, "duedate": 1460725200000, "tcdate": 1456579932869, "id": "ICLR.cc/2016/workshop/-/paper/15/review/11", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "reply": {"pdf": null, "forum": "jZ9WrEWPmsnlBG2XfGLl", "replyto": "jZ9WrEWPmsnlBG2XfGLl", "writers": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)"}, "signatures": {"values-regex": "(~.*)|ICLR.cc/2016/workshop/paper/[0-9]+/reviewer/[0-9]+)", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "readers": ["everyone", "ICLR.cc/2016/workshop/paper/15/reviewer/11", "ICLR.cc/2016/workshop"], "expdate": 1468501200000}}}, {"tddate": null, "number": null, "replyto": null, "ddate": null, "cdate": null, "tmdate": 1455501491210, "tcdate": 1455501491210, "id": "jZ9WrEWPmsnlBG2XfGLl", "invitation": "ICLR.cc/2016/workshop/-/submission", "forum": "jZ9WrEWPmsnlBG2XfGLl", "signatures": ["~Zhaopeng_Tu1"], "readers": ["everyone"], "writers": ["~Zhaopeng_Tu1"], "content": {"CMT_id": "", "title": "Coverage-based Neural Machine Translation", "abstract": "Attention mechanism advanced state-of-the-art neural machine translation (NMT) by jointly learning to align and translate. However, attentional NMT ignores past alignment information, which leads to over-translation and under-translation problems. In response to this problem, we maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust the future attention, which guides NMT to pay more attention to the untranslated source words. Experiments show that coverage-based NMT significantly improves both translation and alignment qualities over NMT without coverage.", "pdf": "/pdf/jZ9WrEWPmsnlBG2XfGLl.pdf", "paperhash": "tu|coveragebased_neural_machine_translation", "conflicts": ["huawei.com", "ict.ac.cn"], "authors": ["Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "authorids": ["tu.zhaopeng@huawei.com", "lu.zhengdong@huawei.com", "liuyang2011@tsinghua.edu.cn", "liuxiaohua3@huawei.com", "hangli.hl@huawei.com"]}, "nonreaders": [], "details": {"replyCount": 2, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "tmdate": null, "cdate": 1454464564200, "ddate": null, "super": null, "final": null, "duedate": 1455833700000, "tcdate": 1454464564200, "id": "ICLR.cc/2016/workshop/-/submission", "writers": ["ICLR.cc/2016/workshop"], "signatures": ["ICLR.cc/2016/workshop"], "readers": ["everyone"], "reply": {"pdf": null, "forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"order": 4, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv.", "value-regex": "upload|http://arxiv.org/pdf/.+"}, "title": {"order": 3, "description": "Title of paper.", "value-regex": ".{0,500}"}, "abstract": {"order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"order": 1, "description": "Comma separated list of author names, as they appear in the paper.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "author_emails": {"order": 2, "description": "Comma separated list of author email addresses, in the same order as above.", "value-regex": "[^,\\n]+(,[^,\\n]+)*"}, "conflicts": {"order": 100, "description": "Semi-colon separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.).", "value-regex": "^([a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))+(;[a-zA-Z0-9][a-zA-Z0-9-_]{0,61}[a-zA-Z0-9]{0,1}\\.([a-zA-Z]{1,6}|[a-zA-Z0-9-]{1,30}\\.[a-zA-Z]{2,3}))*$"}, "CMT_id": {"order": 5, "value-regex": ".*", "description": "If the paper is a resubmission from the ICLR 2016 Conference Track, enter its CMT ID; otherwise, leave blank."}}}, "invitees": [], "nonreaders": [], "noninvitees": [], "expdate": 1463609700000}}}], "count": 3}