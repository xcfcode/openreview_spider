{"notes": [{"id": "rkgl51rKDB", "original": "BygeUdCdPr", "number": 1865, "cdate": 1569439624103, "ddate": null, "tcdate": 1569439624103, "tmdate": 1577168254942, "tddate": null, "forum": "rkgl51rKDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "f5ddPv_r1", "original": null, "number": 1, "cdate": 1576798734448, "ddate": null, "tcdate": 1576798734448, "tmdate": 1576800901966, "tddate": null, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "invitation": "ICLR.cc/2020/Conference/Paper1865/-/Decision", "content": {"decision": "Reject", "comment": "This paper combines PEARL with HAC to create a hierarchical meta-RL algorithm that operates on goals at the high level and learns low-level policies to reach those goals. Reviewers remarked that it\u2019s well-presented and well-organized, with enough details to be mostly reproducible. In the experiments conducted, it appears to show strong results.\n\nHowever there was strong consensus on two major weaknesses that render this paper unpublishable in its current form: 1) the continuous control tasks used don\u2019t seem to require hierarchy, and 2) the baselines don\u2019t appear to be appropriate. Reviewers remarked that a vital missing baseline is HER, and that it\u2019s unfair to compare to PEARL, which is a more general meta-RL algorithm. The authors don\u2019t appear to have made revisions in response to these concerns.\n\nAll reviewers made useful and constructive comments, and I urge the authors to take them into consideration when revising for a future submission.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722745, "tmdate": 1576800274108, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1865/-/Decision"}}}, {"id": "r1lVZ2-ZqH", "original": null, "number": 3, "cdate": 1572047868431, "ddate": null, "tcdate": 1572047868431, "tmdate": 1574389501307, "tddate": null, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "invitation": "ICLR.cc/2020/Conference/Paper1865/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #2", "review": "Update 11/21\nI maintain my score. I like the idea and hope the authors improve the paper and submit to a future conference.\n\nSummary\nThis paper combines hierarchical RL with meta-learning. The idea is that high-level plans transfer across settings (e.g. picking up a mug), while low-level execution may differ across tasks (e.g. different robot morphologies). To this end, the approach meta-learns a two-level hierarchical policy. The higher level policy conditions on a latent task context to produce high-level actions, or goals for the lower level policy. The lower level policy is trained via HER to reach these goals (it may need to be completely re-trained at test time). \n\nConcerns and Questions\nI am very concerned about the experimental results. I do not think that these tasks require hierarchy to solve, as the exact same tasks (with the same simulated robot) were solved in Hindsight Experience Replay, Andrychowicz al. 2017. Thus HER (preferably implemented with SAC rather than DDPG for fair comparison) is a vital baseline that is missing from Figure 3. Could the authors please address this point?\nWhile the introduction and Section 4 claim that one important benefit of such a hierarchical approach is that one could transfer to more disparate tasks, there are no experiments supporting this idea. I think the addition of these experiments would greatly strengthen the paper.\nIn Figure 3, does \u201cPEARL with sparse reward\u201d refer to only the encoder receiving sparse rewards or also the actor-critic?\n\nWriting suggestions\nA suggestion about the title: consider including the word \u201chierarchical\u201d \nIn some places the writing is quite informal, I suggest revising it (in the intro: \u201cDRL barely works\u201d.\nI disagree with the sentence in the intro \u201cIntuitively this is quite similar to how a human behaves\u201d, which is said in support of the idea of transferring high-level goals instead of low-level execution. Human behavior also supports the opposite view - that we reuse primitive motions over and over in support of new goals. So I think it\u2019s best to avoid the appeal to human behavior here (as well as in related work).\nThe first paragraph of section 4.2 is redundant and can be removed, or at least moved to the beginning of Section 4.\n\nIn conclusion, my current impression is that while the idea is interesting, the results achieve the same performance as a non-hierarchical method, which is not included as a baseline.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1865/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1865/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575334014732, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1865/Reviewers"], "noninvitees": [], "tcdate": 1570237731185, "tmdate": 1575334014745, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1865/-/Official_Review"}}}, {"id": "SyllxgTtFS", "original": null, "number": 1, "cdate": 1571569640464, "ddate": null, "tcdate": 1571569640464, "tmdate": 1574179670426, "tddate": null, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "invitation": "ICLR.cc/2020/Conference/Paper1865/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "This paper studies the problem of leveraging past experience to quickly solve new control tasks. The starting point (and perhaps the main contribution) is the observation that some tasks have similar high-level goals, while differing in how those goals are achieved. To that end, the paper introduces an meta-RL algorithm that, given a new task, attempts to solve it by adapting a high-level, goal-setting module, and learn a new, low-level policy to reach each commanded goal. The proposed method might be viewed as a combination of PEARL [Rakelly 19] and HAC [Levy 19]. The proposed method is compared against state-of-the-art hierarchical RL and meta-RL methods on four robotic manipulation tasks. The proposed method outperforms the baselines on each task.\n\nWhile the proposed method is quite strong empirically, I am leaning towards rejecting this paper because many of the claims made in the paper are not empirically validated. While much emphasis is put on the hierarchical aspect of the algorithm, I don't think that the tasks used in the experiments require hierarchy to solve (see [Plappert 18]). In the introduction, the second claim is that the proposed method \"focus[es] on meta learning the overall strategy \u2026 [and] provides a simpler and better way for meta RL.\" While the experiments show that the proposed method learns better than baselines, I don't think the paper show that the proposed method learns some sort of \"overall strategy.\" I don't think that the proposed method is simpler than the baselines.\n\nA second concern is that I'm confused about the experimental protocol. If the high-level policy outputs a desired XYZ position for the gripper (Section 5.1), how can the high-level policy indicate when the gripper should be closed to pick up the block? How is the reward function for the low-level policy defined? PEARL doesn't have access to this extra information (the reward function), right?\n\nA third concern is the large number of grammatical errors in the paper.\n\nI would consider increasing my review if (1) a new plot were added to visualize the commanded subgoals (I have a hunch that the high-level policy directly outputs the true goal, obviating the need for hierarchy and contradicting the claim that the method \"learns to generate high-level meta-strategies over subgoals\"); (2) the experimental protocol were clarified; and (3) the number of grammatical errors were significantly reduced.\n\nOther comments:\n* \"inefficient to to directly learn such a meta policy\" -- Why? Also, \"to to\" is repeated.\n* \"Deep Reinforcement learning\" -- \"Reinforcement\" shouldn't be capitalized.\n* \"failing to generalize\" -- Can you add a citation?\n* \"it would be quite inefficient to directly learn such a policy\u2026\": Doesn't [Plappert 18] do exactly this?\n* \"When the tasks distribution is much wider \u2026 these methods can hardly be effective\u2026\" -- Where is this claim substantiated? Also, \"tasks\" should be singular.\n* \"sparse reward settings which is\" -> \"sparse reward settings, which are\"\n* \"the above mentioned problems\" -> \"the problems mentioned above\"\n* \"our algorithm focus on meta learning \u2026 which provides a much simpler \u2026\" -- Where is it shown that the proposed method is simpler? Also, \"focus\" should be \"focuses\"\n* \"1991),which\" -- Missing space\n* \"complex tasks which requires\" -> \"complex tasks that require\"\n* \"Nachum et al \u2026 set of sub-policies\" -- Run on sentence.\n* \"... human leverage\u2026\" -- Don't humans also transfer low-level knowledge across tasks, in addition to high-level knowledge? Also, \"human\" should be plural.\n* \"algorithms.The\" -- Missing a space, I think.\n* \"PEARL leverages \u2026 latent variable Z\" -- This sentence doesn't make sense as written.\n* \"z's\" -- This should not be a possessive.\n* \"Good sample efficiency enables fast adaptation \u2026 and performs structured exploration \u2026\" -- Isn't the first part true by definition? Why does good sample efficiency perform structured exploration?\n* \"a goal is a 3-d vector\" -- If the goal output by the high-level policy is the XYZ coordinates of the \n* \"SAC, such non-hierarchical\" -- Grammar doesn't make sense here.\n* \"such non-hierarchical RL method has been proved to perform badly before on \u2026\" -- Can you add a citation? Generally, \"proved\" is reserved for mathematical proofs.\n* \"In this paper, We have\" -- \"We\" should not be capitalized.\n\n------------------------ UPDATE AFTER AUTHOR RESPONSE ------------------\nI thank the authors for at least reading the reviews. My concerns with experiments and clarify remain unaddressed, and are amplified by reading the other reviews. I therefore vote to \"reject\" this paper.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper1865/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1865/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575334014732, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1865/Reviewers"], "noninvitees": [], "tcdate": 1570237731185, "tmdate": 1575334014745, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1865/-/Official_Review"}}}, {"id": "HkxcTp05iS", "original": null, "number": 4, "cdate": 1573739970040, "ddate": null, "tcdate": 1573739970040, "tmdate": 1573739970040, "tddate": null, "forum": "rkgl51rKDB", "replyto": "SyllxgTtFS", "invitation": "ICLR.cc/2020/Conference/Paper1865/-/Official_Comment", "content": {"title": "Thank you for carefully reviewing the paper", "comment": "We appreciate the reviewer's valuable and constructive reviews. We will improve our paper as suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper1865/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgl51rKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference/Paper1865/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1865/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1865/Reviewers", "ICLR.cc/2020/Conference/Paper1865/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1865/Authors|ICLR.cc/2020/Conference/Paper1865/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149766, "tmdate": 1576860545350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference/Paper1865/Reviewers", "ICLR.cc/2020/Conference/Paper1865/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1865/-/Official_Comment"}}}, {"id": "r1x9iTAcsH", "original": null, "number": 3, "cdate": 1573739938300, "ddate": null, "tcdate": 1573739938300, "tmdate": 1573739938300, "tddate": null, "forum": "rkgl51rKDB", "replyto": "rkg-Q1gy9S", "invitation": "ICLR.cc/2020/Conference/Paper1865/-/Official_Comment", "content": {"title": "Thank you for carefully reviewing the paper", "comment": "We appreciate the reviewer's valuable and constructive reviews. We will improve our paper as suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper1865/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgl51rKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference/Paper1865/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1865/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1865/Reviewers", "ICLR.cc/2020/Conference/Paper1865/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1865/Authors|ICLR.cc/2020/Conference/Paper1865/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149766, "tmdate": 1576860545350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference/Paper1865/Reviewers", "ICLR.cc/2020/Conference/Paper1865/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1865/-/Official_Comment"}}}, {"id": "r1lfqFR5iS", "original": null, "number": 2, "cdate": 1573738890140, "ddate": null, "tcdate": 1573738890140, "tmdate": 1573738914928, "tddate": null, "forum": "rkgl51rKDB", "replyto": "r1lVZ2-ZqH", "invitation": "ICLR.cc/2020/Conference/Paper1865/-/Official_Comment", "content": {"title": "Thank you for carefully reviewing the paper", "comment": "We appreciate the reviewer's valuable and constructive reviews. We will improve our paper as suggested."}, "signatures": ["ICLR.cc/2020/Conference/Paper1865/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rkgl51rKDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference/Paper1865/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1865/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1865/Reviewers", "ICLR.cc/2020/Conference/Paper1865/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1865/Authors|ICLR.cc/2020/Conference/Paper1865/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504149766, "tmdate": 1576860545350, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1865/Authors", "ICLR.cc/2020/Conference/Paper1865/Reviewers", "ICLR.cc/2020/Conference/Paper1865/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1865/-/Official_Comment"}}}, {"id": "rkg-Q1gy9S", "original": null, "number": 2, "cdate": 1571909400855, "ddate": null, "tcdate": 1571909400855, "tmdate": 1572972413595, "tddate": null, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "invitation": "ICLR.cc/2020/Conference/Paper1865/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "### Summary ###\n\nIn this paper, the authors focus on the problem of meta-reinforcement learning (meta-RL). Specifically, the authors consider the setting of meta-RL for goal reaching tasks where each task corresponds to an unknown goal. Existing meta-RL algorithms directly train for a policy that output low level actions, which might be inefficient in this goal-reaching setting. In this paper, the authors combine the hierarchical RL framework of HAC[1] with the probabilistic task context inference method of PEARL[2], and propose the meta-goal generation for hierarchical RL (MGHRL) algorithm. In this algorithm, a two layer hierarchical policy is used where the high level policy generate goals for the low level goal-reaching policy to reach. In order to adapt to an unknown goal, the high level policy is conditioned on the output of a task inference module to generate goals for the unknown ground truth goal. The goal-reaching policy would then use the generated goal to interact with the environment.\n\nThe authors evaluated the proposed method on simulated robotic manipulation tasks and compare to PEARL as baseline. The experiment results show that the proposed method outperforms the baseline method significantly, especially under sparse reward settings.\n\n\n### Review ###\n\nOverall I think this paper presents an interesting idea in learning fast adapting goal-reaching policies. The idea is very well presented and authors include many empirical evidence to support the proposed method. However I do find a number of shortcomings that need to be addressed.\n\nPro:\n\n1. The idea for this paper is really well presented. The structure of the paper is well organized and  the authors include informative illustration to explain the architecture of the hierarchy of policies. The experiment results are easy to interpret.\n\n2.  The authors provide a detailed description of the configurations and the hyperparameters for each experiments. Such description would be very helpful if the results in this paper are to be reproduced.\n\nCon:\n\n1. The experiments presented in this paper do not include appropriate comparisons to baseline methods. While indeed the proposed method outperforms PEARL, this comparison is inherently unfair. PEARL is a general meta-RL algorithm, which can adapt to arbitrary variations of reward functions and dynamics in the distribution of tasks. The proposed method only applies in the setting of goal reaching meta-RL, where each task corresponds to an unknown goal. With this information artificially encoded into the hierarchical architecture, the proposed method should certainly perform better than any general meta-RL algorithms. Therefore, directly comparing the proposed method to any general meta-RL algorithm is unfair. Instead, the authors could compare with baseline methods with builtin goal-reaching components, such as the following one: train a goal reaching policy using HER[3], and then meta-train a goal conditioned reward function using standard supervised meta-learning methods. At test time, find the goal that maximizes the adapted reward function, and then feed that goal into the HER policy for evaluation. Note that this baseline is different from the proposed method in the way that the goal reaching and goal inference were done separately both using existing methods.\n\n2. I\u2019m not convinced about the novelty of this paper. The proposed method seems like a straightforward combination of HAC and PEARL, and it seems to me that the two methods are combined in order to apply an existing meta-RL algorithm in a goal reaching setting rather than to create a better general meta-RL algorithm.\n\nThe idea in the paper is well presented and carefully investigated. However, I am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement given the lack of proper baselines. Therefore, I would not recommend acceptance before these problems are addressed. \n\nReferences\n\n[1] Levy, Andrew, et al. \"Learning multi-level hierarchies with hindsight.\" (2018).\n\n[2] Rakelly, Kate, et al. \"Efficient off-policy meta-reinforcement learning via probabilistic context variables.\" arXiv preprint arXiv:1903.08254 (2019).\n\n[3] Andrychowicz, Marcin, et al. \"Hindsight experience replay.\" Advances in Neural Information Processing Systems. 2017.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1865/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1865/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Efficient meta reinforcement learning via meta goal generation", "authors": ["Haotian Fu", "Hongyao Tang", "Jianye Hao"], "authorids": ["haotianfu@tju.edu.cn", "bluecontra@tju.edu.cn", "jianye.hao@tju.edu.cn"], "keywords": [], "abstract": "Meta reinforcement learning (meta-RL) is able to accelerate the acquisition of new tasks by learning from past experience. Current meta-RL methods usually learn to adapt to new tasks by directly optimizing the parameters of policies over primitive actions. However, for complex tasks which requires sophisticated control strategies, it would be quite inefficient to to directly learn such a meta-policy. Moreover, this problem can become more severe and even fail in spare reward settings, which is quite common in practice. To this end, we propose a new meta-RL algorithm called meta goal-generation for hierarchical RL (MGHRL) by leveraging hierarchical actor-critic framework. Instead of directly generate policies over primitive actions for new tasks, MGHRL learns to generate high-level meta strategies over subgoals given past experience and leaves the rest of how to achieve subgoals as independent RL subtasks. Our empirical results on several challenging simulated robotics environments show that our method enables more efficient and effective meta-learning from past experience and outperforms state-of-the-art meta-RL and Hierarchical-RL methods in sparse reward settings.", "pdf": "/pdf/96dea0af2faf882ea23a25cc37f844df887b6243.pdf", "paperhash": "fu|efficient_meta_reinforcement_learning_via_meta_goal_generation", "original_pdf": "/attachment/4c74e740d85ef297e215af071eadb5b83d344c39.pdf", "_bibtex": "@misc{\nfu2020efficient,\ntitle={Efficient meta reinforcement learning via meta goal generation},\nauthor={Haotian Fu and Hongyao Tang and Jianye Hao},\nyear={2020},\nurl={https://openreview.net/forum?id=rkgl51rKDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rkgl51rKDB", "replyto": "rkgl51rKDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1865/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575334014732, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1865/Reviewers"], "noninvitees": [], "tcdate": 1570237731185, "tmdate": 1575334014745, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1865/-/Official_Review"}}}], "count": 8}