{"notes": [{"id": "B1xY-hRctX", "original": "BJlh-os5K7", "number": 1193, "cdate": 1538087937197, "ddate": null, "tcdate": 1538087937197, "tmdate": 1556261661463, "tddate": null, "forum": "B1xY-hRctX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SklUxuqqeV", "original": null, "number": 19, "cdate": 1545410542395, "ddate": null, "tcdate": 1545410542395, "tmdate": 1545410542395, "tddate": null, "forum": "B1xY-hRctX", "replyto": "Syx7d_H1Tm", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Thanks for your comprehensive comments. We will revise the paper accordingly.", "comment": "Dear AC,\n\nThanks for your careful reading of our manuscript and consideration.  We appreciate your remarkably comprehensive comments and suggestions. We promise to provide an inclusive revision in the camera-ready version accordingly.\n\nMany thanks,\nAuthors."}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Authors"], "readers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "Syx7d_H1Tm", "original": null, "number": 1, "cdate": 1541523563160, "ddate": null, "tcdate": 1541523563160, "tmdate": 1545354475347, "tddate": null, "forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Meta_Review", "content": {"metareview": "\npros:\n- The paper presents an interesting forward chaining model which makes use of meta-level expansions and reductions on predicate arguments in a neat way to reduce complexity.  As Reviewer 3 points out, there are a number of other papers from the neuro-symbolic community that learn relations (logic tensor networks is one good reference there). However using these meta-rules you can mix predicates of different arities in a principled way in the construction of the rules, which is something I haven't seen.\n- The paper is reasonably well written (see cons for specific issues)\n- There is quite a broad evaluation across a number of different tasks.  I appreciated that you integrated this into an RL setting for tasks like blocks world.\n- The results are good on small datasets and generalize well\n\ncons:\n- (scalability) As both Reviewers 1 and 3 point out, there are scalability issues as a function of the predicate arity in computing the set of permutations for the output predicate computation.\n- (interpretability) As Reviewer 2 notes, unlike del-ILP, it is not obvious how symbolic rules can be extracted.  This is an important point to address up front in the text. \n- (clarity) The paper is confusing or ambiguous in places:\n\n-Initially I read the 1,2,3 sequence at the top of 3 to be a deduction (and was confused) rather than three applications of the meta-rules.  Maybe instead of calling that section \"primitive logic rules\" you can call them \"logical meta-rules\".\n\n-Another confusion, also mentioned by reviewer 3 is that you are assuming that free variables (e.g. the \"x\" in the expression \"Clear(x)\") are implicitly considered universally quantified in your examples but you don't say this anywhere.  If I have the fact \"Clear(x)\" as an input fact, then presumably you will interpret this as \"for all x Clear(x)\" and provide an input tensor to the first layer which will have all 1.0's along the \"Clear\" relation dimension, right?\n\n-It seems that you are making the assumption that you will never need to apply a predicate to the same object in multiple arguments?  If not, I don't see why you say that the shape of the tensor will be m x (m-1) instead of m^2.  You need to be able to do this to get reflexivity for example: \"a <= a\".\n\n-I think you are implicitly making the closed world assumption (CWA) and should say so.\n\n-On pg. 4 you say \"The facts are tensors that encode relations among multiple objectives, as described in Sec. 2.2.\".  What do you mean by \"objectives\"?  I would say the facts are tensors that encode relations among multiple objects.\n\n-On pg. 5 you say \"We finish this subsection, continuing with the blocks world to illustrate the forward\npropagation in NLM\".  I see no mention of blocks world in this paragraph. It just seems like a description of what happens at one block, generically.\n\n-In many places you say that this model can compute deduction on first-order predicate calculus (FOPC) but it seems to me that you are limited to horn logic (rule logic) in which there is at most one positive literal per clause (i.e. rules of the form: b1 AND b2 AND ... AND bn => h).  From what I can tell you cannot handle deduction on clauses such as b1 AND b2 => h1 or (h2 and h3).\n\n-There is not enough description of the exact setup for each experiment. For example in blocks world, how do you choose predicates for each layer?  How many exactly for each experiment?  You make it seem on p3 that you can handle recursive predicates but this seems to not have been worked out completely in the appendix.  You should make this clear.\n\n-In figure 1 you list Move as if its a predicate like On but it's a very different thing. On is  predicate describing a relation in one state.  Move is an action which updates a state by changing the values of predicates.  They should not be presented in the same way.\n\n-You use \"min\" and \"max\" for \"and\" and \"or\" respectively.  Other approaches have found that using the product t-norm t-norm(x,y) = x * y helps with gradient propagation.  del-ILP discusses this in more detail on p 19.  Did you try these variations?\n\n-I think it would be helpful to somewhere explicitly describe the actual MLP model you use for deduction including layer sizes and activation functions.\n\n-p. 5. typo: \"Such a parameter sharing mechanism is crucial to the generalization ability of NLM to\nproblems ov varying sizes.\" (\"ov\" -> \"of\")\n\n-p. 6. sec 3.1 typo: \"For \u2202ILP, the set of pre-conditions of the symbols is used direclty as input of the system.\" (\"direclty\" -> \"directly\")\n\nI think this is a valuable contribution and novel in the particulars of the architecture (eg. expand/reduce) and am recommending acceptance.  But I would like to see a real effort made to sharpen the writing and make the exposition crystal clear.  Please in particular pay attention to Reviewer 3's comments.\n\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "Interesting forward chaining approach to neural deduction"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1193/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352931639, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352931639}}}, {"id": "r1ee4DycyV", "original": null, "number": 14, "cdate": 1544316711966, "ddate": null, "tcdate": 1544316711966, "tmdate": 1544316711966, "tddate": null, "forum": "B1xY-hRctX", "replyto": "HJxOd0tDJE", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Thanks for your pointers", "comment": "Thanks for your pointers to the related papers. We will discuss them in the next version of our paper."}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "HJxOd0tDJE", "original": null, "number": 1, "cdate": 1544162928399, "ddate": null, "tcdate": 1544162928399, "tmdate": 1544162928399, "tddate": null, "forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Public_Comment", "content": {"comment": "... although it is not a differentiable model or even a neural model, the idea of learning to sort infinite arrays from short examples has been explored in the \"Generalized Planning\" literature, for example, \nhttp://rbr.cs.umass.edu/shlomo/papers/SIZaij11.pdf\nhttps://www.ijcai.org/Proceedings/11/Papers/159.pdf\nhttps://www.dtic.upf.edu/~jonsson/ker18.pdf\n", "title": "Potential related work"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311656774, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "B1xY-hRctX", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311656774}}}, {"id": "S1e1Do49Cm", "original": null, "number": 6, "cdate": 1543289686741, "ddate": null, "tcdate": 1543289686741, "tmdate": 1543289760647, "tddate": null, "forum": "B1xY-hRctX", "replyto": "r1e_izwX0m", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Reply to authors' response", "comment": "Thanks for the clarification about the details and the scalability. I would like to keep my rating. This is an interesting direction and worth pursuing, so I support acceptance. But it is still unclear to me how the proposed approach can move beyond toy datasets. "}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "rklvumwmCQ", "original": null, "number": 5, "cdate": 1542841199018, "ddate": null, "tcdate": 1542841199018, "tmdate": 1542841199018, "tddate": null, "forum": "B1xY-hRctX", "replyto": "rkgpGkN52Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "1. Running time / training time.\nThe number of examples/episodes used is shown in Table 4. We plan to add training time / inference speed in our revision. Here, we show our results on Blocks World. We train our model on 12 CPUs (Xeon E5) and a single GPU (GTX 1080), It takes 3 hours to train our model (26000 episodes). During inference, the model runs in 1.43s per episode when the number of blocks is 50.\n\n2. Rules are not expressed in a logical formalism.\nThanks for the comment and suggestion --- Yes, your understanding is correct. Although the design of NLM\u2019s neural architecture is highly motivated by FOPC logic formalism, NLM models do not explicitly encode FOPC logic forms. In contrast, the weights of the MLPs encodes how models should perform the deduction, and the output of the NLM can be regarded as the conclusions (0/1 indicating whether we should move the block, in a Blocks World).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "r1e_izwX0m", "original": null, "number": 4, "cdate": 1542840992260, "ddate": null, "tcdate": 1542840992260, "tmdate": 1542840992260, "tddate": null, "forum": "B1xY-hRctX", "replyto": "rylWbydT3Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "1. Model details.\nDetailed implementation details including the number of layers (a.k.a. the depth) can be found in Table 4 (Appendix B.2). As for the hyper-parameters of the MLPs, we use no hidden layer, and the hidden dimension (number of intermediate predicates) of each layer is set to 8 across all our experiments.\nWe thank the reviewer for the suggestion, and will make these information more clear in our revision. Moreover, we plan to release our code upon acceptance.\n\n2. Scalability\nIt should be clarified that scalability mentioned in the paper mainly refers to the complexity of reasoning (e.g., number of steps before producing a desired predicate), not the number of objects/entities or relations. For example, as shown in our general clarification, learning predicates that have a complex structure (such as the ShouldMove in the example) pose a scalability challenge to existing ILP methods. We also refer the reviewer to our clarification on scalability for more detailed analysis.\nIn general, we agree with the reviewer that an inductive logic system should be able to handle both complex reasoning rules (e.g., as the settings explored in our paper) and large-scale entity sets (e.g., as in knowledge graph-related literature). We hope the methods and insights we presented in this paper could help the whole community in this interesting direction.\n\n3. Permutation in MLP.\nPermutation is needed in two places.  Consider two n-ary MLPs at a particular layer of the NLM (called \u201cp\u201d).  As the reviewer correctly points out, the [m, m-1, \u2026, m-n+1] dimensions represent permutations in the input of p.   On the other hand, the permutation before MLP is to create new predicates that only differs from the existing one in the variable order, in order to compute composition of these two predicates; this is the second place where permutation is needed.\n\nAs an example, suppose \u201cp\u201d is the predicate HasEdge(x, y).  By permuting its variables, we get another predicate, HasReverseEdge(x, y), which is TRUE if there is an edge from y to x.  These two predicates can be used to compose a more complex predicate\n    HasBidirectionalEdge(x, y) \u2190 HasEdge(x, y) \u2227 HasReverseEdge(x, y)\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "S1l4CCI66Q", "original": null, "number": 3, "cdate": 1542446796224, "ddate": null, "tcdate": 1542446796224, "tmdate": 1542509707291, "tddate": null, "forum": "B1xY-hRctX", "replyto": "rJxii0IT6Q", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Response to AnonReviewer3 Continued", "comment": "6. The scalability discussion with ILP systems and SRL methods.\nThank you for the comment.  Please see our response to the scalability claim.  We will revise the paper accordingly to clarify.\n\n7. Generalization w.r.t. the number of objects.\nDifferent from the reviewer\u2019s hypothesis, our results actually verify that NLM models do generalize well to larger test instances.  For example, Table 2 shows that our learned model achieves 100% accuracy on test instances with more blocks, and the same for Table 1. We have also conducted experiments testing this ability using several trained model in extreme cases which consist of 500 blocks (1000 numbers for sorting), no failure cases were found. The models will be made public along with our code after the paper decision. This ability is one of our main findings, as highlighted in the abstract (\u201cNLMs ... generalize to arbitrarily large-scale tasks\u201d).\n\n8. The goal configuration of Blocks World.\nWe present the generation of Blocks World instances in Appendix B.4. We will make it more clear in the revision. The goal configuration is randomly and independently generated as the initial configuration. One can compute that the expected optimal number of steps needed for solving the Blocks World is approximately 2m - o(m) steps are required to solve the case, where m is the number of blocks, which is 50 in the test instances. In average 84 steps means the model learns a fairly good solution. The reviewer is also welcome to check our demo in the footnote of Page 8: https://sites.google.com/view/neural-logic-machines .\n\n9. MDP formulation of the Blocks World.\nThanks for the nice suggestion. We discuss the MDP formulation in section 3.4, and we will make it more clear. We input the current world and the target world with tensors describing relations between objects. At each time step, the agents take actions to move one block onto another. We use sparse rewards to train the agents: The agents get the reward only when they finish the task.\n\n10. NLM learns the underlying logical rules.\nThanks for the comment.  We intend to mean that the learned NLM generalizes well to problems with varying sizes, in the same way logical rules do.  We will reword the sentence to avoid confusions, and discuss rule extraction as future work.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "rJxii0IT6Q", "original": null, "number": 2, "cdate": 1542446755312, "ddate": null, "tcdate": 1542446755312, "tmdate": 1542446755312, "tddate": null, "forum": "B1xY-hRctX", "replyto": "r1gMP1TKnQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "We thank the reviewer for many comments and pointers, and will revise our paper to emphasize further our contributions and novelties compared to previous work.\n\n1. Section 2.1 and the handling of free variables.\nSection 2.1 lists three primitive rules that serve as building blocks in later subsections to implement a Neural Logic Machine.  This is necessary for providing terminology and notation used throughout the rest of the paper.  We are not claiming them as novel contributions.\nSection 2.1 does *not* describe propositional logic.  The rule for \u201cBoolean logic\u201d is used in NLM as a component for realizing first-order logic (probabilistically, as described in section 2.2): they are used to operate on predicates grounded on objects.  An example in the Blocks World domain may look like:\n      IsGround(A) V Clear(A) -> Placeable(A)\nwhere A is one object in the Blocks World domain; and notably, IsGround(.), Clear(.) and Placeable(.) are not manually specified but are learned by the network.\nOur model supports free variables.  The arity of a predicate is its number of free variable.  For example, the arity of a binary predicate is 2, and NLM uses a matrix (a tensor of dimension 2) to represent the predicate\u2019s values for all possible grounding; the 1st paragraph of section 2.2 gives further details.  The three rules (eqns 1-3) keep the same number of free variables, increase it by 1, and decrease it by 1, respectively.\n\n2. The probability distributions modeled by MLPs.\nWe would like to thank R3 for the comment about \u201cjoint distribution\u201d, and briefly clarify technical details in Section 2.2 & 2.3 to avoid potential misunderstanding.\n\nLet\u2019s define the input of each layer k as H_k (whose each element is in [0, 1]) recursively in the following:\n\n(1) The initial layer is H_1 = prob(B) representing boolean values 0 or 1, where B is a set of base predicates.\n(2) For each layer k, the probabilistic boolean expression in the building block is defined above Eqn. 4:\n   Expression(H_1, ... , H_k)  ==>  H_{k+1}\nwhere Expression in NLM is represented by some neural network structure. As illustrated in Figure 2 & 3, we use (a) grouped MLP with weights \\theta_k and activation \\sigma, and (b) ReduceOrExpand that computes\n   H'_k = \\sigma(MLP(H_1, ... , H_k; \\theta_k)),\n   H_{k+1} = ReduceOrExpand(H'_k).\nThis building block keeps all elements of H_{k+1} in [0, 1] and becomes the input of next layer k+1. Therefore, such a series of building blocks is able to model a complex expression. \n\nWe will not use \u201cjoint distribution\u201d to avoid confusion, and make it more clear in the revision.\n\n3. The difference with other approaches that encode the weight of weighted logic rules using neural networks.\nThanks for the pointers. We will cite and discuss the papers in the revision. Our work differs substantially from MLN with weights computed by NNs, e.g., the mentioned L&F paper:\nTheir logic rules (called \u201cknowledge base\u201d in L&F) are designed by experts; see sec 2.3 of L&F).  Here, our NLM uses deep NNs to learn such rules from data. The Blocks World example in our response to the scalability question shows the complexity of the rules that NLMs can handle.\nConsequently, our NLM needs to learn weights that form those rules. In contrast, MLN only needs to learn a real-valued weight for each hand-designed logic rule.\n\n4. The difference with the unrolled computation graph of MLN.\nOne of our main contributions is to use deep NN to learn logic rules.  Unrolling NN-parameterized MLNs is limited by the need and quality of expert-designed logic rules.\n\n5. The encoding of objects.\nIt is unclear to us what the reviewer means by \u201cobjects are \u2026 vector encodings\u201d and hence the similarity to DeepProbLog, as we do *not* encode objects by vectors.  Data representations in NLM are all tensors that encode the (probabilistic) true/false values of grounded predicates; see the 1st paragraph of section 2.2 (page 3).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "H1g19nL6Tm", "original": null, "number": 1, "cdate": 1542446215291, "ddate": null, "tcdate": 1542446215291, "tmdate": 1542446215291, "tddate": null, "forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "content": {"title": "Clarification on Scalability", "comment": "We thank all reviewers for their thoughts and comments. In addition to the specific responses below, here we clarify on the scalability question asked by some reviewers. We will include related discussions in our revision.\n\nIt should be clarified that scalability mentioned in the paper mainly refers to the complexity of reasoning (e.g., number of steps before producing a desired predicate), not the number of objects/entities or relations. This is highlighted in #2 at the bottom of page 1: \u201cWe expect the learning system to scale with the number of logic rules. Existing logic-based algorithms like ILP suffer an exponential computational complexity with respect to the number of logic rules\u201d.\n\nKnowledge-graph tasks involve many entities (e.g. > 10M) and relations as reviewers pointed out, but the rules involved in the reasoning steps are usually restricted. For example, the rules considered in the knowledge base reasoning work (Yang et al., 2017) are restricted in a \u201cchain-like\u201d form (their eqn 1.), which is query(Y,X)<-Rn (Y,Zn) \u2227 \u00b7 \u00b7 \u00b7 \u2227 R1 (Z1,X), while R1, . . . , Rn are *known* relations in the knowledge base. Such knowledge-graph reasoning tasks represent an interesting yet different class of problems outside of the current scope of our paper.\n\nIn contrast, learning predicates that have a complex structure (such as the ShouldMove example below) pose a scalability challenge to existing ILP methods.  In dILP [Evans et al.], for example, suppose each rule has C possible choices from the templates and R rules are need to be learned, then the possible space is at least O(C^R) --- the number of the set of possible rules is exponential w.r.t. the number of rules.  On the other hand, our method is only quadratic in the number of rules (or in this case, equivalently, number of predicates).\n\n**********************************************************************\n                          A Blocks World Example                             \n**********************************************************************\nThis example shows what we mean by complex reasoning in the seemingly simple Blocks World domain.  Suppose we are interested in knowing whether a block should be moved in order to reach the target configuration.  Here, a block should be moved if (1) it is moveable; and (2) there is at least one block below it that does not match the target configuration.  Call the desired predicate \u201cShouldMove(x)\u201d.\n\nInputs Relations (as specified in the last paragraph of page 7):\nSameWorldID, SmallerWorldID, LargerWorldID;\nSameID, SmallerID, LargerID;\nLeft, SameX, Right, Below, SameY, Above.\nThe relations are given on all pairs of objects across both worlds.\n\nHere is one way to produce the desired predicate by defining several helper predicates, designed by \u201chuman experts\u201d:\n1. IsGround(x) \u2190 \u2200y Above(y, x)\n2. SameXAbove(x, y) \u2190 SameWorldID(x, y) \u2227 SameX(x, y) \u2227 Above(x, y)\n3. Clear(x) \u2190 \u2200y \u00acSameXAbove(y, x)\n4. Moveable(x) \u2190 Clear(x) \u2227 \u00acIsGround(x)\n5. InitialWorld(x) \u2190 \u2200y \u00acSmallerWorldID(y, x)\n6. Match(x, y) \u2190 \u00acSameWorldID(x, y) \u2227 SameID(x, y) \u2227 SameX(x, y) \u2227 SameY(x, y)\n7. Matched(x) \u2190 \u2203y Match(x, y)\n8. HaveUnmatchedBelow(x) \u2190 \u2203y SameXAbove(x, y) \u2227 \u00acMatched(y) \n9. ShouldMove(x) \u2190 InitialWorld(x) \u2227 Moveable(x) \u2227 HaveUnmatchedBelow(x)\nWe can also write the logic forms in one line:\nShouldMove(x) \u2190 (\u2200y \u00acSmallerWorldID(y, x)) \u2227 (\u2200y \u00ac(SameWorldID(y, x) \u2227 SameX(y, x) \u2227 Above(y, x))) \u2227 \u00ac(\u2200y Above(y, x)) \u2227 ((\u2203y SameWorldID(x, y) \u2227 SameX(x, y) \u2227 Above(x, y)) \u2227 \u00ac(\u2203z \u00acSameWorldID(y, z) \u2227 SameID(y, z) \u2227 SameX(y, z) \u2227 SameY(y, z)) )\n\nNote that this is only a part of the logic needed to complete the Blocks World challenge. The learner also needs to figure out where should the block be moved onto. The proposed NLM can learn policies that solve the Blocks World from the sparse reward signal indicating only whether the agent has finished the game. More importantly, the learned policy generalizes well to larger instances (consisting more blocks).\n**********************************************************************\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625734, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xY-hRctX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1193/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1193/Authors|ICLR.cc/2019/Conference/Paper1193/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers", "ICLR.cc/2019/Conference/Paper1193/Authors", "ICLR.cc/2019/Conference/Paper1193/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625734}}}, {"id": "rylWbydT3Q", "original": null, "number": 3, "cdate": 1541402361166, "ddate": null, "tcdate": 1541402361166, "tmdate": 1541533345373, "tddate": null, "forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Review", "content": {"title": "Interesting approach to model FOL in NN, with concerns in scalability", "review": "This paper presents a model to combine neural network and logic programming. It proposes to use 3 primitive logic rules to model first-order predicate calculus in the neural networks. Specifically, relations with different numbers of arguments over all permutations of the groups of objects are represented as tensors with corresponding dimensions. In each layer, a MLP (shared among different permutations) is applied to transform the tensor. Multiple layers captures multiple steps of deduction. On several synthetic tasks, the proposed method is shown to outperform the memory network baseline and shows strong generalization.  \n\nThe paper is well written, but some of the contents are still a bit dense, especially for readers who are not familiar with first-order predicate calculus. \n\nThe small Python example in the Appendix helps to clarify the details. It would be good to include the details of the architectures, for example, the number of layers, and the number of hidden sizes in each layer, in the experiment details in the appendix. \n\nThe idea of using the 3 primitive logic rules and applying the same MLP to all the permutations are interesting. However, due to the permutation step, my concern is whether it can scale to real-world problems with a large number of entities and different types of relations, for example, a real-world knowledge graph.\n\nSpecifically:\n\n1. Each step of the reasoning (one layer) is applied to all the permutations for each predicate over each group of objects, which might be prohibitive in real-world scenario. For example, although there are usually only binary relations in real-world KG, the number of entities is usually >10M. \n\n2. Although the inputs or preconditions could be sparse, thus efficient to store and process, the intermediate representations are dense due to the probabilistic view, which makes the (soft) deduction computationally expensive. \n\nSome clarification questions: \n\nIs there some references for the Remark on page 3? \n\nWhy is there a permutation before MLP? I thought the [m, m-1, \u2026, m-n+1] dimensions represent the permutations. For example, if there are two objects, {x1, x2}. Then the [0, 1, 0] represents the first predicate applied on x1, and x2. [1, 0, 0] represents the first predicate applied on x2 and x1. Some clarifications would definitely help here. \n\nI think this paper presents an interesting approach to model FOPC in neural networks. So I support the acceptance of the paper. However, I am concerned with its scalability beyond the toy datasets. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Review", "cdate": 1542234284281, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335893046, "tmdate": 1552335893046, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkgpGkN52Q", "original": null, "number": 2, "cdate": 1541189396907, "ddate": null, "tcdate": 1541189396907, "tmdate": 1541533345162, "tddate": null, "forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Review", "content": {"title": "Review", "review": "In this paper the authors propose a neural-symbolic architecture, called Neural Logic Machines (NLMs), that can learn logic rules.\n\nThe paper is pretty clear and well-written and the proposed system is compelling. I have only some small concerns.\nOne issue concerns the learning time. In the experimental phase the authors do not state how long training is for different datasets.\nMoreover it seems that the \u201crules\u201d learnt by NSMs cannot be expressed in a logical formalism, isn\u2019t it? If I am right, I think this is a major difference between dILP (Evans et. al) and NLMs and the authors should discuss about that. If I am wrong, I think the authors should describe how to extract rules from NLMs.\nIn conclusion I think that, once these little issues are fixed, the paper could be considered for acceptance.\n\n[minor comments]\np. 4\n\u201ctenary\u201d -> \u201cternary\u201d\n p. 5\n\u201cov varying size\u201d -> \u201cof varying size\u201d\n\u201cThe number of parameters in the block described above is\u2026\u201d. It is not clear to me how the number of parameters is computed.\n\u201cIn Eq. equation 4\u201d -> \u201cIn Eq. 4\u201d\n\np. 16\n\u201cEach lesson contains the example with same number of objects in our experiments.\u201d. This sentence sounds odd.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Review", "cdate": 1542234284281, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335893046, "tmdate": 1552335893046, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1gMP1TKnQ", "original": null, "number": 1, "cdate": 1541160794212, "ddate": null, "tcdate": 1541160794212, "tmdate": 1541533344947, "tddate": null, "forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "invitation": "ICLR.cc/2019/Conference/-/Paper1193/Official_Review", "content": {"title": "interesting directions but unclear novelty and some claims that are too strong", "review": "The paper introduces Neural Logic Machines, a particular way to combine neural networks and first order but finite logic. \n\nThe paper is very well written and structured. However, there are also some downsides.\n\nFirst of all, Section 2.1 is rather simple from a logical perspective and hence it is not clear what this gets a special term. Moreover, why do mix Boolean logic (propostional logic) and first order logic? Any how to you deal with the free variables, i.e., the variables that are not bounded by a quantifier? The semantics you define later actually assumes that all free variables (in your notation) are bounded by all quantifiers since you apply the same rule to all ground instances. Given that you argue that you want a neural extension of symbolic logic (\"NLM is a neural realization of (symbolic) logic machines\") this has to be clarified as it would not be an extension otherwise. \n\nFurthermore, Section 2.2 argues that we can use a MLP with a sigmoid output to encode any joint distribution. This should be proven. It particular, given that the input to the network are the marginals of the ground atoms. So this is more like a conditional distribution? Moreover, it is not clear how this is different to other approaches that encode the weight of weighted logical rule (e.g. in a MLN) using neural networks, see\ne.g. \n\nMarco Lippi, Paolo Frasconi:\nPrediction of protein beta-residue contacts by Markov logic networks with grounding-specific weights. \nBioinformatics 25(18): 2326-2333 (2009)\n\nNow of course, and this is the nice part of the present paper, by stacking several of the rules, we could directly specify that we may need a certain number of latent predicates. \nThis is nice but it is not argued that this is highly novel. Consider again the work by Lippi and Frasconi. We unroll a given NN-parameterized MLN for s fixed number of forward chaining steps. This gives us essentially a computational graph that could also be made differentiable and hence we could also have end2end training. The major difference seems to be that now objects are directly attached with vector encodings, which are not present in Lippi and Frasconi's approach. This is nice but also follows from Rocktaeschel and Riedel's differentiable Prolog work (when combined with Lippi and Frasconi's approach).\nMoreover, there have been other combinations of tensors and logic, see e.g. \n\nIvan Donadello, Luciano Serafini, Artur S. d'Avila Garcez:\nLogic Tensor Networks for Semantic Image Interpretation. \nIJCAI 2017: 1596-1602\n \nHere you can also have vector encodings of constants. This also holds for \n\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, Luc De Raedt:\nDeepProbLog: Neural Probabilistic Logic Programming. CoRR abs/1805.10872 (2018)\n\nThe authors should really discuss this missing related work. This should also involve\na clarification of the \"ILP systems do not scale\" statement. At least if one views statistical relational learning methods as an extension of ILP, this is not true. Probabilistic ILP aka statistical relational learning has been used to learn models on electronic health records, see e.g., the papers collectively discussed in \n\nSriraam Natarajan, Kristian Kersting, Tushar Khot, Jude W. Shavlik:\nBoosted Statistical Relational Learners - From Benchmarks to Data-Driven Medicine. Springer Briefs in Computer Science, Springer 2014, ISBN 978-3-319-13643-1, pp. 1-68\n\nSo the authors should either discuss SRL and its successes, separating SRL from ILP, or they cannot argue that ILP does not scale. In the related work section, they decided to view both as ILP, and, in turn, the statement that ILP does not scale is not true. Moreover, many of the learning tasks considered have been solved with ILP, too, of course in the ILP setting. Any ILP systems have been shown to scale beyond those toy domains.   \nThis also includes the blocks world. Here relational MDP solvers can deal e.g. with BW worlds composed of 10 blocks, resulting in MDPs with several million states. And the can compute relational policies that solve e.g. the goal on(a,b) for arbitrary number of blocks. This should be incorporated in the discussion of the introduction in order to avoid the wrong impression that existing methods just work for toy examples. \n\nComing back to scaling, the current examples are on rather small datasets, too, namely <12 training instances. Moreover, given that we learn a continuous approximation with a limit depth of reasoning, it is also very likely that the models to not generate well to larger test instances. So the scaling issue has to be qualified to avoid to give the wrong impression that the present paper solves this issue. \n\nFinally, the BW experiments should indicate some more information on the goal configuration. This would help to understand whether an average number of moves of 84 is good or bad. Moreover, some hints about the MDP formulation should be provided, given that there have been relational MDPs that solve many of the probabilistic planning competition tasks. And, given that the conclusions argue that NLMs can learn the \"underlying logical rules\", the learned rules should actually be shown. \n\nNevertheless, the direction is really interesting but there several downsides that have to be addressed. ", "rating": "5: Marginally below acceptance threshold", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1193/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers.  After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "keywords": ["Neuro-Symbolic Computation", "Logic Induction"], "authorids": ["dhh19951@gmail.com", "maojiayuan@gmail.com", "tianlin@google.com", "chongw@google.com", "lihongli.cs@gmail.com", "dennyzhou@gmail.com"], "authors": ["Honghua Dong", "Jiayuan Mao", "Tian Lin", "Chong Wang", "Lihong Li", "Denny Zhou"], "TL;DR": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning.", "pdf": "/pdf/8a829c8ef35e714bd6102abea6eea9257436955f.pdf", "paperhash": "dong|neural_logic_machines", "_bibtex": "@inproceedings{\ndong2018neural,\ntitle={Neural Logic Machines},\nauthor={Honghua Dong and Jiayuan Mao and Tian Lin and Chong Wang and Lihong Li and Denny Zhou},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xY-hRctX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1193/Official_Review", "cdate": 1542234284281, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xY-hRctX", "replyto": "B1xY-hRctX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1193/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335893046, "tmdate": 1552335893046, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1193/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 14}