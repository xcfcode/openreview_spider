{"notes": [{"id": "HJgEe1SKPr", "original": "BJeqemoOvr", "number": 1501, "cdate": 1569439467547, "ddate": null, "tcdate": 1569439467547, "tmdate": 1577168251826, "tddate": null, "forum": "HJgEe1SKPr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "GAN-based Gaussian Mixture Model Responsibility Learning", "authors": ["Wanming Huang", "Shuai Jiang", "Xuan Liang", "Ian Oppermann", "Richard Yi Da Xu"], "authorids": ["wanming.huang@student.uts.edu.au", "shuai.jiang-1@student.uts.edu.au", "xuan.liang@student.uts.edu.au", "ianopper@outlook.com", "yida.xu@uts.edu.au"], "keywords": ["Generative Adversarial Networks"], "abstract": "Mixture Model (MM) is a probabilistic framework which allows us to define a dataset containing K different modes. When each of the modes is associated with a Gaussian distribution, we refer it as Gaussian MM, or GMM. Given a data point x, GMM may assume the existence of a random index k \u2208 {1, . . . , K } identifying which Gaussian the particular data is associated with. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional like- lihood p(x|k, \u03b8), as well as responsibility probability p(k|x, \u03b8) which describes the distribution index corresponds to the data. Computing the responsibility allows us to retrieve many important statistics of the overall dataset, including the weights of each of the modes. Modern large datasets often contain multiple unlabelled modes, such as paintings dataset containing several styles; fashion images containing several unlabelled categories. In its raw representation, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it\u2019s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, we utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data\u2019s latent space z instead of x. Instead of defining p(x|k, \u03b8) explicitly, we devised a modified GAN to allow us to define the distribution using p(z|k, \u03b8), where z is the corresponding latent representation of x, as well as p(k|x, \u03b8) through an additional classification network which is trained with the GAN in an \u201cend-to-end\u201d fashion. These techniques allow us to discover interesting properties of an unsupervised dataset, including dataset segments as well as generating new \u201cout-distribution\u201d data by smooth linear interpolation across any combinations of the modes in a completely unsupervised manner.", "pdf": "/pdf/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "paperhash": "huang|ganbased_gaussian_mixture_model_responsibility_learning", "original_pdf": "/attachment/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "_bibtex": "@misc{\nhuang2020ganbased,\ntitle={{\\{}GAN{\\}}-based Gaussian Mixture Model Responsibility Learning},\nauthor={Wanming Huang and Shuai Jiang and Xuan Liang and Ian Oppermann and Richard Yi Da Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgEe1SKPr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "wvxD7G-G4N", "original": null, "number": 1, "cdate": 1576798724918, "ddate": null, "tcdate": 1576798724918, "tmdate": 1576800911581, "tddate": null, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "invitation": "ICLR.cc/2020/Conference/Paper1501/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to use GMM as the latent prior distribution of GAN. The reviewers unanimously agree that the paper is not well motivated, explanations are lacking and writing needs to be substantially improved. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN-based Gaussian Mixture Model Responsibility Learning", "authors": ["Wanming Huang", "Shuai Jiang", "Xuan Liang", "Ian Oppermann", "Richard Yi Da Xu"], "authorids": ["wanming.huang@student.uts.edu.au", "shuai.jiang-1@student.uts.edu.au", "xuan.liang@student.uts.edu.au", "ianopper@outlook.com", "yida.xu@uts.edu.au"], "keywords": ["Generative Adversarial Networks"], "abstract": "Mixture Model (MM) is a probabilistic framework which allows us to define a dataset containing K different modes. When each of the modes is associated with a Gaussian distribution, we refer it as Gaussian MM, or GMM. Given a data point x, GMM may assume the existence of a random index k \u2208 {1, . . . , K } identifying which Gaussian the particular data is associated with. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional like- lihood p(x|k, \u03b8), as well as responsibility probability p(k|x, \u03b8) which describes the distribution index corresponds to the data. Computing the responsibility allows us to retrieve many important statistics of the overall dataset, including the weights of each of the modes. Modern large datasets often contain multiple unlabelled modes, such as paintings dataset containing several styles; fashion images containing several unlabelled categories. In its raw representation, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it\u2019s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, we utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data\u2019s latent space z instead of x. Instead of defining p(x|k, \u03b8) explicitly, we devised a modified GAN to allow us to define the distribution using p(z|k, \u03b8), where z is the corresponding latent representation of x, as well as p(k|x, \u03b8) through an additional classification network which is trained with the GAN in an \u201cend-to-end\u201d fashion. These techniques allow us to discover interesting properties of an unsupervised dataset, including dataset segments as well as generating new \u201cout-distribution\u201d data by smooth linear interpolation across any combinations of the modes in a completely unsupervised manner.", "pdf": "/pdf/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "paperhash": "huang|ganbased_gaussian_mixture_model_responsibility_learning", "original_pdf": "/attachment/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "_bibtex": "@misc{\nhuang2020ganbased,\ntitle={{\\{}GAN{\\}}-based Gaussian Mixture Model Responsibility Learning},\nauthor={Wanming Huang and Shuai Jiang and Xuan Liang and Ian Oppermann and Richard Yi Da Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgEe1SKPr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795726716, "tmdate": 1576800278907, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1501/-/Decision"}}}, {"id": "HklCTYa0KH", "original": null, "number": 1, "cdate": 1571899845950, "ddate": null, "tcdate": 1571899845950, "tmdate": 1572972460353, "tddate": null, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "invitation": "ICLR.cc/2020/Conference/Paper1501/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper proposes a modification of GANs where the latent space follows a distribution modelled by a Gaussian Mixture Model. While the idea of using GMMs in GANs is not novel, the main contribution of the paper is to add a classification models that enables posterior inference. The whole model is trained jointly end-to-end using both an adversarial loss and a mutual information loss. The procedure is then tested on MNIST, Fashion-MNIST and a subset of Oxford-102 Flower. \n\nWhile the problem of posterior inference is interesting, the novelty of the paper is quite limited. The overall structure is clear, although writing can be improved. \n\nHowever, a part from the form, I have other concerns about the paper: \n- The main purpose of the paper is to tackle large image datasets that are hard to address by classical GMMs. This being said, the used datasets are composed of small and modal enough images that it seems hard to validate the claim of the paper using only these data. It seems to me that for the claim of the paper to be verified, larger scale/more complex datasets are needed. \n- In all experiments, the authors suppose they have access to the number of classes/modes in the data, which is a huge assumption. It would be interesting to see if it would be possible to automatically accurately select the number of modes, e.g. on a held out validation set.\n- One problem of GANs that the authors do not seem to consider is mode collapse. It would be interesting to do experiments with unbalanced datasets (e.g. MNIST 1 vs all) to see if the proposed architecture will model the data correctly. \n- I am confused about the use of Mutual Information loss. The author claim that they would like to enforce each of the generated images to be from the same class as the input image. This would make sense if the authors used the multinomial sampling for the latent variable generation. However, the K generated samples are from K different Gaussians. It seems unreasonable to require of the classifier to render the same result. \n- On the same note, in order to both use the classical multinomial sampling in GMMs and not break the backpropagation, have the authors considered updating the classifier and the GAN in separately in expectation-maximization fashion?\n- Finally, I don't see the point of weighting the adversarial loss by the weights of Gaussians. All the generated images are fake and should be equally detected as such. \nAlthough the idea is interesting, I think the paper, at its current status, not ready for publication. \n\nMinor: \n- P. 4: Generator: The sampling density from the multinomial distribution seems incorrect. However, as the authors skip the sampling step to be able to back propagate through the model, this is not significant. \n- P. 1: The paragraph before the last: may even synthesizing -> synthesize\n- P. 3: Architecture: Possibility -> Probability\n- P. 7: Figure title: CIFR10 -> Oxford-102\n- Algorithm notation: \n    *Indexes for alpha and alpha_hat from 1 and not 0 to be consistent with the rest of the text \n    *Add hats to the entries of alpha_hat\n    *  LI from alpha_i and alpha_hat_i as in Equation 4 -> maybe change alpha_hat by x_hat to be consistent with the notation of eq.4 (although the meaning is clear here). \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1501/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1501/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN-based Gaussian Mixture Model Responsibility Learning", "authors": ["Wanming Huang", "Shuai Jiang", "Xuan Liang", "Ian Oppermann", "Richard Yi Da Xu"], "authorids": ["wanming.huang@student.uts.edu.au", "shuai.jiang-1@student.uts.edu.au", "xuan.liang@student.uts.edu.au", "ianopper@outlook.com", "yida.xu@uts.edu.au"], "keywords": ["Generative Adversarial Networks"], "abstract": "Mixture Model (MM) is a probabilistic framework which allows us to define a dataset containing K different modes. When each of the modes is associated with a Gaussian distribution, we refer it as Gaussian MM, or GMM. Given a data point x, GMM may assume the existence of a random index k \u2208 {1, . . . , K } identifying which Gaussian the particular data is associated with. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional like- lihood p(x|k, \u03b8), as well as responsibility probability p(k|x, \u03b8) which describes the distribution index corresponds to the data. Computing the responsibility allows us to retrieve many important statistics of the overall dataset, including the weights of each of the modes. Modern large datasets often contain multiple unlabelled modes, such as paintings dataset containing several styles; fashion images containing several unlabelled categories. In its raw representation, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it\u2019s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, we utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data\u2019s latent space z instead of x. Instead of defining p(x|k, \u03b8) explicitly, we devised a modified GAN to allow us to define the distribution using p(z|k, \u03b8), where z is the corresponding latent representation of x, as well as p(k|x, \u03b8) through an additional classification network which is trained with the GAN in an \u201cend-to-end\u201d fashion. These techniques allow us to discover interesting properties of an unsupervised dataset, including dataset segments as well as generating new \u201cout-distribution\u201d data by smooth linear interpolation across any combinations of the modes in a completely unsupervised manner.", "pdf": "/pdf/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "paperhash": "huang|ganbased_gaussian_mixture_model_responsibility_learning", "original_pdf": "/attachment/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "_bibtex": "@misc{\nhuang2020ganbased,\ntitle={{\\{}GAN{\\}}-based Gaussian Mixture Model Responsibility Learning},\nauthor={Wanming Huang and Shuai Jiang and Xuan Liang and Ian Oppermann and Richard Yi Da Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgEe1SKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1501/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1501/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575703127489, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1501/Reviewers"], "noninvitees": [], "tcdate": 1570237736464, "tmdate": 1575703127502, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1501/-/Official_Review"}}}, {"id": "SJlnIgtxqB", "original": null, "number": 2, "cdate": 1572012116494, "ddate": null, "tcdate": 1572012116494, "tmdate": 1572972460309, "tddate": null, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "invitation": "ICLR.cc/2020/Conference/Paper1501/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper considers the Gaussian mixture model at the latent space to have a better GAN training result.  The proposed architecture consists of three networks, a classifier, a generator, and a discriminator. Every input image goes through the classifier and gets the softmax output. The softmax output is considered as the mixture weights of the GMM model and controls the loss function of the generator accordingly. \n\nAlthough this paper has some positive sides, I recommend \"weak reject\" because of the following reasons.\n\n1. This paper is hard to follow. Many concepts are not discussed enough. For instance, how to train mu_k and \\Sigma_k, why should we use Eq. (3) as the loss of the generator, where do we use L^I in Eq.(4),...\n\n2. The experiment section requires more works. It would be much better to do experiments with much higher dimensional data sets and compare with many other GAN algorithms e.g. InfoGAN.\n\n3. The pseudo-code has many errors. For instance,  what is \\alpha_i^0? It is not introduced. What is \\hat{\\alpha}?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1501/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1501/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN-based Gaussian Mixture Model Responsibility Learning", "authors": ["Wanming Huang", "Shuai Jiang", "Xuan Liang", "Ian Oppermann", "Richard Yi Da Xu"], "authorids": ["wanming.huang@student.uts.edu.au", "shuai.jiang-1@student.uts.edu.au", "xuan.liang@student.uts.edu.au", "ianopper@outlook.com", "yida.xu@uts.edu.au"], "keywords": ["Generative Adversarial Networks"], "abstract": "Mixture Model (MM) is a probabilistic framework which allows us to define a dataset containing K different modes. When each of the modes is associated with a Gaussian distribution, we refer it as Gaussian MM, or GMM. Given a data point x, GMM may assume the existence of a random index k \u2208 {1, . . . , K } identifying which Gaussian the particular data is associated with. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional like- lihood p(x|k, \u03b8), as well as responsibility probability p(k|x, \u03b8) which describes the distribution index corresponds to the data. Computing the responsibility allows us to retrieve many important statistics of the overall dataset, including the weights of each of the modes. Modern large datasets often contain multiple unlabelled modes, such as paintings dataset containing several styles; fashion images containing several unlabelled categories. In its raw representation, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it\u2019s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, we utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data\u2019s latent space z instead of x. Instead of defining p(x|k, \u03b8) explicitly, we devised a modified GAN to allow us to define the distribution using p(z|k, \u03b8), where z is the corresponding latent representation of x, as well as p(k|x, \u03b8) through an additional classification network which is trained with the GAN in an \u201cend-to-end\u201d fashion. These techniques allow us to discover interesting properties of an unsupervised dataset, including dataset segments as well as generating new \u201cout-distribution\u201d data by smooth linear interpolation across any combinations of the modes in a completely unsupervised manner.", "pdf": "/pdf/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "paperhash": "huang|ganbased_gaussian_mixture_model_responsibility_learning", "original_pdf": "/attachment/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "_bibtex": "@misc{\nhuang2020ganbased,\ntitle={{\\{}GAN{\\}}-based Gaussian Mixture Model Responsibility Learning},\nauthor={Wanming Huang and Shuai Jiang and Xuan Liang and Ian Oppermann and Richard Yi Da Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgEe1SKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1501/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1501/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575703127489, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1501/Reviewers"], "noninvitees": [], "tcdate": 1570237736464, "tmdate": 1575703127502, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1501/-/Official_Review"}}}, {"id": "rylsua2Q9S", "original": null, "number": 3, "cdate": 1572224371071, "ddate": null, "tcdate": 1572224371071, "tmdate": 1572972460266, "tddate": null, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "invitation": "ICLR.cc/2020/Conference/Paper1501/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes to use GMM as the latent prior distribution of GAN. The model adds the means, covariances and the discrete priors of the GMM as learnable parameters to the GAN, which are jointly optimized with other GAN parameters. The model also adds a discrete classifier to the training process. During training, the classifier predicts the probability of each image falling into each of the GMM clusters, and uses these probabilities to re-weight the GAN generated samples.\n\n# motivation\n\nThe motivation of this work is not clear to me. Even with an isotropic Gaussian prior, a fully connected neural network is already sufficient to (approximately) simulate the GMM sampling. Thus, explicitly modeling GMM doesn't seem to be necessary, and could make the learning more difficult. \n\nI also don't quite understand why the authors have to add a discrete classifier to the modeling. It appears that the discrete classifier is only used for controlling the relative weights of clusters in the GAN training. If that's the case, then what's truly needed is just the prior distribution of each cluster, which doesn't depend on the individual images. For concrete datasets, this prior is usually known. For example, in MNIST each cluster has an equal prior of 10%.\n\n# experiments\n\nThe model is evaluated on MNIST and Oxford-102. I'd like to see it tested on more realistic and higher resolution images, and compared with state-of-the-art GAN models. Since the motivation of the modeling design is unclear, the bar on the empirical results should be much higher.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1501/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1501/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "GAN-based Gaussian Mixture Model Responsibility Learning", "authors": ["Wanming Huang", "Shuai Jiang", "Xuan Liang", "Ian Oppermann", "Richard Yi Da Xu"], "authorids": ["wanming.huang@student.uts.edu.au", "shuai.jiang-1@student.uts.edu.au", "xuan.liang@student.uts.edu.au", "ianopper@outlook.com", "yida.xu@uts.edu.au"], "keywords": ["Generative Adversarial Networks"], "abstract": "Mixture Model (MM) is a probabilistic framework which allows us to define a dataset containing K different modes. When each of the modes is associated with a Gaussian distribution, we refer it as Gaussian MM, or GMM. Given a data point x, GMM may assume the existence of a random index k \u2208 {1, . . . , K } identifying which Gaussian the particular data is associated with. In a traditional GMM paradigm, it is straightforward to compute in closed-form, the conditional like- lihood p(x|k, \u03b8), as well as responsibility probability p(k|x, \u03b8) which describes the distribution index corresponds to the data. Computing the responsibility allows us to retrieve many important statistics of the overall dataset, including the weights of each of the modes. Modern large datasets often contain multiple unlabelled modes, such as paintings dataset containing several styles; fashion images containing several unlabelled categories. In its raw representation, the Euclidean distances between the data do not allow them to form mixtures naturally, nor it\u2019s feasible to compute responsibility distribution, making GMM unable to apply. To this paper, we utilize the Generative Adversarial Network (GAN) framework to achieve an alternative plausible method to compute these probabilities at the data\u2019s latent space z instead of x. Instead of defining p(x|k, \u03b8) explicitly, we devised a modified GAN to allow us to define the distribution using p(z|k, \u03b8), where z is the corresponding latent representation of x, as well as p(k|x, \u03b8) through an additional classification network which is trained with the GAN in an \u201cend-to-end\u201d fashion. These techniques allow us to discover interesting properties of an unsupervised dataset, including dataset segments as well as generating new \u201cout-distribution\u201d data by smooth linear interpolation across any combinations of the modes in a completely unsupervised manner.", "pdf": "/pdf/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "paperhash": "huang|ganbased_gaussian_mixture_model_responsibility_learning", "original_pdf": "/attachment/23edf307dc7ed72cc5c6a937faed3ef40cd7e782.pdf", "_bibtex": "@misc{\nhuang2020ganbased,\ntitle={{\\{}GAN{\\}}-based Gaussian Mixture Model Responsibility Learning},\nauthor={Wanming Huang and Shuai Jiang and Xuan Liang and Ian Oppermann and Richard Yi Da Xu},\nyear={2020},\nurl={https://openreview.net/forum?id=HJgEe1SKPr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HJgEe1SKPr", "replyto": "HJgEe1SKPr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1501/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1501/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575703127489, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1501/Reviewers"], "noninvitees": [], "tcdate": 1570237736464, "tmdate": 1575703127502, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1501/-/Official_Review"}}}], "count": 5}