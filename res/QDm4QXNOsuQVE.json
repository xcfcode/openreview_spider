{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392783300000, "tcdate": 1392783300000, "number": 1, "id": "oV1qV9LKY43KA", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "QDm4QXNOsuQVE", "replyto": "l6rclPZecziqO", "signatures": ["Alireza Makhzani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you very much for your helpful comments. It will be straightforward for us to address the ambiguities you raised about the content of the algorithm box, and produce a version that is clearer in the final manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392783240000, "tcdate": 1392783240000, "number": 1, "id": "IGP-jflb5FImc", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "QDm4QXNOsuQVE", "replyto": "mOSRjNNtRhj91", "signatures": ["Alireza Makhzani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you very much for your feedbacks.\r\n\r\n-We will clarify all the ambiguities that you mentioned in the abstract and the algorithm box. This should improve the clarity of the manuscript.\r\n-Regarding your question about our reimplementation of the dropout and denoising autoencoder, we did the hyperparameter search for the dropout rate and the noise level. Further, we found that the reported results are consistent with those reported in the original papers."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392782820000, "tcdate": 1392782820000, "number": 1, "id": "88wtvqkZBJaOo", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "QDm4QXNOsuQVE", "replyto": "PPfbPOaNgVPWE", "signatures": ["Alireza Makhzani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "We very much appreciate your constructive feedbacks.\r\n\r\n1. 'the method is a bit flawed because it does not control sparsity across samples (yielding to possibly many dead units). It would be very helpful to add experiments with a few different values of code dimensionality. For instance on MNIST, it would be interesting to try: 1000, 2000 and 5000.'\r\n\r\nThank you for raising this concern. All the reported experiments on NORB has been done with 4000 hidden units. We have also done experiments with 4000 hidden units on MNIST and with a proper scheduling of k, we were able to train almost all the filters of the autoencoder and obtain classification result of 1.15% before fine-tuning and 0.99% after fine-tuning. In the case of MNIST, when we start off with a large sparsity level, almost all the filters get trained in the first 50 epochs. As we decrease the sparsity level, the filters start evolving to more global filters and the length of digit strokes start increasing. We didn't report the results with 4000 hidden units on MNIST so that we could have a fair comparison with other works that use 1000 hidden units. Based on your feedback, we will include this result and the details of the experiment in the final manuscript.\r\n\r\n2. 'The paper is fairly incremental in its novelty. There are several other papers that used similar ideas'\r\n\r\nThank you for raising concerns about the related works. We would like to point out that there are important differences between our paper and the works you mentioned.\r\n\r\nWe compared our method to the marginal regression method in response to Phil Bachman's comment (see above). In short, While we are addressing the conventional sparse coding problem with a Euclidean cost function, Krishnakumar's paper defines a different cost function using a non-parametric kernel function applied on data. We derived our operator from iterative hard thresholding which is completely different from marginal regression and behave differently. Another difference is that marginal regression uses an L1 penalty on the absolute value of the least square coefficients to promote sparsity. We have tried using the L1 norm instead of the L0 norm in our algorithm and we were not able to train the model. So, using the L0 norm makes a significant difference. We use our operator to regularize deep neural nets and do supervised learning while they use marginal regression in an unsupervised fashion and then train a SVM on top of that for classification. The analysis we provide for algorithm is quite different and in our view complementary to the results in the paper that you mentioned. Based on your feedback, we will include this comparison in our paper.\r\n\r\nAlthough there are interesting connections between 'Compete to Compute' paper and our paper, The focus and details of the two works are rather different. The focus of our paper is sparse coding while the 'Compete to Compute' paper splits the hidden units to the several groups of two hidden units, and pick the largest hidden unit within each group. So exactly half of the hidden units are always active at any time (similar to 50% dropout) and there is no sparsity in the hidden representation. Our operator is also quite different as it picks the k largest hidden units among all hidden units while they pick one single hard winner within each group.\r\n\r\n3. 'lack of comparison'\r\n\r\nWe have compared our method to several methods including dropout, denoising autoencoder, DBN, DBM and third-order RBM.\r\nRegarding the LISTA and PSD methods, we have cited them and made a brief comparison in the introduction of the paper. Our error rate on MNIST before fine-tuning is 1.38% while LISTA's best error rate is 2.15% and PSD's is 4.5% (1000 samples/class). Based on your feedback, we will also add these numerical comparisons in the final manuscript.\r\nAlso, In the Coates and Ng's paper, the thresholding operator is only used at test time and training is performed using other algorithms, such as OMP-k, which are very slow. Another difference is that they use a fixed and pre-defined soft thresholding operator and do not have control over the sparsity level, while we are using a hard thresholding operator in which the threshold is adaptive and is equal to the k-th largest element of the input.\r\n\r\nThank you again for bringing up these issues, since it helps us better place our work in context."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1392085260000, "tcdate": 1392085260000, "number": 18, "id": "l6rclPZecziqO", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["anonymous reviewer d08c"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of k-Sparse Autoencoders", "review": "The authors propose an auto encoder with linear encoder and decoder, but with sparsity that keeps only k elements in the hidden layer nonzero. They show that it works as well or better then more complicated methods.\r\n\r\nNovelty: Simple but works\r\nQuality: Good\r\n\r\nDetails:\r\n- The paper introduces a very simple idea which I am sure many people not only thought of but implemented, including me. However the main point here is that the authors actually made it work well and made a connection to a sparse coding algorithm. One of tricks of making it work seems to be to start with a large number of allowed nonzero elements and then decrease it, otherwise, many filters would not ever be used. \r\n\r\n- Is there a mistake in the algorithm box as presented x = Wz+b'. Shouldn't the z be replaced by something like z_Gamma where the latter is obtained from z by setting elements that are not in the group of k largest to zero? Because that's what the description in the rest of the paper implies, for example in 2.2.\r\n\r\n- Table - it would be good to explain that the net is in Table 3's caption."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391858040000, "tcdate": 1391858040000, "number": 17, "id": "mOSRjNNtRhj91", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["anonymous reviewer c32b"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of k-Sparse Autoencoders", "review": "* Brief summary of the paper:\r\n\r\nThe paper investigates a very simple heuristic technique for a simple autoencoder to learn a sparse encoding: the nonlinearity consists in only retaining the top-k linear activations among the hidden units and setting the others to zero. With the thus trained k-sparse autoencoers, the authors were able to outperform (on MNIST and NORB) features learned with denoising autoencoders, RBMs or dropout, as well as deep networks pre-trained with those techniques and fine-tuned. \r\n\r\n* Assessment:\r\n\r\nThis is an interesting investigation of a very simple approach to obtaining sparse representations that empirically seem to perform very well. It does have a number of weaknesses:\r\n\r\n- I find it misleading to call this a *linear model* as in the abstract. A piecewise linear function is *not* a linear function. The model does use a non-linear sparsification operation.\r\n- For such a simple approach, I find the actual description of the algorithm (especially in the algorithm box) disappointingly fuzzy, unclear, confusing and probably wrong:\r\n  What is the exact objective being optimized? Is is always squared reconstruction error? It is not written in the box. Also do you really reconstruct x^ from a z that has not been sparsified (as is written in step 1)?? This is contrary to my understanding from reading the rest of the paper. I believe it would be much clearer to introduce an explicit sparsification step before the reconstruction. Similarly, with your definition of supp, it looks like the result of your sparse encoding h is a *set of indices* rather than a sparse vector. Is this intended? Wouldn't it be clearer to define an operation that returns a sparse vector rather than a set of indices? The algorithm box should be rewritten more formally, removing any ambiguity.\r\n- Section 3.3: While I find the discussion on the importance of decoherence interesting, I do not believe you can formally draw your conclusion from it, since you do not have the strict equality x = Wz (perfect reconstruction) that your theorem depends on but only an approximate reconstruction. So I would mitigate the final claims. \r\n- I wonder how thoroughly you have explored the hyper-parameter space for the other pre-training algorithms you compare yourself with, especially those that are expected to influence sparsity or control capacity somehow, as e.g. the noise level for denoising autoencoders and dropout?? Did you but try a single a-priori chosen value? If so the comparisons might be a little unfair since you hyper-optimized your alpha on the validation set.\r\n\r\n* Pros and Cons:\r\n\r\nPros: \r\n+ interesting approach due to its simplicity \r\n+ very good empirical classification performance.\r\n\r\nCons:\r\n- confusing description of the algorithm (in algorithm box); \r\n- possibly insufficient exploration of hyper-parameters of competing algorithms (relative to the amount of tweakings of the proposed approach)."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391728860000, "tcdate": 1391728860000, "number": 16, "id": "PPfbPOaNgVPWE", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["anonymous reviewer b245"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of k-Sparse Autoencoders", "review": "In this paper, the authors propose a new sparse autoencoder. At training time, the input vector is projected onto a set of filters to produce code values. These code values are sorted and only the top k values are retained, the rest is set to 0 to achieve an exact k-sparse code. Then, the code is used to reconstruct the input by multiplying this code by the transpose of the encoding matrix. The parameters are learned via backpropagation of the squared reconstruction error. The authors relate this algorithm to sparse coding and demonstrate its effectiveness in terms of classification accuracy on the MNIST and NORB datasets.\r\n\r\nThe paper is fairly incremental in its novelty. There are several other papers that used similar ideas. Examples of the most recent ones:\r\n- R. K. Srivastava, J. Masci, S. Kazerounian, F. Gomez, J. Schmidhuber. Compete to Compute. In Proc. Neural Information Processing Systems (NIPS) 2013, Lake Tahoe.\r\n- Smooth Sparse Coding via Marginal Regression for Learning Sparse Representations  K Balasubramanian, K Yu, G Lebanon\r\nProceedings of the 30th International Conference on Machine Learning 2013\r\n\r\nThe theoretical analysis is good but straightforward.\r\nWhat worries me the most is that a very important detail of the method (how to prevent 'dead' units) is only slightly mentioned. The problem is that the algorithm guarantees to find k-sparse codes for every input but not to use different codes for different inputs. As the code becomes more and more overcomplete (and filters more and more correlated), there will be more and more units that are not used making the algorithm rather inefficient.\r\nThe authors propose to have a schedule on 'k' but that seems rather hacky to me. What happens if the authors use twice as many codes? what happens if they use 4 times as many codes? My guess is that it will break down easily. \r\nMy intuition is that this is a very simple and effective method when the code has about the same dimensionality of the input but it is less effective in overcomplete settings. This is an issue that can be rather important in practical applications and that should be discussed and better addressed.\r\n\r\nPros:\r\n- simplicity\r\n- clearly written paper\r\n\r\nCons:\r\n- lack of novelty (see comments above)\r\n- lack of comparison\r\n  - I would add a comparison to A. Coates method and to K. Gregor's LISTA (or K. Kavukcuoglu's PSD) (software for these methods is publicly available). These are the most direct competitors of the proposed method because they also try to compute a good and fast approximation to sparse codes.\r\n- the method is a bit flawed because it does not control sparsity across samples (yielding to possibly many dead units). It would be very helpful to add experiments with a few different values of code dimensionality. For instance on MNIST, it would be interesting to try: 1000, 2000 and 5000.\r\n\r\nOverall, this is a nicely written paper proposing a simple method that seems to work fairly well. I have concerns about the novelty of this method and its robustness to highly overcomplete settings."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391370300000, "tcdate": 1391370300000, "number": 1, "id": "BB-qwYPO53cZd", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "QDm4QXNOsuQVE", "replyto": "TTfOb3K5AclE7", "signatures": ["Alireza Makhzani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you Phil for referring us to Krishnakumar's paper. We read this paper and although there are interesting connections between the two works, there are important differences as well:\r\n\r\n1) While we are addressing the conventional sparse coding problem with a Euclidean cost function, Krishnakumar's paper defines a different cost function using a non-parametric kernel function applied on data. As a result, their hidden representation is modeling the neighborhood of training examples rather than reconstructing the individual samples.\r\n\r\n2) Although iterative hard thresholding (that we use) and marginal regression are both alternatives of LASSO, they are quite different algorithms and may behave quite differently. Iterative hard thresholding (IHT) is an iterative procedure for sparse recovery that uses L0 projection and refines the estimated support set at each iteration. If we use only the first iteration of IHT to learn the dictionary, we obtain our K-sparse autoencoder. But we can use more iterations (at the training or test time) and get better results at the price of more computational complexity. See http://www.see.ed.ac.uk/~tblumens/papers/BDIHT.pdf for more details about IHT. Marginal regression, however, is a different algorithm that uses an L1 penalty on the absolute value of the least square coefficients to promote sparsity. We have tried using an L1 norm instead of the L0 norm in our algorithm and we were not able to train the model. So, using the L0 norm makes a significant difference. We have also done experiments using the absolute value of the hidden representation and observed that taking absolute value hurts the performance in our setting. Based on your feedback, perhaps these results should be included?\r\n\r\n3) We have been able to regularize deep neural nets using our method and obtain better results than dropout and the denoising autoencoder on both MNIST and NORB. Using the neural nets gives us the advantage of fine-tuning a 'supervised learning task' with our thresholding operator. But Krishnakumar's paper obtains features modeling the neighborhood in an 'unsupervised fashion' and then uses an SVM on top of that for classification. So our algorithm could also be viewed as a regularization method for deep nets using sparsity.\r\n\r\n4) The analysis we provide for algorithm is quite different and in our view complementary to the results in the papers that you mentioned.\r\n\r\nThank you for bringing this other work to our attention, since it helps us better place our work in context, and also thanks for mentioning the 'marginalized dropout' idea, as we think it is definitely worth trying as an addition to our work."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391159880000, "tcdate": 1391159880000, "number": 15, "id": "tmV9T33Eo0myh", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I apologize for the clutter. The web interface was not responding on my end, though it apparently processed most of my requests on the server side of things. If anyone has moderator privileges, I would appreciate it if all but one of my earlier comments could be removed."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391149020000, "tcdate": 1391149020000, "number": 13, "id": "-1_6TtCJ3iKOz", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391149020000, "tcdate": 1391149020000, "number": 10, "id": "22GAEiwLF02Cg", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391149020000, "tcdate": 1391149020000, "number": 14, "id": "gP7ACWWzucCnT", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391149020000, "tcdate": 1391149020000, "number": 8, "id": "ssEosd49J-s3f", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391149020000, "tcdate": 1391149020000, "number": 12, "id": "IQiNInntNzvtF", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391149020000, "tcdate": 1391149020000, "number": 9, "id": "bbr_bzvcfMSDd", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391149020000, "tcdate": 1391149020000, "number": 11, "id": "P-iz-xpWztV_s", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391148960000, "tcdate": 1391148960000, "number": 5, "id": "U_SZ1ftvTIxvg", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391148960000, "tcdate": 1391148960000, "number": 4, "id": "55fTgHc4IMg4N", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391148960000, "tcdate": 1391148960000, "number": 7, "id": "0Mj2MHxs2CA36", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391148960000, "tcdate": 1391148960000, "number": 3, "id": "TTfOb3K5AclE7", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391148960000, "tcdate": 1391148960000, "number": 6, "id": "01dx0NWMEZGjr", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Phil Bachman"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This paper should be citing: 'Smooth Sparse Coding via Marginal Regression\r\nfor Learning Sparse Representations', by Krishnakumar Balasubramanian, Kai Yu, and Guy Lebanon. In their work, they use a sparse coding subroutine effectively identical to your sparse coding method. The only difference in their encoding step is that they threshold the magnitude-sorted set of potential coefficients by cumulative L1 norm rather than cumulative L0 norm (a minor difference). They also add a regularization term to their objective designed to approximately minimize coherence of the learned dictionary, which may be worth trying as an addition to your current approach.\r\n\r\nThis general class of techniques, i.e. marginal regression, is reasonably well-known and has been investigated previously as a quick-and-dirty approximation to the Lasso. For more detail, see: 'A Comparison of the Lasso and Marginal Regression', by Genovese et. al. I haven't looked at this paper in a while, but it should contribute to your theory, as they present results similar to yours, but in the context of approximating the Lasso in a linear regression setting.\r\n\r\nIt might be interesting to try a 'marginalized dropout' encoding at test time, in which each coefficient is scaled by it's probability of being among the top-k coefficients when the full coefficient set is subject to, e.g., 50% dropout. This would correspond to a simple rescaling of each coefficient by its location in the magnitude-sorted list. The true top-k would still be included fully in the encoding, while coefficients outside the true top-k would quickly shrink towards 0 as you move away from the true top-k. The shrinkage factors could be pre-computed for all possible sorted list positions based on the CDF of a Binomial distribution. If 'true' sparsity is desired, the shrunken coefficients could be hard-thresholded at some small epsilon. This would 'smooth' the encoding, perhaps removing aliasing effects that may occur with a hard threshold. This would be a fairly natural encoding to use if dictionary elements were also subject to (unmarginalized) dropout at training time. The additional computational cost would be trivial, as shrinkage and thresholding would be applied simultaneously to the magnitude-sorted coefficient list with a single element-wise vector product.\r\n\r\nPaper 1: http://www.cc.gatech.edu/~lebanon/papers/ssc_icml13.pdf\r\nPaper 2: http://www.stat.cmu.edu/~jiashun/Research/Year/Marginal.pdf"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1390029480000, "tcdate": 1390029480000, "number": 1, "id": "Wsm0zCOF1isf5", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "QDm4QXNOsuQVE", "replyto": "o1QJezhmJPem2", "signatures": ["Alireza Makhzani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you Markus and David for your constructive feedbacks.\r\n\r\nWe read your paper and found its results in line with that of ours. In your paper, an improved and differentiable sparsity enforcing projection with respect to the Hoyer's sparseness measure has been introduced. This projection is used in a supervised online autoencoder whose cost function has an alpha parameter that controls the trade-off between unsupervised learning and supervised learning. The algorithm has been tested on MNIST and achieved good classification rate. \r\nThis paper is similar to our paper in that our hard thresholding operator could be viewed as an L0 projection and we are also using unsupervised learning to pre-train our discriminative neural net. It is true that by using a more complicated thresholding operator in the sparse recovery stage (as in your paper), we can obtain a better performance on datasets such as MNIST. For example, in Donoho & Maleki's 'approximate message passing' paper, it has been shown that a variant of iterative hard thresholding algorithm that uses a complex thresholding operator derived from message passing can beat even convex relaxation methods in sparse recovery in both performance and complexity. However, as we have discussed in the paper, the main motivation of this work is to propose a fast sparse coding algorithm on GPU that could be applied on larger datasets. We have shown that only the first few iterations of IHT combined with a dictionary update stage is enough to get close to the state of the art results. IHT only requires matrix multiplication in the sparse coding stage which makes it very fast on GPUs. We have discussed how the iterations of IHT algorithm could be viewed in the context of our proposed k-sparse autoencoder and how we can use it to pre-train deep neural nets.\r\n\r\nRegarding the incoherency, Theorem 3.1 establishes a connection between the incoherency of the dictionary and the chances of finding the true support set with the encoder part of the k-sparse autoencoders. We experimentally observe that k-sparse autoencoders converge to a local minimum. At this local minimum, the autoencoder has learnt a sparse code z, that satisfies x = Wz. Also the support set of this sparse code can be estimated using supp(W.T x). Since supp(W.T x) succeeds in finding the support set, according to the Theorem 3.1, the atoms of the dictionary should be well separated from each other and the learnt dictionary must be sufficiently incoherent.\r\nYes, if we were only considering just one single training example, as you mentioned there could be other reasons that the support set could be recovered while the dictionary is coherent. But the only way that the k-sparse autoencoder can recover the true support set for 'all training examples' is that the dictionary be sufficiently incoherent.\r\nTo measure the incoherency of the dictionary, we obtained the following result (see Deterministic Compressed Sensing by Jafarpour & Calderbank). We have experimentally showed that the k-sparse autoencoder learns a dictionary for which we can actually solve sparse recovery problems for other sparse signals and not just the training points. We first learned a dictionary W of size 784*1000 on MNIST using the k-sparse autoencoder with K = 20. We then picked K elements of z uniformly at random and set them to be one and set the rest of the elements to zero. Then we computed x = Wz and tried to reconstruct the support set of z from x using the encoder of the k-sparse autoencoder. We found that the trained dictionary was able to recover the support set of the arbitrary sparse signals perfectly. This is only possible when the dictionary is sufficiently incoherent and the atoms (features) are well separated from each other.\r\n\r\nRegarding the scheduling, It means when we have 100 epochs, k follows a linear function for epochs 1 to 50, and then remains at the minimum level for epochs 51 to 100. In our experiments, increasing k always helped to avoid dead hidden units. Another option is to add a KL penalty to the cost function of the autoencoder as in Ng's sparse autoencoder paper. This KL penalty encourages each hidden unit to be active a small number of times across the training set and avoids the problem of dead hidden units.\r\n\r\nThanks Markus and David for pointing out the typos. We will correct them in the final manuscript."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389824640000, "tcdate": 1389824640000, "number": 1, "id": "o1QJezhmJPem2", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "QDm4QXNOsuQVE", "replyto": "ZpJXvOG3-Cv6U", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "I agree on all the points made about Theorem 3.1.\r\n\r\n'supp_k(z) = supp_k(W^T*x)' would be clearer as well as more succinct.\r\n\r\nI also encourage you to put a little box at the end of the proof.\r\n\r\nAnd If I understand it correctly, you can use a weaker condition, namely:\r\n\r\nk*mu < z_k/z_1 (note strict inequality)"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1389051900000, "tcdate": 1389051900000, "number": 1, "id": "Np1RNxW_L5pw2", "invitation": "ICLR.cc/2014/-/submission/conference/reply", "forum": "QDm4QXNOsuQVE", "replyto": "Gg_aFHZkvdGNC", "signatures": ["Alireza Makhzani"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "reply": "Thank you David for raising this concern.\r\nOMP combined with a dictionary update stage (as in Coates and Ng) is the conventional way of doing sparse coding and is discussed in the introduction of our paper. The problem with OMP is that it is very slow in practical situations. It uses k iterations and at each iteration, it needs to project a residual error onto the orthogonal complement of the subspace created by the span of the dictionary elements at that iteration. This is a very costly matrix operation as it requires k matrix inversions whenever we visit any training example. It works on small datasets such as MNIST but we were not able to get it working properly on larger datasets such as NORB, as it was too slow.\r\n\r\nIndeed the main motivation of proposing the k-sparse autoencoder is to address sparse coding for larger problem sizes, e.g., NORB, where conventional sparse coding approaches such as OMP are not practical. We have shown in the paper that the k-sparse autoencoder is an approximation of iterative hard thresholding (IHT). IHT is much faster than OMP, as it only needs a matrix multiplication in the sparse coding stage that can be efficiently implemented on a GPU. Also we have approximated the dictionary update stage with a single gradient descent step which makes the algorithm very fast. We have discussed in the paper that even with a very naive approximation of IHT, we achieve a fast sparse coding algorithm on GPU that performs as good as the state of the art. By tuning the number of iterations of the IHT algorithm, we can learn better dictionaries and trade off classification performance with computation time. Therefore, although both the k-sparse autoencoder and OMP-k enforce k-sparsity in the hidden representation, they are inherently different in both the dictionary learning and encoding stages.\r\n\r\nOur thresholding operator is also different from that of Coates and Ng's. The main difference is that we directly use the operator in both the training and test stages to gain speed-ups, while in the Coates and Ng's, the thresholding operator is only used at test time and training is performed using other algorithms, such as OMP-k, which are very slow. Another difference is that they use a fixed and pre-defined soft thresholding operator and do not have control over the sparsity level, while we are using a hard thresholding operator in which the threshold is adaptive and is equal to the k-th largest element of the input."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1388841360000, "tcdate": 1388841360000, "number": 1, "id": "Gg_aFHZkvdGNC", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["David Krueger"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "I have not read the paper that carefully.  But the idea of the k-sparse autoencoder seems very similar to the Orthogonal Matching Pursuit (OMP-k) training and encoding used in Coates and Ng (http://www.stanford.edu/~acoates/papers/coatesng_icml_2011.pdf).\r\n\r\nThe difference, it seems to me, is that OMP-k allows less than k units to be active, and also the scheduling idea (4.2.1), and the alpha multiplier for the encoding stage.\r\n\r\nIs there something else I am missing that distinguishes your approach?  \r\n\r\nOtherwise, I would like to see comparisons to OMP-k, and to a simple threshold encoding approach as in Coates and Ng."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1387854420000, "tcdate": 1387854420000, "number": 2, "id": "ZpJXvOG3-Cv6U", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "QDm4QXNOsuQVE", "replyto": "QDm4QXNOsuQVE", "signatures": ["Markus Thom"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "", "review": "This is an interesting paper I would like to comment on and ask the authors some questions. The paper studies auto-encoders where the internal representation is approximated online by a vector whose number of non-vanishing coordinates is restricted (in other words, the internal representation is projected onto the set of all vectors with a certain L0 pseudo-norm). The proposed model, 'k-sparse autoencoders', is put in context with Arian Maleki's 'iterative thresholding with inversion' algorithm for inference of sparse code words, and a criterion is given to identify the case when one iteration is enough for perfect inference. Experiments on MNIST and small NORB show that superior classification performance (wrt. RBMs, dropout, denoising auto-encoders) can be achieved when using the proposed model to generate features and processing them with logistic regression, and that the classification performance is competitive when all the models were additionally fine-tuned. This is a cool result, since the only non-linearity used for the features is the projection.\r\n\r\nWe have done something similar, see Section 3 of the paper available at http://jmlr.org/papers/v14/thom13a.html, by using projections onto sets on which certain sparseness measures (including the L0 pseudo-norm) attain a constant value as neural transfer function in a hybrid of an auto-encoder and an MLP. Inference of the internal representation can be understood here as carrying out the first iteration of a projected Landweber algorithm. Perhaps the authors would like to discuss the relationship between both approaches?\r\n\r\nThe last sentence in the discussion after the proof of Theorem 3.1 is a bit puzzling. The theorem shows that if mu is small, then the supports of z and W^T*x are identical. The aforementioned sentence says that these supports are identical, hence mu must be small. I believe this is the converse of the theorem's statement and has not been proven, since there may be reasons other than mu being small.\r\n\r\nThe description of the k schedule in Section 4.2.1 is ambiguous. Does this mean that when we have 100 epochs, say, that k follows a linear function for epochs 1 thru 50, and then remains at the minimum level for epochs 51 to 100, or does it mean that in each epoch for the first halve of the presented samples k is adjusted and stays at the minimum for the remaining samples of the epoch, and then the schedule starts all over again in the next epoch? There are still some dead hidden units in the figures on page 6, even for k = 70 on MNIST. Would it help to increase the initial k value in the schedule, or maybe add some small random numbers to z (with some annealed variance) after setting the small entries to zero, such that backprop adjusts all the hidden units?\r\n\r\nJust a few things I noticed while reading through the manuscript:\r\n  - The formulation of the claim of Theorem 3.1 could be altered to be more succinct, e.g. 'supp_k(z) = supp_k(W^T*x)'. In the proof, i should be from {1, ..., k}, since i = 0 doesn't seem to make sense here.\r\n  - Typo on page 3, left column: 'tarining'"}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387565760000, "tcdate": 1387565760000, "number": 18, "id": "QDm4QXNOsuQVE", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "QDm4QXNOsuQVE", "signatures": ["a.makhzani@gmail.com"], "readers": ["everyone"], "content": {"title": "k-Sparse Autoencoders", "decision": "submitted, no decision", "abstract": "Recently, it has been observed that when representations are learnt in a way that encourages sparsity, improved performance is obtained on classification tasks. These methods involve combinations of activation functions, sampling steps and different kinds of penalties. To investigate the effectiveness of sparsity by itself, we propose the k-sparse autoencoder, which is a linear model, but where in hidden layers only the k highest activities are kept. When applied to the MNIST and NORB datasets, we find that this method achieves better classification results than denoising autoencoders, networks trained with dropout, and restricted Boltzmann machines. k-sparse autoencoders are simple to train and the encoding stage is very fast, making them well-suited to large problem sizes, where conventional sparse coding algorithms cannot be applied.", "pdf": "https://arxiv.org/abs/1312.5663", "paperhash": "makhzani|ksparse_autoencoders", "authorids": ["a.makhzani@gmail.com", "frey@psi.utoronto.ca"], "keywords": [], "conflicts": [], "authors": ["Alireza Makhzani", "Brendan Frey"]}, "writers": [], "details": {"replyCount": 25, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 26}