{"notes": [{"id": "ryl4-pEKvB", "original": "rygOwgwIPH", "number": 371, "cdate": 1569438971886, "ddate": null, "tcdate": 1569438971886, "tmdate": 1577168247938, "tddate": null, "forum": "ryl4-pEKvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "5hSHNqUQkA", "original": null, "number": 1, "cdate": 1576798694506, "ddate": null, "tcdate": 1576798694506, "tmdate": 1576800941015, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposes an RL-based algorithm for training neural networks that is able to match the performance of backprop on CIFAR and MNIST datasets.\n\nThe reviewers generally found the algorithm and motivations interesting, but some had issue with the imprecision of the notion of \"biologically plausible\" used by the authors. One reviewer had issues with missing discussion of related work and also doubts about the meaningfulness of the experiments, since the networks were quite shallow.\n\nFor this type of paper, clarity and precision of exposition is crucial in my opinion, and so I recommend rejection at this time, but encourage the authors to take the feedback into account and resubmit to a future venue.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722424, "tmdate": 1576800273719, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper371/-/Decision"}}}, {"id": "BJej1OMhiS", "original": null, "number": 5, "cdate": 1573820387318, "ddate": null, "tcdate": 1573820387318, "tmdate": 1573820387318, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "HkefKUpSjH", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment", "content": {"title": "Reply", "comment": "Thank you for your reply.\n\nThe equation I was referring to is equation (7). After re-reading the manuscript I now see that is the symmetrical learning rules.\n\nI appreciate authors reply, however I largely keep to my initial evaluation, mostly because of the clarity issues which raised in my initial review.\nI am also confused about why the authors did not cite Akrout et al, 2019 and Bartunov et al, 2018 and, more importantly, did not discuss these results in the first place since they seemed to be aware about these papers.\n\nMy recommendations for future submissions of the manuscript are the following:\n1) Fully integrate all past and recent work on biologically-plausible learning and clearly position the proposed framework within the literature.\n2) Provide a clear derivation of the learning algorithm, starting from explaining the basic principles (e.g. stochastic optimization, Q-learning, etc) and then use it to derive the update rules.\n3) Perform experiments on deeper architectures to confirm the scalability of the proposed method and its ability to perform credit assignment. \n4) Improve mathematical notation, reduce the number of potentially confusing elements such as fb_{y_j^{(1)}}.\n\nI believe that if the submitted paper indeed possess a great importance for the field, following these recommendations will only make it stronger and increase the chance of acceptance. In the current form I do not find it progressing the field and thus cannot recommend acceptance."}, "signatures": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer4", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl4-pEKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper371/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper371/Authors|ICLR.cc/2020/Conference/Paper371/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172437, "tmdate": 1576860547955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment"}}}, {"id": "HkefKUpSjH", "original": null, "number": 4, "cdate": 1573406329914, "ddate": null, "tcdate": 1573406329914, "tmdate": 1573406440370, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "HJegoPYvcB", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment", "content": {"title": "Reply to Reviewer #4", "comment": "When asking for a more formal presentation of the main update equation, we are not quite clear what the reviewer means: we introduce (1) from a line of research papers, after giving an intuitive explanation, afterwards we show that (1) results in approximate EBP in a trial-and-error fashion. \nConcerning the \"two different expressions for updates in each layer\" we are not quite sure what the reviewer is referring to, which equations are at issue? \nMore theoretical properties of the algorithm: the global error, as indicated in the text (mid page 4), is a MSE: $E = \\frac{1}{2}\\delta^2 = \\frac{1}{2}{(r-q_s)}^2 $. Since it is a stochastic optimisation of this error, we do not have any theoretical guarantees (yet), but, as also pointed out in Richards et al (2019), determining the variance of the approach is important. We added this note in the discussion.\nIndeed we had not addressed the weight-transport issue. As indicated by Tim Lillicrap himself in [Akrout 2019], the key issue is not so much symmetric weights, but being able to locally compute the same error both in the forward and backward network. Having different feedforward and feedback weights but symmetrical updates with weight decay leads to a convergence to the same value, i.e. symmetrical weights. For this reason, we have stuck with symmetrical weights. We added this explicit reference in the text. Of course, we are also arguing that supervised learning is not the proper framework for animal learning. \nPresuming the reviewer is referring to [Bartunov et al 2018], \"Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures\" (NeuriPS2018), when saying that we missed to mention recent results, we remark that this and the idea of Target Propagation is focused on replicating the error-backpropagation learning algorithm in a supervised setting. Bartunov in particular examined Target Propagation and Feedback alignment, showing that both perform substantially worse than error-backprop. Also, while Bartunov et al scale up to ImageNet, the results are so extremely poor that while informative of its failure, we are focusing first on getting much better results compared to EBP in a reinforcement learning setting; to scale to ImageNet size networks, a future goal, we will need to find biologically plausible equivalents for learning through a number of standard deep learning structures, like max-pooling. We note so in the update text.\nAnd finally, regarding the credit-assignment, we strongly disagree with the reviewer: the results clearly demonstrate proper credit assignment in deep networks, up to 5 layers, as the results correspond to EBP, as the math also demonstrates (and any bug in the code during development also emphatically showed this). For locally connected, we were mostly interested in determining the performance penalty of increasing the complexity; we agree that the architecture used does not completely factor out the impact of weight sharing; and we note so in the text now, we argue however that adding such complexity in terms of weights still demonstrates the effectiveness of the credit-assignment by DeepAGREL.\n\nAkrout, M. M., Wilson, C., Humphreys, P., Lillicrap, T., & Tweed, D. (2019). Deep learning without weight transport."}, "signatures": ["ICLR.cc/2020/Conference/Paper371/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl4-pEKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper371/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper371/Authors|ICLR.cc/2020/Conference/Paper371/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172437, "tmdate": 1576860547955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment"}}}, {"id": "HkedFl6SoS", "original": null, "number": 3, "cdate": 1573404799862, "ddate": null, "tcdate": 1573404799862, "tmdate": 1573404799862, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "SkxelqZCFH", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment", "content": {"title": "Reply to Reviewer #2", "comment": "We selected the same architectures as in [Amit2018] as they were tested on another rule with biologically plausible components, we added this as a reference in the text. \nRegarding feedback connections, for reasons of space, we point to the elaborate review paper by Roelfsema and Holtmaat (2018); we also added a reference to the very recent Richards et al. (2019) paper. The point about the specificity of forward and backward connections however deserves more study. We would argue that given reciprocal learning and weight decay, as argued by Akrout et al. (2019), specificity will emerge. We added this point (and citation) to the discussion.\n\nRichards, Blake A., et al. \"A deep learning framework for neuroscience.\"\u00a0Nature neuroscience\u00a022.11 (2019): 1761-1770.\nAkrout, M. M., Wilson, C., Humphreys, P., Lillicrap, T., & Tweed, D. (2019). Deep learning without weight transport."}, "signatures": ["ICLR.cc/2020/Conference/Paper371/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl4-pEKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper371/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper371/Authors|ICLR.cc/2020/Conference/Paper371/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172437, "tmdate": 1576860547955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment"}}}, {"id": "BJgd6C2SoS", "original": null, "number": 2, "cdate": 1573404351660, "ddate": null, "tcdate": 1573404351660, "tmdate": 1573404351660, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "HJl1ApMw5r", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment", "content": {"title": "Reply to Reviewer #1", "comment": "Regarding the dependance of learning on the number of classes: we see the cost of \u201ctrial-and-error\u201d learning go up from about 150% for MNIST and CIFAR10 to about 200% for CIFAR100, a very modest increase. In our studies, the main problem we encountered when training CIFAR100 instead of CIFAR10 was not so much due to the number of classes, but rather the 10 fold reduced number of samples per class in CIFAR100. For this reason, we added data augmentation.\nMoreover, for what concerns the extension to larger tasks, we demonstrated that the rule works on the standard image classification tasks used for biologically plausible learning, with essentially equivalent performance to error backpropagation. In order to train networks on larger tasks we would need architectures/elements (e.g. max pooling, model ensembles, memory units, etc) for which biologically plausible counterparts still need to be developed in the AGREL setting. This is a goal for future work; we include this point now explicitly in the discussion. \nFor the statement in the introduction, there may be some confusion, as we agree: we are followed Yann LeCunn in rephrasing \u201cunsupervised\u201d as \u201cself-supervised learning\u201d. We updated the paper to explicitly include \u201cunsupervised\u201d. Instead, regarding convolutional layers, we are mainly concerned with neuroscientific data that explicitly shows that there are clear instances where vision is not learned in a \u201cposition invariant\u201d manner [COX2005]. Clearly, weights are not physically shared in the brain, though one could argue that eye-movements mostly take care of this problem. We added the citation in the paper to be more explicit.\nWhen stating that this approach is very reasonable for mutually exclusive classification task but that these make only for a small subset of the task that EBP could be applied to the reviewer is correct, thus our statement regarding the nature of learning in animals. At the same time, related work like the cited Rombouts et al (2015) paper, demonstrate that this approach applies to learning very many cognitive tasks. Also see the very recent paper by Richards et al. in Nature Neuroscience (2019), which positions AGREL relative to other approaches (we added this citation in the paper).\nFinally, \"Is DeepAGREL equivalent to stochastically selecting a single unit in a softmax layer and then back propagating only its error?\" Effectively yes, except that the error is separated from the activity feedback; feeding back the error would be identical. Specifically, DeepAGREL is equivalent to a version of error backpropagation in a reinforcement learning setting that trains one output unit at a time, using an \u03b5 greedy mechanism for action selection (i.e. 98% of the times the neuron with the highest activity will be selected). We added a sentence to this effect in the paper (end of the Introduction).\n\nCox, D. D., Meier, P., Oertelt, N. & DiCarlo, J. J. \u2018Breaking\u2019 position-invariant object recognition.\u00a0Nat. Neurosci.\u00a08,\u00a01145\u20131147 (2005).\nRichards, Blake A., et al. \"A deep learning framework for neuroscience.\"\u00a0Nature neuroscience\u00a022.11 (2019): 1761-1770."}, "signatures": ["ICLR.cc/2020/Conference/Paper371/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl4-pEKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper371/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper371/Authors|ICLR.cc/2020/Conference/Paper371/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172437, "tmdate": 1576860547955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment"}}}, {"id": "SkxelqZCFH", "original": null, "number": 1, "cdate": 1571850728419, "ddate": null, "tcdate": 1571850728419, "tmdate": 1572972603705, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents DeepAGREL, a framework for biologically plausible deep learning that is modified to use reinforcement learning as a training mechanism. This framework is shown to perform similarly to error-backpropagation on a set of architectures. The idea relies on feedback mechanism that can resemble local connections between real neurons.\n\nThis paper is an interesting approach to provide a reinforcement learning paradigm for training deep networks, it is well written and the experiments are convincing, although more explanation about why these specific architectures were tested would be more convincing. I also think the assumptions about feedback connections in real neurons should be visited and more neuroscientific evidence from the literature should be included in the paper. Do we expect feedback to happen at each level of a neuron-neuron interaction and between each pair of connected neurons? Is there a possibility that feedback is more general to sets of neurons, or skips entire layers of neurons? I think more neuroscience background would help this paper (and others on the topic). Nonetheless, I think the paper does offer an interesting proposal of a more biologically plausible form of deep learning.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575638609043, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper371/Reviewers"], "noninvitees": [], "tcdate": 1570237753123, "tmdate": 1575638609056, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Review"}}}, {"id": "HJl1ApMw5r", "original": null, "number": 2, "cdate": 1572445639213, "ddate": null, "tcdate": 1572445639213, "tmdate": 1572972603662, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper generalizes the AGREL biologically plausible learning algorithm to deeper networks. It presents experiments to show that the method can scale to medium-sized tasks with only a modest slow down in training speed and essentially no change in classification performance.\n\nMajor comments:\n\nThis is an interesting paper which shows that a particular biologically plausible learning method can attain comparable performance to gradient descent on small and mid-size visual classification tasks. The proposed mapping to the biology is of a different flavor from many other recently proposed approaches. \n\nIt is striking that the CIFAR10 network trains about as fast as the EBP network, while the CIFAR100 network trains much more slowly. This is presumably because randomly guessing the right answer out of 100 possibilities is the bottleneck. The paper could be strengthened by studying the speed of learning as a function of the number of output classes. For this approach to scale to ImageNet, with a 1000-way classification (or to our human visual recognition abilities with far more classes), this is an important scaling dimension to consider. \n\nIt should be noted that other biologically plausible schemes like feedback alignment were able to solve CIFAR and other smaller image classification tasks, but struggled when applied to the larger scale ImageNet problem. The paper could be improved by pointing to this limitation of the present work, the possibility that performance could change on larger tasks, and the need to conduct larger experiments in future work.\n\nPersonally I think the statement at the beginning of the introduction that only RL occurs in animals and humans is overstated. Unsupervised learning occurs in some form in critical period plasticity, and in various unrewarded statistical learning paradigms, at a minimum.\n\nI would also caution against categorical statements of biological plausibility, for instance, in saying that shared weights in convolutions are biologically implausible (bottom of pg 6). The same criticism has been leveled at gradient descent learning, and as the present submission shows, these intuitive judgements can be misleading.\n\nThe distinction between RL and supervised learning is a bit blurred in the present submission because it is considering a classification setting in which exactly one out of a number of possible outputs is rewarded on each trial. This looks very similar to the supervised learning scenario, and relies on stochastic softmax-like competition to select a single output. This approach is very reasonable for mutually exclusive classification tasks, but this is a small subset of the tasks that full EBP could be applied to. \n\nThe paper is fairly clear but the explanation of the algorithm could be further streamlined and condensed. Is DeepAGREL equivalent to stochastically selecting a single unit in a softmax layer and then back propagating only its error? It seems so from what I understand, and this may be a straightforward way to explain the algorithm. \n\nTypos:\n\nThe text on page 7 describes MNIST performance as 99.17% but the table has 99.16%.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575638609043, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper371/Reviewers"], "noninvitees": [], "tcdate": 1570237753123, "tmdate": 1575638609056, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Review"}}}, {"id": "HJegoPYvcB", "original": null, "number": 3, "cdate": 1572472727528, "ddate": null, "tcdate": 1572472727528, "tmdate": 1572972603618, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes an backpropagation-free algorithm for training in deep neural networks.\nThe algorithm called DeepAGREL does not use the chain rule for computing derivatives and instead is based on direct reinforcement. \nAuthors conduct experiments on MNIST and CIFAR where they find their method to reach performance similar to BP.\n\nNovelty: the method is built upon existing ideas, however, the exact algorithm seems to be novel.\n\nClarity: although it is possible to understand the algorithm based on the provided textual description, it would not hurt to provide a more formal presentation of the main update equation (1). \n\nIt is not clear why authors provide two different expressions for updates in each layer (in terms of fb^{l} and fb^{l+1}).\n\nI would also appreciate a clear discussion on the theoretical properties of the algorithm, for example, is it guaranteed to converge to a critical point of some loss function? Can we derive the update rule from some known optimisation procedure, e.g. REINFORCE?\n\nQuality: Unfortunately I have a number of major issues with this respect.\n\n1. Authors do not clearly pose what are the properties of back propagation that they find biologically non-plausible and how exactly their algorithm is making progress on them. There seems to be some sort of consensus in the literature about these properties and if authors accept it, then it looks like DeepAGREL is not very plausible too, because it does not address the weight-transport issue at all.\n2. Many recent results are not even mentioned in the paper, for example, (Bartunov et al, 2019). From not so recent \u2014 the whole branch of research around target propagation algorithm (Lee et al, 2014).\n3. The experiments are not convincing to me, as the considered network architectures are quite shallow and thus the ability to perform credit assignment can not be demonstrated. Besides that, the locally-connected architecture has only the first layer locally-connected and it does not make much sense to me, as it does not factor out the impact of weight sharing. \n\nOverall, I think the paper is not ready for publication at ICLR.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575638609043, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper371/Reviewers"], "noninvitees": [], "tcdate": 1570237753123, "tmdate": 1575638609056, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Review"}}}, {"id": "BkexBVvuuS", "original": null, "number": 1, "cdate": 1570432056457, "ddate": null, "tcdate": 1570432056457, "tmdate": 1570432056457, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "SklcfRiNOH", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment", "content": {"comment": "Thank you for your interest and comment. Yes, theoretically there is a savings in computation time since we do not update non-selected output neurons. This savings however would be extremely small, since all other layers, containing most neurons and connections, are fully updated. So in practise, wall-time per epoch for DeepAGREL and EBP are the same. ", "title": "Re: Wall clock training time?"}, "signatures": ["ICLR.cc/2020/Conference/Paper371/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl4-pEKvB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper371/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper371/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper371/Authors|ICLR.cc/2020/Conference/Paper371/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172437, "tmdate": 1576860547955, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper371/-/Official_Comment"}}}, {"id": "SklcfRiNOH", "original": null, "number": 1, "cdate": 1570188818265, "ddate": null, "tcdate": 1570188818265, "tmdate": 1570189184278, "tddate": null, "forum": "ryl4-pEKvB", "replyto": "ryl4-pEKvB", "invitation": "ICLR.cc/2020/Conference/Paper371/-/Public_Comment", "content": {"comment": "Since only neurons connected to selected action neuron are updated for every iteration, does this mean you can save computation time by not computing errors and updates for unrelated neurons? Even though more epochs needed to achieve accuracy similar to error backprop?\n\nHow is the whole training algorithm implemented in code? During matmul, what happend to neurons that don't need updates? \n\nHow is wall clock training time?", "title": "Wall clock training time?"}, "signatures": ["~Raden_Muaz1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Raden_Muaz1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DeepAGREL: Biologically plausible deep learning via direct reinforcement", "authors": ["Isabella Pozzi", "Sander M. Bohte", "Pieter R. Roelfsema"], "authorids": ["pozzi@cwi.nl", "s.m.bohte@cwi.nl", "p.roelfsema@nin.knaw.nl"], "keywords": ["biologically plausible deep learning", "reinforcement learning", "feedback gating", "image claassification"], "TL;DR": "We show how deep learning can be implemented in the brain using direct reinforcement learning just as well as error-backprop for hard tasks, with a surprisingly small penalty to the speed of convergence", "abstract": "While much recent work has focused on biologically plausible variants of error-backpropagation, learning in the brain seems to mostly adhere to a reinforcement learning paradigm; biologically plausible neural reinforcement learning frameworks, however, were limited to shallow networks learning from compact and abstract sensory representations. Here, we show that it is possible to generalize such approaches to deep networks with an arbitrary number of layers.  \nWe demonstrate the learning scheme - DeepAGREL - on classical and hard image-classification benchmarks requiring deep networks, namely MNIST, CIFAR10, and CIFAR100, cast as direct reward tasks, both for deep fully connected, convolutional and locally connected architectures. We show that for these tasks, DeepAGREL achieves an accuracy that is equal to supervised error-backpropagation, and the trial-and-error nature of such learning imposes only a very limited cost in terms of training time. Thus, our results provide new insights into how deep learning may be implemented in the brain. ", "pdf": "/pdf/7bc4a24fe6a931056e66a18e51c6e4e31375435b.pdf", "paperhash": "pozzi|deepagrel_biologically_plausible_deep_learning_via_direct_reinforcement", "original_pdf": "/attachment/03b6495c7dc702fee22e3b3385b20982d740d8d3.pdf", "_bibtex": "@misc{\npozzi2020deepagrel,\ntitle={Deep{\\{}AGREL{\\}}: Biologically plausible deep learning via direct reinforcement},\nauthor={Isabella Pozzi and Sander M. Bohte and Pieter R. Roelfsema},\nyear={2020},\nurl={https://openreview.net/forum?id=ryl4-pEKvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "ryl4-pEKvB", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504210041, "tmdate": 1576860581280, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper371/Authors", "ICLR.cc/2020/Conference/Paper371/Reviewers", "ICLR.cc/2020/Conference/Paper371/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper371/-/Public_Comment"}}}], "count": 11}