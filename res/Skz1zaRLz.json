{"notes": [{"tddate": null, "ddate": null, "original": null, "tmdate": 1521573616580, "tcdate": 1521573616580, "number": 308, "cdate": 1521573616232, "id": "ByKlykJqM", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Skz1zaRLz", "replyto": "Skz1zaRLz", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Accept", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "This paper was invited to the workshop track based on reviews at the main conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Intriguing Properties of Adversarial Examples", "abstract": "It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we show that distributions of logit differences have a universal functional form. This functional form is independent of architecture, dataset, and training protocol; nor does it change during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n", "pdf": "/pdf/0dc93000a5acf4b206b769dbe0cc6de1231fe944.pdf", "TL;DR": "Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.", "paperhash": "cubuk|intriguing_properties_of_adversarial_examples", "_bibtex": "@misc{\ndogus2018intriguing,\ntitle={Intriguing Properties of Adversarial Examples},\nauthor={Ekin Dogus Cubuk, Barret Zoph, Samuel Stern Schoenholz, Quoc V. Le},\nyear={2018},\nurl={https://openreview.net/forum?id=rk6H0ZbRb},\n}", "keywords": ["adversarial examples", "universality", "neural architecture search"], "authors": ["Ekin Dogus Cubuk", "Barret Zoph", "Samuel Stern Schoenholz", "Quoc V. Le"], "authorids": ["cubuk@google.com", "barretzoph@google.com", "schsam@google.com", "qvl@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}, {"tddate": null, "ddate": null, "tmdate": 1518730166608, "tcdate": 1518420442618, "number": 98, "cdate": 1518420442618, "id": "Skz1zaRLz", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Skz1zaRLz", "original": "rk6H0ZbRb", "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "Intriguing Properties of Adversarial Examples", "abstract": "It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we show that distributions of logit differences have a universal functional form. This functional form is independent of architecture, dataset, and training protocol; nor does it change during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n", "pdf": "/pdf/0dc93000a5acf4b206b769dbe0cc6de1231fe944.pdf", "TL;DR": "Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.", "paperhash": "cubuk|intriguing_properties_of_adversarial_examples", "_bibtex": "@misc{\ndogus2018intriguing,\ntitle={Intriguing Properties of Adversarial Examples},\nauthor={Ekin Dogus Cubuk, Barret Zoph, Samuel Stern Schoenholz, Quoc V. Le},\nyear={2018},\nurl={https://openreview.net/forum?id=rk6H0ZbRb},\n}", "keywords": ["adversarial examples", "universality", "neural architecture search"], "authors": ["Ekin Dogus Cubuk", "Barret Zoph", "Samuel Stern Schoenholz", "Quoc V. Le"], "authorids": ["cubuk@google.com", "barretzoph@google.com", "schsam@google.com", "qvl@google.com"]}, "nonreaders": [], "details": {"replyCount": 1, "writable": false, "overwriting": [], "revisions": false, "tags": [], "original": {"tddate": null, "ddate": null, "tmdate": 1518730166608, "tcdate": 1509133892775, "number": 742, "cdate": 1518730166596, "id": "rk6H0ZbRb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "rk6H0ZbRb", "original": "Sknr0-WAW", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Intriguing Properties of Adversarial Examples", "abstract": "It is becoming increasingly clear that many machine learning classifiers are vulnerable to adversarial examples. In attempting to explain the origin of adversarial examples, previous studies have typically focused on the fact that neural networks operate on high dimensional data, they overfit, or they are too linear. Here we show that distributions of logit differences have a universal functional form. This functional form is independent of architecture, dataset, and training protocol; nor does it change during training. This leads to adversarial error having a universal scaling, as a power-law, with respect to the size of the adversarial perturbation. We show that this universality holds for a broad range of datasets (MNIST, CIFAR10, ImageNet, and random data), models (including state-of-the-art deep networks, linear models, adversarially trained networks, and networks trained on randomly shuffled labels), and attacks (FGSM, step l.l., PGD). Motivated by these results, we study the effects of reducing prediction entropy on adversarial robustness. Finally, we study the effect of network architectures on adversarial sensitivity. To do this, we use neural architecture search with reinforcement learning to find adversarially robust architectures on CIFAR10. Our resulting architecture is more robust to white \\emph{and} black box attacks compared to previous attempts.\n", "pdf": "/pdf/0dc93000a5acf4b206b769dbe0cc6de1231fe944.pdf", "TL;DR": "Adversarial error has similar power-law form for all datasets and models studied, and architecture matters.", "paperhash": "cubuk|intriguing_properties_of_adversarial_examples", "_bibtex": "@misc{\ndogus2018intriguing,\ntitle={Intriguing Properties of Adversarial Examples},\nauthor={Ekin Dogus Cubuk and Barret Zoph and Samuel Stern Schoenholz and Quoc V. Le},\nyear={2018},\nurl={https://openreview.net/forum?id=rk6H0ZbRb},\n}", "keywords": ["adversarial examples", "universality", "neural architecture search"], "authors": ["Ekin Dogus Cubuk", "Barret Zoph", "Samuel Stern Schoenholz", "Quoc V. Le"], "authorids": ["cubuk@google.com", "barretzoph@google.com", "schsam@google.com", "qvl@google.com"]}, "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}, "tauthor": "ICLR.cc/2018/Workshop"}], "count": 2}