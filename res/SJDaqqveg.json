{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488555468478, "tcdate": 1478105790877, "number": 48, "id": "SJDaqqveg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SJDaqqveg", "signatures": ["~Dzmitry_Bahdanau1"], "readers": ["everyone"], "content": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 23, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396325475, "tcdate": 1486396325475, "number": 1, "id": "SJaiofUue", "invitation": "ICLR.cc/2017/conference/-/paper48/acceptance", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "Originality, Significance:\n \n The paper proposes to use an actor-critic RL methods to train sequence to sequence tasks, as applied to NLP tasks. \n A key aspect is that the critic network can be conditioned on the ground-truth output of the actor network. The idea is quite novel.\n \n Quality, Clarity:\n \n The major concern is with respect to the evaluation, specifically with respect to baseline results for other state-of-the-art methods for BLEU-scored translation tasks. The final rebuttal tackles many of these issues, although the reviewers have not commented on the rebuttal. \n \n I believe that the method demonstrates significant promise, given the results that can be achieved with quite a different approach from previous work.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396325963, "id": "ICLR.cc/2017/conference/-/paper48/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SJDaqqveg", "replyto": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396325963}}}, {"tddate": null, "tmdate": 1484973505902, "tcdate": 1484973505902, "number": 15, "id": "Bk9arPePe", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "r113A61wx", "signatures": ["~Dzmitry_Bahdanau1"], "readers": ["everyone"], "writers": ["~Dzmitry_Bahdanau1"], "content": {"title": "Very good question!", "comment": "Hi, you are asking a great question. Indeed, Q(a;\\hat{y}_1 ... \\hat{y}_{t-1}) is a matrix (for each example), and 3D tensor if you consider the whole batch. The targets q_t are only available for the sampled actions, i.e. for Q(\\hat{y}_t;\\hat{y}_1 ... \\hat{y}_{t - 1}). The actions that are not sampled would not have got any credit, unless for the value penalty the we apply to them. That's why value penalty is crucial: it keeps the values of the actions that aren't sampled under control.\n\nYou asked if this results in sample inefficiency. I would not call this sample inefficiency, because we get rewards only for the sampled sequences and we use them the best we can.\n\nYou also asked how well policy is able to explore all actions. This is what we need the log-likelihood pretraining for: it gives us a reasonable exploration strategy. But I can imagine that it would be beneficial to sample once in a while random ground-truth token to improve the exploration. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1484934838389, "tcdate": 1484934822580, "number": 14, "id": "r113A61wx", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Clarification on the training method of Critic", "comment": "Hi Dzmitry,\n\nI'm trying to replicate the model. I suppose the critic is a normal seq2seq model that outputs a score for every vocabulary at each time step (the output size would be [batch_size, time, vocab_size]), which makes sense in the step 5 of Algorithm 1 to multiply with the output of policy (probability distribution over all vocabs).\n\nHowever, q_t target value we computed is a single value for each timestep. How is it used with Q(y^hat_t) as a loss? Is Q(y^hat_t) simply the score critic computed for a specific y_hat (and it is what the policy generated at that timestep)?\n\nIf so, is it a loss function that only slices a single dimension of the output, compare it to computed q_value, and train the network? Does this result in sample inefficiency, and how well policy is able to \"explore\" all actions so that all dimensions of critic can be updated?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1484324781403, "tcdate": 1484324781403, "number": 12, "id": "BkS3Jt8Ux", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["~Dzmitry_Bahdanau1"], "readers": ["everyone"], "writers": ["~Dzmitry_Bahdanau1"], "content": {"title": "Rebuttal", "comment": "Dear reviewers, please find our formal rebuttal below. \n\nWe thank all the reviewers for their valuable comments. We will start by addressing the concerns about the experimental evaluation of the proposed methods by reporting additional results:\n\n1) We found that combining actor-critic (AC) training with log-likelihood (LL) training by considering a mixture of the two objectives (AC+LL) can be helpful. This is aligned with the previous work by [Ranzato et al], [Wiseman et al] and [Shen at al]. The methods proposed in all these papers have the aspect of continuing to increase the log-probability (or the score) of the ground-truth sequence throughout the training. We also consider combining our version of REINFORCE (RF-C) with log-likelihood training (RF-C+LL).\n\nWith AC+LL training we obtained 27.49 (28.53 with beam search) BLEU points on the TED dataset with a bidirectional GRU encoder, which constitutes a 1.7 (1.0) BLEU improvement over LL training. This is comparable to the 1.3 (1.6) BLEU improvement reported for a similar model in [Wiseman et al]. Note, that our baseline is 3.5 BLEU stronger. RF-C+LL worked on par. For the model with a convolutional encoder that we considered originally and that is similar to [Ranzato et al] AC+LL training did not improve upon AC training, seemingly because it increased the overfitting.\n\nOn our toy spelling correction task both AC+LL and RF-C+LL improve upon AC and RF-C. The gap between AC+LL and RF-C+LL is less than between AC and RF-C (now it is 0.4, 0.2, 0.3, 0.4 CER respectively) but is consistent over the four settings.\n\n2) We did additional experiments on a large (7 million sentences) WMT14 En -> Fr dataset, a very popular data for neural MT. AC+LL brought a 1.4 (0.3 with beam search) improvement over LL training, with the final performance being 30.7 (31.0). These new results can be compared with Minimum Risk Training (MRT) by [Shen et al], that reportedly improves the beam search performance of a similar model from 29.9 to 31.3. We note that our baseline is again stronger (30.7 vs 29.9) and that we use 1 sample per-example, whereas in [Shen et al] 100 samples are used. We also note that RF-C and RF-C+LL performed much worse than AC and AC+LL on this dataset.\n\nWe argue that these updated experimental results, when combined with the results already in the paper, clearly show the feasibility of our approach. Since the proposed method is novel, it is likely that our results can be even further improved. We believe the idea of the paper and the experimental results make a strong case for acceptance, and should be shared with ICLR attendees.\n\nFinally, we also edited the paper to incorporate the feedback of the reviewers. We have added an explicit description of REINFORCE to Section 3, along with a brief discussion of the bias-variance trade-off in gradient estimation, results of an ablation study and some other edits for improved clarity. We also split the experimental section into parts as Reviewer 3 suggested. Needless to say, the new experimental results will be added to the paper if it is accepted.\n\nReferences:\nRanzato et al, \u201cSequence Level Training with Recurrent Neural Networks\u201d, ICLR 2015\nWiseman et al, \u201cSequence-to-Sequence Learning as Beam-Search Optimization\u201d, EMNLP 2016\nShen et al, \u201cMinimum Risk Training for Neural Machine Translation\u201d, ACL 2016\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1482876861216, "tcdate": 1482876861216, "number": 11, "id": "HkrpvvxSg", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "HJ7hJloNl", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Re: results with bidirectional GRU encoder", "comment": "really impressive results !!! "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1482518444181, "tcdate": 1482518444181, "number": 10, "id": "HJ7hJloNl", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "SJVUqOFEx", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "results with bidirectional GRU encoder", "comment": "We report our results with a bidirectional GRU encoder.\n\n+--------------------+--------------------+-------------------+\n|       Model        | Greedy search BLEU |  Beam search BLEU |\n+--------------------+--------------------+-------------------+\n| Wiseman et al, LL  |              22.53 |             23.87 |\n| Wiseman et al, BSO |            23.83   |             25.48 |\n| this work, LL      |              25.82 |             27.56 |\n| this work, AC      |              27.27 |             27.75 |\n| this work, RF-C    |              27.42 |             27.75 |\n+--------------------+--------------------+-------------------\n\nwhere LL is log-likelihood training, BSO is beam-search optimization, AC is actor-critic, RF-C is reinforce-critic.\n\nOur baseline system is again much stronger than the baseline used in the prior work (3.5 BLEU). We double-checked that we use the same data processing and evaluation procedure as Wiseman et al.. \n\nAs for the performance of the methods proposed in the paper, one can see that both actor-critic and REINFORCE-critic significantly improve the performance with greedy search (1.4 and 1.6 BLEU respectively). This improvement is comparable to the improvement brought by BSO (1.3 BLEU). A log-likelihood trained model catches up when the beam search is used. \n\nThese results can be added to the paper if the reviewer deems it necessary.\n\nFinally, we would like to emphasize that the goal of this paper is not to set a new state-of-the-art on a particular dataset. What we are aiming for is to propose new ideas of how actor-critic methods from RL can be applied for sequence prediction. Our experiments show that the proposed approaches are feasible. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1482422878127, "tcdate": 1482422859783, "number": 9, "id": "SJVUqOFEx", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "rkGO3vtNg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Re. bidirectional vs. convolutional", "comment": "I think the paper should at least implement as a baseline, the best structure to date, which is the recurrent encoder+attention here. Otherwise it is just so unclear what benefits the proposed model brings. As I said before, I do like the idea of this paper. I think it is interesting (and also necessary) to see whether LSTM+RL outperforms LSTM. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1482421389720, "tcdate": 1482419305979, "number": 8, "id": "rkGO3vtNg", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "Hk_VSHYEe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "bidirectional vs. convolutional", "comment": "Please note, that in the paper by Wiseman et al, mentioned above, a bidirectional recurrent network is used as an encoder. For the purpose of making our results more comparable to the most relevant prior work by Ranzato et al., we conducted our machine translation experiments with a convolutional encoder (as mentioned in the first sentence, second paragraph, Section 5.2). The results with models of very different capacity and expressivity should not be compared directly. We did some preliminary experiments with bidirectional recurrent encoders as well and will report the results soon. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1482409263676, "tcdate": 1482409263676, "number": 3, "id": "Hk_VSHYEe", "invitation": "ICLR.cc/2017/conference/-/paper48/official/comment", "forum": "SJDaqqveg", "replyto": "rJG15ouEl", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer3"], "content": {"title": "baselines", "comment": "in light of this fact, I have lower my score one point, as this is even worst than I thought. Indeed it is not clear whether RL is the way to go at all. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750797, "id": "ICLR.cc/2017/conference/-/paper48/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750797}}}, {"tddate": null, "tmdate": 1482409194041, "tcdate": 1481969108782, "number": 3, "id": "Sk6CTFMNe", "invitation": "ICLR.cc/2017/conference/-/paper48/official/review", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer3"], "content": {"title": "In summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes to use an actor-critic RL technique to train sequence to sequence tasks in natural language processing. \nIn particular, experiments are shown in a synthetic denoising task as well as in machine translation. \n\nI like the idea of the paper, however, the experimental evaluation is not convincing. Why is the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different?\nIf one extrapolates the numbers, it seems that MIXER will be much better than the proposed approach. I'd like to see a head-to-head comparison, either by reproducing the same setting or by running the mixer baseline.\n\nThe authors should also compare their results to the state-of-the-art. How good is their machine translation system? Only comparing to a single baseline and without reproducing the numbers is not sufficient. \n\nWhile the idea makes sense, the authors needed to use many heuristics to make the model to work, e.g., using a delayed actor, update \\phi' with interpolation, penalize the variance, reducing the value of rare actions, etc. \nFurthermore, there is no in depth analysis of how much performance each of these heuristics brings. \nIt seems that the authors need more work to make the model work without so many heuristics.\n\nThe authors also mentioned several optimization difficulties (some of which are non-intuitive), \n1) why does the critic assign very high value to actions with very low probability according to the actor?\n2) why is a lower square error on Q resulting in much worst performance?\n\nThe paper will benefit from a serious re-write. The technical part is not clearly written. The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text. This will help the reader understand how to use the critic within this framework.\nAlso the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. \n\nThe paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this?\n\nThe text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. \n\nI'll revise my score if the authors address my questions.\n\nIn summary, an interesting idea, however many heuristics are used and the experimental evaluation is not sufficient.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512715678, "id": "ICLR.cc/2017/conference/-/paper48/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper48/AnonReviewer2", "ICLR.cc/2017/conference/paper48/AnonReviewer1", "ICLR.cc/2017/conference/paper48/AnonReviewer3"], "reply": {"forum": "SJDaqqveg", "replyto": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512715678}}}, {"tddate": null, "tmdate": 1482369596698, "tcdate": 1482369497775, "number": 7, "id": "rJG15ouEl", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "comparing with \"Sequence-to-Sequence Learning as Beam-Search Optimization\"", "comment": "I generally like the idea of this paper. But I feel the experimental evaluation is way off standards. An LSTM model with a simple attention mechanism (BLEU 23.87)(https://arxiv.org/pdf/1606.02960v2.pdf) beats all results by RL based models on the same benchmark by a large margin .  The best result presented in the aforementioned paper achieves a huge boost of +3.8 BLEU than the best-reported score in the paper. This even makes me wonder whether RL is a good direction to explore in MT. \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1482275971615, "tcdate": 1482275971615, "number": 6, "id": "Bk2KhVvVx", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "Sk6CTFMNe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Thank you for feedback, the paper is updated", "comment": "We thank the reviewer for their valuable feedback!\n\nThe reviewer has asked a number of important questions, and we will try to answer many of them here. Furthermore, we addressed the concerns regarding the structure of the paper and uploaded a new version.\n\nQ: Why are the LL numbers in Ranzato et al. 2015 and your paper so different? Is the metric different? is it the scheduler? are the parameters different?\nA: Our intention was to build a system similar to the one used in Ranzato et al. 2015. The architecture, the number of units, the metric - all these are the same. The differences include (a) GRU instead of LSTM (b) ADAM vs SGD as a training algorithm. There might be others as their implementation is using another framework. We think that it would not be right to try and down-clock our system\u2019s performance to match theirs.\n\nQ: The authors should also compare their results to the state-of-the-art. \nA: The rapid progress in the field of neural machine translation has made it infeasible to compare to SOTA systems. Such models are typically trained using abundant computational resources of large companies like Baidu and Google (see e.g. https://arxiv.org/pdf/1609.08144v2.pdf, a model is trained using 12 replicas with 8 GPU each). Our primary aim is to propose innovative algorithms, and provide proof-of-concept.\n\nQ: It seems that the authors need more work to make the model work without so many heuristics.\nA: We fully agree that it would be nice to find a way to apply deep RL methods with less heuristics. However, as of now, most papers using deep RL techniques rely on them. Please note, that we borrowed almost all our tricks from an actor-critic paper by Lillicrap et al. 2016 (https://arxiv.org/abs/1509.02971) published at ICLR 2016. The only additional trick that we used was the value penalty C from Equation 9, which the reviewer refers to as \u201cpenalize the variance, reducing the value of rare actions\u201d.\n\nQ: Furthermore, there is no in depth analysis of how much performance each of these heuristics brings. \nA: We report in the last paragraph of Section 5 which of the heuristics were strongly necessary for the training to work. Furthermore, we have added results of an ablation study to Appendix A.\n\nQ: Why does the critic assign very high value to actions with very low probability according to the actor?\nA: The action space contains 28,222 words. Many of these words are rarely sampled and thus their Q-value estimates only rarely enter in the Bellman error. Such Q-estimates have high variance because of that, and since there is a lot of such Q-estimates, some of them end up being highly overestimated, unless a value penalty from Equation 9 is applied.\n\nQ: Why is a lower square error on Q resulting in much worst performance?\nA: The low squared temporal difference error doesn\u2019t guarantee good performance. If all Q-estimates are at 0, the squared error can be low. But, it is minimizing the square error *with the right-hand side fixed* that makes it possible to train a useful, even though not perfectly accurate, Q-function approximation. This is what distinguishes temporal difference learning from Monte-Carlo methods, and in particular when a nonlinear function estimator is used, there\u2019s no guarantee that the square error will go down during training.\n\nQ: The manuscript also assumes that the reader knows algorithms such as REINFORCE. I strongly suggest to include a brief description in the text.\nA: We have added an explicit description of REINFORCE to Section 3, along with a brief discussion of the bias-variance trade-off in gradient estimation. \n\nQ: Also the experimental section will benefit from dividing it by experiment. Right now is cumbersome to look at the details of each experiment as things are mixed up in the text. \nA: We thank the reviewer, and we have followed this recommendation.\n\nQ: The paper will benefit from a serious re-write. The technical part is not clearly written.\nA: We are happy to incorporate changes into the writing if the reviewer has specific concerns (in addition to the two already addressed).\n\nQ: The paper criticizes the REINFORCE algorithm a lot, particularly for its high variance, however the best results in the real setting are achieved with this algorithm (+ the critic). How do you explain this?\nA: We have already received a similar question from Reviewer 1, please find a detailed answer in the respective pre-review discussion.\n\nQ: The text is also not consistent with what the results show. The discussion claims that using the critic on REINFORCE reduces the gap with the actor critic. However, it is better than the proposed approach. \nA: Consider the training curves reported in Figure 2. Here we can see that REINFORCE strongly underfits on the training data, especially with a linear baseline, and less with a novel baseline that we propose. Actor-Critic training fits the training data faster, thus causing overfitting. However, we agree that the wording was ambiguous and changed it to \u201creduce the gap in the training speed and achieve a better test error\u201c.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1481930076933, "tcdate": 1481930076933, "number": 2, "id": "ryrvSlGVx", "invitation": "ICLR.cc/2017/conference/-/paper48/official/review", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer1"], "content": {"title": "novel approach, well motivated, results sufficient", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper introduces an actor-critic approach for sequence prediction, and shows experiments on spelling correction and machine translation.  While previous works e.g. Ranzato et al. 2015 have used an RL-based approach such as REINFORCE for sequence prediction, the main contribution of this work is the use of actor-critic as a novel approach for how to determine the target of network predictions, given the setting that the network should be trained to generate correctly given outputs already produced by the model and not ground-truth reference outputs.  Specifically, the actor is the main prediction network and the critic is trained to output the value of specific tokens.\n\nThe motivations for the approach are well-presented, and while a somewhat natural extension, it is still novel and justified. There are a number of details that are necessary for successful training, that are discussed well.  While the full Actor-Critic model does not show strong improvements over REINFORCE with critic, the critic-based models still outperform other baselines.  It would be nice to include more discussion of the bias-variance tradeoff and future advantages of Actor-Critic (from the pre-review question response) in the paper.  The paper is solid and deserves acceptance", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512715678, "id": "ICLR.cc/2017/conference/-/paper48/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper48/AnonReviewer2", "ICLR.cc/2017/conference/paper48/AnonReviewer1", "ICLR.cc/2017/conference/paper48/AnonReviewer3"], "reply": {"forum": "SJDaqqveg", "replyto": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512715678}}}, {"tddate": null, "tmdate": 1481899069698, "tcdate": 1481899069698, "number": 1, "id": "SyLHhObEg", "invitation": "ICLR.cc/2017/conference/-/paper48/official/review", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer2"], "content": {"title": "Review", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The paper presents a nice application of actor-critic method for conditional sequence prediction. The critic is trained conditional to target sequence output, while the actor is conditional on input sequence. The paper presents a number of interesting design decisions in order to tackle non-standard RL problem with actor-critic (conditional sequence generation with sequence-level reward function, large action space, reward at final step) and shows encouraging results for applying RL in sequence prediction. \n\nThe interaction of actor and critic is an interesting aspect of this paper. Each has different pieces of information (input sequence, target output sequence), and effectively the actor gets target label information only through greedy optimization of the critic. Letting the critic having access to information only available at train time is interesting and may be applicable to other applications that tie RL with supervised learning. Pre-review discussion on Q-learning vs actor-critic has been good, and indeed I agree that making the critic having access to structured output label may be quite useful. \n\nThe pros include reasonable improvement over prior attempts at using RL to fine-tune sequence models. One possible con is that the actor-critic is likely more unstable than simpler prior methods, thus requiring a number of tricks to alleviate, and it would be nice to see discussion on stability and hyper-parameter sensitivity. Another possible con is that this is an application paper, but it explores a non-traditional approach in a widely applicable field. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512715678, "id": "ICLR.cc/2017/conference/-/paper48/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper48/AnonReviewer2", "ICLR.cc/2017/conference/paper48/AnonReviewer1", "ICLR.cc/2017/conference/paper48/AnonReviewer3"], "reply": {"forum": "SJDaqqveg", "replyto": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512715678}}}, {"tddate": null, "tmdate": 1481885547582, "tcdate": 1481885547582, "number": 3, "content": {"title": "N/A", "question": "N/A"}, "id": "HJEdvBW4l", "invitation": "ICLR.cc/2017/conference/-/paper48/pre-review/question", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer3"], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481885548362, "id": "ICLR.cc/2017/conference/-/paper48/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper48/AnonReviewer2", "ICLR.cc/2017/conference/paper48/AnonReviewer1", "ICLR.cc/2017/conference/paper48/AnonReviewer3"], "reply": {"forum": "SJDaqqveg", "replyto": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481885548362}}}, {"tddate": null, "tmdate": 1481597490128, "tcdate": 1481597490120, "number": 5, "id": "BJ94f1TQl", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "ryqSZTimg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Using Y is important", "comment": "Yes, in theory it is possible to train a Q-network conditioned only on X. However, we doubt that this would work. Even though Y can be thought as \"contained in X\", the mapping from X to Y is a complex one, and learning this mapping is the original problem we are trying to solve. Without knowing Y the Q-network would have a hard time to predict the values of actions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1481523521646, "tcdate": 1481523521642, "number": 2, "id": "ryqSZTimg", "invitation": "ICLR.cc/2017/conference/-/paper48/official/comment", "forum": "SJDaqqveg", "replyto": "SJXnKAfQe", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer2"], "content": {"title": "Q-learning", "comment": "If we consider Y information to be contained in X, then is it possible to train Q condition on X with Q-learning? This may not be ideal since there is no explicit conditioning on Y, which provides valuable information. \n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750797, "id": "ICLR.cc/2017/conference/-/paper48/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750797}}}, {"tddate": null, "tmdate": 1481235050134, "tcdate": 1481235050129, "number": 4, "id": "Hyzd5UPQl", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "HJfn0l87e", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Thanks for your question", "comment": "The critic network in our work is an Encoder-Decoder with an architecture similar to the one of the critic. It differs in what its inputs and outputs are. The critic's inputs are the ground-truth translation Y, which is fed to the encoder, and the prediction \\hat{Y}, which is consumed by the decoder. The decoder RNN also outputs the Q-values. The encoder and the decoder are connected by an attention mechanism.\n\nThe relevant information is also provided in the first paragraph of Section 3 and Figure 1."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1481146026447, "tcdate": 1481146026441, "number": 3, "id": "HJfn0l87e", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Ground-truth input to Critic Network", "comment": "It appears to be a great work bridging reinforcement learning and Seq2Seq model!\nIn a normal actor-critic setting, or in DDPG, Q network takes in an action(next output)-observation(current output) pair as input, and then calculate the Q value, can you elaborate on how the critic network is able to take in the ground truth Y? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1480939947263, "tcdate": 1480939947258, "number": 2, "id": "SJXnKAfQe", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "HJdvy_Jme", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Nice idea!", "comment": "Thank you for your suggestion!\n\nIn general, we like your idea that Q-learning could be used to train deterministic sequence predictors. The particular approach that you propose could not be applied though, because our critic uses the ground-truth output Y, which is not available at the test time. One could imagine a training procedure with a deterministic actor and a critic evaluates its Q-values by accessing the ground-truth. We have not tried such an approach yet. Exploration is one potential issue for approaches of this kind."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1480939466172, "tcdate": 1480939445180, "number": 1, "id": "HkT3wRMme", "invitation": "ICLR.cc/2017/conference/-/paper48/public/comment", "forum": "SJDaqqveg", "replyto": "Syo2uRxQe", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Bias-variance trade-off", "comment": "Thank you for your question.\n\nIn general, Actor-Critic and REINFORCE are on the opposite ends of the bias-variance trade-off spectrum. REINFORCE is unbiased but has very high variance, as we discuss in Section 3. Actor-Critic has much lower variance, but is biased because a biased parametric estimate of the expected return is used instead of the sum of future rewards. What method works better in practice depends on the success of the particular variance/bias reduction scheme used, which is our answer to your question of why in principle REINFORCE can perform on-par with Actor-Critic. Another factor is that on a small dataset the larger variance of REINFORCE can strongly regularize the model, as one can clearly see from Figure 2.\n\nIn this work we propose methods for reducing the bias of the Actor-Critic estimate for sequence prediction task, such as the particular architecture of the critic and the use of a penalty for the values of rare words. In all but one setups we show that Actor-Critic is better at fitting the model on the training data. It is true that REINFORCE-critic has a better test set performance in the machine translation task, but as we said before, we believe this is because training by REINFORCE introduces a lot of noise. A potential future advantage of Actor-Critic methods is that further progress in the estimation of the value function will directly translate into improved performance of the actor, whereas REINFORCE by design has fundamental limitations that (a) only the sampled action is considered (b) rewards for the later actions contribute as much as the reward for the current action.\n\nFinally, we would like to note that using a critic that has access to the groundtruth as a baseline for REINFORCE is also a novelty of this work (although a similar technique has concurrently been proposed in https://arxiv.org/abs/1606.01541 in a different context).\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287750926, "id": "ICLR.cc/2017/conference/-/paper48/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SJDaqqveg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper48/reviewers", "ICLR.cc/2017/conference/paper48/areachairs"], "cdate": 1485287750926}}}, {"tddate": null, "tmdate": 1480808627423, "tcdate": 1480808627419, "number": 2, "id": "Syo2uRxQe", "invitation": "ICLR.cc/2017/conference/-/paper48/pre-review/question", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer1"], "content": {"title": "actor-critic vs. REINFORCE with critic", "question": "In the experiments, REINFORCE with critic performs in many cases comparable to or better than Actor-Critic. Can you provide more insight into why this may be the case, and where Actor-Critic has advantages?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481885548362, "id": "ICLR.cc/2017/conference/-/paper48/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper48/AnonReviewer2", "ICLR.cc/2017/conference/paper48/AnonReviewer1", "ICLR.cc/2017/conference/paper48/AnonReviewer3"], "reply": {"forum": "SJDaqqveg", "replyto": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481885548362}}}, {"tddate": null, "tmdate": 1480716127585, "tcdate": 1480716127581, "number": 1, "id": "HJdvy_Jme", "invitation": "ICLR.cc/2017/conference/-/paper48/pre-review/question", "forum": "SJDaqqveg", "replyto": "SJDaqqveg", "signatures": ["ICLR.cc/2017/conference/paper48/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper48/AnonReviewer2"], "content": {"title": "actor-critic vs Q-learning", "question": "It's a nice paper that bridges actor-critic methods and sequence prediction tasks. The results are encouraging. Have you tried pre-training the critic as you do, and then doing Q-learning instead of actor-critic to train the critic only? The critic directly provides policy, either argmax or softmax if entropy-regularized. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a textit{critic} network that is trained to predict the value of an output token, given the policy of an textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling. ", "pdf": "/pdf/35163ec9e0a4092ca39ad729b83880b89f7fd33f.pdf", "TL;DR": "Adapting Actor-Critic methods from reinforcement learning to structured prediction", "paperhash": "bahdanau|an_actorcritic_algorithm_for_sequence_prediction", "keywords": ["Natural language processing", "Deep learning", "Reinforcement Learning", "Structured prediction"], "conflicts": ["umontreal.ca", "google.com", "mcgill.ca"], "authors": ["Dzmitry Bahdanau", "Philemon Brakel", "Kelvin Xu", "Anirudh Goyal", "Ryan Lowe", "Joelle Pineau", "Aaron Courville", "Yoshua Bengio"], "authorids": ["dimabgv@gmail.com", "pbpop3@gmail.com", "iamkelvinxu@gmail.com", "anirudhgoyal9119@gmail.com", "lowe.ryan.t@gmail.com", "jpineau@cs.mcgill.ca", "aaron.courville@gmail.com", "yoshua.bengio@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481885548362, "id": "ICLR.cc/2017/conference/-/paper48/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper48/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper48/AnonReviewer2", "ICLR.cc/2017/conference/paper48/AnonReviewer1", "ICLR.cc/2017/conference/paper48/AnonReviewer3"], "reply": {"forum": "SJDaqqveg", "replyto": "SJDaqqveg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper48/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481885548362}}}], "count": 24}