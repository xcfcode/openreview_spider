{"notes": [{"id": "Uh0T_Q0pg7r", "original": "SSsvSRB2MqJ", "number": 2484, "cdate": 1601308274565, "ddate": null, "tcdate": 1601308274565, "tmdate": 1614985768577, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8ZZnloLfoSv", "original": null, "number": 1, "cdate": 1610040364350, "ddate": null, "tcdate": 1610040364350, "tmdate": 1610473954758, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "invitation": "ICLR.cc/2021/Conference/Paper2484/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes an approach for active learning in CNNs. The method computes the expected reduction in the predictive variance across a representative set of points and selects the next data point to be queried from the same set. \n\nPros:\n- The method is rather simple and seems practical. \n- The paper is generally well-written.\n\nCons:\n- The novelty of the paper is limited, as it essentially applies a known approach to CNNs.\n- The performance gains presented in experiments seem rather mild, and may not justify using this method."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040364336, "tmdate": 1610473954740, "id": "ICLR.cc/2021/Conference/Paper2484/-/Decision"}}}, {"id": "KpLW8ytLb26", "original": null, "number": 3, "cdate": 1603896835957, "ddate": null, "tcdate": 1603896835957, "tmdate": 1606808267971, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "invitation": "ICLR.cc/2021/Conference/Paper2484/-/Official_Review", "content": {"title": "The paper combines dropout-based variational inference and integrated variance minimization", "review": "The paper proposes a method for pool-based active learning in CNNs, selecting the next (batch of) data from an unlabeled pool to query their labels to expedite the learning process. The method computes the expected reduction in the predictive variance across a representative set of points and selects the next data point to be queried from the same set. In batch settings, the data points are sequentially selected in a batch (in a greedy way), with predictive variance representation updated after each selection. Experiments are performed on MNIST classification, and regression tasks of alternative splicing prediction, and face age prediction.\n\nPros:\n\nThe method is rather simple and (up to some extent) computationally efficient. The paper is generally well-written and puts the proposed method into context. The proposed method performs better than maximum variance acquisition and random acquisition in the regression experiments. For the classification tasks, its performance is comparable with maximum entropy, batch BALD and robust k-Center acquisition.\n\nConcerns:\n\nOverall, the novelty of the paper is limited. It employs dropout-based variational inference and applies the integrated variance minimization idea. The main contribution seems to be considering a joint normal distribution (whose level of validity is not completely clear/discussed). It is not clear if the proposed acquisition is the statistically optimal one that minimizes the expected MSE. It seems to me that requires taking into account the distribution of the (unobserved) label. Also, assuming all predicted class probabilities across all sample points as jointly Gaussian is strange as the probabilities are bounded and should sum to 1. In fact, the performance comparison in the classification setup is limited to one dataset and the results do not seem to confirm that the proposed method outperforms the baselines. \n\nFigure 2 seems to suggest that with increasing batch size, the performance gap shrinks. Have the authors tried a larger batch size or checked the sensitivity to it? \n\nAdditionally, how sensitive is the performance to the number of randomly sampled data points for the representative set?\n\nMy current rating is based on the aforementioned concerns.\n\nUpdate:\nI thank the authors for their response. I read the authors\u2019 responses and the updated paper. As the authors have addressed some of my concerns and questions, I have adjusted my rating. However, I still have my concerns regarding the novelty and methodological contribution, and the classification setup. I also agree with other reviewers that the presentation can be improved.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2484/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095327, "tmdate": 1606915762451, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2484/-/Official_Review"}}}, {"id": "bYtyEMBBvhR", "original": null, "number": 5, "cdate": 1606271238044, "ddate": null, "tcdate": 1606271238044, "tmdate": 1606295474491, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": "QEOdILF63ia", "invitation": "ICLR.cc/2021/Conference/Paper2484/-/Official_Comment", "content": {"title": "Authors' Response to Reviewer 1", "comment": "We thank the reviewer for their comments.\n\nRegarding our contribution: Our main contribution is not deriving the EI; as the reviewer observes, EI-based algorithms have been formulated for Bayesian optimization. We view our main contribution as extending integrated variance approaches, which attempt to construct statistically optimal active learning queries, to CNNs. \n\nIn response to the major concern: We believe that the reviewer may have misunderstood our definition of expected improvement (see Eq 5 and Eq 6 and the paragraph following each of these two equations). The notion of expected improvement used in our paper is different from that used in (Jones et al 1998) and, consequently $E(y_{train} | x_{new})$ does not appear in our objective function. As stated in the paper, we define expected improvement as the reduction in predictive variance across a large, representative sample of points that is expected to result from querying x_{new} under our assumptions. Section 3.1 of the paper shows that the $x_{new}$ maximizing expected improvement (as we define it) is the same $x_{new}$ that minimizes expected prediction error after it is queried. Since our definition of the expected improvement involves the variance in the prediction of Y rather than the value of Y itself, the expected value of Y does not come into play.\n\nIn response to concerns that the improvement is not significant compared to the baselines: Multiple experiments presented in the paper show that the proposed algorithm outperforms the baselines by a statistically significant margin. All p-values comparing performance of our active learning method to the baselines in the regression setting are below 0.01. The proposed algorithm has outperformed random acquisition and maximum variance acquisition by a statistically significant margin in all active learning experiments we have conducted for regression tasks. Additionally Table 2 shows that the proposed approach performs better than all baselines, including state-of-the-art methods such as core-set active learning, in MNIST 7 vs. 9 classification with batch size 1. Although the proposed approach does not exhibit statistically significant improvement over all baselines in MNIST 0-9 classification for batch size 100, it is on par with top-performing existing methods. In MNIST 0-9 classification with batch size 25, the proposed method outperforms random and maximum entropy acquisition ($p<0.01$, Figure 3(b)).\n\nIn response to the minor concern: We believe the reviewer may have misunderstood the proposed active learning approach. Our method does, importantly, utilize the full joint covariance of model predictions across the representative sample of points. It is correct that Eq 2 states that the point acquired minimizes the point-wise variance such that it minimizes expected squared prediction error. However, the way in which we choose $x^* $ and estimate the right hand side of Eq 2 involves the covariance between model predictions, as is discussed in Sections 3.3 and 3.4. We believe that acquiring points based on covariance between model predictions is an important advantage of our method. For instance, uncertainty sampling methods typically query uncertain points, but no claim can be made about the resulting improvement in model predictions. On the other hand, our formulation utilizes the joint covariance of model predictions across a large sample of points to query the point that, accounting for correlations between model predictions across points, minimizes expected prediction error upon being queried.\n\nThank you for pointing us to the paper on Predictive Variance Reduction Search; it is certainly relevant and we will further revise the paper to discuss it."}, "signatures": ["ICLR.cc/2021/Conference/Paper2484/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Uh0T_Q0pg7r", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2484/Authors|ICLR.cc/2021/Conference/Paper2484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847862, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2484/-/Official_Comment"}}}, {"id": "Be3amOnWzDx", "original": null, "number": 4, "cdate": 1606270929326, "ddate": null, "tcdate": 1606270929326, "tmdate": 1606295021242, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": "VTPEarWmtRf", "invitation": "ICLR.cc/2021/Conference/Paper2484/-/Official_Comment", "content": {"title": "Authors' Response to Reviewer 3", "comment": "We thank the reviewer for their thoughtful feedback.\n\nThank you for the feedback regarding experiments in which the proposed algorithm does not significantly outperform random acquisition. Figure 2, which was mentioned in your feedback, in fact shows that in the regression setting the proposed method does consistently outperform random acquisition by a statistically significant margin. However, MNIST 0-9 classification with batch size 100 is a task where many active learning methods, including the proposed approach, do not outperform random acquisition significantly. We have replaced the MNIST 0-9 classification results for batch size 100 with results from MNIST 0-9 classification with batch size 25 (Figure 3(b)), in which our method outperforms maximum entropy acquisition and random acquisition ($p<0.01$). We will revise Figure 3(b) to include the performance of robust k-Center acquisition.\n\nThe procedure by which the representative sample of points is chosen in our experiments is detailed in Appendix D. The representative sample of points is simply a random sample of specified size from $X_{train} \\cup X_{pool}$. We will incorporate these details into the main paper to make the discussion of our experiments clearer.\n\nIt is true that greedy batch selection methods often query correlated points, which results in reduced performance. Although our batch-mode active learning algorithm is a greedy algorithm, querying batches of diverse points, via iterative updates to the predictive covariance, is one of its key advantages. The proposed batch-mode active learning algorithm sequentially assembles a batch by adding the point that is expected to maximally reduce predictive variance conditioned on all existing points in the batch. Therefore, we explicitly construct batches such that each new point added to a batch is expected to maximally reduce the predictive uncertainty that remains after querying the existing points in the batch. We have found empirically that the points queried by the proposed method are diverse even in batch-mode active learning with moderately large batch sizes (see, for instance, Figure 3(c)). In order to further analyze the proposed method\u2019s effectiveness in assembling diverse batches of points, we have run new experiments on MNIST 7 vs. 9 and 0-9 classification (batch size 25) analyzing batch diversity as measured by the average symmetric KL divergence between the predicted class probability distributions of pairs of images in the queried batch. In both classification tasks, we found that EI acquisition has a far higher batch diversity than alternative methods including Batchbald and maximum entropy acquisition ($p<0.01$) at virtually every stage in the active learning experiments. The paper has been updated to include discussion of the results from these batch diversity experiments (Figure 3(d), 3(e); Section 5.3). We will further revise the paper to include core-set AL in our batch diversity analysis.\n\nMC dropout is used for uncertainty estimation because (i) it is computationally efficient compared to ensemble methods (ii) it enables estimation of covariance between different model predictions as opposed to simply estimating the point-wise variance and (iii) can be used even for pretrained models that used dropout.\n\nThank you for the feedback regarding the writing. We will revise the paper such that the definition of expected improvement is made explicit early on."}, "signatures": ["ICLR.cc/2021/Conference/Paper2484/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Uh0T_Q0pg7r", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2484/Authors|ICLR.cc/2021/Conference/Paper2484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847862, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2484/-/Official_Comment"}}}, {"id": "P3osBxODk3", "original": null, "number": 3, "cdate": 1606270687314, "ddate": null, "tcdate": 1606270687314, "tmdate": 1606293827780, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": "KpLW8ytLb26", "invitation": "ICLR.cc/2021/Conference/Paper2484/-/Official_Comment", "content": {"title": "Authors' Response to Reviewer 4", "comment": "We thank the reviewer for their constructive feedback.\n\nThe main contribution of our work is extending integrated variance-based approaches to active learning to CNNs. Previously, integrated variance-based approaches have typically been applied to Gaussian processes (GP) rather than neural networks. The joint Gaussian assumption can be motivated as approximating the CNN by a GP thereby enabling tractable estimation of the variance of a large, representative sample of points conditioned on an unlabeled data point.\n\nUnder the assumptions stated in the paper, the proposed acquisition function is the statistically optimal one, in that the queried point minimizes expected MSE conditioned on that point. This is shown in Section 3.1. The distribution of the unobserved label does not affect the validity of the argument presented in Section 3.1 because the distribution of the unobserved label only affects the second and third terms of the expansion in Eq (1), both of which are constant in $x_{new}$.\n\nThe proposed approach is motivated by the regression setting, for which relatively few active learning algorithms have been proposed for neural networks. The presentation of the classification setting along with empirical results is intended to demonstrate that the method extends to classification, although that is not our main contribution. You are correct in noting that the joint Gaussian assumption in the classification setting is an approximation given that probabilities are bounded. As stated in the paper, we investigated alternative formulations of the proposed approach in classification tasks such that the joint Gaussian assumption is applied to unbounded quantities such as logits. However, our empirical results demonstrated that our method in classification was most effective when applied to probabilities in spite of the apparent limitations of this approximation. We address the approximate nature of the joint Gaussian assumption in classification in the third paragraph of Section 3.4 of the paper.\n\nWe have observed that our method performs well compared to the baselines for batch sizes of up to several hundred points. The method tends to outperform baselines by a lesser extent for batch sizes greater than several hundred points. However, we believe that the strong performance of the proposed method for batch sizes up to several hundred points is sufficient for it to be useful in a variety of applications. \n\nWe have found empirically that performance is not very sensitive to the number of points in the representative set. We have run new experiments on MNIST 7 vs. 9 binary classification in order to gauge the sensitivity of the proposed method to the representative sample size. Hypothesis tests based on linear mixed models for batch size 25 found no statistically significant difference in the active learning performance of our method run with representative sample sizes of 500, 2500, and 5000 points (and for all representative sample sizes, our method significantly outperformed random and robust k-Center acquisition). These results have been added to the paper (Figure 3(f))."}, "signatures": ["ICLR.cc/2021/Conference/Paper2484/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Uh0T_Q0pg7r", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2484/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2484/Authors|ICLR.cc/2021/Conference/Paper2484/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923847862, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2484/-/Official_Comment"}}}, {"id": "QEOdILF63ia", "original": null, "number": 1, "cdate": 1603751433315, "ddate": null, "tcdate": 1603751433315, "tmdate": 1605024200655, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "invitation": "ICLR.cc/2021/Conference/Paper2484/-/Official_Review", "content": {"title": "Official Blind Review #1", "review": "The paper considers the active leaning problem by proposing a new acquisition function based on Expected improvement (EI). The paper shows that acquiring the point maximizing the expected reduction in predictive uncertainty across all points is equivalent to maximizing the expected improvement (EI).\n\n\nThe paper considers the setting for CNN classification and regression where the authors derive the variance estimations. The paper also extends to the batch mode to select multiple points simultaneously. The uncertainty is estimated using MC dropout.\n\nThe papers show that the improvement in the experiment is not significant comparing to the baselines.\n\n=============================\nMajor concern: although the paper claims to derive the EI-based acquisition function for active learning. The resulting derivation shows that the final acquisition function only depends on the uncertainty of the training data given the select point (x_new) while the relative contribution of the x_new toward the final performance E(y_train | x_new) has been vanished. Note that in the original form of the EI (Jones et al 1998), the acquisition function does not only depend on the uncertainty, but also the expected function value at the x_new.\n\nThe reviewer thinks that it may be better for this paper to position the contribution (abstract/introduction\u2026) as the uncertainty based approach for active learning, then claim the minor/secondary contribution as showing the connection to the EI for the active learning setting. \n\nAt the current form, the reviewer thinks the paper is under the acceptance threshold.\n\n\nMinor concern:\n\nThe paper claims that they utilize full joint covariance rather than just point-wise variances used in previous work. However, the reviewer thinks this may be a wrong claim. In particular, the trace of variance considered in Eq 2 is equivalent to point-wise variance.\n\n\n\nOther comments:\n\nThe intuition in the resulting acquisition function is very much related to the Predictive Variance Reduction Search [1] \u2013 it is worth mentioning. Both approaches rely on:\n(1) uncertainty estimation of the some targets;\n(2) the objective function is defined using the sum of the uncertainty reduction \u2013 the trace of variance in Eq (2) and the original form in Eq (1) share this spirit. \n(3) similar in the batch setting where both approaches will sequentially select point to fill in a batch.\n\nThe difference is that [1] considers another problem in Bayes Opt while this paper considers the active learning task.\n\n\n\n[1]  Nguyen, V., Gupta, S., Rana, S., Thai, M., Li, C., & Venkatesh, S.  Efficient Bayesian Optimization for Uncertainty Reduction Over Perceived Optima Locations. In IEEE International Conference on Data Mining (ICDM) (pp. 1270-1275). 2019.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2484/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095327, "tmdate": 1606915762451, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2484/-/Official_Review"}}}, {"id": "VTPEarWmtRf", "original": null, "number": 2, "cdate": 1603836428572, "ddate": null, "tcdate": 1603836428572, "tmdate": 1605024200597, "tddate": null, "forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "invitation": "ICLR.cc/2021/Conference/Paper2484/-/Official_Review", "content": {"title": "Official Review #3", "review": "Paper Summary\n\nThe paper considers the problem of active learning for training convolutional neural networks (CNN) in a sample-efficient manner. The proposed approach is built upon the existing idea of selecting points that maximally reduce expected mean squared error (MSE) on a large representative sample of points. MC-dropout is used for obtaining the estimates of model uncertainty. This idea is used for active learning in regression and classification problems with CNNs. A greedy method is proposed to select a batch of points by maximizing the acquisition function score sequentially obtained by updating the covariance matrix on previous points selected in the batch. Experiments are performed on two regression and one classification task. \n\n\nDetailed Comments\n\n- The paper tackles an important problem relevant for many practical applications. Although the proposed approach is based on an existing idea, the application to CNNs specifically seems novel. \n\n- It is a little worrying to see that random acquisition performs equally well with the proposed acquisition function (Figure 2 for instance). Please provide more discussion on this point and/or run one experiment for more than 3 (current) random runs to get a clear picture. Random acquisition is a simple to implement method and if the gains by the proposed acquisition function are not large enough, random acquisition will be a preferable option for a user. \n\n- There is limited description about how is the ''representative sample chosen'' in the experimental section. Please provide more details on this. \n\n- Greedy batch selection methods are known to select similar points losing the batch advantage. Please provide more details on the diversity of the points in batch selection.  \n\n- Why MC-dropout specifically is used for uncertainty estimation? The proposed approach should ideally be agnostic to the method for uncertainty estimation. Please provide more discussion on this choice. \n\n- In my opinion, although it is a minor point, it is better to show the visualization of EI acquisition function on a standard synthetic function like a sinusoidal function, Branin function etc. instead of a random neural network which might seem a little contrived.\n\n- The writing of the paper can be improved. For example, the usage of the term 'Expected improvement' is spread out across the paper without a proper technical description. Please try to provide a clear technical description of all the key concepts\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2484/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2484/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Learning in CNNs via Expected Improvement Maximization", "authorids": ["~Udai_G._Nagpal1", "~David_A._Knowles1"], "authors": ["Udai G. Nagpal", "David A. Knowles"], "keywords": ["active learning", "batch-mode active learning", "deep learning", "convolutional neural networks", "supervised learning", "regression", "classification", "MC dropout", "computer vision", "computational biology"], "abstract": "Deep learning models such as Convolutional Neural Networks (CNNs) have demonstrated high levels of effectiveness in a variety of domains, including computer vision and more recently, computational biology. However, training effective models often requires assembling and/or labeling large datasets, which may be prohibitively time-consuming or costly. Pool-based active learning techniques have the potential to mitigate these issues, leveraging models trained on limited data to selectively query unlabeled data points from a pool in an attempt to expedite the learning process. Here we present \"Dropout-based Expected IMprOvementS\" (DEIMOS), a flexible and computationally-efficient approach to active learning that queries points that are expected to maximize the model's improvement across a representative sample of points. The proposed framework enables us to maintain a prediction covariance matrix capturing model uncertainty, and to dynamically update this matrix in order to generate diverse batches of points in the batch-mode setting. Our active learning results demonstrate that DEIMOS outperforms several existing baselines across multiple regression and classification tasks taken from computer vision and genomics.", "one-sentence_summary": "An efficient batch-mode active learning algorithm for CNNs is proposed based on acquisition of points expected to maximize the model\u2019s improvement upon being queried, and is found to perform well across regression and classification tasks.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "nagpal|active_learning_in_cnns_via_expected_improvement_maximization", "supplementary_material": "/attachment/a54dec0b11c386b92efadf1155e8ff24bea208bb.zip", "pdf": "/pdf/01719ebc0cd4b20dde88a2b7c7e3fb33ef812923.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=PUGPI7Wlaw", "_bibtex": "@misc{\nnagpal2021active,\ntitle={Active Learning in {\\{}CNN{\\}}s via Expected Improvement Maximization},\nauthor={Udai G. Nagpal and David A. Knowles},\nyear={2021},\nurl={https://openreview.net/forum?id=Uh0T_Q0pg7r}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Uh0T_Q0pg7r", "replyto": "Uh0T_Q0pg7r", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2484/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538095327, "tmdate": 1606915762451, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2484/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2484/-/Official_Review"}}}], "count": 8}