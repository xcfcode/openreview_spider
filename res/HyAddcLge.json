{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396317072, "tcdate": 1486396317072, "number": 1, "id": "r1SisGUug", "invitation": "ICLR.cc/2017/conference/-/paper33/acceptance", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Counter to the current wisdom, this work proposes that synchronous training may be advantageous over asynchronous training (provided that \"backup workers\" are available). This is shown empirical and without theoretical results. The contribution is somewhat straightforward and designed for a specific large-scale hardware scenario, but this is often an important bottleneck in the learning process. However, there is some concern about the long-term impact of this work, due to its dependence on the hardware."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396317627, "id": "ICLR.cc/2017/conference/-/paper33/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HyAddcLge", "replyto": "HyAddcLge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396317627}}}, {"tddate": null, "tmdate": 1483559562971, "tcdate": 1483559562971, "number": 2, "id": "rkQqGC5rg", "invitation": "ICLR.cc/2017/conference/-/paper33/official/comment", "forum": "HyAddcLge", "replyto": "S1zYdhqSx", "signatures": ["ICLR.cc/2017/conference/paper33/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper33/AnonReviewer4"], "content": {"title": "thanks for clarification", "comment": "thanks "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756793, "id": "ICLR.cc/2017/conference/-/paper33/official/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper33/(AnonReviewer|areachair)[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper33/(AnonReviewer|areachair)[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756793}}}, {"tddate": null, "tmdate": 1483552890088, "tcdate": 1483552890088, "number": 12, "id": "S1zYdhqSx", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "rJW29ruSg", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Thank you for the review", "comment": "We thank the reviewer for the review and suggestions, and address the concerns raised in detail below.\n\n1. The reviewer suggests that stragglers are not an issue with dedicated machines and proper implementation of communications, which is counter to our experiences. The issue of stragglers is exacerbated with larger number of machines, posing a challenge to scalability -- perhaps the reviewer has been using a relatively smaller number of machines? Our experiments were all conducted on Google's internal cluster with containers (https://research.google.com/pubs/pub43438.html). We also point out that the straggler phenomenon has been observed by others (Dean & Barroso (2013)) in production clusters. As discussed in both our paper and Dean & Barroso (2013), stragglers may occur from multiple reasons, including failing hardware, software bugs, contention of resources, pre-emption of jobs. Communication bottlenecks which the reviewer referred to are only one possible cause.\n\nFurthermore, not everyone has access to dedicated machines and clusters. In our cluster, containers share underlying physical hardware with imperfect isolation that shows up in the tail when pushing higher utilization. Even with elastic cloud resources such as EC2, one has to share network resources with other jobs and processes. Pre-emption of machines are also possible with spot instances. Our solution is a simple, elegant solution that attempts to address the *effect* of stragglers, rather than having multiple technically challenging approach to solve each cause.\n\nThe reviewer's suggestion of an \"MPI_Reduce\" (or AllReduce) is interesting and could help to alleviate some communication bottlenecks (which we emphasize again is not the only cause of stragglers) in synchronous training. We do also point out that a similar approach for asynchronous training could be technically harder to implement and analyze.\n\n\n3. It is true that our solution only addresses straggling workers but not parameter servers. In practice, however, one typically has an order of magnitude more workers than parameter servers. Hence, we have found that that straggling workers are a greater problem than straggling parameter servers. The issue of slow parameter servers is something we would leave for future work, should it become a significant bottleneck in practice.\n\n\n4. The reviewer is correct that hyperparameters need to be tuned with each batch size. (For more discussions, we refer the reviewer to the below comments / discussion with Ioannis Mitliagkas on tuning.) In general, we tried our best to tune the hyperparameters for asynchronous training; in contrast we spent less effort tuning the synchronous training, but nevertheless found it to produce better results for the reported models and datasets. Hence, we feel confident that our experiments demonstrate that synchronous training of deep models can outperform asynchronous training.\n\nWe agree with the reviewer that our results could be strengthened by averaging over multiple runs. Unfortunately, doing so is rather expensive -- 10 runs of the Inception experiments could cost ~150,000 GPU hours. We would like to reassure the reviewer that we did not cherry-pick results, i.e. we reported results from a random run. We also found results to be pretty consistent as we were fine-tuning our implementation. In addition, other real users within the company, training similar models for their work, have also observed similar results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1483393704705, "tcdate": 1483393704705, "number": 3, "id": "rJW29ruSg", "invitation": "ICLR.cc/2017/conference/-/paper33/official/review", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["ICLR.cc/2017/conference/paper33/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper33/AnonReviewer4"], "content": {"title": "Interesting Idea, but can be improved and I have a few concerns about experiments", "rating": "6: Marginally above acceptance threshold", "review": "This paper was easy to read, the main idea was presented very clearly.\n\nThe main points of the paper (and my concerns are below) can be summarized as follows:\n1. synchronous algoriths suffer from some struggeling nodes, for which the algorithm has to wait. From my own experience, this has never happend for me on e.g. Amazon EC2 cloud, however, it happens on our own cluster at my university, if the cluster is shared and some users make some nodes very busy. So maybe if the nodes would be dedicated to just user's job, it wouldn't be such a big concer (I am not sure what kind of cluster was used to produce Figure 3 and 4). Also how many experiments have you run? In my own experience, most of the time I get the gradient on time from all nodes equality fast, but maybe just in less than 0.1% of iterations I observe that it took maybe twice as long for some node. Also the increasing shape of the curve is somehow implying some weird implementation of communication. Isn't it only because you are somehow serialize the communication? And it would be maybe much faster if a \"MPI_Reduce\" would be used (even if we wait for the slowest guy)?\n2. asynchronous algorithms are cutting the waiting time, however, the convergence speed may be slower. Moreover, those algorithms can be divergence it special care is not given to stale gradients. Also they have a nice guarantees for convex functions, but the non-convex DNN may cause pain.\n3.they propose to take gradient from the first \"N\" workers out of \"N+b\" \nworkers available. My concern here is that they focused only on the \nworkers, but what if the \"parameter server\" will became to slow? What \nif the parameter server would be the bottleneck? How would you address \nthis situation? But still if the number of nodes (N) is not large, and \nthe deep DNN is used, I can imagine that the communciation will not \ntake more than 30% of the run-time.\n\n\nMy largest concern is with the experiments. Different batch size \nimplies that different learning rate should be chosen, right? How did \nyou tune the learning rates and other parameters for e.g. Figure 5 you \nprovide some formulas in (A2) but clearly this can bias your Figures, \nright? meaning, that if you tune \"\\gamma, \\beta\" for each N, it could \nbe somehow more representative? also it would be nicer if you run the \nexperiment many times and then report average, best and worst case \nbehaviour. because now it can be just coinsidence, right? \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483393705494, "id": "ICLR.cc/2017/conference/-/paper33/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper33/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper33/AnonReviewer1", "ICLR.cc/2017/conference/paper33/AnonReviewer3", "ICLR.cc/2017/conference/paper33/AnonReviewer4"], "reply": {"forum": "HyAddcLge", "replyto": "HyAddcLge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483393705494}}}, {"tddate": null, "tmdate": 1482476201867, "tcdate": 1482476201867, "number": 11, "id": "HJzncBqVg", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "HJ9ud6I4e", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Thank you for the review", "comment": "We thank the reviewer for the comments and suggestion.\n\nIndeed, our solution was designed with the specific distribution of learners' efficiencies in mind, and may not necessarily be the ideal solution for other distributions. However, we contend that this particular distribution is the one that matters in practice, since it is what we empirically observe in real production workloads (see Figures 3, 4), and it is what makes a fully synchronous (no backups) solution difficult to use in practice. Similar tail distributions have been observed by Dean & Barroso (2013) in other production systems with similar latency-related problems. Our contribution lies in demonstrating the feasibility of our solution in a synchronous machine learning system.\n\nFurthermore, we believe that distributions that are likely to cause problems for our solution, e.g. extremely high variability in learners' efficiencies, are also problematic for Async-Opt and naive synchronous optimization (with no backups), due to increased staleness and straggler effects. That said, we again emphasize that these are not distributions we observe in practice, and thus of lesser concern to us."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1482475750922, "tcdate": 1482475750922, "number": 10, "id": "S11lFB54x", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "Hy8b0jINl", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Thank you for the review", "comment": "We thank the reviewer for the comments and suggestions.\n\nWe would like to point out that our solution does not require extra budget. Indeed, given a fixed number of machines N, we can reserve a small number b, so that running with b out of N workers as backups would be faster than a fully synchronous solution with N workers. This is the premise of discussion in Section 3.1 on straggler effects.\n\nWhile we agree that the proposed solution looks straightforward in retrospect, we would argue that our contributions lie in motivating and demonstrating the feasibility of such a solution in a real-world production machine learning system. It is counter-intuitive and not immediately obvious that discarding the work of a few workers can in fact improve overall running time, since this reduces throughput relative to Async-SGD, and increases number of iterations to convergence relative to synchronous training (see Figure 5).\n\nThe reviewer's suggestion for controlling Async-SGD's staleness is an interesting one, and provides a different trade-off between accuracy and throughput, similar to the \"softsync\" approach of Zhang et al. (2015b). Our main goal in this paper, however, is to demonstrate that a synchronous solution is competitive with, and can outperform asynchronous solutions.\n\nWe also contend that asynchronous approaches do not completely resolve the staleness issues highlighted in Section 2.1, making analysis and understanding more difficult. On the other hand, synchronous solutions are straightforward to analyze, and more predictable behavior that aids debugging and improves reproducibility."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1482246258438, "tcdate": 1482246258438, "number": 2, "id": "HJ9ud6I4e", "invitation": "ICLR.cc/2017/conference/-/paper33/official/review", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["ICLR.cc/2017/conference/paper33/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper33/AnonReviewer3"], "content": {"title": "no title", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposed a synchronous parallel SGD by employing several backup machines. The parameter server does not have to wait for the return from all machines to perform the update on the model, which reduce the synchronization overhead. It sounds like a reasonable and straightforward idea. \n\nMy main concern is that this approach is only suitable for some very specific scenario, that is, most learners (except a small number of learners) are at the same pace to return the results. If the efficiency of learners does not follow such distribution, I do not think that the proposed algorithm will work. So I suggest two revisions:\n\n- provide more experiments to show the performance with different efficiency distributions of learners.\n- assuming that all learners follow the same distribution of efficiency and show the expected idle time is minor by using the proposed algorithm.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483393705494, "id": "ICLR.cc/2017/conference/-/paper33/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper33/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper33/AnonReviewer1", "ICLR.cc/2017/conference/paper33/AnonReviewer3", "ICLR.cc/2017/conference/paper33/AnonReviewer4"], "reply": {"forum": "HyAddcLge", "replyto": "HyAddcLge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483393705494}}}, {"tddate": null, "tmdate": 1482239485637, "tcdate": 1482239485637, "number": 1, "id": "Hy8b0jINl", "invitation": "ICLR.cc/2017/conference/-/paper33/official/review", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["ICLR.cc/2017/conference/paper33/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper33/AnonReviewer1"], "content": {"title": "Official Review", "rating": "5: Marginally below acceptance threshold", "review": "The paper claim that, when supported by a number of backup workers, synchronized-SGD \nactually works better than async-SGD. The paper first analyze the problem of staled updates\nin async-SGDs, and proposed the sync-SGD with backup workers. In the experiments, the \nauthors shows the effectiveness of the proposed method in applications to Inception Net\nand PixelCNN.\n\nThe idea is very simple, but in practice it can be quite useful in industry settings where \nadding some backup workders is not a big problem in cost. Nevertheless, I think the \nproposed solution is quite straightforward to come up with when we assume that \neach worker contains the full dataset and we have budge to add more workers. So, \nunder this setting, it seems quite natural to have a better performance with the additional \nbackup workers that avoid the staggering worker problem. And, with this assumtion I'm not \nsure if the proposed solution is solving difficult enough problem with novel enough idea. \n\nIn the experiments, for fair comparison, I think the Async-SGD should also have a mechanism \nto cut off updates of too much staledness just as the proposed method ignores all the remaining \nupdates after having N updates. For example, one can measure the average time spent to \nobtain N updates in sync-SGD setting and use that time as the cut-off threashold in Async-SGD \nso that Async-SGD does not perform so poorly.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483393705494, "id": "ICLR.cc/2017/conference/-/paper33/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper33/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper33/AnonReviewer1", "ICLR.cc/2017/conference/paper33/AnonReviewer3", "ICLR.cc/2017/conference/paper33/AnonReviewer4"], "reply": {"forum": "HyAddcLge", "replyto": "HyAddcLge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483393705494}}}, {"tddate": null, "tmdate": 1482096116782, "tcdate": 1482096116782, "number": 9, "id": "rkaxR_44e", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "HybKKdENx", "signatures": ["~Asim_Kadav1"], "readers": ["everyone"], "writers": ["~Asim_Kadav1"], "content": {"title": "thanks", "comment": "Thanks! I enjoyed reading the paper! "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1482094968645, "tcdate": 1482094968645, "number": 8, "id": "HybKKdENx", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "S1rLQ_4Ng", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Sync-with-backups faster, but same output, as sync-without-backups", "comment": "Our proposed solution (sync with backups) is indeed faster than sync without backups. My earlier comment about \"equivalence\" was in terms of the algorithm's output and convergence -- running 50+3 sync with backups gives the same algorithmic convergence than running 50 sync without backups, but has a faster per-iteration time. Apologies if the earlier statement was unclear.\n\nYour question about proportional or marginal gains does not have a straightforward answer, unfortunately, as it depends on the percentage of backup workers used. For example, if we had used 90+10 instead of 100 workers (see Figure 3, 4), we get a gain of ~30%, going from 2.6s to <1.8s, which is more than proportional. However, dropping more stragglers from 90+10 to 80+10 provides only marginal gains. Thus, our largest gains are when dropping only the final few stragglers.\n\nWe could also consider the problems in terms of additional resources instead of dropping stragglers. Although we do not have the results available, one can expect that using 99 sync-without-backups has almost the same running time as 100 sync-without-backup. If we were to use 99+1 sync-with-backups instead, Figures 3,4 suggest we would achieve a >1% improvement in per-iteration time.\n\nFor the Inception experiments in Section 4.3, we do get faster speeds and convergence for async even after 53 machines. This can be seen from the fact that using 106 machines for async is faster than using 53 machines for async."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1482093389515, "tcdate": 1482093389515, "number": 7, "id": "S1rLQ_4Ng", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "H1XINIVNx", "signatures": ["~Asim_Kadav1"], "readers": ["everyone"], "writers": ["~Asim_Kadav1"], "content": {"title": "comparison with sync", "comment": "Thanks for your response. It is impressive that you have a solution to address the variability in worker completion times. My understanding is that this variability increases with #workers.\n\nMy question was how does this compare with simply using sync. You mention in your response above that \"using 50+3 workers for sync-with-backup is equivalent to using 50 workers for sync-without-backup\". However, as you describe in the paper just using 50 workers for sync without backup can have significant straggler slowdown arising from contention and synchronization with the server. So, you should be going faster than sync without backup for the extra 6% overhead of the backup workers to make sense. Is the speedup proportional to the resource costs of the extra workers you are using or marginal? \n\nAn additional minor point is that if the dataset achieves its peak speedup with say 50 workers (i.e. beyond 50 workers the increase in speedup is negligible), running async with 53 workers can actually hurt its performance (This seems like your baseline in 8(a)). The extra 3 workers in the async case will cause extra noise leading to divergence for the async case; and will cause unnecessary straggler/synchronization costs for the sync case. Hence, I was wondering how will the speedup look like for an appropriately chosen N (sync) with N+b (your method). \n\n\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1482085451548, "tcdate": 1482085451548, "number": 6, "id": "H1XINIVNx", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "SJuUIa7Ng", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Efficiency", "comment": "Figure 3 and 4 show that it takes extremely long to collect the final few gradients, and as such allow us to achieve high efficiency. For example, using 90 of 100 machines has a median epoch of < 1.8s, but using 100 of 100 machines has a media of about 2.6s.\n\nWe are a little confused about the comment on Figure 8a, which does show sync (with backup) results. These results are equivalent to running a fully synchronous training (without backups) -- for example, using 50+3 workers for sync-with-backup is equivalent to using 50 workers for sync-without-backup."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1482049104298, "tcdate": 1482049104298, "number": 5, "id": "SJuUIa7Ng", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["~Asim_Kadav1"], "readers": ["everyone"], "writers": ["~Asim_Kadav1"], "content": {"title": "efficiency", "comment": "Is this technique efficient? If you are using 5-10% extra resources (backup workers), do you get at least 5-10% speedup over sync SGD? Figure 8(a) does not have any sync results. It is well understood that async may sometimes never converge to the correct results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481776248388, "tcdate": 1478039670172, "number": 33, "id": "HyAddcLge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HyAddcLge", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "content": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 19, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481301432133, "tcdate": 1481301432129, "number": 4, "id": "H1lppI_Qe", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "By2MMx_7e", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "All workers access all data.", "comment": "Yes, every worker has access to the same entire dataset, which is a typical setting for distributed TensorFlow jobs."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1481301371351, "tcdate": 1481301371346, "number": 3, "id": "Sy7Y6IdQe", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "SJuqCK7mx", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Configurations tuned for asynchronous training", "comment": "We thank Ioannis for his suggestions and feedback.\n\nAs discussed in our private communications, the learning rate of 0.045 for Inception was tuned for asynchronous training and thus the best known configuration to our team. It is of course impossible to discount the possibility that there are better configurations that we have not tried, but this is an issue faced by all researchers and practitioners. Nevertheless, we have tried our best to obtain the best learning rates for asynchronous training.\n\nIn light of the work uncovering the relation between asynchrony and momentum, we are currently running some experiments with Inception with lower momentum, but have yet to find a setting of lower momentum that outperforms our current results."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1481273876036, "tcdate": 1481273876003, "number": 2, "id": "By2MMx_7e", "invitation": "ICLR.cc/2017/conference/-/paper33/pre-review/question", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["ICLR.cc/2017/conference/paper33/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper33/AnonReviewer1"], "content": {"title": "question", "question": "Do you assume that each of the workers have the same full replica of the dataset?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481273876702, "id": "ICLR.cc/2017/conference/-/paper33/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper33/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper33/AnonReviewer3", "ICLR.cc/2017/conference/paper33/AnonReviewer1"], "reply": {"forum": "HyAddcLge", "replyto": "HyAddcLge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481273876702}}}, {"tddate": null, "tmdate": 1481152357259, "tcdate": 1481152357252, "number": 2, "id": "BJpPDf8Xg", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "HkAXr3kme", "signatures": ["~Xinghao_Pan1"], "readers": ["everyone"], "writers": ["~Xinghao_Pan1"], "content": {"title": "Weak assumption", "comment": "While there is an assumption that there are less than b stragglers, we would like to point out the following:\n1. Typically we set b to be about 5% of the total number of machines, and empirically we observe that the number of slow workers is less than this threshold (see Section 3.1). Thus, we believe that this is a weak assumption that almost always holds in practice.\n2. Furthermore, regardless of the true number of stragglers, our approach always shaves off the difference between the time taken to collect n gradient and (n-b) gradients, where the most significant savings occur.\n3. In the off-chance that there are more than b stragglers within an iteration, the correctness of the algorithm is not compromised."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1480986256288, "tcdate": 1480986256281, "number": 1, "id": "SJuqCK7mx", "invitation": "ICLR.cc/2017/conference/-/paper33/public/comment", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["~Ioannis_Mitliagkas1"], "readers": ["everyone"], "writers": ["~Ioannis_Mitliagkas1"], "content": {"title": "Some discussion on tuning", "comment": "The proposed technique of over-provisioning a few workers to improve the hardware efficiency of synchronous training gives a very welcome boost, as evidenced beautifully by Figure 6.\n\nHowever, as I have discussed in private communication with the authors, the comparison with asynchronous methods leaves a few questions regarding tuning.\u00a0\n\nLearning rate:\u00a0\n\nThe authors carefully describe the process of tuning the synchronous implementation. However no tuning is reported for the asynchronous implementation: the value is set to 0.045 for all configurations. Figure 7(a) shows that tuning the learning rate can cause a difference of almost a whole percentage point in test precision.\n1. What other values of LR have the authors tried for asynchronous training?\n2. Is 0.045 the best one for all configurations they report?\n3. Is there any chance that the 0.5% difference in precision between sync and async is due to insufficient LR tuning (as suggested by Figure 7(a))?\u00a0\n\n\nMomentum:\n\nOur results (https://arxiv.org/abs/1605.09774), that we discussed with the authors in August, suggest that asynchrony interacts with momentum, and that it is important for performance to tune your momentum when running asynchronously. The standard value 0.9 used throughout the paper is likely too much for asynchronous training.\u00a0\n\nThe authors have now kindly suggested that they might be able to test a few configurations with momentum less than 0.9. I think those results, if added, would be a great addition to this very good paper and contribute an extremely useful point to our understanding of this space.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756978, "id": "ICLR.cc/2017/conference/-/paper33/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "HyAddcLge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper33/reviewers", "ICLR.cc/2017/conference/paper33/areachairs"], "cdate": 1485287756978}}}, {"tddate": null, "tmdate": 1480733990068, "tcdate": 1480733990059, "number": 1, "id": "HkAXr3kme", "invitation": "ICLR.cc/2017/conference/-/paper33/pre-review/question", "forum": "HyAddcLge", "replyto": "HyAddcLge", "signatures": ["ICLR.cc/2017/conference/paper33/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper33/AnonReviewer3"], "content": {"title": "assumption", "question": "Do you assume that the number of slow learners are less than b?\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Revisiting Distributed Synchronous SGD", "abstract": "Distributed training of deep learning models on large-scale training data is typically conducted with asynchronous stochastic optimization to maximize the rate of updates, at the cost of additional noise introduced from asynchrony. In contrast, the synchronous approach is often thought to be impractical due to idle time wasted on waiting for straggling workers. We revisit these conventional beliefs in this paper, and examine the weaknesses of both approaches. We demonstrate that a third approach, synchronous optimization with backup workers, can avoid asynchronous noise while mitigating for the worst stragglers. Our approach is empirically validated and shown to converge faster and to better test accuracies.\n", "pdf": "/pdf/968d0270d1c93ee2aaecd3af5797c86de9de3106.pdf", "TL;DR": "We proposed distributed synchronous stochastic optimization with backup workers, and show that it converge faster and to better test accuracies.", "paperhash": "chen|revisiting_distributed_synchronous_sgd", "keywords": ["Optimization", "Deep learning", "Applications"], "conflicts": ["google.com", "berkeley.edu"], "authors": ["Jianmin Chen*", "Xinghao Pan*", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "authorids": ["jmchen@google.com", "xinghao@google.com", "rajatmonga@google.com", "bengio@google.com", "rafal@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481273876702, "id": "ICLR.cc/2017/conference/-/paper33/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper33/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper33/AnonReviewer3", "ICLR.cc/2017/conference/paper33/AnonReviewer1"], "reply": {"forum": "HyAddcLge", "replyto": "HyAddcLge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper33/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481273876702}}}], "count": 20}