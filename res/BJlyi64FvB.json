{"notes": [{"id": "BJlyi64FvB", "original": "rkxWt8AwPS", "number": 728, "cdate": 1569439126903, "ddate": null, "tcdate": 1569439126903, "tmdate": 1577168243051, "tddate": null, "forum": "BJlyi64FvB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["dg2893@columbia.edu", "guyga@google.com"], "title": "Wider Networks Learn Better Features", "authors": ["Dar Gilboa", "Guy Gur-Ari"], "pdf": "/pdf/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "TL;DR": "We visualize the hidden states of wide networks, finding that they contain more information about the inputs than narrow networks with equal performance, and show that wide networks fine-tuned to perform novel tasks outperform narrow networks.", "abstract": "Transferability of learned features between tasks can massively reduce the cost of training a neural network on a novel task. We investigate the effect of network width on learned features using activation atlases --- a visualization technique that captures features the entire hidden state responds to, as opposed to individual neurons alone. We find that, while individual neurons do not learn interpretable features in wide networks, groups of neurons do. In addition, the hidden state of a wide network contains more information about the inputs than that of a narrow network trained to the same test accuracy. Inspired by this observation, we show that when fine-tuning the last layer of a network on a new task, performance improves significantly as the width of the network is increased, even though test accuracy on the original task is independent of width. ", "keywords": ["Interpretability", "transfer learning"], "paperhash": "gilboa|wider_networks_learn_better_features", "original_pdf": "/attachment/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "_bibtex": "@misc{\ngilboa2020wider,\ntitle={Wider Networks Learn Better Features},\nauthor={Dar Gilboa and Guy Gur-Ari},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlyi64FvB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "bfu6GlPwMw", "original": null, "number": 1, "cdate": 1576798704421, "ddate": null, "tcdate": 1576798704421, "tmdate": 1576800931638, "tddate": null, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "invitation": "ICLR.cc/2020/Conference/Paper728/-/Decision", "content": {"decision": "Reject", "comment": "This paper investigated the effect of network width on learned features using activation atlases. From the current view of deep learning, the novelty of the paper is limited.\n\nAs all reviews rejected the paper and the authors gave up rebuttal, I choose to reject the paper.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dg2893@columbia.edu", "guyga@google.com"], "title": "Wider Networks Learn Better Features", "authors": ["Dar Gilboa", "Guy Gur-Ari"], "pdf": "/pdf/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "TL;DR": "We visualize the hidden states of wide networks, finding that they contain more information about the inputs than narrow networks with equal performance, and show that wide networks fine-tuned to perform novel tasks outperform narrow networks.", "abstract": "Transferability of learned features between tasks can massively reduce the cost of training a neural network on a novel task. We investigate the effect of network width on learned features using activation atlases --- a visualization technique that captures features the entire hidden state responds to, as opposed to individual neurons alone. We find that, while individual neurons do not learn interpretable features in wide networks, groups of neurons do. In addition, the hidden state of a wide network contains more information about the inputs than that of a narrow network trained to the same test accuracy. Inspired by this observation, we show that when fine-tuning the last layer of a network on a new task, performance improves significantly as the width of the network is increased, even though test accuracy on the original task is independent of width. ", "keywords": ["Interpretability", "transfer learning"], "paperhash": "gilboa|wider_networks_learn_better_features", "original_pdf": "/attachment/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "_bibtex": "@misc{\ngilboa2020wider,\ntitle={Wider Networks Learn Better Features},\nauthor={Dar Gilboa and Guy Gur-Ari},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlyi64FvB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728703, "tmdate": 1576800281160, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper728/-/Decision"}}}, {"id": "HkgkHmv5FS", "original": null, "number": 1, "cdate": 1571611446801, "ddate": null, "tcdate": 1571611446801, "tmdate": 1572972559728, "tddate": null, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "invitation": "ICLR.cc/2020/Conference/Paper728/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper considers the effect of network width of the neural network and its ability to capture various intricate features of the data. In particular, the central claim of this paper is what the title claims \"Wider networks learn features that are better\". They make this claim using the visualization technique called \"activation atlasses\". They find that wider networks learn features in the hidden neurons that are more \"interpretable\" in this visualization framework. Additionally, they also notice that fine-tuning a _linear model_ using the learned features for the wider networks provide better accuracy for new (but related) tasks over the shallower counterparts. For most experiments of this paper, \"shallow network\" refers to a width of 64 and \"wide network\" refers to a width of 2048. The main datasets used for the experiments are MNIST, CIFAR 10/100 and a \"translated\" version of MNIST images.\n\n\nOverall the paper is written well and the ideas and results are communicated crisply. I have a few comments. First, regarding the related work, I think that the reader would be served better if the authors also list the recent works related to effect of network width on convergence and generalization (e.g., [1] and references that cite this). The reason I say this is so that the reader should not (wrongly) interpret that this is the first work that finds \"favorable\" properties of wider networks (the paper does not make this claim, but it is easy for a reader to interpret it). Second, I find it slightly concerning that a lot of findings have been extrapolated from just one architecture. In particular, I find the experiments in section 5 to be the most informative (and also objective), since it is a single number which is easy to think about. To be clear, I like the visualization experiments and it gives credibility to the claim about interpretability. Given that there are many levers in a neural net (batch norm, architectural choices, hyper-params etc.) one could fiddle with, to make the claim made in the introduction one needs a more extensive set of experiments. I acknowledge that the authors say they haven't explored the possibility of fine-tuning the hyper-params for instance, but I think considering some of these choices is really helpful. This will help _isolate_ the effect of width independent of the architecture choice.\n\nGiven the above observations, my current decision of this paper is that it doesn't meet the bar. I find the results promising but the paper is not yet ready. \n\n\n[1] - https://papers.nips.cc/paper/8076-neural-tangent-kernel-convergence-and-generalization-in-neural-networks.pdf"}, "signatures": ["ICLR.cc/2020/Conference/Paper728/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper728/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dg2893@columbia.edu", "guyga@google.com"], "title": "Wider Networks Learn Better Features", "authors": ["Dar Gilboa", "Guy Gur-Ari"], "pdf": "/pdf/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "TL;DR": "We visualize the hidden states of wide networks, finding that they contain more information about the inputs than narrow networks with equal performance, and show that wide networks fine-tuned to perform novel tasks outperform narrow networks.", "abstract": "Transferability of learned features between tasks can massively reduce the cost of training a neural network on a novel task. We investigate the effect of network width on learned features using activation atlases --- a visualization technique that captures features the entire hidden state responds to, as opposed to individual neurons alone. We find that, while individual neurons do not learn interpretable features in wide networks, groups of neurons do. In addition, the hidden state of a wide network contains more information about the inputs than that of a narrow network trained to the same test accuracy. Inspired by this observation, we show that when fine-tuning the last layer of a network on a new task, performance improves significantly as the width of the network is increased, even though test accuracy on the original task is independent of width. ", "keywords": ["Interpretability", "transfer learning"], "paperhash": "gilboa|wider_networks_learn_better_features", "original_pdf": "/attachment/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "_bibtex": "@misc{\ngilboa2020wider,\ntitle={Wider Networks Learn Better Features},\nauthor={Dar Gilboa and Guy Gur-Ari},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlyi64FvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575622748190, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper728/Reviewers"], "noninvitees": [], "tcdate": 1570237747953, "tmdate": 1575622748204, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper728/-/Official_Review"}}}, {"id": "BklafWnaFB", "original": null, "number": 2, "cdate": 1571827989261, "ddate": null, "tcdate": 1571827989261, "tmdate": 1572972559694, "tddate": null, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "invitation": "ICLR.cc/2020/Conference/Paper728/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors observed that the wider deep neural networks can learn much rich representative features than shallower deep neural networks while both networks show similar level of the test performance. They show feature visualization about their observations using two different networks n=20/n=2048.\n\nAt the first, I feel that the visualizations on figure 1 about two different width are too marginal. Almost they look similar, it's hard to say that significantly show difference.\n\nAlso there is no guarantee that the quality of the feature visualization follows linear relationship according to width. Comparison with just two different width is not enough to analyze the situation.\n\nI wonder if human-interpretable features are always better. Machine-interpretable information also do important role, as adversarial attack.\n\nEven the total number of parameter is preserved, the performance will be largely vary according to the network architecture, such as the number of the layers. Then, I have a doubt whether experiments on Sec 5.1 are meaningful not. More, the model can suffer from the gradient vanishing problem when the network has a number of layers. I wonder that the results on figure 4 are caused from this problem.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper728/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper728/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dg2893@columbia.edu", "guyga@google.com"], "title": "Wider Networks Learn Better Features", "authors": ["Dar Gilboa", "Guy Gur-Ari"], "pdf": "/pdf/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "TL;DR": "We visualize the hidden states of wide networks, finding that they contain more information about the inputs than narrow networks with equal performance, and show that wide networks fine-tuned to perform novel tasks outperform narrow networks.", "abstract": "Transferability of learned features between tasks can massively reduce the cost of training a neural network on a novel task. We investigate the effect of network width on learned features using activation atlases --- a visualization technique that captures features the entire hidden state responds to, as opposed to individual neurons alone. We find that, while individual neurons do not learn interpretable features in wide networks, groups of neurons do. In addition, the hidden state of a wide network contains more information about the inputs than that of a narrow network trained to the same test accuracy. Inspired by this observation, we show that when fine-tuning the last layer of a network on a new task, performance improves significantly as the width of the network is increased, even though test accuracy on the original task is independent of width. ", "keywords": ["Interpretability", "transfer learning"], "paperhash": "gilboa|wider_networks_learn_better_features", "original_pdf": "/attachment/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "_bibtex": "@misc{\ngilboa2020wider,\ntitle={Wider Networks Learn Better Features},\nauthor={Dar Gilboa and Guy Gur-Ari},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlyi64FvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575622748190, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper728/Reviewers"], "noninvitees": [], "tcdate": 1570237747953, "tmdate": 1575622748204, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper728/-/Official_Review"}}}, {"id": "Hyl9rKAatB", "original": null, "number": 3, "cdate": 1571838274036, "ddate": null, "tcdate": 1571838274036, "tmdate": 1572972559652, "tddate": null, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "invitation": "ICLR.cc/2020/Conference/Paper728/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper investigates wider networks using a recent feature visualization technique named activation atlases. By analyzing what the hidden layers of wider networks respond to, the authors showed that wider networks learn more transferable features. However, I tend to reject this paper since it doesn\u2019t show very compelling evidence through experiments.\n\n1. This paper does not present any novel methods, and so the experiments need to be very solid. But all the datasets and architectures used in this paper are quite simple from the view of deep learning. It is not clear whether the conclusions will still be valid for larger datasets or deeper networks. \n\n2. The most important observation of this paper is to find that wider networks can be easily transferred to a new task. But in Section 5.1, all layers except the last classification layer are fixed when fine-tuning the networks for the second task. It is so obvious that wider networks with fewer previous layers can perform better. In Section 5.2, the authors did not show the network details, and also it is not fair to compare the networks with a linear classifier. The authors should include more competitive baselines.\n\n3. I encourage the authors to show the training/validation curves to testify the data efficiency of the wider networks when training or fine-tuning on a new task."}, "signatures": ["ICLR.cc/2020/Conference/Paper728/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper728/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["dg2893@columbia.edu", "guyga@google.com"], "title": "Wider Networks Learn Better Features", "authors": ["Dar Gilboa", "Guy Gur-Ari"], "pdf": "/pdf/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "TL;DR": "We visualize the hidden states of wide networks, finding that they contain more information about the inputs than narrow networks with equal performance, and show that wide networks fine-tuned to perform novel tasks outperform narrow networks.", "abstract": "Transferability of learned features between tasks can massively reduce the cost of training a neural network on a novel task. We investigate the effect of network width on learned features using activation atlases --- a visualization technique that captures features the entire hidden state responds to, as opposed to individual neurons alone. We find that, while individual neurons do not learn interpretable features in wide networks, groups of neurons do. In addition, the hidden state of a wide network contains more information about the inputs than that of a narrow network trained to the same test accuracy. Inspired by this observation, we show that when fine-tuning the last layer of a network on a new task, performance improves significantly as the width of the network is increased, even though test accuracy on the original task is independent of width. ", "keywords": ["Interpretability", "transfer learning"], "paperhash": "gilboa|wider_networks_learn_better_features", "original_pdf": "/attachment/97d49c1c9b17621e004cd603942aa525511f2002.pdf", "_bibtex": "@misc{\ngilboa2020wider,\ntitle={Wider Networks Learn Better Features},\nauthor={Dar Gilboa and Guy Gur-Ari},\nyear={2020},\nurl={https://openreview.net/forum?id=BJlyi64FvB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BJlyi64FvB", "replyto": "BJlyi64FvB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper728/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575622748190, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper728/Reviewers"], "noninvitees": [], "tcdate": 1570237747953, "tmdate": 1575622748204, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper728/-/Official_Review"}}}], "count": 5}