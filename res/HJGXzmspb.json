{"notes": [{"tddate": null, "ddate": null, "tmdate": 1522216763800, "tcdate": 1522206452380, "number": 3, "cdate": 1522206452380, "id": "B1hgwtOcG", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Public_Comment", "forum": "HJGXzmspb", "replyto": "HJGXzmspb", "signatures": ["~herbert_chen1"], "readers": ["everyone"], "writers": ["~herbert_chen1"], "content": {"title": "have you try (mod P) congruence class, which is a typical field", "comment": "maybe it can change  classification to regression\uff0c1/2=2 mod\uff083\uff09\uff0c1/1=1 mod\uff083\uff09\u3002with some Convolution \uff0cit seems interesting"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791694356, "id": "ICLR.cc/2018/Conference/-/Paper47/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Area_Chair"], "cdate": 1512791694356}}}, {"tddate": null, "ddate": null, "tmdate": 1518730190757, "tcdate": 1508745754535, "number": 47, "cdate": 1518730190741, "id": "HJGXzmspb", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HJGXzmspb", "original": "HyG7zXoTZ", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "tmdate": 1518523631466, "tcdate": 1518523631466, "number": 5, "cdate": 1518523631466, "id": "SywlHIgDz", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "forum": "HJGXzmspb", "replyto": "B14ObUeDz", "signatures": ["ICLR.cc/2018/Conference/Paper47/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper47/Authors"], "content": {"title": "Reply to Gradient of quantitized integer values/variables ", "comment": "Please refer to the algorithm in Appendix A, we use back-propagation and quantize deltaW and e_{q}^i within each layer."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740120, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper47/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper47/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper47/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740120}}}, {"tddate": null, "ddate": null, "tmdate": 1518522732342, "tcdate": 1518522732342, "number": 2, "cdate": 1518522732342, "id": "B14ObUeDz", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Public_Comment", "forum": "HJGXzmspb", "replyto": "HJGXzmspb", "signatures": ["~Rudra_Poudel1"], "readers": ["everyone"], "writers": ["~Rudra_Poudel1"], "content": {"title": "Gradient of quantitized integer values/variables", "comment": "How do you calculate the gradient of (quantized/discontinuous) integer weight (W_{q}^i) and activation (e_{q}^i)? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791694356, "id": "ICLR.cc/2018/Conference/-/Paper47/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Area_Chair"], "cdate": 1512791694356}}}, {"tddate": null, "ddate": null, "tmdate": 1518241865061, "tcdate": 1518241865061, "number": 4, "cdate": 1518241865061, "id": "rk-I_-nLG", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "forum": "HJGXzmspb", "replyto": "H1EkHsHIf", "signatures": ["ICLR.cc/2018/Conference/Paper47/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper47/Authors"], "content": {"title": "Reply to Similar paper + reference ", "comment": "Thanks for your comments !\n--  I would like to see how this paper differs from the above mentioned papers.\n(1) In Gupta et.al. 2015 (https://arxiv.org/abs/1502.02551), they use WL = <IL + FL> to format the fixed-point parameters and intermediate variables. And in most of their experiments, the WL equals to 16, e.g., <6, 10>, <4, 12>. While in our paper, we only have IL part for sign bits, i.e., we use WL = 8 = <1 + 7>. In that case, we have to use distribution shifting. Compared to the half-precision floating-point (FP16), the computation efficiency and overhead of 16-bit fixed-point doesn't improve that much. However, we are considering to cite this paper and modify our wording. \n(2) Flexpoint (Koster et.al. 2017) use flexN+M to completely replace FP32 format in both training and inference, it addresses little about quantization. Besides, the distribution shifting in WAGE discard the exponent bits and only keeps the relatively orders of magnitude, which is quite different from the above papers.\n(3) Courbariaux et. al. 2015 (https://arxiv.org/pdf/1412.7024.pdf) use a higher precision for the parameters during the updates than during the forward and backward propagations, which can really improve the performance. Please see http://papers.nips.cc/paper/7163-training-quantized-nets-a-deeper-understanding.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740120, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper47/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper47/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper47/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740120}}}, {"tddate": null, "ddate": null, "tmdate": 1517823238499, "tcdate": 1517823196105, "number": 1, "cdate": 1517823196105, "id": "H1EkHsHIf", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Public_Comment", "forum": "HJGXzmspb", "replyto": "HJGXzmspb", "signatures": ["~Tijmen_Blankevoort1"], "readers": ["everyone"], "writers": ["~Tijmen_Blankevoort1"], "content": {"title": "Similar paper + reference", "comment": "The idea of applying quantization this way on the weights, activations, gradients and errors was already discussed in Gupta et.al. 2015 (https://arxiv.org/abs/1502.02551). This paper applies stochastic rounding, in a fixed-point way on the above calculations/parameters to quantise a network while training. This paper does not appear in the reference, but given the similarity of the two papers, it's worthwhile including. It is also counter to the statement in this paper that this is the first bidirectional quantisation scheme that allows learning and inference in reduced bit-precision.\nOne of the things this paper does over Gupta et.al. 2015 is Distribution shifting. This is not novel, as it was already done in the flexpoint paper (Koster et.al. 2017) and before that in Courbariaux et. al. 2015 (https://arxiv.org/pdf/1412.7024.pdf)\nI would like to see how this paper differs from the above mentioned papers."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791694356, "id": "ICLR.cc/2018/Conference/-/Paper47/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Area_Chair"], "cdate": 1512791694356}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260101388, "tcdate": 1517249183364, "number": 4, "cdate": 1517249183345, "id": "HkDjf1prG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HJGXzmspb", "replyto": "HJGXzmspb", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "High quality paper, appreciated by reviewers, likely to be of substantial interest to the community. It's worth an oral to facilitate a group discussion.", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642453468, "tcdate": 1511535705617, "number": 1, "cdate": 1511535705617, "id": "SkzPEnBeG", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Review", "forum": "HJGXzmspb", "replyto": "HJGXzmspb", "signatures": ["ICLR.cc/2018/Conference/Paper47/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "It is not clear if this work obtains significant improvements in comparison with previous works", "rating": "7: Good paper, accept", "review": "This paper proposes a method to train neural networks with low precision. However, it is not clear if this work obtains significant improvements over previous works. \n\nNote that:\n1)\tWorking with 16bit, one can train neural networks with little to no reduction in performance. For example, on ImageNet with AlexNet one gets 45.11% top-1 error if we don\u2019t do anything else, and 42.34% (similar to the 32-bit result) if we additionally adjust the loss scale (e.g., see Boris Ginsburg, Sergei Nikolaev, and Paulius Micikevicius. \u201cTraining of deep networks with halfprecision float.\u201d NVidia GPU Technology Conference, 2017). \n2)\tImageNet with AlexNet top-1 error (53.5%) in this paper seems rather high in comparison to previous works. Specifically, DoReFA and QNN, which used mostly lower precision (k_W=1, k_A=2 and k_E=6, k_G=32)  one can get much lower performance (47% and 49%, respectively). So, the main innovation here, in comparison, is k_G=12.\n3)\tComparison using other datasets is made with different architectures then previous works, so it is hard to quantify what is the contribution of the proposed method. For example, on MNIST, the authors use a convolutional neural network, while BC and BNN used a fully connected neural network (the so called \u201cpermutation invariant mnist\u201d problem).\n4)\tCifar performance is good, but may seem less remarkable, given that \u201cGated XNOR Networks: Deep Neural Networks with Ternary Weights and Activations under a Unified Discretization Framework\u201d already showed that k_G=k_W=k_A=2, k_E=32 is sufficient to get 7.5% error on CIFAR. So the main novelty, in comparison, is that k_E=12.\n\nTaking all the above into account, it hard to be sure whether the proposed methods meaningfully improve existing methods. Moreover, I am not sure if decreasing the precision from 16bit to 12bit (as was done on ImageNet) is very useful for hardware applications, especially if there is such a degradation in accuracy. If, for example, the authors would have demonstrated all-8bit training on all datasets with little performance degradation, this would seem much more useful.\n\nMinor: there are some typos that should be corrected, e.g.: \u201cEmpirically, We demonstrates\u201d in abstract.\n\n%%% Following the authors response %%%\nThe authors have improved their results and have addressed my concerns. I therefore raised my scores.\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642453379, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper47/AnonReviewer1", "ICLR.cc/2018/Conference/Paper47/AnonReviewer2", "ICLR.cc/2018/Conference/Paper47/AnonReviewer3"], "reply": {"forum": "HJGXzmspb", "replyto": "HJGXzmspb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642453379}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642453432, "tcdate": 1511668649852, "number": 2, "cdate": 1511668649852, "id": "rJG2o3wxf", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Review", "forum": "HJGXzmspb", "replyto": "HJGXzmspb", "signatures": ["ICLR.cc/2018/Conference/Paper47/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "The paper describe a method for how to train and make inference in a network using only integer values.", "rating": "7: Good paper, accept", "review": "The authors describe a method called WAGE, which quantize all operands and operators in a neural network, specifically, the weights (W), activations (A), gradients (G), and errors (E) . The idea is using quantizers with clipping (denoted in the paper with Q(x,k)) and some additional operators like shift (denoted with shift(x)) and stochastic rounding. The main motivation of the authors in this work is to reduce the number of bits for representation in a network for all the WAGE operations and operands which influences the power consumption and silicon area in hardware implementations.\n\nAfter introducing the idea and related work, the authors in Section 3 give details about how to perform the quantization. They introduce the additional operators needed for training in such network. Since quantization may loss some information, the authors need to quantize the signals in the network around the dynamic range in order not to \"kill\" the signal. The authors describe how to do that. Afterward, as in other techniques for quantization, they describe how to initialize the network values. Also, they argue that batch normalization in this network is replaced with the shift-quantize operations, and what is matter in this case is (1) the relative values (\u201corientations\u201d) and not the absolute values and (2) small values in errors are negligible.\n\nAfterward, the authors conduct experiments on MNIST, SVHN, CIFAR10, and ILSVRC12 datasets, where they show promising results compared to the errors provided by previous works. The WAGE parameters (i.e., the quantized no. of bits used) are 2-8-8-8, respectively. For understand more the WAGE, the authors compare on CIFAR10 the test error rate with vanilla CNN and show is small loss in using their network. The authors investigate mainly the bitwidth of errors and gradients.\n\nIn overall, this paper is an accept since it shows good performance on standard problems and invent some nice tricks to implement NN in hardware, for *both* training and inference. For inference only, other works has more to offer but this is a promising technique for learning. The things that are still missing in this work are some power reduction estimates as well as area reduction estimations. This will give the hardware community a clear vision of how such methods may be implemented both in data centers as well as on end portable devices. \n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642453379, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper47/AnonReviewer1", "ICLR.cc/2018/Conference/Paper47/AnonReviewer2", "ICLR.cc/2018/Conference/Paper47/AnonReviewer3"], "reply": {"forum": "HJGXzmspb", "replyto": "HJGXzmspb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642453379}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642453395, "tcdate": 1511830125065, "number": 3, "cdate": 1511830125065, "id": "SyrOMN9eM", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Review", "forum": "HJGXzmspb", "replyto": "HJGXzmspb", "signatures": ["ICLR.cc/2018/Conference/Paper47/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "a thorough and flexible approach towards discretizing neural networks", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The authors propose WAGE, which discretized weights, activations, gradients, and errors at both training and testing time. By quantization and shifting, SGD training without momentum, and removing the softmax at output layer as well, the model managed to remove all cumbersome computations from every aspect of the model, thus eliminating the need for a floating point unit completely. Moreover, by keeping up to 8-bit accuracy, the model performs even better than previously proposed models. I am eager to see a hardware realization for this method because of its promising results. \n\nThe model makes a unified discretization scheme for 4 different kinds of components, and the accuracy for each of the kind becomes independently adjustable. This makes the method quite flexible and has the potential to extend to more complicated networks, such as attention or memory. \n\nOne caveat is that there seem to be some conflictions in the results shown in Table 1, especially ImageNet. Given the number of bits each of the WAGE components asked for, a 28.5% top 5 error rate seems even lower than XNOR. I suspect it is due to the fact that gradients and errors need higher accuracy for real-valued input, but if that is the case, accuracies on SVHN and CIFAR-10 should also reflect that. Or, maybe it is due to hyperparameter setting or insufficient training time?\n\nAlso, dropout seems not conflicting with the discretization. If there are no other reasons, it would make sense to preserve the dropout in the network as well.\n\nIn general, the paper was written in good quality and in detail, I would recommend a clear accept.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642453379, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper47/AnonReviewer1", "ICLR.cc/2018/Conference/Paper47/AnonReviewer2", "ICLR.cc/2018/Conference/Paper47/AnonReviewer3"], "reply": {"forum": "HJGXzmspb", "replyto": "HJGXzmspb", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642453379}}}, {"tddate": null, "ddate": null, "tmdate": 1513250204546, "tcdate": 1513164955350, "number": 3, "cdate": 1513164955350, "id": "HJ7oecRZf", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "forum": "HJGXzmspb", "replyto": "SkzPEnBeG", "signatures": ["ICLR.cc/2018/Conference/Paper47/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper47/Authors"], "content": {"title": "Reply to AnonReviewer1", "comment": "We sincerely appreciate the reviewer for the comments, which indeed helps us to improve the quality of this paper. \n\nIn our revised manuscript, we keep the last layer in full precision for ImageNet task (both BNN and DoReFa keep the first and the last layer in full precision). Our results have been improved from 53.5/28.6 with 28CC to 51.7/28.0 with 2888 bits setting. Results of other patterns are updated in Table4.  We have now revised the paper accordingly and would like to provide point-by-point response on how these comments have been addressed:\n\n(1) Working with 16bit, one can train neural networks with little to no reduction in performance.\n\nWe introduce a thorough and flexible approach (from AnonReviewer3) towards training DNNs with fixed-point (8bit) integers, so there is no floating-point operands or operations in both inference and training phases. This is the key difference between our work and the previous works. As shown in Table5 in the revised manuscript, 5x reduction of energy and area costs can be achieved in this way, which we believe will greatly benefit the application of our method especially in mobile devices.\n\n(2) ImageNet with AlexNet top-1 error (53.5%) in this paper seems rather high in comparison to previous works.\n\nThe significant differences between WAGE and existing works (DoReFa, QNN, BNN) lie in that:\n\n    1. WAGE does not need to store real-valued weights (DoReFa, QNN, BNN need).\n    2. WAGE calculates both gradients and errors with 8-bit integers (QNN, BNN use float32).\n    3. Many of the techniques, say for example, batch normalization and Adam optimizer that are hard to be \n         implemented on mobile devices are avoided by WAGE.  \n\nThrough experiments, we find that, if we store real-valued weights and do not quantize back propagation, the performance on ImageNet is at the same level (although not the same specification) as that of DoReFa, QNN and BNN.  Please refer to more detailed results in Table4.\n\n(3) Comparison using other datasets is made with different architectures then previous works\n\nPlease refer to the comparison between TWN and WAGE in Table1 where we show a better result with the same CNN architecture. \n\n(4) Cifar performance is good, but may seem less remarkable.\n\nIn fact,  k-E is set as 8 in WAGE. Gated-XNOR uses a batch size of 1000 and totally trains for 1000 epochs, so the total training time and memory consumption are unsatisfactory. Besides, they use float32 to calculate gradients and errors, and batch normalization layer is kept to guarantee the convergence.\n\n(5)  If, for example, the authors would have demonstrated all-8bit training on all datasets\n\nIn our experiments, we find that it is necessary to set k-G>k-W, otherwise the updates of weights will directly influence the forward propagation and cause instability. Most of the previous works store real-valued weights (32-bits k-G), so they meet this restriction automatically. By considering this comment, we focus on 2-8-8-8 training and the results for ImageNet are updated in Table1 and Table4.  \n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740120, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper47/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper47/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper47/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740120}}}, {"tddate": null, "ddate": null, "tmdate": 1513249785465, "tcdate": 1513164800979, "number": 2, "cdate": 1513164800979, "id": "r1t-e5CZf", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "forum": "HJGXzmspb", "replyto": "rJG2o3wxf", "signatures": ["ICLR.cc/2018/Conference/Paper47/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper47/Authors"], "content": {"title": "Reply to AnonReviewer2", "comment": "We thank the reviewer for the constructive suggestion:\n\n(1) The things that are still missing in this work are some power reduction estimates as well as area reduction estimations.\n\nWe have taken this suggestion and added Table5 in Discussion, and made a rough estimation. \n\nFor future work, we have tapped out our neuromorphic processors lately using phase-change memory to store weights and designed the ability to do some on-chip and on-site learning. The processor has 8-bit weights and 8-bit activation without any floating-point design. The real power consumption and area reduction of the processor has been simulated and estimated. It is very promising to implement some interesting application with continual learning demands on that chip as an end portable device.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740120, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper47/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper47/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper47/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740120}}}, {"tddate": null, "ddate": null, "tmdate": 1513249685365, "tcdate": 1513164681300, "number": 1, "cdate": 1513164681300, "id": "ryW51cAbG", "invitation": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "forum": "HJGXzmspb", "replyto": "SyrOMN9eM", "signatures": ["ICLR.cc/2018/Conference/Paper47/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper47/Authors"], "content": {"title": "Reply to AnonReviewer3", "comment": "We thank the reviewer for the insightful comments. Please find our responses to individual questions below:\n\n(1) One caveat is that there seem to be some conflictions in the results shown in Table 1, especially ImageNet ...\n\nIn our revised manuscript, we keep the last layer in full precision for ImageNet task (BNN and DoReFa kept both the first and the last layer), the accuracy for 2-8-8-8 is 51.7/28.0 compared to original results 53.5/28.6 with 2-8-C-C bits setting. Results of other patterns are updated in Table4.\n\nWe find that the Softmax layer in the AlexNet model and 1000 categories jointly cause the conflictions. Since we make no exception for the first or the last layer, weights in the last layer will be limited to {-0.5,0,+0.5} and scaled by Equation(8), so the outputs of the last layer also obey a normal distribution N(0,1). The problem is that these values are small for a Softmax layer with 1000 categories. \n\nExample: \n\nx1=[0,0,0,\u2026,1] (one-hot 1000 dims)\ny1=Softmax(x1)=[9.9e-4, 9.9e-4, \u2026, 2.7e-3]\ne1 = z \u2013 x1, still a long way to train\nx2=[0, 0, 0,\u2026,8] (one-hot 1000 dims)\ny2=Softmax(x2)=[1e-4, 1e-4, \u2026, 0.75]\ne2 = z \u2013 x2, much closer to the label now\nlabel=z=[0,0,0,\u2026,1].\n\nIn this case, we observe that 80% weights in the last FC layer are trained greedily to {+0.5} to magnify the outputs. Therefore, the last layer would be a bottleneck for both inference and backpropagation. That might be why previous works do not quantize the last layer. The experiments on CIFAR10 and SVHN did not use Softmax cross-entropy and had only 10 categories, which indicates no accuracy drop. \n\n\n(2)Also, dropout seems not conflicting with the discretization...\n\nYes, it is an additional method to alleviate over-fitting. Because we are working on designing a new neuromorphic computing chip, dropout will make the pipeline of weights and MAC calculations a little bit weird. Anyone who has no concern of that can easily add dropout to the WAGE graph.\n\n\t\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Training and Inference with Integers in Deep Neural Networks", "abstract": "Researches on deep neural networks with discrete parameters and their deployment in embedded systems have been active and promising topics. Although previous works have successfully reduced precision in inference, transferring both training and inference processes to low-bitwidth integers has not been demonstrated simultaneously. In this work, we develop a new method termed as ``\"WAGE\" to discretize both training and inference, where weights (W), activations (A), gradients (G) and errors (E) among layers are shifted and linearly constrained to low-bitwidth integers. To perform pure discrete dataflow for fixed-point devices, we further replace batch normalization by a constant scaling layer and simplify other components that are arduous for integer implementation. Improved accuracies can be obtained on multiple datasets, which indicates that WAGE somehow acts as a type of regularization. Empirically, we demonstrate the potential to deploy training in hardware systems such as integer-based deep learning accelerators and neuromorphic chips with comparable accuracy and higher energy efficiency, which is crucial to future AI applications in variable scenarios with transfer and continual learning demands.", "pdf": "/pdf/516345e75eb2cf918642c571d05976a33898d715.pdf", "TL;DR": "We apply training and inference with only low-bitwidth integers in DNNs", "paperhash": "wu|training_and_inference_with_integers_in_deep_neural_networks", "_bibtex": "@inproceedings{\nwu2018training,\ntitle={Training and Inference with Integers in Deep Neural Networks},\nauthor={Shuang Wu and Guoqi Li and Feng Chen and Luping Shi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=HJGXzmspb},\n}", "keywords": ["quantization", "training", "bitwidth", "ternary weights"], "authors": ["Shuang Wu", "Guoqi Li", "Feng Chen", "Luping Shi"], "authorids": ["wus15@mails.tsinghua.edu.cn", "liguoqi@mail.tsinghua.edu.cn", "chenfeng@mail.tsinghua.edu.cn", "lpshi@mail.tsinghua.edu.cn"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825740120, "id": "ICLR.cc/2018/Conference/-/Paper47/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJGXzmspb", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper47/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper47/Authors|ICLR.cc/2018/Conference/Paper47/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper47/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper47/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper47/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper47/Reviewers", "ICLR.cc/2018/Conference/Paper47/Authors", "ICLR.cc/2018/Conference/Paper47/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825740120}}}], "count": 13}