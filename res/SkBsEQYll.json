{"notes": [{"tddate": null, "replyto": null, "ddate": null, "writable": true, "revisions": true, "tmdate": 1486409539399, "tcdate": 1478206620665, "number": 85, "replyCount": 12, "id": "SkBsEQYll", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "SkBsEQYll", "signatures": ["~Franziska_Horn1"], "readers": ["everyone"], "content": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396347833, "tcdate": 1486396347833, "number": 1, "id": "rk4piGUul", "invitation": "ICLR.cc/2017/conference/-/paper85/acceptance", "forum": "SkBsEQYll", "replyto": "SkBsEQYll", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "There is consensus among the reviewers that the novelty of the paper is limited, and that the experimental evaluation of the proposed method needs to be improved."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396348358, "id": "ICLR.cc/2017/conference/-/paper85/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "SkBsEQYll", "replyto": "SkBsEQYll", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396348358}}}, {"tddate": null, "tmdate": 1482180240026, "tcdate": 1482180240026, "number": 3, "id": "SyOc8pS4e", "invitation": "ICLR.cc/2017/conference/-/paper85/official/review", "forum": "SkBsEQYll", "replyto": "SkBsEQYll", "signatures": ["ICLR.cc/2017/conference/paper85/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper85/AnonReviewer2"], "content": {"title": "Novelty claim is false, evaluation is partial", "rating": "3: Clear rejection", "review": "This paper presents a method for embedding data instances into a low-dimensional space that preserves some form of similarity.\n\nAlthough the paper presents this notion as new, basically every pre-trained embedding (be it auto-encoders or word2vec) has been doing the same: representing items in a low-dimensional space that inherently encodes their similarities. Even when looking at the specific case of word/context embeddings, the method is not novel either: this method is almost identical to one of the similarity functions presented in \"A Simple Word Embedding Model for Lexical Substitution\" (Melamud et al., 2015). The novelty claim must be more accurate and position itself with respect to existing work.\n\nIn addition, I think the evaluation could be done better. There are plenty of benchmarks for word embeddings in context, for example: \n* http://veceval.com/ (Nayak et al., RepEval 2016).\n* Lexical Substitution in Context\nAnd many higher-level tasks where word similarity in context could be a game-changer:\n* Semantic Text Similarity\n* Recognizing Textual Entailment / Natural Language Inference\nI was disappointed that none of these were even brought up.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512702965, "id": "ICLR.cc/2017/conference/-/paper85/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper85/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper85/AnonReviewer1", "ICLR.cc/2017/conference/paper85/AnonReviewer3", "ICLR.cc/2017/conference/paper85/AnonReviewer2"], "reply": {"forum": "SkBsEQYll", "replyto": "SkBsEQYll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512702965}}}, {"tddate": null, "tmdate": 1482004350998, "tcdate": 1482004350998, "number": 2, "id": "rkDtPG7Ee", "invitation": "ICLR.cc/2017/conference/-/paper85/official/review", "forum": "SkBsEQYll", "replyto": "SkBsEQYll", "signatures": ["ICLR.cc/2017/conference/paper85/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper85/AnonReviewer3"], "content": {"title": "marginal novelty", "rating": "2: Strong rejection", "review": "this paper proposes to use feed-forward neural networks to learn similarity preserving embeddings. They also use the proposed idea to represent out-of-vocabulary words using the words in given context. \n\nFirst, considering the related work [1,2] the proposed approach brings marginal novelty. Especially\nContext Encoders is just a small improvement over word2vec. \n\nExperimental setup should provide more convincing results other than visualizations and non-standard benchmark for NER evaluation with word vectors [3].\n\n[1] http://papers.nips.cc/paper/5477-scalable-non-linear-learning-with-adaptive-polynomial-expansions.pdf\n[2] http://deeplearning.cs.cmu.edu/pdfs/OJA.pca.pdf\n[3] http://www.anthology.aclweb.org/P/P10/P10-1040.pdf", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512702965, "id": "ICLR.cc/2017/conference/-/paper85/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper85/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper85/AnonReviewer1", "ICLR.cc/2017/conference/paper85/AnonReviewer3", "ICLR.cc/2017/conference/paper85/AnonReviewer2"], "reply": {"forum": "SkBsEQYll", "replyto": "SkBsEQYll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512702965}}}, {"tddate": null, "tmdate": 1481982090325, "tcdate": 1481982090325, "number": 4, "id": "Skz9gaMVg", "invitation": "ICLR.cc/2017/conference/-/paper85/public/comment", "forum": "SkBsEQYll", "replyto": "ryTDQsW4g", "signatures": ["~Franziska_Horn1"], "readers": ["everyone"], "writers": ["~Franziska_Horn1"], "content": {"title": "Please elaborate / provide a reference", "comment": "Albeit negative, thank you for your review.\n\nWe will add more results on larger corpora for the analogy task. As we all know, even the simplest models can often perform on par with more complex models if they are trained on a very large dataset, however if you have only limited time or data available for training it doesn't hurt to be able to give your simple model a little boost, especially if it's as easy as weighting your existing word embeddings with some context vectors.\nFor the NER task, we wanted to isolate the three factors that lead to the increased performance when using ConEcs: a) using ConEc in general, i.e. enhancing the word2vec embeddings with aggregated context information, b) being able to distinguish between words with multiple meanings by including local contexts, and c) being able to easily generate out-of-vocabulary representations. It is not possible to demonstrate that ConEc can generate meaningful OOV embeddings, if the word embeddings in general were trained on such a large corpus, that there are no OOV words in the test set anymore. You could argue, that in reality the word embeddings will always trained on a larger corpus, yet if only a fixed corpus is available for the task, we prefer to only train the embeddings on the actual training data and exclude the documents from the test set to allow for a realistic estimation of the generalisation capabilities of the model.\n\nCan you please elaborate why you think the SimEc \"architecture and approach is far from novel\"? In my literature search I have not come across a paper with such an approach. Yes, the network to generate the embeddings after the training procedure is complete has a simple feed-forward structure, similar to an auto-encoder, which, after training, is also just a vanilla NN. The main insight lies in how the training is performed. For auto-encoders, this was to realise that by mirroring the feed forward NN structure, you can approximate the original input and thereby at the bottleneck of the network generate low dimensional embeddings that capture much of the datas' variance. With SimEcs, we want to learn embeddings that retain pairwise relations between the data points instead of the variance. For this, only during training, we add an additional layer to the feed-forward NN, with which we multiply the embeddings to linearly approximate the target similarities. After the training is complete, this additional layer is not needed anymore, as we're only interested in the low dimensional embeddings generated in the layer before (output of the normal FFNN). The end result is similar to the solution found by kernel PCA (w.r.t. the corresponding kernel function / target similarities), as we're also projecting the data pints into a feature space where the similarities can be linearly approximated. However, unlike KPCA we know the function (namely our tuned FFNN) with which to project also new data points into this feature space and additionally there are no limitations on the target similarity matrix (doesn't have to be positive semi-definite, etc.). If you know of a paper which shows how to use a NN to recreate a KPCA solution similarly as auto-encoders can replace (and extended) traditional PCA, I would be very thankful for a reference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287734545, "id": "ICLR.cc/2017/conference/-/paper85/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkBsEQYll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper85/reviewers", "ICLR.cc/2017/conference/paper85/areachairs"], "cdate": 1485287734545}}}, {"tddate": null, "tmdate": 1481909093559, "tcdate": 1481909093559, "number": 1, "id": "ryTDQsW4g", "invitation": "ICLR.cc/2017/conference/-/paper85/official/review", "forum": "SkBsEQYll", "replyto": "SkBsEQYll", "signatures": ["ICLR.cc/2017/conference/paper85/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper85/AnonReviewer1"], "content": {"title": "Standard feed-forward neural net with unconvincing experimental results", "rating": "3: Clear rejection", "review": "This paper introduces a similarity encoder based on a standard feed-forward neural network with the aim of generating similarity-preserving embeddings. The approach is utilized to generate a simple extension of the CBOW word2vec model that transforms the learned embeddings by their average context vectors. Experiments are performed on an analogy task and named entity recognition.\n\nWhile this paper offers some reasonable intuitive arguments for why a feed-forward neural network can generate good similarity-preserving embeddings, the architecture and approach is far from novel. As far as I can tell, the model is nothing more than the most vanilla neural network trained with SGD on similarity signals.\n\nSlightly more original is the idea to use context embeddings to augment the expressive capacity of learned word representations. Of course, using explicit contextual information is not a new idea, especially for tasks like word sense disambiguation (see, e.g., 'Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space' by Neelakantan et al, which should also be cited), but the specific method used here is original, as far as I know.\n\nThe evaluation of the method is far from convincing. The corpora used to train the embeddings are far smaller than would ever be used in practice for unsupervised or semi-supervised embedding learning. The performance on the analogy task says little about the benefit of this method for larger corpora, and, as the authors mentioned in the comments, they expect \"the gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training.\"\n\nThe argument can be made (and the authors do claim) that extrinsic evaluations are more important for real-world applications, so it is good to see experiments on NER. However, again the embeddings were trained on a very small corpus and I am not convinced that the observed benefit will persist when trained on larger corpora.\n\nOverall, I believe this paper offers little novelty and weak experimental evidence supporting its claims. I cannot recommend it for acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512702965, "id": "ICLR.cc/2017/conference/-/paper85/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper85/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper85/AnonReviewer1", "ICLR.cc/2017/conference/paper85/AnonReviewer3", "ICLR.cc/2017/conference/paper85/AnonReviewer2"], "reply": {"forum": "SkBsEQYll", "replyto": "SkBsEQYll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512702965}}}, {"tddate": null, "tmdate": 1480758982528, "tcdate": 1480758982525, "number": 3, "id": "H10aIMeXl", "invitation": "ICLR.cc/2017/conference/-/paper85/public/comment", "forum": "SkBsEQYll", "replyto": "SyG5W01mx", "signatures": ["~Franziska_Horn1"], "readers": ["everyone"], "writers": ["~Franziska_Horn1"], "content": {"title": "Thanks!", "comment": "Thank you for pointing these papers out, I'll look into them and include them accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287734545, "id": "ICLR.cc/2017/conference/-/paper85/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkBsEQYll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper85/reviewers", "ICLR.cc/2017/conference/paper85/areachairs"], "cdate": 1485287734545}}}, {"tddate": null, "tmdate": 1480758837800, "tcdate": 1480758837795, "number": 2, "id": "rkRN8zgQg", "invitation": "ICLR.cc/2017/conference/-/paper85/public/comment", "forum": "SkBsEQYll", "replyto": "r1Zd2t1Qe", "signatures": ["~Franziska_Horn1"], "readers": ["everyone"], "writers": ["~Franziska_Horn1"], "content": {"title": "Thank you for your questions and suggestions.", "comment": "Concerning the linear embedding layer: an additional non-linearity on this layer would transform the embedding coordinates to be within a certain interval, e.g. [0, 1] if you add a sigmoid activation function. However, with this additional constraint, it becomes even more difficult to find embedding vectors whose scalar product can approximate the target similarities. Especially if you think in terms of approximating the kPCA solution, where the kernel matrix gets decomposed into the low dimensional embeddings YY^T based on the eigenvectors belonging to the d largest eigenvalues, the embeddings coordinates in Y are not constrained to be within a specific range; in fact this would make it impossible to find a solution even for the linear kernel, which can contain negative values. Does this make it more clear? In general, dimensionality reduction algorithms don't find representations whose coordinates lie only in a specific interval; SimEcs don't do this either.\n\nConcerning the learning slowdown when adding a non-linearity on the output layer and therefore switching to the cross-entropy error function instead of the mean squared error, I was mainly inspired by this blog post: http://neuralnetworksanddeeplearning.com/chap3.html (see the examples that you can run in the beginning): while you can still get a good solution with both error functions eventually, the optimisation with the MSE can be very slow initially, while you quickly get better with cross-entropy. But yes, talking about vanishing gradients might be misleading here, I guess it's just small gradients in general.\n\nI will include a reference to the Oja paper, thanks.\n\nDo you have a specific task in mind for further evaluation, especially one where there are comparable baseline results available? As mentioned in the comment on the question below, we feel that testing the word embeddings on a \"real world\" task is critical, as a good performance on some word similarity or analogy task does not always mean the word embeddings are also useful for other problem settings. And in the end, I suppose we should be learning representations because they serve as useful features for such extrinsic tasks as well."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287734545, "id": "ICLR.cc/2017/conference/-/paper85/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkBsEQYll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper85/reviewers", "ICLR.cc/2017/conference/paper85/areachairs"], "cdate": 1485287734545}}}, {"tddate": null, "tmdate": 1480741257712, "tcdate": 1480741257704, "number": 3, "id": "SyG5W01mx", "invitation": "ICLR.cc/2017/conference/-/paper85/pre-review/question", "forum": "SkBsEQYll", "replyto": "SkBsEQYll", "signatures": ["ICLR.cc/2017/conference/paper85/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper85/AnonReviewer2"], "content": {"title": "Related literature on context embeddings", "question": "There are several recent papers by Oren Melamud that deal with embedding contexts and using them to generate context-sensitive representations or similarities:\n* Modeling Word Meaning in Context with Substitute Vectors\n* A Simple Word Embedding Model for Lexical Substitution\n* context2vec: Learning Generic Context Embedding with Bidirectional LSTM\nI think it is worth discussing how this work relates to Melamud's, and perhaps even compare the methods empirically."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959471212, "id": "ICLR.cc/2017/conference/-/paper85/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper85/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper85/AnonReviewer1", "ICLR.cc/2017/conference/paper85/AnonReviewer3", "ICLR.cc/2017/conference/paper85/AnonReviewer2"], "reply": {"forum": "SkBsEQYll", "replyto": "SkBsEQYll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959471212}}}, {"tddate": null, "tmdate": 1480723561284, "tcdate": 1480723561279, "number": 2, "id": "r1Zd2t1Qe", "invitation": "ICLR.cc/2017/conference/-/paper85/pre-review/question", "forum": "SkBsEQYll", "replyto": "SkBsEQYll", "signatures": ["ICLR.cc/2017/conference/paper85/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper85/AnonReviewer3"], "content": {"title": "questions and suggestions", "question": "- why do you enforce the last layer of the first part of the NN be linear?\n- I don't understand the argument related to vanishing gradient since your model is not deep or have backprop-through-time. can you elaborate on that?\n\nsuggestions:\n- you should refer to Oja's work on showing the relationship between neural networks and pca http://deeplearning.cs.cmu.edu/pdfs/OJA.pca.pdf\n\n- I feel like to evaluate representation power of the context or modelling unknown words, an open-vocabulary language modelling on morphologically rich languages or word similarity in context tasks would be better option than an extrinsic task NER."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959471212, "id": "ICLR.cc/2017/conference/-/paper85/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper85/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper85/AnonReviewer1", "ICLR.cc/2017/conference/paper85/AnonReviewer3", "ICLR.cc/2017/conference/paper85/AnonReviewer2"], "reply": {"forum": "SkBsEQYll", "replyto": "SkBsEQYll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959471212}}}, {"tddate": null, "tmdate": 1480712319666, "tcdate": 1480712319662, "number": 1, "id": "SkdKxwJQx", "invitation": "ICLR.cc/2017/conference/-/paper85/public/comment", "forum": "SkBsEQYll", "replyto": "SyY49IJQl", "signatures": ["~Franziska_Horn1"], "readers": ["everyone"], "writers": ["~Franziska_Horn1"], "content": {"title": "Will include results from larger corpus", "comment": "Thanks for pointing this out. We will include results obtained by training on a larger corpus in an updated version of the manuscript. \nMy guess is that the performance gain will be less significant, as the global context statistics brought in by the ConEc can also be picked up by word2vec with more training. \nSince there is growing scepticism about the validity of evaluating word representations on similarity and analogy tasks instead of \"real world applications\" (see e.g. papers from a recent ACL workshop https://sites.google.com/site/repevalacl16/accepted-papers), we consider the performance on the NER task to be more relevant and included the results on the analogy task only for completeness."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287734545, "id": "ICLR.cc/2017/conference/-/paper85/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "SkBsEQYll", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper85/reviewers", "ICLR.cc/2017/conference/paper85/areachairs"], "cdate": 1485287734545}}}, {"tddate": null, "tmdate": 1480710705148, "tcdate": 1480710705145, "number": 1, "id": "SyY49IJQl", "invitation": "ICLR.cc/2017/conference/-/paper85/pre-review/question", "forum": "SkBsEQYll", "replyto": "SkBsEQYll", "signatures": ["ICLR.cc/2017/conference/paper85/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper85/AnonReviewer1"], "content": {"title": "Analogy task", "question": "Why did you train your embedding model on such a small corpus (text8)? Most other embedding models are trained on much larger corpora (say, ~1B tokens), so it makes it difficult to interpret your results in the broader context of the literature.\n\nDo you still see substantial gains from the context encoder when training on larger corpora?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning similarity preserving representations with neural similarity and context encoders", "abstract": "We introduce similarity encoders (SimEc), which learn similarity preserving representations by using a feed-forward neural network to map data into an embedding space where the original similarities can be approximated linearly. The model can easily compute representations for novel (out-of-sample) data points, even if the original pairwise similarities of the training set were generated by an unknown process such as human ratings. This is demonstrated by creating embeddings of both image and text data.\nFurthermore, the idea behind similarity encoders gives an intuitive explanation of the optimization strategy used by the continuous bag-of-words (CBOW) word2vec model trained with negative sampling. Based on this insight, we define context encoders (ConEc), which can improve the word embeddings created with word2vec by using the local context of words to create out-of-vocabulary embeddings and representations for words with multiple meanings. The benefit of this is illustrated by using these word embeddings as features in the CoNLL 2003 named entity recognition task.", "pdf": "/pdf/5779204f81750d711baecf5fca47c5fc7cb2705c.pdf", "TL;DR": "Neural network way of doing kernel PCA and an extension of word2vec to compute out-of-vocabulary embeddings and distinguish between multiple meanings of a word based on its local context.", "paperhash": "horn|learning_similarity_preserving_representations_with_neural_similarity_and_context_encoders", "conflicts": ["tu-berlin.de"], "keywords": ["Natural language processing", "Unsupervised Learning", "Supervised Learning"], "authors": ["Franziska Horn", "Klaus-Robert M\u00fcller"], "authorids": ["franziska.horn@campus.tu-berlin.de", "klaus-robert.mueller@tu-berlin.de"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959471212, "id": "ICLR.cc/2017/conference/-/paper85/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper85/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper85/AnonReviewer1", "ICLR.cc/2017/conference/paper85/AnonReviewer3", "ICLR.cc/2017/conference/paper85/AnonReviewer2"], "reply": {"forum": "SkBsEQYll", "replyto": "SkBsEQYll", "writers": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper85/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959471212}}}], "count": 12}