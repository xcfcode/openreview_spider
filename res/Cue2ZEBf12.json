{"notes": [{"id": "Cue2ZEBf12", "original": "q6EuCHc-tev", "number": 773, "cdate": 1601308090088, "ddate": null, "tcdate": 1601308090088, "tmdate": 1614985618308, "tddate": null, "forum": "Cue2ZEBf12", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "C66p8Z3VAwg", "original": null, "number": 1, "cdate": 1610040442042, "ddate": null, "tcdate": 1610040442042, "tmdate": 1610474043211, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper improves on previous work (adv-BNN) with hierarchical variational inference. It observes that mean-field VI training for BNNs often result in close-to-deterministic approximate posterior distributions for weights, which effectively makes the BNN closer to deterministic neural network, thereby loosing the robustness advantage of stochastic neural networks. To address this, a hierarchical prior is proposed on the weights, which, together with the corresponding approximate posterior design, aims at preventing the collapse of the variances of the weights towards zero. This improved version of adv-BNN is shown to be reasonably better than the original adv-BNN and their deterministic counter-part against the PGD and EOT attacks on a various of benchmark dataset in the adversarial robustness literature.\n\nReviewers initially had questions about whether the comparison is fair to the original adv-BNN since the reported results were very different. This issue has been addressed by the authors during the author feedback period, after that reviewers agreed that the proposed approach is a good extension of adv-BNN towards making it more robust. They also agree that the analysis of the original adv-BNN in terms of posterior variance collapse is interesting and potentially useful, although they also pointed out the link of increased variance (with the proposed method) and better uncertainty estimation is unclear.\n\nIn revision, I would encourage the authors to clear up the confusions of the reviewers by clearly stating the comparison setting with the original adv-BNN, and better clarify the methodology."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "expdate": 1610718299323, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040442028, "tmdate": 1610718299497, "id": "ICLR.cc/2021/Conference/Paper773/-/Decision"}}}, {"id": "vp6VvPN7d8", "original": null, "number": 4, "cdate": 1604023325587, "ddate": null, "tcdate": 1604023325587, "tmdate": 1607008466083, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Review", "content": {"title": "Improving Adversarial-BNN with hierarchical variational inference", "review": "Updates:\nThe author addressed my concerns about the experiments. Though the improvement is marginal and I still have some concerns, I\u2019m ok to accept the paper. I\u2019ll change my score to 6.\n========================\n\nSummary:\nThe paper studied the adversarial Bayesian Neural Network and found that the stochasticity of it vanished. As stochasticity can help improve robustness against adversarial examples, the author proposed to use conjugate prior of Gaussian posterior to improve stochasticity of the model and robustness at the same time.\n\nStrength:\nExperiments show that the proposed method outperforms adversarial-BNN and adversarial training on several benchmark datasets and the stochasticity of the proposed model is larger than adversarial-BNN.\n\nWeakness:\nThe evaluation is somehow questionable. Checked the original paper of adversarial-BNN and found that the performances of adversarial-BNN is much better than reported in this paper. In both papers, VGG16 is the base structure of BNN, but the reported performances of adv-BNN and adversarial training are different in two papers on CIFAR10.\n\nCIFAR10 Results\n\n| $\\epsilon$ | adv-BNN | adv-BNN(in this paper) | adv | adv (in this paper)|\n|:------:|:-----:|:------:|:-----:|:------:|\n| 0 | 79.7 | 62 | 80.3| 72 |\n|0.015 | 68.7 | 54| 58.3 | 60 |\n\nIt could be a problem of hyper-parameter tuning. Could the author provide some explanation on this?\n\nIn experiments, the models on CIFAR10 and STL10 are trained with $L_\\infty$ perturbation magnitude of 0.03. They are evaluated under PGD and EOT-PGD with $L_\\infty$ in $[0,0.03]$. The range of attack perturbation magnitude on CIFAR10 and STL10 could be larger, such as $[0,0.08]$, to better compare the baselines with the proposed method.\n\nClarity and Correctness:\nThe paper is well written and easy to follow but the experiments might be problematic.\n\nReproducibility:\nCode of the method is not available.\n\nConclusion:\nThe idea is clear and novel but experiment results need more elaboration. Overall, I think the paper is marginally below the acceptance threshold. I like the idea of using conjugate prior to improve stochasticity and robustness. However, I'm a little bit concerned about the experiment results. If that can be addressed, I'm willing to accept the paper.\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper773/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135376, "tmdate": 1606915784361, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper773/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Review"}}}, {"id": "y_Xl8eyYieW", "original": null, "number": 3, "cdate": 1603886260808, "ddate": null, "tcdate": 1603886260808, "tmdate": 1606775513795, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Review", "content": {"title": "Nice results and important direction, but too many misnomers. [Edited Post-Rebuttal]", "review": "In this paper the authors study the adversarial robustness of BNNs on large scale datasets. BNNs have been shown to be a more robust learning paradigm due to their uncertainty/stochasticity. Given the empirical observation that adversarially trained BNN posterior variances converge to zero (which the authors need to do much more to show as this is not a well-established phenomena), the authors propose a hierarchical prior where they put a prior over the parameters of the Gaussian prior normally used  in mean-field variational inference. The authors show that performing approximate inference with a hierarchical prior leads to an increased variational posterior variance, which the authors claim is correlated to the observation of increased adversarial robustness. \n\nI think the empirical results are solid, and the direction of the paper is ultimately an important one. I truly encourage the authors to continue to pursue this topic. \n\nUnfortunately, this paper is severely handicapped by its lack of clarity in terms of accurate contextualization, and its pervasive use of misnomers.  These misnomers are so prevalent that they would certainly lead uninformed readers to incorrect conclusions about Bayesian deep learning. \n\n* On the \u201ctrue posterior:\u201d  There are many places throughout the paper where the authors discuss/reference the computation of the true posterior for a Bayesian deep neural network. This can only be done by exact Bayesian inference. That includes (1) proper marginalization over the space of parameters (2) normalization wrt p(X). Given a DNN with non-linear activations, computation of the true posterior is intractable. Despite this, the authors claim that one can infer the true posterior via variation inference methods (bottom of pg 1). Variational inference makes a closed form approximation of the posterior that one tries to learn the parameters of. Even learning the optimal parameters does not guarantee convergence, outside of the case of conjugation.  Further on this, computing the true adversarial posterior is even more intractable given that the intractability of computing the optimal adversarial example compounds the issue of performing exact Bayesian inference.\n\n* Conjugate priors and hierarchical priors are distinct under the Bayesian framework. Despite this, the authors name their hierarchical prior the \u201cconjugate\u201d prior. In this work, the authors suggest placing a prior distribution over the parameters of their prior distribution (i.e. a hierarchical prior), yet call it a conjugate prior. A conjugate prior in the standard Bayesian literature is a prior which is known to be in the same family as the true posterior. It is not known, and likely not true, that for general approximate Bayesian neural networks (e.g. mean-field approximations) the true or approximate posterior is Gaussian. Thus, it is likely false to call a mean-field prior approximation a \u201cconjugate\u201d prior.\n\n * Robust Optimization is the special case of adversarial training where only adversarial data is used. The authors conflate adversarial optimization (optimization with respect to an adversarial objective) with robust optimization which has a rich history in optimization prior to its application to deep learning. The end of section 2.1 should have its terminology corrected.  \n\n* On the notion of \u2018regularization\u2019 in Bayesian deep learning. In several places the authors refer to the regularization term of the ELBO objective. This regularization term is the KL divergence with the prior distribution. While the prior distribution could be said to have a regularization effect on the posterior, saying that the prior distribution is a regularizer is reductive and probably misleading.\n\n\n----------------------------------------------\n\nFollowing the author's rebuttal I think the paper has benefitted from further experiments and from further clarifications. I would like to thank the authors for carefully considering my feedback and for modifying their paper in the directions I suggested. Ultimately, like I said in my original review, I think this is a very interesting and well-motivated problem, but I still have a few doubts. In particular, the doubt about the paper's use of the term of \"conjugate\" remains. In their rebuttal the authors use the term approximate-conjugate prior, but I am not sure that this is satisfactory as being conjugate means you have knowledge of the form of the true posterior's closed form, which is not the case for BNNs. \n\nI have increased my score to reflect that I think the authors are moving in a promising direction and I hope that they will continue with this work. One thing I will note on the experimental side of things is that having greater variance is indeed interesting, but it may or may not be correlated with increased uncertainty and this may be interesting to investigate in a future version of this work.\n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper773/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135376, "tmdate": 1606915784361, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper773/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Review"}}}, {"id": "buAl4XO_88G", "original": null, "number": 2, "cdate": 1603885813931, "ddate": null, "tcdate": 1603885813931, "tmdate": 1606741779809, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Review", "content": {"title": "An extension to Adv-BNN to improve the robustness of BNN", "review": "Summary:\n\nThis paper presents a new adversarial training for BNNs with variational inference (VI). Specifically, Adv-BNN training of Liu et al. 2019 uses a standard normal prior for VI of BNNs.  The paper observes that the above method may have vanished stochasticity that reduces the robustness and the proposed method extends it with a conjugate prior constructed by a normal distribution and an inverse gamma distribution. This extension results in a stronger regularisation of the weights of BNNs, which can enhance the robustness against adv attacks and leads to a hierarchical inference. The method is reported to have better performance than vanilla adv training and adv-BNN training on several benchmark datasets.\n\nPros:\n- It is an interesting and motivative observation that AdV-BNN has vanished stochasticity issue, which is important for BNNs.\n\n- The proposed method is a straightforward way to address the vanished stochasticity issue.\n\n- The results in Table 1 are intuitive, which directly shows the proposed method has more stochasticity than Adv-BNNs.\n\nCons:\n\n- It is concerning that the reported performance of Adv-BNN in this paper has a significant difference than that reported in the original paper. In this paper, Adv-BNN performs worse than the vanilla adv training with a large margin, which is a bit surprised. Therefore, it is unclear whether it is Adv-BNN not working or it is the settings/implementations of this paper having something wrong. Given this fact, the performance reported in this paper seems to be ungrounded. It is hard to justify the true performance advantage reported in this paper.\n\n- The approach might be less related to the topic of \"hierarchical inference\", as it's only replacing the standard Gaussian with a normal-inverse-gamma distribution, which only affects the KL divergence in this case, where there are no intermediate variables are inferred.\n\n- Minor: Some of the notations and equations are a bit unclear. For example, q has been used to denote the posterior but it denotes the prior in Eq. (5)\n\n--------------------------------------------------------------------------------------------------------------------------------------------------\n\nThe author response addresses my major concern on the experimental results. Therefore, I have updated my rating from 5 to 6.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper773/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135376, "tmdate": 1606915784361, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper773/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Review"}}}, {"id": "yZuA3QFEXSj", "original": null, "number": 5, "cdate": 1605890147181, "ddate": null, "tcdate": 1605890147181, "tmdate": 1606305557032, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "buAl4XO_88G", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment", "content": {"title": "Responses for AnonReviewer2 (cont')", "comment": "We thank you for your insightful and helpful comments. As AnonReviewer3 also noted, your major concern is the difference between reported performance of adv-BNN in the original paper and ours. Also, we appreciate your comments on the topic of hierarchical inference for clarifying the advantages of the proposed method.\n\n***\n\n**Q1. It is concerning that the reported performance of adv-BNN in this paper has a significant difference than that reported in the original paper. In this paper, adv-BNN performs worse than the vanilla adv training with a large margin, which is a bit surprised. Therefore, it is unclear whether it is adv-BNN not working or it is the settings/implementations of this paper having something wrong. Given this fact, the performance reported in this paper seems to be ungrounded. It is hard to justify the true performance advantage reported in this paper.**\n\nA1. In a nutshell, we used different experimental settings in the scope of layers with Bayesian inference applied. Zimmermann (2019) [1] pointed out the open source code of the original adv-BNN did not use the full Bayesian inference to the whole layer ([https://github.com/xuanqing94/BayesianDefense/issues/5](https://github.com/xuanqing94/BayesianDefense/issues/5)). He noted that although the authors of adv-BNN calculated the KL divergence for all layers in the network in their paper, but they did not use the sum of the KL divergences for all layers. Rather, the original code actually used just the KL divergence of the last layer ([https://github.com/xuanqing94/BayesianDefense/blob/master/models/vgg_vi.py#L39](https://github.com/xuanqing94/BayesianDefense/blob/master/models/vgg_vi.py#L39)). Thus, according to the public open source code of adv-BNN, their proposed regularization is only applied to the last layer during training and not the entire network. Since our proposed regularization is theoretically proved and described in the sense of applying Bayesian inference to the whole layer in the submitted paper, we utilized hierarchical Bayesian inference in the entire network, not just the last layer.\n\nTo avoid controversy, we can start from currently available open source code for the adv-BNN ([https://github.com/xuanqing94/BayesianDefense](https://github.com/xuanqing94/BayesianDefense)). When we didn\u2019t change anything in the open source code of the original adv-BNN, we were able to reproduce a performance similar to the original adv-BNN as below. However, switching to using the sum of KL divergence for all layers did not allow us to achieve the performance in the original paper. \n\nWhen we used the calculation of only the last layer KL divergence for the proposed method, it still outperformed adv-BNN in small margins with the calculation for the last layer KL divergence. We think this small difference comes from the different Bayesian inference applying only the last layer. If we use the full Bayesian inference to the whole layer, then the difference becomes huge. \n\nIn short, the described Figure 2-4 (also even Figure 3 newly added in updated paper for ablation study) were fairly compared figures with adv, adv-BNN, and adv-Ours on the equal evaluation settings where we utilized the whole layer summation of KL divergence for full Bayesian.\n\n* Comparison between adv-BNN and adv-Ours on CIFAR10\n| $\\delta$ \t| adv-BNN (origianl paper) \t| adv-BNN (KLD Last Only) \t| adv-BNN (KLD Sum) \t| adv-Ours (KLD Last Only) \t| adv-Ours (KLD Sum) \t|\n|:--------:\t|:------------------------:\t|:-----------------------:\t|:-----------------:\t|:------------------------:\t|:------------------:\t|\n|     0    \t|           79.7           \t|          79.94          \t|        65.10       \t|           81.91          \t|        74.00       \t|\n|   0.015  \t|           68.7           \t|          68.52          \t|       56.00       \t|           69.67          \t|        64.73       \t|\n|   0.035  \t|           45.4           \t|          46.13          \t|       47.08       \t|           48.18          \t|        52.16       \t|\n|   0.055  \t|           26.9           \t|          27.35          \t|       30.23       \t|           27.71          \t|        32.03       \t|\n|   0.07            |          18.6            |          18.23                   |      20.67              |           18.64                  |        22.64           |\n  \n***\n\nReference\n* [1] Zimmermann, R. S. (2019). Comment on\" Adv-BNN: Improved Adversarial Defense through Robust Bayesian Neural Network\". arXiv preprint arXiv:1907.00895."}, "signatures": ["ICLR.cc/2021/Conference/Paper773/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cue2ZEBf12", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper773/Authors|ICLR.cc/2021/Conference/Paper773/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867356, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment"}}}, {"id": "kLJc5U4pL5w", "original": null, "number": 7, "cdate": 1605893357974, "ddate": null, "tcdate": 1605893357974, "tmdate": 1606103499142, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "vp6VvPN7d8", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment", "content": {"title": "Responses for AnonReviewer3", "comment": "We are thankful for your detailed and discerning assessment of our work. As with AnonReviewer2, your major concern is also the difference between the reported performances of the adv-BNN and adversarial training in the original paper and ours.\n\n***\n\n**Q1. The evaluation is somehow questionable. Checked the original paper of adversarial-BNN and found that the performances of adversarial-BNN is much better than reported in this paper. In both papers, VGG16 is the base structure of BNN, but the reported performances of adv-BNN and adversarial training are different in two papers on CIFAR10. It could be a problem of hyper-parameter tuning.**\n\nA1-1. The issue of different reported performances of adv-BNN\n\nFor this issue, you can refer to [\"A1.\"](https://openreview.net/forum?id=Cue2ZEBf12&noteId=yZuA3QFEXSj) in Responses for AnonReviewer2.\n\nA1-2. The issue of different reported performances of normal adversarial training\n\nFor the performance of normal adversarial training you pointed out, we used a different experiment setting for optimizer. We used an Adam optimizer for all methods whereas the original source code used an SGD optimizer only for normal adversarial training ([https://github.com/xuanqing94/BayesianDefense/blob/master/main_adv.py#L143](https://github.com/xuanqing94/BayesianDefense/blob/master/main_adv.py#L143)). The original source code used different optimizers for adversarial training(SGD) and adv-BNN(Adam) ([https://github.com/xuanqing94/BayesianDefense/blob/master/main_adv_vi.py#L150](https://github.com/xuanqing94/BayesianDefense/blob/master/main_adv_vi.py#L150)). When we did not change anything in the open source code of the original adv-BNN, the original source code actually used Adam optimizer for adv-BNN in the training process. However, they used an SGD optimizer for the normal adversarial training. If we use an SGD optimizer for adv-BNN in the original source code, then the training becomes unstable and fails to implement with a technical issue. In our study, we used an Adam optimizer for all methods (adv, adv-BNN, and adv-Ours) in order to achieve coherent experimental settings. Therefore, performance differences of adversarial training arise from that difference.\n\nIn short, the illustrated Figure 2-4 (also even Figure 3 newly added in updated paper for ablation study) were fairly compared figures with adv, adv-BNN, and adv-Ours on the equal evaluation settings where we utilized the whole layer summation of KL divergence for full Bayesian (for the first issue), and equally employed Adam optimizer (for the second issue).\n\n***\n\n**Q2. In experiments, the models on CIFAR10 and STL10 are trained with $L_{\\infty}$ perturbation magnitude of 0.03. They are evaluated under PGD and EOT-PGD with $L_{\\infty}$ in [0,0.03]. The range of attack perturbation magnitude on CIFAR10 and STL10 could be larger, such as [0,0.08], to better compare the baselines with the proposed method.**\n\nA2. We added ablation study for attacking with higher perturbation magnitude in page 8-9. As you noted, the range of attack perturbation magnitude on CIFAR10 and STL10 could be larger. We conducted additional experiments on this larger perturbation range [0, 0.08]. The results of PGD attack are summarized as the following Tables. These results demonstrated that the proposed method outperforms the baseline models as well. The detailed performance against EOT-PGD attack was added in Figure 3 of newly uploaded paper.\n\n* Large range PGD attack on VGG16 trained with $\\delta = 8/255$ for CIFAR10\n| $\\delta$     \t|   0   \t| 0.018 \t| 0.031 \t| 0.043 \t| 0.062 \t|  0.08 \t|\n|:--------:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|\n|    adv   \t| 71.37 \t| 57.17 \t| 45.57 \t| 34.26 \t| 18.85 \t|  8.98 \t|\n|  adv-BNN \t| 65.17 \t| 54.51 \t| 46.80 \t| 38.30 \t| 26.31 \t| 15.58 \t|\n| adv-Ours \t| 74.00 \t| 61.21 \t| 51.56 \t| 41.65 \t| 27.48 \t| 15.89 \t|\n\n* Large range PGD attack on Model A trained with $\\delta = 8/255$ for STL10\n|     $\\delta$ \t|   0   \t| 0.018 \t| 0.031 \t| 0.043 \t| 0.062 \t| 0.08 \t|\n|:--------:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:----:\t|\n|    adv   \t| 60.08 \t| 32.95 \t| 20.37 \t| 13.22 \t|  6.09 \t|  2.8 \t|\n|  adv-BNN \t| 62.23 \t| 38.27 \t| 26.69 \t| 18.29 \t|  9.06 \t| 4.17 \t|\n| adv-Ours \t| 61.86 \t| 43.84 \t| 32.85 \t| 22.07 \t| 12.23 \t| 6.69 \t|"}, "signatures": ["ICLR.cc/2021/Conference/Paper773/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cue2ZEBf12", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper773/Authors|ICLR.cc/2021/Conference/Paper773/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867356, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment"}}}, {"id": "l3HmjnChrFH", "original": null, "number": 8, "cdate": 1605893886280, "ddate": null, "tcdate": 1605893886280, "tmdate": 1606103310202, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "y_Xl8eyYieW", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment", "content": {"title": "Responses for AnonReviewer4", "comment": "We really appreciate for your helpful comments and critiques on terminology. After receiving your comments, we thoroughly read the entire paper to address the enhanced clarity and readability. Thanks to your helpful comments, we believe that you will find the revised version of our paper superior to the last in the sense of using proper mathematical terminology.\n***\n\n**Q1. Misnomers with \u201ctrue posterior\u201d, \u201cconjugate prior\u201d, and \u201cregularization\u201d.**\n\nA1. To clearly deliver mathematical terms without confusion, we modified the misnomers in terms of \u201ctrue posterior\u201d, \u201cconjugate prior\u201d, and \u201cregularization\u201d. Updated version to your responses has been uploaded. We mostly modified the expressions of \u201cfind/obtain true posterior\u201d to those of \u201capproximate true posterior\u201d. Furthermore, we denoted $q(w)$ as an approximate posterior to approximate the true posterior, and modified the previous expression of $q(\\mu, \\sigma^2)$ to denote an approximate conjugate prior to approximate the conjugate prior as a same distribution family of the true posterior. Lastly, we modified the expression of \u201ca regularization term for the Gaussian parameters\u201d in page 4 to that of \u201ca regularization term for an approximate posterior\u201d. Besides, other modifications of mathematical terminology have been applied in the updated version of our paper. \n\n***\n\n**Q2. Robust Optimization is the special case of adversarial training where only adversarial data is used. The authors conflate adversarial optimization (optimization with respect to an adversarial objective) with robust optimization which has a rich history in optimization prior to its application to deep learning. The end of section 2.1 should have its terminology corrected.**\n\nA2. As you noted, Robust Optimization has a rich history in optimization theory prior to its application to deep learning. Therefore, adversarial training is just the special case of robust optimization as you mentioned. Our criteria for dividing the adversarial defense into three methods were to follow the standard categorization of previous studies. Recent papers provided a comprehensive review of adversarial attacks and defenses [1-4]. They also divided adversarial defense approaches into a number of categories including robust optimization. We contemplate that\u2019s why some misunderstandings seem to have occurred in the process of delivering our contents to readers. Thanks to your helpful comments, we modified and clarified the general description of robust optimization and adversarial training at the end of section 2.1 to prevent further misunderstanding for readers. And, we added more references to justify our statements on the terminology. The following sentences represent the end of section 2.1.\n\n*Robust optimization: it is a well-known paradigm that aims to obtain solutions under bounded feasible regions. Especially, its main interest from an adversarial perspective, is improving the classifier's robustness by changing the learning scheme of deep neural networks.*\n\n***\nReferences\n\n* [1] Wong, E., & Kolter, Z. (2018, July). Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning (pp. 5286-5295). PMLR.\n* [2] Dong, Y., Fu, Q. A., Yang, X., Pang, T., Su, H., Xiao, Z., & Zhu, J. (2020). Benchmarking Adversarial Robustness on Image Classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 321-331).\n* [3] Hao Chen, H. X. Y. M., Deb, L. D., Anil, H. L. J. L. T., & Jain, K. (2020). Adversarial attacks and defenses in images, graphs and text: A review. International Journal of Automation and Computing, 17(2), 151-178.\n* [4] Silva, S. H., & Najafirad, P. (2020). Opportunities and challenges in deep learning adversarial robustness: A survey. arXiv preprint arXiv:2007.00753."}, "signatures": ["ICLR.cc/2021/Conference/Paper773/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cue2ZEBf12", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper773/Authors|ICLR.cc/2021/Conference/Paper773/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867356, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment"}}}, {"id": "kGzv6PTNkbM", "original": null, "number": 3, "cdate": 1605887950281, "ddate": null, "tcdate": 1605887950281, "tmdate": 1606010689352, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment", "content": {"title": "Paper update overview", "comment": "We would like to show our sincere gratitude for the benefits that we derived from all reviewers. We took all the comments very seriously and regarded them as great opportunities to enhance the overall quality of our paper. We tried to address all of concerns and revised our paper accordingly. We will also answer directly to each review comment. We summarized the most notable changes in our paper:\n\n* We fixed all misnomers and correspondingly clarified the use of each terminology to avoid any misunderstanding throughout the entire paper.\n* We added additional ablation studies to address reviewers\u2019 concerns and make elaborate experiments in the modified paper.\n\nWe sincerely hope that this revision effort meets your expectations. We are happy to provide further clarification if any of the reviewers\u2019 concerns are not answered.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper773/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cue2ZEBf12", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper773/Authors|ICLR.cc/2021/Conference/Paper773/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867356, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment"}}}, {"id": "N0oYVHSWMcu", "original": null, "number": 4, "cdate": 1605889198386, "ddate": null, "tcdate": 1605889198386, "tmdate": 1606009877515, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "-YIQpmKWob3", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment", "content": {"title": "Responses for AnonReviewer1", "comment": "We are grateful for your comments and suggestions as they helped us further clarify the reason why the near zero variance occurs in the adv-BNN and normal BNN. We also appreciate for your valuable comments on the experimental settings and proper comparison with state-of-the-art models. \n***\n**Q1. This paper argues that the previous method (adv-BNN) learns the posterior distribution that has near zero variance, but does not analyze why adv-BNN causes this phenomenon. Does normal BNN models (without adversarial training) also have this issue?**\n\nA1. As you pointed out, we modified and concretized the mathematical analysis on the near zero variance phenomenon at Theoretical Analysis section in page 7. Furthermore, for your question regarding normal BNN models, you can refer to the Appendix E (clean training without adversarial examples), the statistical summaries in Clean training at Table 2, and Table 3 empirically showed normal BNN also has same phenomenon of the vanished stochasticity.\n***\n**Q2. The experimental settings are unclear in the context. The parameters of PGD and EOT-PGD are not stated (e.g., number of steps, step size, number of samples in EOT, etc.). Therefore, it is hard to judge the significance of the results.**\n\nA2. We newly changed and added detailed information of the parameters at Hyper-parameter section in page 7. For PGD attack, we set the number of steps T to 10 in adversarial training stage and 30 in test stage, and set the step size $\\eta$ to $\\frac{2.3}{T}\\delta$. For EOT-PGD attack, we equally use the hyper-parameters of PGD attack and set the number of samples to 10.\n***\n**Q3. This paper lacks the comparison with the state-of-the-art methods. A common practice is to use Wide ResNet models for adversarial training and set the $L_\\infty$ norm of perturbation as 16/255. I suggest the authors to compare with the public adversarial training models.**\n\nA3. We added ablation study for comparison with Wide ResNet models in page 8-9. You can check the modification on the newly uploaded paper. As you already mentioned, since Madry et al. (2017) [1] used Wide ResNet [2] which is a residual convolutional neural network consisting of a number of residual units and a fully connected layer [1], we performed the additional experiment for CIFAR10 and CIFAR100. We trained and compared three methods on the Wide ResNet models with adversarial training and set the $L_\\infty$ norm of perturbation as 16/255 as you suggested. We summarized results of the additional experiments on Wide ResNet against PGD attack as seen in the following Tables. The results demonstrate that the proposed method outperforms the baseline methods even on the state-of-the-art model as well. The detailed performance against EOT attack and statistical summaries of adv-BNN and adv-Ours on Wide ResNet were added in Figure 3 and Table 2 of the newly uploaded paper.\n\n* PGD attack on WideResNet trained with $\\delta = 16/255$ for CIFAR10\n| $\\delta$\t|   0   \t| 0.018 \t| 0.031 \t| 0.043 \t| 0.062 \t|  0.08 \t|\n|:--------:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|\n|    adv   \t| 57.56 \t| 49.52 \t| 44.01 \t| 38.17 \t| 29.12 \t| 21.14 \t|\n|  adv-BNN \t| 53.30 \t| 46.49 \t| 42.36 \t| 38.39 \t| 32.38 \t| 26.47 \t|\n| adv-Ours \t| 66.12 \t|  58.4 \t|  52.7 \t| 46.55 \t| 37.38 \t| 28.14 \t|\n\n* PGD attack on WideResNet trained with $\\delta = 16/255$ for CIFAR100\n| $\\delta$\t|   0   \t| 0.018 \t| 0.031 \t| 0.043 \t| 0.062 \t|  0.08 \t|\n|:--------:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|:-----:\t|\n|    adv   \t| 33.40 \t| 27.03 \t| 23.21 \t| 19.82 \t| 14.55 \t| 10.20 \t|\n|  adv-BNN \t| 26.10 \t| 21.45 \t| 19.12 \t| 16.69 \t| 13.57 \t| 10.51 \t|\n| adv-Ours \t| 37.00 \t| 29.84 \t| 25.85 \t| 21.79 \t| 16.44 \t| 12.05 \t|\n\n***\nReferences\n\n * [1] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., & Vladu, A. (2017). Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\n\n * [2] Zagoruyko, S., & Komodakis, N. (2016). Wide residual networks. arXiv preprint arXiv:1605.07146.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper773/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cue2ZEBf12", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper773/Authors|ICLR.cc/2021/Conference/Paper773/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867356, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment"}}}, {"id": "jVmnywlKoaE", "original": null, "number": 6, "cdate": 1605891201510, "ddate": null, "tcdate": 1605891201510, "tmdate": 1605981381314, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "yZuA3QFEXSj", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment", "content": {"title": "Responses for AnonReviewer2 (cont')", "comment": "**Q2. The approach might be less related to the topic of \"hierarchical inference\", as it's only replacing the standard Gaussian with a normal-inverse-gamma distribution, which only affects the KL divergence in this case, where there are no intermediate variables are inferred.**\n\nA2. Figure 1(b) represents hierarchical inference to sample weight parameters with $\\mu_0$, $\\sigma_0$, $\\psi$, and $\\nu$ in the approximate conjugate prior, such that $w=\\mu(=\\mu_{0}+\\sigma_{0}\\epsilon_{\\mu})+\\sigma (=(\\psi\\epsilon_{\\sigma^2})^{1/2})\\epsilon$. By doing the optimization described in Section 3, Theoretical Analysis in page 7 demonstrated that the variance converges to zero: $\\sigma^2 \\approx 0$, then it satisfies $w=\\mu$. Here, the mean $\\mu$ is sampled from the Normal distribution $q(\\mu)$ in the approximate conjugate prior $q(\\mu, \\sigma^2)$ such that $w=\\mu=\\mu_0+\\sigma_0\\epsilon_{\\mu}$. Although the formulations of sampling the weight parameters are explicitly same between variational inference and hierarchical variational inference, $\\mu$ and $\\mu_0$ are implicitly different parameters because $\\mu$ denotes the mean in the approximate posterior for variational inference, and $\\mu_0$ indicates the mean prior knowledge in the approximate conjugate prior for hierarchical variational inference. In short, the proposed method deals with hierarchical parameters to sample the weight parameters where $\\mu$ and $\\mu_0$ are different in nature. Furthermore, this zero variance in the conjugate view has an advantage of no additional computational complexity compared with adversarial-BNN, while achieving more robust performance.\n***\n**Q3. Minor: Some of the notations and equations are a bit unclear. For example, q has been used to denote the posterior but it denotes the prior in Eq. (5).**\n\nA3. We modified the mathematical terms to clearly deliver them. As the approximate posterior $q(w)$ approximates the true posterior, we modified previous expression of $q(\\mu, \\sigma^2)$ to denote it as an approximate conjugate prior which is approximating the conjugate prior as a same distribution family of the true posterior. And, our original intention of denoted $q$ more focuses the meaning of \"approximate\" rather than whether it is posterior or prior. Updated version has been uploaded. And, you can check our modification that you were concerned with before."}, "signatures": ["ICLR.cc/2021/Conference/Paper773/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Cue2ZEBf12", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper773/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper773/Authors|ICLR.cc/2021/Conference/Paper773/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923867356, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Comment"}}}, {"id": "-YIQpmKWob3", "original": null, "number": 1, "cdate": 1603873056844, "ddate": null, "tcdate": 1603873056844, "tmdate": 1605024609212, "tddate": null, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "invitation": "ICLR.cc/2021/Conference/Paper773/-/Official_Review", "content": {"title": "A sound method with unclear experimental settings", "review": "This paper studies the adversarial robustness of DNNs with Bayesian neural networks. Although BNN has been integrated with adversarial training for better robustness, this paper argues that the previous method lacks the stochasticity (i.e., the posterior tends to have zero variance), thus limiting the robustness performance. In this paper, a new hierarchical variational inference is proposed to enhance the robustness when integrating with adversarial training. The proposed method is presented well. The experiments show the effectiveness of the proposed method.\n\nBesides, I have few concerns about this paper.\n\n1. This paper argues that the previous method (ADV-BNN) learns the posterior distribution that has near zero variance, but does not analyze why ADV-BNN causes this phenomenon. Does normal BNN models (without adversarial training) also have this issue?\n\n2. The experimental settings are unclear in the context. The parameters of PGD and EOT-PGD are not stated (e.g., number of steps, step size, number of samples in EOT, etc.). Therefore, it is hard to judge the significance of the results.\n\n3. This paper lacks the comparison with the state-of-the-art methods. A common practice is to use Wide ResNet models for adversarial training and set the L_infty norm of perturbation as 16/255. I suggest the authors to compare with the public adversarial training models.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper773/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper773/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference", "authorids": ["~Byung-Kwan_Lee1", "~Youngjoon_Yu1", "~Yong_Man_Ro1"], "authors": ["Byung-Kwan Lee", "Youngjoon Yu", "Yong Man Ro"], "keywords": [], "abstract": "Recent works have applied Bayesian Neural Network (BNN) to adversarial training, and shown the improvement of adversarial robustness via the BNN's strength of stochastic gradient defense. However, we have found that in general, the BNN loses its stochasticity after its training with the BNN's posterior. As a result, the lack of the stochasticity leads to weak regularization effect to the BNN, which increases KL divergence in ELBO from variational inference. In this paper, we propose an enhanced Bayesian regularizer through hierarchical variational inference in order to boost adversarial robustness against gradient-based attack. Furthermore, we also prove that the proposed method allows the BNN's stochasticity to be elevated with the reduced KL divergence. Exhaustive experiment results demonstrate the effectiveness of the proposed method by showing the improvement of adversarial robustness, compared with adversarial training (Madry et al., 2018) and adversarial-BNN (Liu et al., 2019) under PGD attack and EOT-PGD attack to the $L_{\\infty}$ perturbation on CIFAR-10/100, STL-10, and Tiny-ImageNet.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "lee|towards_adversarial_robustness_of_bayesian_neural_network_through_hierarchical_variational_inference", "supplementary_material": "/attachment/52a4231e8b47fa50d5d7f8003d07525a6cfdbb7c.zip", "pdf": "/pdf/3f20076ebd7a838557827524a961ad3780512d2d.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=5SJ0aogyNp", "_bibtex": "@misc{\nlee2021towards,\ntitle={Towards Adversarial Robustness of Bayesian Neural Network through Hierarchical Variational Inference},\nauthor={Byung-Kwan Lee and Youngjoon Yu and Yong Man Ro},\nyear={2021},\nurl={https://openreview.net/forum?id=Cue2ZEBf12}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Cue2ZEBf12", "replyto": "Cue2ZEBf12", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper773/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538135376, "tmdate": 1606915784361, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper773/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper773/-/Official_Review"}}}], "count": 12}