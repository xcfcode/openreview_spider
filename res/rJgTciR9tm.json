{"notes": [{"id": "rJgTciR9tm", "original": "r1xdOVh9tQ", "number": 576, "cdate": 1538087829252, "ddate": null, "tcdate": 1538087829252, "tmdate": 1545355440343, "tddate": null, "forum": "rJgTciR9tm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "r1xDD0cLl4", "original": null, "number": 1, "cdate": 1545150046731, "ddate": null, "tcdate": 1545150046731, "tmdate": 1545354477041, "tddate": null, "forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Meta_Review", "content": {"metareview": "The reviewers reached a consensus that the paper is not ready for publication in ICLR. (see more details in the reviews below. )", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper576/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper576/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353165092, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper576/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353165092}}}, {"id": "rklKBAMq0m", "original": null, "number": 6, "cdate": 1543282241436, "ddate": null, "tcdate": 1543282241436, "tmdate": 1543282241436, "tddate": null, "forum": "rJgTciR9tm", "replyto": "SJesJZWtRQ", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "content": {"title": "Clarification", "comment": "We would like to thank the reviewer for reading our revised version and for the feedback. We believe that the current version of the paper, in which we have incorporated a small change, clarifies the ambiguity regarding the convergence of the procedure.\n\n1-     Due to space limitations, we have just restricted ourselves to a brief overview of concepts (IB and sentiment analysis) that have been introduced previously in the literature + citing all the most relevant papers that provide all the details.\n\n2-    To recall, the idea of finding a solution is to use the self-consistent equations derived in Theorem 1 to develop the iterative procedure stated in Corollary 1. The convergence of the proposed procedure is claimed in Lemma 1. By construction, the iterative procedure is convergent to a stationary point since the self-consistent equations are derived by setting the derivatives of the functional (4) to zero. To avoid further confusion, we clarify this by explicitly writing it in the statement of Lemma 1. Indeed, if the procedure converges, then the point of convergence is necessarily a stationary point, that satisfies the self-consistent equations hence the derivatives of the functional (4) with respect to p(B_{k}|X_{k-1}) and p(B_{k+1}|B_{k}) are zero. We are confident that the proof of the convergence is correct, and we observe the same behavior in numerical as well as real-world results. We would also like to emphasize that the key idea in the developed proof is somewhat (but not directly) similar to the Expectation-Maximization (EM) technique, as mentioned by the cited works of Blahut-Arimoto and Tishby. In the original EM, the E- and M-steps can both be alternatively interpreted as the maximization of the objective (log likelihood), as observed in many existing works, for example, equation (7a), (7b) in [1]. Hence, one EM iteration is marginal maximization over two variables; (i) conditional probability distribution, and (ii) parameters of the model. In our case, one update of the algorithm in Corollary 1, involves maximization over four variables, p_1, p_2, \\phi_1, \\phi_2, as detailed in Appendix C, which is performed marginally as well.\n\nAs noted in the well-developed literature of EM algorithm, the convergence is almost always to a stationary point with such marginal maximization (or minimization in our case) technique due to the self-consistent nature of the equations ([2] Section 3.4.1 Page-79). We also like to emphasize that in such techniques of marginal maximization over multiple variables (here probability distributions), it has been realized that in most of the cases, the stationary convergence point is indeed a local maximum (for EM), and thereby local minimum in our case. A small random perturbation at an arbitrary stationary point can lead the marginal maximization algorithm to diverge (Page-80, [2]). We agree that directly claiming local minimum in our case is optimistic and hence we rephrase the sentence to be more careful. We want to stress that this does not change any novel formulation, results, and proofs of the work.\n\n3-    Finally, we would like to emphasize that the conditions presented in Theorem 1 are derived by equating the derivative to zero, as presented in the proof. Our iterative procedure is derived from Theorem 1, and it converges to a point where the derivatives are zero (or stationary point), since it performs minimization based on the gradient approach. Indeed, we are not just using the monotonicity and lower bound to claim the convergence to a stationary point but also, by definition, we use the structure of the problem where the self-consistent equations are derived from minimizing the functional in (4). Therefore, the example provided by the reviewer will not occur in our analysis as \u20181\u2019 is not a stationary point of f(x) = x^2.\n\n[1] Sam Roweis and Zoubin Ghahramani, \u201cAn EM algorithm for identification of nonlinear dynamical systems,\u201d 2000.\n\n[2] G. McLachlan and T. Krishnan, \u201cThe EM Algorithm and Extensions.\u201d John Wiley & Sons, New York, 1996."}, "signatures": ["ICLR.cc/2019/Conference/Paper576/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607209, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTciR9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper576/Authors|ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607209}}}, {"id": "SJesJZWtRQ", "original": null, "number": 5, "cdate": 1543209186563, "ddate": null, "tcdate": 1543209186563, "tmdate": 1543209186563, "tddate": null, "forum": "rJgTciR9tm", "replyto": "SkeHfY9qaQ", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "content": {"title": "Still confusing", "comment": "I read the authors' response. It clarifies a few things, and they have revised the text, which I have skimmed, and it is better, but I think it still could use more clarification.  As most reviewers agree, the writing could be more clear, and you could discuss things more slowly, e.g., the IB background, then multimodal sentiment intensity (even in the revised version, there is still not nearly enough setup for this -- it is completely new to me, and to many other readers as well I would guess).\n\nThe math is still sloppy in places, even if it's not necessarily wrong. For example, lemma 1 seems straightforward, and I have no issues with the proof technique (lower bound + monotonicity). To recall, it says: \"Lemma 1. The iterative procedure in Corollary 1 to minimize the functional F in (4) is convergent.\"\n\nBut then the text says \"The convergence of the iterations in Corollary 1, and hence existence of a minimizing solution to (4), is established through the following result. ... [then states Lemma 1]\"\n\nI take issue with the \"hence existence of a minimizing solution\". How did you infer this? Corollary 1 says nothing about *what* the sequence converges too. You could be minimizing the function x^2, and your sequence could be x_n = 1 + 1/n. This is monotonic and lower bounded, but it converges to x=1, not x=0.  I am guessing that the authors are aware of this, and it is just an issue of how the phrased things, but as written, this paper is still just too confusing.\n\nWhile I am not that excited about this version of the paper, I think the ideas and results are good, so I strongly recommend that the authors resubmit a longer, more clear version to a different venue."}, "signatures": ["ICLR.cc/2019/Conference/Paper576/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper576/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607209, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTciR9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper576/Authors|ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607209}}}, {"id": "r1eaEaAeAX", "original": null, "number": 4, "cdate": 1542675764671, "ddate": null, "tcdate": 1542675764671, "tmdate": 1542675764671, "tddate": null, "forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "content": {"title": "Changes Summary ", "comment": "We address the reviewers comments, in detail, individually in the responses, and a brief overview of the changes in the revised version are as follows:\n\n1. The abstract and introduction are modified to reflect the problem setup and our proposed novel ideas better.\n\n2. We further elaborate the Figure 1, thanks to the suggestions of the Reviewer-3\n\n3. The constraints are further elaborated for the optimization problem in (3)\n\n4. The proof of the Theorem 1 has been moved to the Appendix\n\n5. The proposed iterative solution (initially present in the Appendix) is now termed as Corollary 1 for a better connection with the text of Section 3.1\n\n6. The real-world experiment of sentiment prediction in Section 5 is further elaborated in terms of the problem setup, prior works. Further, we add more explanation of the applied techniques of the proposed work"}, "signatures": ["ICLR.cc/2019/Conference/Paper576/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607209, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTciR9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper576/Authors|ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607209}}}, {"id": "S1gaSt9caQ", "original": null, "number": 3, "cdate": 1542265156568, "ddate": null, "tcdate": 1542265156568, "tmdate": 1542265156568, "tddate": null, "forum": "rJgTciR9tm", "replyto": "B1gi5FcQsX", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "content": {"title": "Response to Reviewer-3", "comment": "We are extremely grateful for the careful reading of the manuscript and would like to thank the reviewer for appreciating the novelty of the proposed results. We also would like to clarify the concerns raised by the reviewer regarding the technical contribution and hope that we would converge by utilizing this discussion period. We now address the comments point-wise as follows.\n\n1. We thank the reviewer for the suggestion of drawing the complete Markov Chain, and to explain the part of the problem that we are solving. We have modified the Figure 1 in the revised manuscript. We now like to mention that one of the main contributions of the paper is to write the IBH as an optimization problem. An advantage of such formulation is that it can be very well generalized to any number of hops (as mentioned in the manuscript, ending of Sec-3.1). In this work, we have introduced all these concepts and solved the problem for three variable case i.e. X_{k-1}-X_{k}-X_{k+1}. The IBH solution can be derived for the general case, by appropriately differentiating the information terms. For example, a generalized differentiation can be written as \n\\begin{equation}\n\\frac{\\delta I({\\bf B}_{k};{\\bf B}_{k+1})}{\\delta p({\\bf B}_{j+1}\\vert {\\bf B}_{j})} = p({\\bf B}_{j})(\\mathbb{E}_{{\\bf B}_{j+2}\\vert {\\bf B}_{j+1}}\\ldots\\mathbb{E}_{{\\bf B}_{k}\\vert {\\bf B}_{k-1}}[D_{KL}(p({\\bf B}_{k+1}\\vert {\\bf B}_{k})\\vert\\vert p({\\bf B}_{k+1}))]-1),\\quad\\forall 1\\leq j < k.\n\\end{equation}\n\nThe mathematical generalization of the IBH is already derived, and now we are looking for some interesting datasets which can benefit from larger than three hop dynamical systems. The generalization with new real-world datasets will be presented in the future as the full version.\n\n2. We agree with the reviewer that the optimization problem (3) is critical to understand the IBH principle. Towards that, we would like to mention the following.\na) There is a typo in the manuscript, what we meant to say is \"\\epsilon_1 bounds the accuracy of the prediction of X_k by B_k\", since B_k is is the bottleneck corresponding to the first hop (X_{k-1}-X_k), therefore it is designed such that it better predict the X_k by compressing the information from X_{k-1}.\n\nb) The Markov chains are correct in the sense that, we design B_{k+1} such that all the information that B_{k+1} received is only through B_{k}, to construct an alternate dynamical process of B_{k}'s. In other words, this means that B_{k} is the parent of B_{k+1} (from a Bayesian network perspective) and hence B_{k+1} is independent of all other RV in consideration given B_{k}.\n\nc) The primary purpose of the constraints in the optimization problem (3) is to lower bound the prediction accuracies (to guarantee accurate predictions) and upper bound the compression at each hop of the alternate dynamical system (IBH). We note that only signs of the associated terms matter in this respect (as far as writing the Lagrangian is concerned), however, the mutual information constants in the constraints are carefully chosen by reverse-engineering from the proof of the Lemma 1. It has been shown in equation (21)-(22) that these differences are actually KL-divergence terms and hence guaranteed to be positive. This formulation also guarantees the lower boundedness of the functional in (4). Therefore, there is no need for the absolute difference in our framework as we only need the inequalities in one appropriate direction.\n\n3. The reviewer is right in saying that \\Phi matrix captures information flow from X_{k-1} to B_{k}, and \\Delta is from B_{k} to B_{k+1}. We now note that B_{k} is designed to tap the information flow from X_{k-1} to X_{k} (here, from text to audio), and B_{k+1} wish to receive information from B_{k} to explain X_{k+1} (visual). Hence, \\Delta has correlations from all three modalities because it receives correlations from text-audio and uses them to explain the visual modality. This is why \\Delta is used for the sentiment prediction. A further explanation is added in the revised manuscript in the experiments sections.\n\n4. Also, we have made a small cosmetic change of moving the Theorem 1 proof to the appendix."}, "signatures": ["ICLR.cc/2019/Conference/Paper576/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607209, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTciR9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper576/Authors|ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607209}}}, {"id": "SkeHfY9qaQ", "original": null, "number": 2, "cdate": 1542265100909, "ddate": null, "tcdate": 1542265100909, "tmdate": 1542265100909, "tddate": null, "forum": "rJgTciR9tm", "replyto": "BkebP2PthQ", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "content": {"title": "Response to Reviewer-2", "comment": "We want to thank the reviewer for the detailed reading of the manuscript. We agree with the reviewer that the presentation of the novel results can be improved with further clarifications/modification of the text. With the unique opportunity provided by open-review and the discussion period, we are positive that all the raised comments and concerns can be addressed to further strengthen the manuscript. We have revised the paper, and now we further elaborate the comments as follows.\n\n1. The details of the used compression term were first provided in the original IB paper (Tishby et a. 2000), where the authors have explained that the bottleneck variable (B) is determined via a stochastic mapping p(B|X) such that it produces a soft-clustering of the input variable (X). It has been realized that we are not looking at minimization of the entropy of the bottleneck variable (which is the lower bound on the number of bits needed to specify B in a codebook). Instead, we look for the expected number of bits needed to specify the new variable (B) in a codebook without confusion, i.e., a mapping from X to B, such that X's can be clustered to B (hence compression), and the clusters are distinguishable. In other words, this is equivalent to the number of B's in the codebook such that they represent distinguishable clustering of X, which is equal to vol(X)/vol(X mapped to the same B) = 2^I(X; B) using standard asymptotic equipartition property arguments.\n\nIt can be noted that such compression can lead to spurious results, as we can very well choose to throw away all the details and reduce the required bits to zero (by mapping all of the input to a single cluster). So, the compression has to be done with some constraints of prediction (correlation) accuracies which appear through the second term in equation (2). The combined tradeoff is now a variational problem which is what is known as Information bottleneck problem, or equation (2). Further details are provided in the original IB paper.\n\n2. The objective in equation (4) is a functional (function of functions), and the arguments are the associated probability distributions in the optimization problem (3). We have further clarified by explicitly writing the variables in the equation (4) in the revised manuscript.\n\n3. In Theorem 1 what we meant to say is the conditions for the optimal solution (now revised). However, since we noted that the functional (4) might not be convex in the product space of the probabilities, we can only come up with a locally optimal solution.\n\nWe have made some cosmetic changes in this section, by moving the Theorem 1 proof to the appendix (to get more space for explanation). We also specified clearly the iterative solution (which previously was a section in the Appendix) as Corollary 1. We also emphasize that in the proof of the Lemma 1 we have already proved that the functional is lower bounded. We also show that each iteration monotonically reduces the functional, hence we are guaranteed to find a solution since convergence is guaranteed.\n\n4. The experimental section is clarified further in the revision. The goal of the multimodal sentiment intensity dataset is to predict the sentiment of the speaker using the three available modalities (text, audio and visual). We have mapped the modalities into a dynamical system and then used IBH to compress the high-dimensional data and identify the complex correlations across the modalities using the \\Delta matrix.\n\n5. The manuscript is revised to take care of the presentation of the paper, especially Introduction, Sec-3.1, Sec-3.2.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper576/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607209, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTciR9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper576/Authors|ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607209}}}, {"id": "S1gJpd59pQ", "original": null, "number": 1, "cdate": 1542265015190, "ddate": null, "tcdate": 1542265015190, "tmdate": 1542265015190, "tddate": null, "forum": "rJgTciR9tm", "replyto": "B1lW4mEC2X", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "content": {"title": "Response to Reviewer-1", "comment": "We appreciate the reviewers' views on the novelty and usefulness of the proposed results. We are also thankful for the careful reading of the manuscript, and comments which have led us to improve the presentation of the results further. We provide clarifications as follows, and would like to emphasize that the raised concerns can be addressed by taking advantage of the discussion period.\n\n1. The abstract and introduction were modified to present the original ideas and the problem statement better.\n2. The experiments section is further elaborated to have prior works and further intuition of the developed approach. We wish to claim that the proposed approach (by design) can identify complex inter- and intra- dependencies across spatio-temporal states of the dynamical system. Hence, representing the modalities as a linear dynamical system, then allow us to come up with an alternate compact dynamical system which can be used to perform discriminative learning of the speaker sentiment using machine learning classifiers like SVM.\n3. By one-BN, we mean designing an Information bottleneck between any consecutive pair of RVs in the given Markov Chain. The IBH designs series of bottlenecks jointly across the entire given dynamical system. In Figure 2, we try to emphasize that the joint design performs better than the local (or individually/independently designed) bottlenecks across the hops of the dynamical system. We elaborate this further in the simulation results section of the revised manuscript.\n4. Also, we have made a small cosmetic change of moving the Theorem 1 proof to the appendix.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper576/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621607209, "tddate": null, "super": null, "final": null, "reply": {"forum": "rJgTciR9tm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper576/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper576/Authors|ICLR.cc/2019/Conference/Paper576/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers", "ICLR.cc/2019/Conference/Paper576/Authors", "ICLR.cc/2019/Conference/Paper576/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621607209}}}, {"id": "B1lW4mEC2X", "original": null, "number": 3, "cdate": 1541452584991, "ddate": null, "tcdate": 1541452584991, "tmdate": 1541533875470, "tddate": null, "forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Review", "content": {"title": "review", "review": "This paper studies the problem of compactly represent the model of a complex dynamic system while preserving information. The method is based on the information bottleneck method. Basically, for a dynamic system whose states changing from X_{k-1}, X_k to X_{k+1}, the \"information bottleneck hierarchy\" method learns a variable B_k and B_{k+1} such that B_k predicts B_{k+1} well, B_k predicts X_k well, and B_{k+1} predicts X_{k+1} well, while minimizing the information of X_{k-1} contained in B_k. \n\nIn my opinion, this is a very interesting framework for representing and learning a dynamic system. The paper then considers simple examples on a linear model with Gaussian noise and show that the IBH method performs better than the one-BN method. The simulation and the experiments on real data all show very good performance (even with the simple linear Gaussian estimator).\n\nThe reason that I give such a rating is that of the confusing writing.\n* In the abstract, it is unclear what the goal is. For example, the second and third sentence do not explain the first sentence: \"the task is a crucial task\".\n* Introduction is also very confusing. It seems there is not a good logic connecting each sentence.\n* The paper does not give a good survey of other methods performing similar tasks, e.g., the ones the paper are comparing to in the experiment section. Therefore, it is hard to compare or to understand why the previous methods are worse.\n* Figure 2: one-BN is not well defined. How do you design the IB locally? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper576/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Review", "cdate": 1542234429592, "expdate": 1552335264000, "duedate": 1541116800000, "reply": {"forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335755907, "tmdate": 1552335755907, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkebP2PthQ", "original": null, "number": 2, "cdate": 1541139545365, "ddate": null, "tcdate": 1541139545365, "tmdate": 1541533875264, "tddate": null, "forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Review", "content": {"title": "Preliminary, with 1 promising experiment, but unclear and vague", "review": "The paper proposes a method to learn the conditional distribution of a random variable in order to minimize and maximize certain mutual information terms.  Interestingly, the proposed method can be applied to sentiment prediction and outperforms a 2018 method based on SVM.\n\nOverall, the ideas seem intriguing, and the results seem promising, but I really cannot understand what the paper is saying, and I think the paper would be much stronger if it was written more clearly (to make individual sentences more clear, but also to make the broader picture more clear). Not only is the writing hard to understand (some sentences lack a verb!), but it is vague, and the notion of a \"complex system\" is never defined.  It seems that the technique can be applied to any (potentially non-stationary) Markov process?\n\nAdditionally, due to the lack of clarity in the writing and lack of mathematical rigor, Theorem 1 does not seem to be true as stated. I think this is an issue of stating the assumptions, and not due to a mistake in the derivation.  Right now, the actual conclusion of theorem 1 is not even clear to me.\n\nQuality: poor/unclear\nClarity: very poor\nOriginality: unclear, perhaps high? Not clear how related it is to the methods of Tishby et al.\nSignificance: unclear, as clarity was poor, and there was minimal discussion of alternative methods.\n\nSpecific points:\n\n- Eq (2), the first term is included because it is for the \"information compression task\", but I do not understand that. Where is the actual compression?  This is not traditional compression (turning a large vector into a smaller vector), but more like turning one PDF into a PDF with lower entropy?\n\n- This paper seems to fall into the subfield of system identification (at which I am not an expert), so I'd expect to see some related literature in the field. The only compared method was the IF method of Tishby et al. from 18 years ago (and the current work seems to be a generalization of that).\n\n- Equation (4): what exactly is the object being minimized? Is it a PDF/probability measure? Is it an *instance* of a random variable?  If it is a PDF, is it the PDF of B_k | X_{k-1} ?\n\n- The statement of Theorem 1 is either too vague or wrong. To say \"The solution... is given by\" makes it sound like you are giving equations that define a unique solution. Perhaps you mean, \"Any solution ... must necessarily satisfy...\" ? And that is not clearly true without more work. You are basically saying that any minimizer must be a stationary point of the objective (since you are not assuming convexity). It seems everything is differentiable?  How do you know solutions even exist -- what if it is unbounded? In that case, these are not necessary conditions.\n\n- Lemma 1: \"The iterative procedure... is convergent.\"  The iterative procedure was never defined, so I don't even know what to make of this.\n\n- Section 3.2: \"As proved by prior work, the optimum solution obtained by a stochastic transformation that is jointly Gaussian with bottleneck's input.\"  I do not know what you are trying to say here. There's no predicate.\n\n- Section 4 wasn't that interesting to me yet, since it was abstract and it seemed possible that you make a model to fit your framework well. But section 5 is much better, since you apply it to a real problem. However, what you are actually solving in section 5 is unclear. The entire setup is poorly described, so I am very confused.\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper576/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Review", "cdate": 1542234429592, "expdate": 1552335264000, "duedate": 1541116800000, "reply": {"forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335755907, "tmdate": 1552335755907, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "B1gi5FcQsX", "original": null, "number": 1, "cdate": 1539709331199, "ddate": null, "tcdate": 1539709331199, "tmdate": 1541533875013, "tddate": null, "forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "invitation": "ICLR.cc/2019/Conference/-/Paper576/Official_Review", "content": {"title": " ICLR 2019 Conference Paper576 AnonReviewer3", "review": "This paper studied an extension of the Information Bottleneck Principle called Information Bottleneck Hierarchy (IBH).  The goal of IBH is to extract meaningful information from a Markov Chain. Then the authors studied case of the Gaussian linear dynamic and proposed an algorithm for computing the IBH. Then an experiment was conducted to show the usage of IBH to practical problems.\n\nGenerally I like the idea of extending Information Bottleneck to dynamic systems and I think the experiment is interesting. But I have some major questions to the paper and these questions are important about the principle you are proposing.\n\n1. About Figure 1, there is a link between X_{k-1} and B_k, but there are no link between X_k and B_{k+1}. I understand what you said --- B_k needs to compress X_{k-1} and delivers information to B_{k+1}. My question is ---- Figure 1 can not be generated to a longer Markov Chain. It seems that the principle you proposed only works for 3 random variables X_{k-1}-X_k-X_{k+1}, which weaken the principle a lot. Please draw a longer Markov Chain like Figure 1 to illustrate your principle.\n\n2.  About the \\epsilon_{1,2,3} in formula (3). \\epsilon_1 is claimed to bound the accuracy of the prediction of X_k by B_{k-1}, but where not B_{k-1} appear in the formula (actually B_{k-1} is not even in Figure 1)? \\epsilon_3 is claimed to define the closeness of prediction of X_{k+1} by B_{k+1}, but why does I(X_{k-1},X_{k+1}) need to be small? In the \"Markov chains are considered\" before formula (3), there are some typos, for example, X_{k+1}-B_k-B_{k+1} seems not a Markov Chain. Also why you are bounding the difference of two mutual informations, but not take the absolute value (I think the difference you are considered are not guaranteed to be non-negative)? I think formula (3) is the key to understand the IBH principle, but it is not well illustrated for the readers to understand.\n\n3. I understand that you can only derive an algorithm for Gaussian linear dynamic, since non-Gaussian case might be difficult and Gaussian linear dynamic might be good enough for modeling real random processes. But I wonder what is the physical or practical meaning for the matrices \\Psi and \\Delta? Why \\Delta can be used to predict sentiment intensity in your experiment? It seems that \\Delta carries the information from B_k to B_{k+1}, so it is only one-hop information and the sentiment intensity involves multi-hop information. How do you combine the different \\Delta for different hops to predict sentiment intensity? These questions are not well illustrated in the paper.\n\nSo I think the paper can be accepted if the author can provide some more insightful illustrations, especially for Figure 1, formula (3) and the experiment. But overall I think the idea in this paper is interesting, if well illustrated.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper576/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy", "abstract": "Extracting relevant information, causally inferring and predicting the future states with high accuracy is a crucial task for modeling complex systems. The endeavor to address these tasks is made even more challenging when we have to deal with high-dimensional heterogeneous data streams. Such data streams often have higher-order inter-dependencies across spatial and temporal dimensions. We propose to perform a soft-clustering of the data and learn its dynamics to produce a compact dynamical model while still ensuring the original objectives of causal inference and accurate predictions. To efficiently and rigorously process the dynamics of soft-clustering, we advocate for an information theory inspired approach that incorporates stochastic calculus and seeks to determine a trade-off between the predictive accuracy and compactness of the mathematical representation. We cast the model construction as a maximization of the compression of the state variables such that the predictive ability and causal interdependence (relatedness) constraints between the original data streams and the compact model are closely bounded. We provide theoretical guarantees concerning the convergence of the proposed learning algorithm. To further test the proposed framework, we consider a high-dimensional Gaussian case study and describe an iterative scheme for updating the new model parameters. Using numerical experiments, we demonstrate the benefits on compression and prediction accuracy for a class of dynamical systems. Finally, we apply the proposed algorithm to the real-world dataset of multimodal sentiment intensity and show improvements in prediction with reduced dimensions.", "keywords": ["compact representation", "perception", "dynamical systems", "information bottleneck"], "authorids": ["ggaurav@usc.edu", "znaidi@usc.edu", "pbogdan@usc.edu"], "authors": ["Gaurav Gupta", "Mohamed Ridha Znaidi", "Paul Bogdan"], "TL;DR": "Compact perception of dynamical process", "pdf": "/pdf/e5791f91c11f0cd936d11349d7798de9de7636eb.pdf", "paperhash": "gupta|learning_information_propagation_in_the_dynamical_systems_via_information_bottleneck_hierarchy", "_bibtex": "@misc{\ngupta2019learning,\ntitle={Learning Information Propagation in the Dynamical Systems via Information Bottleneck Hierarchy},\nauthor={Gaurav Gupta and Mohamed Ridha Znaidi and Paul Bogdan},\nyear={2019},\nurl={https://openreview.net/forum?id=rJgTciR9tm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper576/Official_Review", "cdate": 1542234429592, "expdate": 1552335264000, "duedate": 1541116800000, "reply": {"forum": "rJgTciR9tm", "replyto": "rJgTciR9tm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper576/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335755907, "tmdate": 1552335755907, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper576/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}