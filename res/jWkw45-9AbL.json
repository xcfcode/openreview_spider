{"notes": [{"id": "jWkw45-9AbL", "original": "zSdzofRhNFy", "number": 2042, "cdate": 1601308224936, "ddate": null, "tcdate": 1601308224936, "tmdate": 1615769499669, "tddate": null, "forum": "jWkw45-9AbL", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "ZvDVfBMj5D7", "original": null, "number": 1, "cdate": 1610040470898, "ddate": null, "tcdate": 1610040470898, "tmdate": 1610474074911, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Oral)", "comment": "The paper studies the problem of being able to control text generated by pre-trained language models.\nThe problem is timely and important. The paper   frames the problem as constraint satisfaction over a probability distribution. Both pointwise and distributional constraints can be imposed. The proposed algorithm,  Generation with Distributional Control (GDC), is elegant, and is an interesting new addition to this line of work. Overall, the paper brings forth news ideas, and could have impact.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040470885, "tmdate": 1610474074893, "id": "ICLR.cc/2021/Conference/Paper2042/-/Decision"}}}, {"id": "BnstUZEm_ix", "original": null, "number": 2, "cdate": 1604011191027, "ddate": null, "tcdate": 1604011191027, "tmdate": 1607383925976, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Review", "content": {"title": "Novel idea for controlled text generation", "review": "The authors addressed my concern so I increased my score to 8. \n\n-----------------------\n\nThis is a very interesting idea for controlling a pretrained model for some sort desired criteria. The authors argue that existing approaches for this have taken a pointwise view for instance using REINFORCE to optimize for a particular reward. This can lead models to over-optimize on the criteria and sacrifice diversity and other criteria. \n\nThe authors instead propose to take a distributional view. Given the pretrained LM distribution a, they would like to find a distribution c as:\n\np = arg min_{c\u2208C} D_KL(c, a)\n\nwhere C is a set of distributions that pass the constraints. Some of these constraints are point-wise but some are distributional. For instance when generating biographies, the authors would like a constraint e.g. X% should talk about a certain gender or occupation. \n\nThe authors describe how their approach leads them to an EBM (energy based model) and subsequent derivations. I think some of this section could be better written for those who are not familiar with EBMs.\n\nThe experiments are quite interesting and show how the author's \"soft\" approach allows them to elegantly adjust the distribution of the LM without degeneration.\n\nPros:\n-Very interesting idea. \n-Thorough experiments. In addition to comparing with REINFORCE based methods,  the authors also compare with CTRL and PPLM in the appendix. \n\nCons:\n-I think the method section (especially the optimization part)  could be explained better for readers who are not familiar with EBM, and allow the paper to have more accessibility. ", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105374, "tmdate": 1606915792271, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2042/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Review"}}}, {"id": "LTPuqAeR6WN", "original": null, "number": 9, "cdate": 1606052690174, "ddate": null, "tcdate": 1606052690174, "tmdate": 1606052730344, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "SV4CQDHKuCk", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment", "content": {"title": "The response and update are helpful ", "comment": "The authors' response addressed my questions. Updating the paper by following the discussions in the response helps. Thank you! "}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jWkw45-9AbL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2042/Authors|ICLR.cc/2021/Conference/Paper2042/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment"}}}, {"id": "aIGqpnOoNR", "original": null, "number": 5, "cdate": 1605824622867, "ddate": null, "tcdate": 1605824622867, "tmdate": 1605866529966, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "u-5rrhMrpgv", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "Thank you for the valuable feedback. we answer the questions in order:\n\n\n> In the Figure 4 ... By corpus does it mean the original training corpus? or the generated corpus by GPT-2?\n\nFigure 4 is a ``Zipf-like\u2019\u2019 analysis that reports token frequency across a set of 68000 samples generated by each method. In other words, we sample a collection of texts from each of the four models. Then, we compute the frequency of each token across each of the four generated corpora and its associated frequency rank. The goal is to show which approach generates more diverse tokens (longer tails, meaning less mass is concentrated on high-frequency tokens), and therefore has higher overall text diversity.\n\n> What does the freq signifies here? The authors should provide some discussion regarding the same.\n\nWe note that figure 4 is computed only over samples that satisfy the constraint (in order to guarantee a fair comparison of diversities, since all models don't have the same constraint satisfaction level and more constraint satisfaction means less diversity). The frequency here is how often a  certain vocabulary token (not a sentence) is repeated throughout the generated collection of samples. As indicated above, longer tails mean that more vocabulary tokens are represented, indicating more textual diversity. \nWe have expanded the caption to clarify the meaning of Figure 4.\n\n> Are the sentences generated sequentially keeping the context of the previously generated sentences? or they do not have any context of the previously generated sentence? If each generated sentences are independent from previously generated sentences, how meaningful it is to impose distribution constraint on that?\n\nThroughout our evaluation, we generate textual samples with a fixed length, where a given sample is independent of the previous samples (the model\u2019s hidden states are re-initialized each time). We also note that one sample can contain one or more sentences. In that sense, distributional constraints are imposed over sets of independent textual samples, each of which may consist of several sentences.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jWkw45-9AbL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2042/Authors|ICLR.cc/2021/Conference/Paper2042/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment"}}}, {"id": "VJeZz7s8Clw", "original": null, "number": 7, "cdate": 1605825599776, "ddate": null, "tcdate": 1605825599776, "tmdate": 1605861562379, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment", "content": {"title": "General answer to reviewers.", "comment": "We sincerely appreciate the reviewers for their thorough reading, helpful feedback and overall appreciation of many aspects of the work. We have tried as best we can to provide clarifications and answer questions.\n\nAdditionally, we have uploaded an adapted version of the manuscript containing the following:\n\n- Expanded and clarified notation in the method section 2.2 and 2.3 as suggested by AnonReviewer1 to increase accessibility for readers.\n\n- Added section A.3 in the appendix replying to an interesting question of AnonReviewer3, containing a figure and a proof showing that according to the transitivity property of Generalized MaxEnt [Csiszar 1996], incrementally adding new constraints can be done directly from $p$ and $\\pi_\\theta$ without the need of restarting the whole process. \n\n- Updated caption of Figure 4 to clarify the process of performing the token frequency analysis (AnonReviewer4).\n  \n- Fixed typos overall in the main paper and the appendix.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jWkw45-9AbL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2042/Authors|ICLR.cc/2021/Conference/Paper2042/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment"}}}, {"id": "SV4CQDHKuCk", "original": null, "number": 8, "cdate": 1605826987653, "ddate": null, "tcdate": 1605826987653, "tmdate": 1605826987653, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "XkRa8Wwy-7e", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3 ", "comment": "Thank you for the valuable feedback and very interesting questions. \n \n> The paper may benefit from some human evaluation for text generation.\n\nHuman evaluation can usefully complement automatic evaluation metrics, but there are some nuances here that one should be cautious about. Humans could be ill-placed for distribution-level evaluation, such as sample diversity and distributional constraint satisfaction (e.g. 50% female scientists), as reviewing these will require each annotator to look into a large set of samples collectively. For pointwise constraint satisfaction, human evaluation can be redundant in our situation, as opposed to simply evaluating $E\\phi(x)$. \nThat said, for individual sample quality, indeed human evaluation could complement such automatic measures as $KL(\\pi_\\theta|a)$ and GPT-2 perplexity but would have to be differentiated from the underlying quality of the original pretrained GPT-2 (which can be intrinsically poor for certain constraints). We, however, provide a long list of randomly sampled generations (not cherry-picked) in the appendix to demonstrate the sample quality.  \n\n> from figure 2, ... It seems that Ziegler is superior in generating attribute-related sentences while inferior in diversity.\n\nArguments  for the advantages of our method GDC over the Ziegler baseline can be summarized as follows: \n\n- Handling distributional and hybrid constraints: We would like to note that Ziegler baseline (the PPO + KL penalty)  follows an optimization objective, which makes it only suitable for imposing pointwise constraints but not suitable for distributional constraints. A peculiarity of GDC is that it can naturally handle pointwise, distributional and hybrid constraints, a distinctive feature of the proposed approach.    \n\n- Distance from the optimal distribution p: figure 2 doesn\u2019t show the full story on its own, there is a competing objective between constraint satisfaction and deviation from the original GPT2. In  figure 3 we plot $KL(p,\\pi)$. the deviation from the optimal target distribution p, which seems to us to be the more important measure: there we can see clearly that GDC converges better than Ziegler towards p. \n\n- Stability: Ziegler suffers from stability issues during training, as can be seen in figures 19-34 in the appendix, which poses serious challenges on when to stop training. By contrast, GDC has smooth training curves over all the experiments.   \n \n> if a task has a large number of constraints to consider or if the constraints are more complicated than what are tested in Section 3?\n\nWe expect that more numerous or more complicated constraints could present some challenges:\n\n- More complicated constraints. In principle, the framework can exploit any feature $\\phi_i$, but some features that could be useful in principle (e.g. a feature checking for parsability) could seriously slow the process, and it is not obvious how one could improve the situation there, but the problem is not specific to our approach and could appear with any technique checking for complicated conditions.\n\n- Contradicting constraints. Our approach assumes that the constraints are not contradictory, in which case no solution can be found. For instance, if we ask for a model producing 30% of scientific biographies as well as 50% of biographies about physicists, then we have a logical contradiction. We have not studied the question of automatically detecting such contradictions, but it might be possible either from certain symptoms of the optimization procedure of section 2.2 or even, in theory, based on deductive principles. A possible technique for avoiding contradictions would be to base the constrained moments on statistics of an actual dataset, either given naturally or produced just for the purpose of checking that the constraints are compatible.\n\n- More constraints. Some optimizations are possible there. For instance, pointwise features can be grouped before the distributional constraints and solved directly in the first phase (from constraints to EBM, section 2.2) of the process, through the shortcut mentioned at the end of section 2.2. As you suggest in your next question, it is also possible to move incrementally by adding one (or a few) constraints at a time on top of a solution to a fewer number of constraints (see our answer below). This looks like a potentially promising technique to accommodate larger sets of constraints than we have considered in the submission.\n\n> which would be better if she/he wants to add a new attribute to generation: starting scratch from GPT2 or continuing with the adjusted model?\n\nVery interesting question, thank you for pointing to an opportunity to analyse this aspect! \nShort answer: continuing from an adjusted model is expected to lead to faster convergence. We updated the paper to provide a detailed formal explanation in a new section A.3.1. in the Appendix (referred to in the main text in footnote 7).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jWkw45-9AbL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2042/Authors|ICLR.cc/2021/Conference/Paper2042/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment"}}}, {"id": "S1W1JC1HVMv", "original": null, "number": 6, "cdate": 1605825059173, "ddate": null, "tcdate": 1605825059173, "tmdate": 1605825059173, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "BnstUZEm_ix", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1  (Revised the submission to allow more accessibility)", "comment": "Thanks for your reviews, appreciation of the work, and to your constructive suggestions. \n\n> I think the method section (especially the optimization part) could be explained better for readers who are not familiar with EBM, and allow the paper to have more accessibility.\n\nThank you for this suggestion. Indeed, we have revised the submission in section 2.2 to provide more details and to clarify the optimization aspects. We also try to explain a bit better the status/terminology of Energy-Based Models in footnote 6."}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "jWkw45-9AbL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2042/Authors|ICLR.cc/2021/Conference/Paper2042/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852942, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Comment"}}}, {"id": "u-5rrhMrpgv", "original": null, "number": 1, "cdate": 1603640233620, "ddate": null, "tcdate": 1603640233620, "tmdate": 1605024302831, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Review", "content": {"title": "A Distributional Approach to Controlled Text Generation", "review": "In this paper the authors have proposed a mechanism for controlled text \ngeneration both pointwise and distributional. That is they not only can\ngenerate each sentences bearing some specified contraint or attribute \nbut also takes care of overall property distribution of the generates \nset of sentences. Though pointwise or per sentence level control is well \nexplored, the distributional control is a new and promising direction \nwhich the authors have proposed.\n\nThe authors proposed a method Generation with Distributional Control (GDC), \nwhich is nothing but a constraint satisfaction problem over the probability \ndistribution p representing the desired target Language Model. \n\n\nOverall I find the problem challenging and promising. This is a nicely written paper. \nHowever, I have some quetions regarding experimental evaluation.\n\n1. In the Figure 4, the authors have reported the generated sentences controlling\nsentiment and also report the frequency of the sentence present in the corpus. \nBy corpus does it mean the original training corpus? or the generated corpus by GPT-2?\n\n\n2. The proposed method is imposing a constraint so that the generation distribution \nbecomes closer to the original distribution (in this case GPT-2) and still satisfy the \npointwise and distributional constraints. If the distributional constraints are not imposed,\nthe generated sentences should be similar to that of the original GPT-2 generated sentences \nbearing which satisfy the pointwise constraint. What does the freq signifies here? \nThe authors should provide some discussion regarding the same. \n\n3. Are the sentences generated sequentially keeping the context of the previously \ngenerated sentences? or they do not have any context of the  previously generated \nsentence? If each generated sentences are independent from previously generated \nsentences, how meaningful it is to impose distribution constraint on that ?", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105374, "tmdate": 1606915792271, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2042/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Review"}}}, {"id": "XkRa8Wwy-7e", "original": null, "number": 3, "cdate": 1604107757163, "ddate": null, "tcdate": 1604107757163, "tmdate": 1605024302712, "tddate": null, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "invitation": "ICLR.cc/2021/Conference/Paper2042/-/Official_Review", "content": {"title": "Solid paper providing a formal distributional view for controlled text generation and a framework of solution", "review": "The paper studies the controlled sequence generation problem based on pretrained language models, i.e., controlling a generic pretrained LM to satisfy certain constraints, e.g., removing certain biases in language models. Specifically, the paper proposes a distributional view and imposes constraints based on collective statistical properties. The problem is formalized as a constraint satisfaction problem, minimizing a divergence objective. The paper proposes to use KL-Adaptive DPG algorithm for approximating the optimal energy-based model distribution. Experiments were conducted over both pointwise constraints and distributional constraints, showing the effectiveness of the model over the compared baselines.\n \nPros:\n- The problem under study is an important problem and can have extensive impact on many downstream language generation applications. \n- This paper makes solid contributions by proposing a formal view on generation controlling. It provides a framework to handle pointwise, distributional, and hybrid constraints. \n- The method proposed to sample from the sequential EBM makes sense and is empirically vilified to be effective.\n- The experiments and analyses support the claims and conclusions.\n- Overall, the paper is well organized and easy to understand. \n\nCons: \n- The paper may benefit from some human evaluation for text generation. \n- It is somehow not easy to tell which model is better from figure 2, GDC or Ziegler. It seems that Ziegler is superior in generating attribute-related sentences while inferior in diversity. The sentence quality might be similar as the converged values of (\u03c0, a) are close.\n- The current submission contains a number of typos, grammatical and other style issues, in both the main sections and appendixes, but these are rather easy to fix.\n\nQuestions:\n-  For real-life applications, whether the proposed framework has scalability issue; e.g., if a task has a large number of constraints to consider or if the constraints are more complicated than what are tested in Section 3? \n- Assuming one has already got an adjusted LM with some attributes based on GPT2, which would be better if she/he wants to add a new attribute to generation: starting scratch from GPT2 or continuing with the adjusted model?\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2042/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2042/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "A Distributional Approach to Controlled Text Generation", "authorids": ["~Muhammad_Khalifa2", "~Hady_Elsahar2", "~Marc_Dymetman1"], "authors": ["Muhammad Khalifa", "Hady Elsahar", "Marc Dymetman"], "keywords": ["Controlled NLG", "Pretrained Language Models", "Bias in Language Models", "Energy-Based Models", "Information Geometry", "Exponential Families"], "abstract": "We propose a  Distributional  Approach for addressing  Controlled  Text  Generation from pre-trained Language Models (LM). This approach permits to specify, in a single formal framework, both \u201cpointwise\u2019\u201d and \u201cdistributional\u201d constraints over the target LM \u2014 to our knowledge, the first model with such generality \u2014while minimizing KL divergence from the initial LM distribution.  The optimal target distribution is then uniquely determined as an explicit EBM (Energy-BasedModel) representation. From that optimal representation, we then train a target controlled Autoregressive LM through an adaptive distributional variant of PolicyGradient.  We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the pretrained LM.  We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models.  Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.\nCode available at https://github.com/naver/gdc", "one-sentence_summary": "We propose a novel approach to Controlled NLG, relying on Constraints over Distributions, Information Geometry, and Sampling from Energy-Based Models.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "khalifa|a_distributional_approach_to_controlled_text_generation", "pdf": "/pdf/11f0063ca9b22ee0ed8462004057c2417891ade2.pdf", "venue": "ICLR 2021 Oral", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nkhalifa2021a,\ntitle={A Distributional Approach to Controlled Text Generation},\nauthor={Muhammad Khalifa and Hady Elsahar and Marc Dymetman},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=jWkw45-9AbL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "jWkw45-9AbL", "replyto": "jWkw45-9AbL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2042/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538105374, "tmdate": 1606915792271, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2042/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2042/-/Official_Review"}}}], "count": 10}