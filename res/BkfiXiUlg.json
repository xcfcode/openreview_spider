{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396317754, "tcdate": 1486396317754, "number": 1, "id": "H1LjoMLux", "invitation": "ICLR.cc/2017/conference/-/paper34/acceptance", "forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "All three reviewers point to significant deficiencies. No response or engagement from the authors (for the reviews). I see no basis for supporting this paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396318227, "id": "ICLR.cc/2017/conference/-/paper34/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396318227}}}, {"tddate": null, "tmdate": 1482158317153, "tcdate": 1481910122603, "number": 2, "id": "H1zuwsZVe", "invitation": "ICLR.cc/2017/conference/-/paper34/official/review", "forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "signatures": ["ICLR.cc/2017/conference/paper34/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper34/AnonReviewer1"], "content": {"title": "review", "rating": "3: Clear rejection", "review": "This paper proposes to use a hierarchical softmax to speed up attention based memory addressing in memory augmented network (e.g. NTM, memNN\u2026).\n\nThe model build a hierarchical softmax on top of the input sequence then at each time step SEARCH for the most relevant input to predict the next output (this search is discrete), and use its corresponding embedding to update the state of an LSTM that will then produce the output. Finally the embedding of the used input is update by a WRITE function (an LSTM working that takes hidden state of the other LSTM as an input). The model has a discrete component (the SEARCH) and is thus trained with REINFORCE. In the experimental section they test their approach on several algorithmic tasks such as search, sort...\n\nThe main advantage of replacing the full softmax by a hierarchical softmax is that during inference, the complexity goes from O(N) to O(log(N)). It would be great to see if the gain in complexity allows to tackle problem which are a few orders of magnitude bigger than the one addressed with full softmax. However the authors only test on toy sequences up to 32 tokens, which is quite small. \n\nThe model requires a relatively complex search mechanism that can only be trained with REINFORCE. While this seems to work on problems with relatively small and simple sequences, it would be great to see how performance changes with the size of the problem. \n\nOverall, while the idea of replacing the softmax in the attention mechanism by a hierachical softmax is appealing, this work is not quite convincing yet. Their approach is not very natural, may be hard to train and may not be that simple to scale. The experiment section is very weak.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721373, "id": "ICLR.cc/2017/conference/-/paper34/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper34/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper34/AnonReviewer3", "ICLR.cc/2017/conference/paper34/AnonReviewer1", "ICLR.cc/2017/conference/paper34/AnonReviewer2"], "reply": {"forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721373}}}, {"tddate": null, "tmdate": 1481930164068, "tcdate": 1481930164068, "number": 3, "id": "Hy2hrgMEe", "invitation": "ICLR.cc/2017/conference/-/paper34/official/review", "forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "signatures": ["ICLR.cc/2017/conference/paper34/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper34/AnonReviewer2"], "content": {"title": "interesting idea, weak experiments", "rating": "5: Marginally below acceptance threshold", "review": "This paper introduces a novel hierarchical memory architecture for neural networks, based on a binary tree with leaves corresponding to memory cells.  This allows for O(log n) memory access, and experiments additionally demonstrate ability to solve more challenging tasks such as sorting from pure input-output examples and dealing with longer sequences.\n\nThe idea of the paper is novel and well-presented, and the memory structure seems reasonable to have advantages in practice. However, the main weakness of the paper is the experiments. There is no experimental comparison with other external memory-based approaches (e.g. those discussed in Related Work), or experimental analysis of computational efficiency given overhead costs (beyond just computational complexity) despite that being one of the main advantages. Furthermore, the experimental setups are relatively weak, all on artificial tasks with moderate increases in sequence length.  Improving on these would greatly strengthen the paper, as the core idea is interesting.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721373, "id": "ICLR.cc/2017/conference/-/paper34/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper34/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper34/AnonReviewer3", "ICLR.cc/2017/conference/paper34/AnonReviewer1", "ICLR.cc/2017/conference/paper34/AnonReviewer2"], "reply": {"forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721373}}}, {"tddate": null, "tmdate": 1481818651355, "tcdate": 1481818651349, "number": 1, "id": "Hk7QGrgEl", "invitation": "ICLR.cc/2017/conference/-/paper34/official/review", "forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "signatures": ["ICLR.cc/2017/conference/paper34/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper34/AnonReviewer3"], "content": {"title": "interesting idea and model but not clear that it actually works for long sequences", "rating": "5: Marginally below acceptance threshold", "review": "The authors introduce a new memory model which allows memory access in O(log n) time.\n\nPros:\n* The paper is well written and everything is clear.\n* It's a new model and I'm not aware of a similar model.\n* It's clear that memory access time is an issue for longer sequences and it is clear how this model solves this problem.\n\nCons:\n* The motivation for O(log n) access time is to be able to use the model on very long sequences. While it is clear from the definition that the computation time is low because of its design, it is not clear that the model will really generalize well to very long sequences.\n* The model was also not tested on any real-world task.\n\nI think such experiments should be added to show whether the model really works on long sequences and real-world tasks, otherwise it is not clear if this is a useful model.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512721373, "id": "ICLR.cc/2017/conference/-/paper34/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper34/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper34/AnonReviewer3", "ICLR.cc/2017/conference/paper34/AnonReviewer1", "ICLR.cc/2017/conference/paper34/AnonReviewer2"], "reply": {"forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512721373}}}, {"tddate": null, "tmdate": 1481575218170, "tcdate": 1481575218161, "number": 2, "id": "Hy9EjFnXg", "invitation": "ICLR.cc/2017/conference/-/paper34/public/comment", "forum": "BkfiXiUlg", "replyto": "SJacj-JXg", "signatures": ["~Karol_Kurach1"], "readers": ["everyone"], "writers": ["~Karol_Kurach1"], "content": {"title": "Thank you for the comments", "comment": ">Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes / Dynamic Neural Turing Machine with\n>Continuous and Discrete Addressing Schemes uses also hard addressing with NTMs, thus constant memory\n>access and they were successful.\n\n\nPlease note that the hard addressing in this paper is O(n), not constant. As described in Section 5, the sampling is done according to the vector of probabilities \u201cw\u201d (Eq. 7).  Computing this vector requires O(n) operations, where n is the memory size.  \n\n[all references w.r.t. the version https://arxiv.org/abs/1607.00036]\n\n\n>The Sparse Access Memory (SAM) scheme from Scaling Memory-Augmented Neural Networks with Sparse Reads and\n> Writes also can use constant time memory access.\n\n\nThank you for mentioning SAM. We didn't discuss this model in our paper, because it was published just before the submission deadline. This is a very interesting model also allowing to access the memory in O(log n) time, but based on a completely different idea than our model.\n\n>The HAM depth depends on $n$. How does it generalize to different depth?\n\n\nWe train the model on input sequences of length up to 32 using the tree with 32 leafs and then checked how well it performs on input sequences of length up to 128 using a tree with 128 leafs. The model generalized almost perfectly on all problems apart from binary addition. The details may be found in Table 1.\n\n>Why are $x$ and $y$ binary vectors? Where do you use that?\n\n\nThis assumption is not important, it was introduced only to simplify the model description.\n\n>In Table 1, you compare HAM to encoder-decoder and encoder-attention-decoder models. However, only HAM\n>has writeable memory. Wouldn't it be more fair to use some model like NTM instead for comparison?\n\nIt would be interesting to compare our model and NTM on some set of tasks. We decided not to do this comparison, because NTM has a very high number of hyperparameters and may be difficult to train. Even if we had failed to tune it, it would not mean that it can not solve these tasks.\n\nMoreover, the main advantage of our model is O(log n) memory access, so it is not essential for our point whether a model with linear memory access can solve similar problems or not.\n\n>Your main argument for HAM is that it can handle much longer sequences because the memory access \n>time is more efficient. Where do you have experiments on longer sequences?\n\n\nWe don\u2019t have experiments on sequences longer than 128 input symbols. Our solution works in time O(log n) and the constant hidden in the O notation is relatively small - our memory requires log n MLP forward/backward passes per timestep, while normal attention requires linear number of MLP passes. It is therefore clear that our solution is much faster on long input sequences (e.g. long articles / books).\n\n> Did you do any experiments on real-world tasks such as Facebook bAbI or translation?\n\nNo. It is a topic of future work.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756342, "id": "ICLR.cc/2017/conference/-/paper34/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkfiXiUlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper34/reviewers", "ICLR.cc/2017/conference/paper34/areachairs"], "cdate": 1485287756342}}}, {"tddate": null, "tmdate": 1481575067380, "tcdate": 1481575067372, "number": 1, "id": "HyQj9K27e", "invitation": "ICLR.cc/2017/conference/-/paper34/public/comment", "forum": "BkfiXiUlg", "replyto": "By_Z8J-Xx", "signatures": ["~Karol_Kurach1"], "readers": ["everyone"], "writers": ["~Karol_Kurach1"], "content": {"title": "Thank you for the comments", "comment": ">The paper claims that memory access is more efficient, can you comment on the computational overhead of >sampling O(log n) Bernoulli at each timestep?\n\nThe cost of sampling from Bernoulli distribution is very efficient (we don't need cryptographically secure randomness), and it's much faster than forward propagation in a small MLP which is also required for every traversed node.\n\n\n>In Section 3.3, you mention three different tricks for training, what is the contribution of each of these >to the performance, and the baseline without any of the tricks?\n\nCurriculum is essential for the training, we couldn't get the model to work without it apart from very simple tasks or very short input sequences. Regarding different reward functions and entropy bonus, they seemed to improve performance in our experiments, but we didn't perform extensive evaluation of their effect.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287756342, "id": "ICLR.cc/2017/conference/-/paper34/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BkfiXiUlg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper34/reviewers", "ICLR.cc/2017/conference/paper34/areachairs"], "cdate": 1485287756342}}}, {"tddate": null, "tmdate": 1480812032444, "tcdate": 1480812032440, "number": 2, "id": "By_Z8J-Xx", "invitation": "ICLR.cc/2017/conference/-/paper34/pre-review/question", "forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "signatures": ["ICLR.cc/2017/conference/paper34/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper34/AnonReviewer2"], "content": {"title": "efficiency and ablation studies", "question": "The paper claims that memory access is more efficient, can you comment on the computational overhead of sampling O(log n) Bernoulli at each timestep?\n\nIn Section 3.3, you mention three different tricks for training, what is the contribution of each of these to the performance, and the baseline without any of the tricks?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959500376, "id": "ICLR.cc/2017/conference/-/paper34/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper34/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper34/AnonReviewer3", "ICLR.cc/2017/conference/paper34/AnonReviewer2"], "reply": {"forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959500376}}}, {"tddate": null, "tmdate": 1480690581009, "tcdate": 1480690581003, "number": 1, "id": "SJacj-JXg", "invitation": "ICLR.cc/2017/conference/-/paper34/pre-review/question", "forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "signatures": ["ICLR.cc/2017/conference/paper34/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper34/AnonReviewer3"], "content": {"title": "related work, longer sequences, real-world tasks", "question": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes / Dynamic Neural Turing Machine with Continuous and Discrete Addressing Schemes uses also hard addressing with NTMs, thus constant memory access and they were successful.\nThe Sparse Access Memory (SAM) scheme from Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes also can use constant time memory access.\n\nThe HAM depth depends on $n$. How does it generalize to different depth?\n\nWhy are $x$ and $y$ binary vectors? Where do you use that?\n\nIn Table 1, you compare HAM to encoder-decoder and encoder-attention-decoder models. However, only HAM has writeable memory. Wouldn't it be more fair to use some model like NTM instead for comparison?\n\nYour main argument for HAM is that it can handle much longer sequences because the memory access time is more efficient. Where do you have experiments on longer sequences?\n\nDid you do any experiments on real-world tasks such as Facebook bAbI or translation?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959500376, "id": "ICLR.cc/2017/conference/-/paper34/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper34/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper34/AnonReviewer3", "ICLR.cc/2017/conference/paper34/AnonReviewer2"], "reply": {"forum": "BkfiXiUlg", "replyto": "BkfiXiUlg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper34/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959500376}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478042610903, "tcdate": 1478042522262, "number": 34, "id": "BkfiXiUlg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BkfiXiUlg", "signatures": ["~Karol_Kurach1"], "readers": ["everyone"], "content": {"title": "Learning Efficient Algorithms with Hierarchical Attentive Memory", "abstract": "In this paper, we propose and investigate a novel memory architecture for neural networks called Hierarchical Attentive Memory (HAM).  It is based on a binary tree with leaves corresponding to memory cells. This allows HAM to perform memory access in O(log n) complexity, which is a significant improvement over the standard attention mechanism that requires O(n) operations, where n is the size of the memory.  \n\nWe show that an LSTM network augmented with HAM can learn algorithms for problems like merging, sorting or binary searching from pure input-output examples.  In particular, it learns to sort n numbers in time O(n log n) and generalizes well to input sequences much longer than the ones seen during the training.  We also show that HAM can be trained to act like classic data structures: a stack, a FIFO queue and a priority queue.", "pdf": "/pdf/1f865d9b000a8b89c7aefd275c22b413e6f84d38.pdf", "TL;DR": "fast attention in O(log n); learned sorting algorithm that generalizes", "paperhash": "andrychowicz|learning_efficient_algorithms_with_hierarchical_attentive_memory", "keywords": [], "conflicts": ["google.com", "openai.com", "mimuw.edu.pl"], "authors": ["Marcin Andrychowicz", "Karol Kurach"], "authorids": ["marcin@openai.com", "kkurach@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}