{"notes": [{"id": "H1gpET4YDB", "original": "rJeKxOEvwr", "number": 504, "cdate": 1569439029347, "ddate": null, "tcdate": 1569439029347, "tmdate": 1577168239067, "tddate": null, "forum": "H1gpET4YDB", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "DMVgMvn62z", "original": null, "number": 1, "cdate": 1576798698335, "ddate": null, "tcdate": 1576798698335, "tmdate": 1576800937463, "tddate": null, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes blockwise masked attention mechanisms to sparsify Transformer architectures, the main motivation being reducing the memory usage with long sequence inputs. The resulting model is called BlockBERT. The paper falls in a trend of recent papers compressing/sparsifying/distilling Transformer architectures, a very relevant area of research given the daunting resources needed to train these models.\n\nWhile the proposed contribution is very simple and interesting, it also looks a rather small increment over prior work, namely Sparse Transformer and Adaptive Span Transformer, among others. Experiments are rather limited and the memory/time reduction is not overwhelming (18.7-36.1% less memory, 12.0-25.1% less time), while final accuracy is sometimes sacrificed by a few points. No comparison to other adaptively sparse attention transformer architectures (Correia et al. EMNLP 19 or Sukhbaatar et al. ACL 19) which should as well provide memory reductions due to the sparsity of the gradients, which require less activations to be cached. I suggest addressing this concerns in an eventual resubmission of the paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717920, "tmdate": 1576800268312, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper504/-/Decision"}}}, {"id": "rJx4LsnKsr", "original": null, "number": 3, "cdate": 1573665611754, "ddate": null, "tcdate": 1573665611754, "tmdate": 1573667909593, "tddate": null, "forum": "H1gpET4YDB", "replyto": "SJlkRGil9B", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment", "content": {"title": "To AnonReviewer2. Thank you for your feedback and insightful discussions.", "comment": "We appreciate your feedback and will keep polishing our work. Thank you very much.\n\nWe believe the reviewer has misunderstood the scope of the paper\u2019s contribution. This paper addresses an issue with *self-attention*, which is used (via transformers) in many other applications other than BERT, such as machine translation, speech recognition, and even computer vision. Therefore, our proposed blockwise self-attention is not only applicable to \u201coptimising BERT models (Bertology)\u201d - but can potentially assist any domain that involves modeling long sequences via self-attention.\n\nRegarding the test time efficiency, BlockBERT does achieve speedup and memory reduction during test time. The test-time (sec) statistics is as follows (OOM indicates out-of-memory):\n| Model/Batch x Length | 8 x 1024         | 16 x 1024      | 24 x 1024     | 32 x 1024 |\n| RoBERTa                         | 0.137146792  | OOM             | OOM            | OOM        |\n| BlockBERT (n=2)            | 0.098985166 | 0.186883354 | OOM            | OOM        |\n| BlockBERT (n=3)            | 0.095399417 | 0.179042275 | 0.26340443 | OOM        |\n\nTake 8x1024 (Batch size=8, Sequence Length=1024) as an example, we can see that BlockBERT with 2 blocks saves 27.8% of test time, and BlockBERT with 3 blocks saves more (30.4%). As for memory, we can observe that RoBERTa can not handle an input of 16x1024, while it is possible for BlockBERT to work on it.\n\nAll experiments are run 30 times on a 32GB V100 GPU with half precision (FP16). We report the average running time. We have added detailed test efficiency analysis at A.5.\n\nAs for the code, we have released the code publicly and will include the link in the paper once deanonymized.\n\nThanks for pointing out the typo. We have fixed it. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gpET4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper504/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper504/Authors|ICLR.cc/2020/Conference/Paper504/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170444, "tmdate": 1576860551550, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment"}}}, {"id": "rkeazEaFor", "original": null, "number": 4, "cdate": 1573667861289, "ddate": null, "tcdate": 1573667861289, "tmdate": 1573667861289, "tddate": null, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment", "content": {"title": "To All Reviewers. Revision Log.", "comment": "Dear reviewers, we have updated our submission according to your helpful comments. We made the following changes:\n\n1. We added test efficiency analysis at Appendix A.5. According to our analysis, we can see that BlockBERT achieves memory and time saving in both training and testing.\n\n2. We polished the presentation in section 4.3 to make the ablation study section more readable.\n\n3. We added detailed pre-training and fine-tuning setting in Appendix A.1 and A.4, respectively.\n\n4. We added some references as suggested by reviewers in related work section (Section 5). \n\n5. We fixed some typos.\n\nThank you very much.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gpET4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper504/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper504/Authors|ICLR.cc/2020/Conference/Paper504/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170444, "tmdate": 1576860551550, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment"}}}, {"id": "B1g2WKhKjS", "original": null, "number": 2, "cdate": 1573665027552, "ddate": null, "tcdate": 1573665027552, "tmdate": 1573665050170, "tddate": null, "forum": "H1gpET4YDB", "replyto": "SkgHIb4G9B", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment", "content": {"title": "To AnonReviewer4. Thank you for your positive feedback.", "comment": "Thank you for your comments. We updated the paper to add detailed experimental setting and related work, and also polish the presentation according to your feedback. Here are the replies to your comments.\n\n1.Vocab size.\nWe pretrained our RoBERTa/BlockBERT/SparseBERT with the same vocabulary as Google\u2019s BERT (uncased version). All of them are of vocabulary size 30,522. \n\n2. Fine-tuning settings.\nWe allow fine-tuning task to input sequences as long as the pre-training model, i.e., if the pre-trained model has max_sequence_length=1024, the fine-tuning task can feed a 1024 token sequence to it. If the input sequence is too long to fit the max sequence length constraints, we use a sliding window of stride 128 to split it. We grid search learning rate from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5} and batch size from {16, 32}. The fine-tuning is performed for 4 epoches. We update detailed fine-tuning settings at Appendix A.1.\n\n3. Training time and memory of SparseBERT.\nWe implement SparseBERT in a direct way, with the goal of comparing performance, not speed. We first compute the N^2 attention matrix, and then mask it to be a sparse matrix according to the sparse pattern defined in Sparse Transformer paper. Consequently, this implementation of SparseBERT has very close training time/memory cost as RoBERTa (as it can not avoid the O(N^2) attention computation). We did so because the code released by Sparse Transformer is based on Tensorflow and relies on customized CUDA kernels, but our pre-training is done using PyTorch. \n\n4. About Section 4.3\nIn section 4.3, we mainly discuss how the distribution of attention heads affects the model performance. The main conclusion is that we need far more attention heads for identity permutation (which learns local dependencies) than non-identity permutations (which learns long-term dependencies). We have polished the presentation according to your feedback.\n\n5. Redundancy in attention heads\nThanks for the reference. And yes, our method can be viewed as a way to compress (or factorize) redundant attention heads. We have added it to reference and discuss it in related work.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gpET4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper504/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper504/Authors|ICLR.cc/2020/Conference/Paper504/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170444, "tmdate": 1576860551550, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment"}}}, {"id": "r1gHbOnFir", "original": null, "number": 1, "cdate": 1573664765176, "ddate": null, "tcdate": 1573664765176, "tmdate": 1573664765176, "tddate": null, "forum": "H1gpET4YDB", "replyto": "HJgqUsBatS", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment", "content": {"title": "To AnonReviewer2. Thank you for your positive feedback. ", "comment": "We appreciate your positive feedback and will keep polishing our work."}, "signatures": ["ICLR.cc/2020/Conference/Paper504/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1gpET4YDB", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper504/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper504/Authors|ICLR.cc/2020/Conference/Paper504/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504170444, "tmdate": 1576860551550, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper504/Authors", "ICLR.cc/2020/Conference/Paper504/Reviewers", "ICLR.cc/2020/Conference/Paper504/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper504/-/Official_Comment"}}}, {"id": "SJlkRGil9B", "original": null, "number": 3, "cdate": 1572020934627, "ddate": null, "tcdate": 1572020934627, "tmdate": 1572972587199, "tddate": null, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review": "This paper introduces a optimisation for BERT models based on using block matrices for the attention layers. This allows to reduce the memory footprint and the processing  time during training while reaching state-of-the-art results on 5 datasets. An interesting study on memory consumption in BERT is conducted. No results are given at test time : is there also a memory and processing time reduction ?\n\nEven if the proposition is interesting, the impact of the paper is limited to the (flourishing) scope optimising Bert models (\"Bertology\"). The authors do not mention if their code is available.\n\n\nTable 3 : Humam -> Human\t"}, "signatures": ["ICLR.cc/2020/Conference/Paper504/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper504/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574904616466, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper504/Reviewers"], "noninvitees": [], "tcdate": 1570237751178, "tmdate": 1574904616481, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper504/-/Official_Review"}}}, {"id": "HJgqUsBatS", "original": null, "number": 1, "cdate": 1571801938080, "ddate": null, "tcdate": 1571801938080, "tmdate": 1572972587156, "tddate": null, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The paper propose to sparsify the attention matrix to decrease memory usage and to speed up training. The authors experiment the model on multiple tasks. The model gains ~20% efficiency with ~20% decrease in memory use while maintaining comparable performance to the state of the art model. To keep the performance comparable, the authors use the same training corpus. The authors also discuss how block size could change the performance of the model. The paper is clear and well organized with good experiment results."}, "signatures": ["ICLR.cc/2020/Conference/Paper504/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper504/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574904616466, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper504/Reviewers"], "noninvitees": [], "tcdate": 1570237751178, "tmdate": 1574904616481, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper504/-/Official_Review"}}}, {"id": "SkgHIb4G9B", "original": null, "number": 4, "cdate": 1572122957429, "ddate": null, "tcdate": 1572122957429, "tmdate": 1572972587112, "tddate": null, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "invitation": "ICLR.cc/2020/Conference/Paper504/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors propose BlockBERT, a model that makes the attention matrix of Transformer models sparse by introducing block structure. This has the dual benefit of reducing memory and reducing training time. The authors show on various question answering tasks that their model is competitive with RoBERTa. \n\n1. Can the authors add additional details about the training of their model in Section 4.1? It is not clear for example the vocabulary size - is that the RoBERTa vocabulary size of around 50k or the BERT vocabulary size that is smaller? I believe this will affect the memory and training speed. \n\n2. For the datasets such as TriviaQA and SearchQA, how is RoBERTa finetuned on these tasks? By doing the window approach? \n\n3. The authors compare to RoBERTa and Sparse BERT as baselines for the section on performance. However, can the authors also include metrics on training time and memory in Table 2 for Sparse BERT as well as other sparse attention transformer architectures proposed (for example the Correia paper or the Sukhbaatar paper)? It is not clear the savings from this architecture compared to sparse Transformers in general. \n\n4. The analysis in Section 4.3 is quite unclear due to how compressed the description is and how tiny the graphs are. \n\n5. The authors mention that attention heads can be sparsified due to the memory usage and quadratic time complexity. Other work has also shown that the attention heads are quite redundant and can be pruned away, so attention head dropout is effective. For example https://arxiv.org/abs/1905.09418 \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper504/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper504/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Blockwise Self-Attention for Long Document Understanding", "authors": ["Jiezhong Qiu", "Hao Ma", "Omer Levy", "Scott Wen-tau Yih", "Sinong Wang", "Jie Tang"], "authorids": ["qiujz16@mails.tsinghua.edu.cn", "gabe.hao.ma@gmail.com", "omerlevy@gmail.com", "scottyih@gmail.com", "sinongwang@fb.com", "jietang@tsinghua.edu.cn"], "keywords": ["BERT", "Transformer"], "TL;DR": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies.", "abstract": "We present BlockBERT, a lightweight and efficient BERT model that is designed to better modeling long-distance dependencies. Our model extends BERT by introducing sparse block structures into the attention matrix to reduce both memory consumption and training time, which also enables attention heads to capture either short- or long-range contextual information. We conduct experiments on several benchmark question answering datasets with various paragraph lengths. Results show that BlockBERT uses 18.7-36.1% less memory and reduces the training time by 12.0-25.1%, while having comparable and sometimes better prediction accuracy, compared to an advanced BERT-based model, RoBERTa.", "pdf": "/pdf/9ec5ccd2471b84b9db3d91407d6cff5a94cd4282.pdf", "paperhash": "qiu|blockwise_selfattention_for_long_document_understanding", "original_pdf": "/attachment/315181a34bb41bae94dfbabb79deb28e62f46933.pdf", "_bibtex": "@misc{\nqiu2020blockwise,\ntitle={Blockwise Self-Attention for Long Document Understanding},\nauthor={Jiezhong Qiu and Hao Ma and Omer Levy and Scott Wen-tau Yih and Sinong Wang and Jie Tang},\nyear={2020},\nurl={https://openreview.net/forum?id=H1gpET4YDB}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1gpET4YDB", "replyto": "H1gpET4YDB", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper504/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574904616466, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper504/Reviewers"], "noninvitees": [], "tcdate": 1570237751178, "tmdate": 1574904616481, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper504/-/Official_Review"}}}], "count": 9}