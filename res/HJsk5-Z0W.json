{"notes": [{"tddate": null, "ddate": null, "tmdate": 1518730167798, "tcdate": 1509132770905, "number": 704, "cdate": 1518730167786, "id": "HJsk5-Z0W", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "HJsk5-Z0W", "original": "ryF15bZ0Z", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Structured Deep Factorization Machine: Towards General-Purpose Architectures", "abstract": "In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size\u2014this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.", "pdf": "/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf", "TL;DR": "Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.", "paperhash": "gonz\u00e1lezbrenes|structured_deep_factorization_machine_towards_generalpurpose_architectures", "_bibtex": "@misc{\np.2018structured,\ntitle={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\nauthor={Jos\u00e9 P. Gonz\u00e1lez-Brenes and Ralph Edezhath},\nyear={2018},\nurl={https://openreview.net/forum?id=HJsk5-Z0W},\n}", "authorids": ["jgonzalez@chegg.com", "redezhath@chegg.com"], "keywords": ["factorization", "general-purpose methods"], "authors": ["Jos\u00e9 P. Gonz\u00e1lez-Brenes", "Ralph Edezhath"]}, "nonreaders": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260079391, "tcdate": 1517250080410, "number": 748, "cdate": 1517250080393, "id": "B1O78yTHG", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper has been withdrawn by the authors."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Deep Factorization Machine: Towards General-Purpose Architectures", "abstract": "In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size\u2014this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.", "pdf": "/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf", "TL;DR": "Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.", "paperhash": "gonz\u00e1lezbrenes|structured_deep_factorization_machine_towards_generalpurpose_architectures", "_bibtex": "@misc{\np.2018structured,\ntitle={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\nauthor={Jos\u00e9 P. Gonz\u00e1lez-Brenes and Ralph Edezhath},\nyear={2018},\nurl={https://openreview.net/forum?id=HJsk5-Z0W},\n}", "authorids": ["jgonzalez@chegg.com", "redezhath@chegg.com"], "keywords": ["factorization", "general-purpose methods"], "authors": ["Jos\u00e9 P. Gonz\u00e1lez-Brenes", "Ralph Edezhath"]}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642494851, "tcdate": 1511654365290, "number": 1, "cdate": 1511654365290, "id": "rkSyVFDeG", "invitation": "ICLR.cc/2018/Conference/-/Paper704/Official_Review", "forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "signatures": ["ICLR.cc/2018/Conference/Paper704/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "proposes a model that is equivalent to known work", "rating": "3: Clear rejection", "review": "This paper proposes to improve time complexity of factorization machine. Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong. Specifically, the dot product can be computed as (which is linear to feature size)\n\n(\\sum x_i \\beta_i)^T (\\sum x_i \\beta_i) - \\sum_i x_i^2 beta_i^T beta_i\n\nThe projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one. When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Deep Factorization Machine: Towards General-Purpose Architectures", "abstract": "In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size\u2014this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.", "pdf": "/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf", "TL;DR": "Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.", "paperhash": "gonz\u00e1lezbrenes|structured_deep_factorization_machine_towards_generalpurpose_architectures", "_bibtex": "@misc{\np.2018structured,\ntitle={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\nauthor={Jos\u00e9 P. Gonz\u00e1lez-Brenes and Ralph Edezhath},\nyear={2018},\nurl={https://openreview.net/forum?id=HJsk5-Z0W},\n}", "authorids": ["jgonzalez@chegg.com", "redezhath@chegg.com"], "keywords": ["factorization", "general-purpose methods"], "authors": ["Jos\u00e9 P. Gonz\u00e1lez-Brenes", "Ralph Edezhath"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642494753, "id": "ICLR.cc/2018/Conference/-/Paper704/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper704/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper704/AnonReviewer3", "ICLR.cc/2018/Conference/Paper704/AnonReviewer2", "ICLR.cc/2018/Conference/Paper704/AnonReviewer1"], "reply": {"forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper704/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642494753}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642494810, "tcdate": 1512010742347, "number": 2, "cdate": 1512010742347, "id": "Bk0lEg6eG", "invitation": "ICLR.cc/2018/Conference/-/Paper704/Official_Review", "forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "signatures": ["ICLR.cc/2018/Conference/Paper704/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "Novel model for Collaborative filtering. Seems reasonable overall. Empirical study would be more convincing by including classic recsys datasets\u02da.", "rating": "4: Ok but not good enough - rejection", "review": "The authors introduce a novel novel for collaborative filtering. The proposed model combines some of the strengths of factorization machines and of polynomial regression. Another way to understand this model is that it's a feed forward neural network with a specific connection structure (i.e., not fully connected).\n\nThe paper is well written overall and relatively easy to understand. The study seems fairly thorough (both vanilla and cold-start experiments are reported).\n\nOverall the paper feels a little bit incomplete . This is particularly apparent in the empirical study. Given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental results. Here are some suggestions about how to achieve that: \n\n1) Methodically report results for MF, FM, CTR (when meaningful), other strong baselines (maybe SLIM?) and all your methods for all datasets.\n\n2) Report results on well-known CF datasets. Movielens comes to mind.\n\n3) Shed some light on some of the poor CTR results (last paragraph of Section 4.2.2)\n\n4) Explore the models and shed some lights on where the gains are coming from.\n\n\nMinor: \n\n- How do you deal with unobserved preferences in the implicit case?\n\n- I found the idea of Figure 1 very good but in its current form I didn't find it particularly insightful (these \"clouds\" are hard to interpret).\n\n- It may also be worth adding this reference when discussing neural factorization:\nhttp://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Deep Factorization Machine: Towards General-Purpose Architectures", "abstract": "In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size\u2014this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.", "pdf": "/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf", "TL;DR": "Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.", "paperhash": "gonz\u00e1lezbrenes|structured_deep_factorization_machine_towards_generalpurpose_architectures", "_bibtex": "@misc{\np.2018structured,\ntitle={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\nauthor={Jos\u00e9 P. Gonz\u00e1lez-Brenes and Ralph Edezhath},\nyear={2018},\nurl={https://openreview.net/forum?id=HJsk5-Z0W},\n}", "authorids": ["jgonzalez@chegg.com", "redezhath@chegg.com"], "keywords": ["factorization", "general-purpose methods"], "authors": ["Jos\u00e9 P. Gonz\u00e1lez-Brenes", "Ralph Edezhath"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642494753, "id": "ICLR.cc/2018/Conference/-/Paper704/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper704/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper704/AnonReviewer3", "ICLR.cc/2018/Conference/Paper704/AnonReviewer2", "ICLR.cc/2018/Conference/Paper704/AnonReviewer1"], "reply": {"forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper704/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642494753}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642494771, "tcdate": 1512661669250, "number": 3, "cdate": 1512661669250, "id": "Sk6iGkvbG", "invitation": "ICLR.cc/2018/Conference/-/Paper704/Official_Review", "forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "signatures": ["ICLR.cc/2018/Conference/Paper704/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "context and contribution of this work is not very clear to me", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a method for matrix factorization using DNNs. The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7). From the methodology point of view, such extensions are relatively straightforward. As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically. The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features.\n\nWhile the application problems look quite interesting, in my opinion, the paper needs to make the context and contribution clearer. In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem). I think this paper does not connect very well with that literature. When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework. I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods, or if the success of this paper is mostly due to clever processing of text features using DNNs.\n\nSome detailed comments:\n1. eqn 4 does not indicate any rank-r factors. \n2. some statements do not seem straightforward/justified to me:  \n    -- the paper uses the word \"inference\" several times without definition\n    -- \"if we were interested in interpreting the parameters, we could constrain w to be non-negative ... \". Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples?\n    -- \"Note that if the dot product is replaced with a neural function, fast inference for cold-start ...\". \n3. the experimental setup seems quite unusual to me: \"since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency\". This seems very problematic if most of the entries are not observed. Why cannot you use the typical evaluation procedure for collaborative filtering, where you hide some known entries during model training, and evaluate on these entries during test? ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Deep Factorization Machine: Towards General-Purpose Architectures", "abstract": "In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size\u2014this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.", "pdf": "/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf", "TL;DR": "Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.", "paperhash": "gonz\u00e1lezbrenes|structured_deep_factorization_machine_towards_generalpurpose_architectures", "_bibtex": "@misc{\np.2018structured,\ntitle={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\nauthor={Jos\u00e9 P. Gonz\u00e1lez-Brenes and Ralph Edezhath},\nyear={2018},\nurl={https://openreview.net/forum?id=HJsk5-Z0W},\n}", "authorids": ["jgonzalez@chegg.com", "redezhath@chegg.com"], "keywords": ["factorization", "general-purpose methods"], "authors": ["Jos\u00e9 P. Gonz\u00e1lez-Brenes", "Ralph Edezhath"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642494753, "id": "ICLR.cc/2018/Conference/-/Paper704/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper704/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper704/AnonReviewer3", "ICLR.cc/2018/Conference/Paper704/AnonReviewer2", "ICLR.cc/2018/Conference/Paper704/AnonReviewer1"], "reply": {"forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper704/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642494753}}}, {"tddate": null, "ddate": null, "tmdate": 1515179426266, "tcdate": 1515179293355, "number": 1, "cdate": 1515179293355, "id": "SJrX6Hpmz", "invitation": "ICLR.cc/2018/Conference/-/Paper704/Official_Comment", "forum": "HJsk5-Z0W", "replyto": "HJsk5-Z0W", "signatures": ["ICLR.cc/2018/Conference/Paper704/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper704/Authors"], "content": {"title": "Withdrawing paper", "comment": "Dear reviewers,\n\nThank you for your very insightful comments. We are withdrawing this paper and using some of these results to supplement a different paper.  \n\nWe are not clicking on \"withdrawing\" paper yet, as this submission would be de-anonymized immediately.\n\nThanks,\nAuthor"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Structured Deep Factorization Machine: Towards General-Purpose Architectures", "abstract": "In spite of their great success, traditional factorization algorithms typically do not support features (e.g., Matrix Factorization), or their complexity scales quadratically with the number of features (e.g, Factorization Machine). On the other hand, neural methods allow large feature sets, but are often designed for a specific application. We propose novel deep factorization methods that allow efficient and flexible feature representation. For example, we enable describing items with natural language with complexity linear to the vocabulary size\u2014this enables prediction for unseen items and avoids the cold start problem. We show that our architecture can generalize some previously published single-purpose neural architectures. Our experiments suggest improved training times and accuracy compared to shallow methods.", "pdf": "/pdf/9995b4ea08f00e971ca7322acd45a3275e00f6ed.pdf", "TL;DR": "Scalable general-purpose factorization algorithm-- also helps to circumvent cold start problem.", "paperhash": "gonz\u00e1lezbrenes|structured_deep_factorization_machine_towards_generalpurpose_architectures", "_bibtex": "@misc{\np.2018structured,\ntitle={Structured Deep Factorization Machine: Towards General-Purpose Architectures},\nauthor={Jos\u00e9 P. Gonz\u00e1lez-Brenes and Ralph Edezhath},\nyear={2018},\nurl={https://openreview.net/forum?id=HJsk5-Z0W},\n}", "authorids": ["jgonzalez@chegg.com", "redezhath@chegg.com"], "keywords": ["factorization", "general-purpose methods"], "authors": ["Jos\u00e9 P. Gonz\u00e1lez-Brenes", "Ralph Edezhath"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825729051, "id": "ICLR.cc/2018/Conference/-/Paper704/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "HJsk5-Z0W", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper704/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper704/Authors|ICLR.cc/2018/Conference/Paper704/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper704/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper704/Authors|ICLR.cc/2018/Conference/Paper704/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper704/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper704/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper704/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper704/Reviewers", "ICLR.cc/2018/Conference/Paper704/Authors", "ICLR.cc/2018/Conference/Paper704/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825729051}}}], "count": 6}