{"notes": [{"tddate": null, "tmdate": 1492897535227, "tcdate": 1492897535227, "number": 5, "id": "S1wZ1IKAg", "invitation": "ICLR.cc/2017/workshop/-/paper146/public/comment", "forum": "S1gNakBFx", "replyto": "Sk5D8tiie", "signatures": ["~Volodymyr_Kuleshov1"], "readers": ["everyone"], "writers": ["~Volodymyr_Kuleshov1"], "content": {"title": "Thanks for your feedback!", "comment": "Thank you so much for your interest in our paper.\n\nFirst, we're sorry for the inconsistencies in our samples. We generated them in several passes (and we were also in a bit of a rush), and so we inadvertently mixed together results from several experiments.\n\nNow on to your questions:\n\nYes, the high resolution samplerate is always 16kHz, and the 2, 4, 6, and 8x tasks correspond to 8, 4, 2.666, and 2kHz input with Nyquist frequencies of 4kHz , 2kHz, 1.333kHz, and 1kHz.\n\nEach sample indeed corresponds to 0.375s. We used the full dataset and did not filter silent frames.\n\nAll of the samples are from the test set, except the very last one, where we demonstrate how the system can sometimes hallucinate sounds.\n\nAs we mentioned, the samples are somewhat inconsistent. We used two methods to generate the downscaled version:\n\nA. Low-pass filtering: x_lr = decimate(x, args.scale)\nB. Naive sub-sampling: x_lr = scipy.decimate(x, args.scale)\n\nWe can train super-resolution algorithms that work in both regimes, as the long as the model is trained and tested with the same downsampling procedure.\n\nWe initially didn't pay much attention to the choice of downscaling technique, because it didn't affect very much our objective performance measures (SNR, PSD). But after listening more carefully to both types of samples, we realize that super-resolving low-pass filtered speech is more challenging, in the sense that all the methods (ours + baselines) don't recover as much higher frequencies as when the input is aliased (like the single-speaker samples that you mention). Interestingly, that doesn't seem to be the case for the piano samples, which we looked at first.\n\nWe have uploaded new single-speaker samples for which the input is not aliased.\nRegarding the MSP-2 samples, we really don't know what happened. We are going to post new samples as soon as the model finishes retraining.\n\nThe banding artifacts are an issue that we are aware of. On some samples, the network creates artifacts that decompose into a sequence of bands at multiples of the same frequency, and are therefore clearly visible in the spectrogram. They are heard as a background buzz. Since we know precisely their frequencies, we added a post-processing heuristic that suppresses these bands. It doesn't affect sound quality but gets rid of some of the noise. This is a temporary hack until we figure out the true cause of the problem.\n\nThanks again for your interest, and let us know if you have any more questions!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367464511, "tcdate": 1487367464511, "id": "ICLR.cc/2017/workshop/-/paper146/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper146/reviewers"], "reply": {"forum": "S1gNakBFx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367464511}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028629946, "tcdate": 1490028629946, "number": 1, "id": "SyR8_K6jg", "invitation": "ICLR.cc/2017/workshop/-/paper146/acceptance", "forum": "S1gNakBFx", "replyto": "S1gNakBFx", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028630512, "id": "ICLR.cc/2017/workshop/-/paper146/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1gNakBFx", "replyto": "S1gNakBFx", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028630512}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489897155745, "tcdate": 1489897057822, "number": 4, "id": "Sk5D8tiie", "invitation": "ICLR.cc/2017/workshop/-/paper146/public/comment", "forum": "S1gNakBFx", "replyto": "S1gNakBFx", "signatures": ["~Kyle_Kastner1"], "readers": ["everyone"], "writers": ["~Kyle_Kastner1"], "content": {"title": "Some questions on the experimental setup", "comment": "Thanks for updating the paper, and providing some samples for the method. We have been looking forward to this paper, and after analyzing the samples we have a few questions with regard to the experimental procedure.\n\nIn the paper it is mentioned \"We normalize all files to 16,000 Hz and generate high-resolution patches of length 6000.\". This means that the high resolution samplerate is always 16kHz, with 2, 4, 6, and 8x tasks corresponding to 8, 4, 2.666, and 2kHz downsampled input, correct? Along with Nyquist frequencies of 4kHz , 2kHz, 1.333kHz, and 1kHz?\n\nThis would also mean each training sample corresponds to .375 seconds. Is any special care taken to avoid training and/or evaluating on silent or near silent frames, or is the full dataset used to build the training and test sets?\n\nAre all of these samples from the test set in each case?\n\nSome of the sample files in the samples github repo (such as samples/sp1/{4,6,8}) are all at the same sample rate (16k), while others (such as samples/msp/4) have the low-resolution (.lr.wav) at 4k sample rate, as might expected, while still others (such as samples/msp/2) have the low resolution at 32k sample rate. Can you clarify a bit how the low-resolution base was generated - tools, filters and so on?\n\nIn particular, we notice that samples/sp1/4/* seems to be processed significantly differently than sp1/{6, 8} - see the plots here ( https://github.com/kastnerkyle/analysis_of_audio_superresolution_using_neural_nets/blob/master/sp1_4_plots/1_4_hr_lr.png vs https://github.com/kastnerkyle/analysis_of_audio_superresolution_using_neural_nets/blob/master/sp1_6_plots/1_6_hr_lr.png ). sp1 6 and 8 appear to have heavy aliasing, but also do not appear to be low-pass filtered like sp1/4. \n\nIn msp/2 the low sample rate sounds somewhat downsampled, but when plotted the spectrum stops at the same place as the hr (samples/msp/2/msp.*.lr.wav all have 32kHz sample rate, vs samples/msp/2/msp.*.hr.wav at 16kHz). See the plots here ( https://github.com/kastnerkyle/analysis_of_audio_superresolution_using_neural_nets/blob/master/msp_2_plots/3_2_hr_lr.png ), noting that lr has 2x the samplerate - this means that the \"upsampling\" would have full access to the same information for the msp 2x task, if these samples match the preprocessing procedures. \n\nWe also see heavy banding artifacts in some of the proposed method's reconstructions - see the plots here ( https://github.com/kastnerkyle/analysis_of_audio_superresolution_using_neural_nets/tree/master/banding_artifacts_plots ). These artifacts seem to show up in piano and msp, but not sp1 (though sp1 has some artifacts which appear to run the duration of the signal, still).\n\nCan the authors comment if they observed this as well, or have an explanation to why this particular strong banding artifact exists? It seems the reconstructions could greatly improve by eliminating this issue - especially in the speech case. The piano harmonics may well be relatively unaffected.\n\nWe also note that some of the reconstructions seem to be creating \"aliases\" across the band - see the inversion pattern on the right part of this plot ( https://github.com/kastnerkyle/analysis_of_audio_superresolution_using_neural_nets/blob/master/alias_plots/piano_1_4.png ). This may just be behavior of the superresolution model, but it is interesting.\n\nIn particular, signal reconstruction from heavily aliased signals (such as what sp1/6 and sp1/8 appear to be) would be useful, but different from what is normally considered superresolution. The issue with msp 2x seems troubling, but may be a result of the procedure to give samples in the repo. \n\nThis could also be an issue with our visualization or analysis (plotting script in Python is included in the repo), but given the differences we were hoping the authors could clarify how the downsampled signals were created, along with some of the other questions."}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367464511, "tcdate": 1487367464511, "id": "ICLR.cc/2017/workshop/-/paper146/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper146/reviewers"], "reply": {"forum": "S1gNakBFx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367464511}}}, {"tddate": null, "tmdate": 1489758993712, "tcdate": 1489758993712, "number": 2, "id": "BJczsDFsg", "invitation": "ICLR.cc/2017/workshop/-/paper146/official/comment", "forum": "S1gNakBFx", "replyto": "ByzdjAOjg", "signatures": ["ICLR.cc/2017/workshop/paper146/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper146/AnonReviewer1"], "content": {"title": "Score updated", "comment": "\nI would like to thanks a lot for their detailed response and for providing audio samples.\n\nI believe that the samples are actually good, although some artifacts can be heard for larger up-sampling scales. It is hard to judge without a solid baseline though. I think that the samples should come along some strong baselines, such an HMM-based system or some recently proposed method, such as the work by Li et al or for example,\n\nPeharz, R. et al. \"Modeling speech with sum-product networks: Application to bandwidth extension.\" ICASSP, 2014.\n\nThe paper would be much stronger with a detailed qualitative comparison with other methods. \n\nIn any case, the samples are good and I find this results interesting given the limitations of the loss function. Thus, I believe that it is interesting to have this paper in the workshop. I updated the score to 6."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367464515, "tcdate": 1487367464515, "id": "ICLR.cc/2017/workshop/-/paper146/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "S1gNakBFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper146/reviewers", "ICLR.cc/2017/workshop/paper146/areachairs"], "cdate": 1487367464515}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489758888627, "tcdate": 1489242981830, "number": 2, "id": "ryRvsKbox", "invitation": "ICLR.cc/2017/workshop/-/paper146/official/review", "forum": "S1gNakBFx", "replyto": "S1gNakBFx", "signatures": ["ICLR.cc/2017/workshop/paper146/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper146/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "The paper proposes an end-to-end method for audio super resolution. The proposed approach treats this problem as a regression task: predict the high resolution audio signal from the lower resolution one operating directly with raw audio samples.\n\nPros:\n\n+ The method is very simple and straight forward (if it works well, it would be a big plus)\n+ The paper is well written.\n\nCons:\n\n+ The method optimizes a measure that is not correlated with perceptual quality.\n+ Current results seem to be competitive with previous approaches in terms of PSNR, but it is not clear the perceptual relevance\n\nIt would be very surprising to me that using an L2 loss in the time domain would lead to good perceptual results, particularly when having large upscaling factors, as there is a lot of uncertainty in the desired reconstruction. While some high frequencies can be predicted based on the statistics of the training data, some aspects of the signal are essentially unpredictable. For instance, predicting in time domain the samples of an unvoiced sound (which is essentially colored noise) would be impossible, thus, the best strategy (in terms of L2) would be to predict the mean. Because of this, I would expect a good reconstruction in terms of MSE would lead to smooth signals. The results shown in Figure 2, show that this is certainly not very dramatic, but ultimately qualitative evaluation is key.\n\nA similar issue is encountered when up-sampling images, where people have looked for better losses, see for instance (many more references look at the same issues):\n\nBruna, et al. \"Super-resolution with deep convolutional sufficient statistics.\" ICLR 2016.\nJohnson, et al. \"Perceptual losses for real-time style transfer and super-resolution.\" ECCV, 2016.\nLedig, Christian, et al. \"Photo-realistic single image super-resolution using a generative adversarial network.\" arXiv preprint arXiv:1609.04802 (2016).\n\nI am not surprised that the model gives good results in terms of SNR, as this is the loss being optimized for. However, it is well known that SNR is not linked to good perceptual quality. It would be interesting to provide PESQ scores for the speech samples, as well as audio examples of the enhanced samples (if not a perceptual evaluation). \n\nHow does the proposed model compare with Li et al in terms of computational complexity? Meaning, operating in raw audio should be much more demanding than in higher level features.\n\nPlease provide more details regarding the architecture. I understand that the up-sampled signal could be a good input to the network, but it is a bit counter intuitive to me to up-sample the audio signal to then use downsampling modules. Why not to just start from the original input?\n\nThe authors mention that this technique would help speed up methods that work directly in the audio domain (I assume that by producing compressed samples and then expanding them with the proposed method). For example in WaveNet type of models, it could be a good idea to replace the L2 norm with the likelihood function given by the learned model. That would be in my a opinion a more sensible loss function. \n\nI am open to reconsider my review based on the the authors response and, in particular, if they provide audio samples.\n\nSCORE UPDATED FROM 4 to 6.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489242982783, "id": "ICLR.cc/2017/workshop/-/paper146/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper146/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper146/AnonReviewer2", "ICLR.cc/2017/workshop/paper146/AnonReviewer1"], "reply": {"forum": "S1gNakBFx", "replyto": "S1gNakBFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489242982783}}}, {"tddate": null, "tmdate": 1489722217739, "tcdate": 1489722217739, "number": 3, "id": "ByzdjAOjg", "invitation": "ICLR.cc/2017/workshop/-/paper146/public/comment", "forum": "S1gNakBFx", "replyto": "ryRvsKbox", "signatures": ["~Volodymyr_Kuleshov1"], "readers": ["everyone"], "writers": ["~Volodymyr_Kuleshov1"], "content": {"title": "We are releasing more samples, as requested by the reviewer, and would be grateful if they could consider them in their review", "comment": "We have just added a dozen more samples to our project webpage: \n\nhttps://kuleshov.github.io/audio-super-res/\n\nThese include piano samples, as well as samples from Speaker 1 from a retrained model that gets rid of the slight background noise.\n\nThe main concern of Reviewer 1 was that the L2 loss cannot lead to good reconstructions, as it would overly smooth the signal. Our samples demonstrate that this is clearly not the case, and the L2 loss works very well.\n\nWe would like to ask the reviewer to please update their review in light of this additional material. Tonight appears to be the last date for doing this."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367464511, "tcdate": 1487367464511, "id": "ICLR.cc/2017/workshop/-/paper146/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper146/reviewers"], "reply": {"forum": "S1gNakBFx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367464511}}}, {"tddate": null, "tmdate": 1489572788410, "tcdate": 1489572788410, "number": 1, "id": "By3h79Usg", "invitation": "ICLR.cc/2017/workshop/-/paper146/official/comment", "forum": "S1gNakBFx", "replyto": "rkv6eYUjl", "signatures": ["ICLR.cc/2017/workshop/paper146/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper146/AnonReviewer2"], "content": {"title": "re-review", "comment": "I was quite annoyed to learn that I spent time reviewing the wrong version of the paper. If the option is given to the authors to submit updated versions of their manuscripts after the submission deadline has passed, this should be communicated to the reviewers so they are always aware of the latest version. As it stands, I printed the manuscripts assigned to me when I received the assignment, and didn't check afterwards. Consider this an OpenReview feature request :)\n\nThat said, I'm not sure I agree with the policy of letting authors upload significantly different versions of their papers after the deadline has passed. It would seem more fair to me to hold all authors to the same deadline, and only allow modifications after that deadline in exceptional circumstances. This would avoid situations like this. As it stands, papers that get assigned very eager reviewers who submit their reviews the next day are at a disadvantage, because the authors would have less time beyond the deadline to update their manuscripts.\n\nAnyway, none of this is the authors' fault of course. Many of my criticisms of the first version of the paper no longer apply, so I took a look at the new version. It looks like I am no longer able to update my rating for the original review, so I'll just include a new rating in this comment and I suppose it will be up to the conference organisers to decide what they do with that.\n\n\n- The new version actually addresses my primary concerns: it includes a baseline and provides multiple evaluation metrics.\n\n- Note that my remark about the definition of R does not apply to a single sentence. 'R' seems to be consistently referred to as a \"sampling rate\" throughout the paper, so either the definition needs to be changed (which would make the most sense to me) or it needs to be replaced by R/T throughout the paper.\n\n- I hadn't seen the single-ReLU residual block before, but it seems to be more widely used than I thought. I don't really understand the advantage of it over the \"full preactivation\" block defined in https://arxiv.org/abs/1603.05027 though (Figure 4e). This seems to be the standard way of doing things nowadays, and is arguably a better way to avoid having nonlinearities in the residual pathways of the network, because it doesn't cost you a nonlinearity and avoids stacking multiple linear layers on top of each other.\n\n- My remark about the spectrogram figure seems to have been incorrect, I was confused because the frequency axis is horizontal and the time axis is vertical, which is nonstandard. To avoid this confusion, it might be a good idea to transpose these figures.\n\n- From the authors' response, the subsampling procedure also seems to be correct after all. However, the new version of the paper still doesn't mention the low pass filtering and the 2nd paragraph of Section 2 still implies that a subset of the original signal samples is taken without filtering. This would need to be fixed.\n\n- The description of Magnatagatune would still need to be fixed, and a random split is really unfortunate with reproducibility in mind, but I guess this is not a huge deal.\n\n\nOverall, the new version is a significant step up which I would rate 7: accept.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367464515, "tcdate": 1487367464515, "id": "ICLR.cc/2017/workshop/-/paper146/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "S1gNakBFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper146/reviewers", "ICLR.cc/2017/workshop/paper146/areachairs"], "cdate": 1487367464515}}}, {"tddate": null, "tmdate": 1489567935081, "tcdate": 1489567935081, "number": 2, "id": "rkv6eYUjl", "invitation": "ICLR.cc/2017/workshop/-/paper146/public/comment", "forum": "S1gNakBFx", "replyto": "SyPAYAeil", "signatures": ["~Volodymyr_Kuleshov1"], "readers": ["everyone"], "writers": ["~Volodymyr_Kuleshov1"], "content": {"title": "We kindly ask AnonReviewer2 to review the latest version of manuscript, which we uploaded about 2-3 weeks ago.", "comment": "We thank the reviewer very much for their detailed and careful review.\n\nPlease note that we have uploaded an updated version of our paper about 2-3 weeks ago. It includes an updated architecture, a comparison to two baselines, new metrics, and many other improvements. We apologize for the confusion, and we would like to kindly ask the reviewer to evaluate the most current version of our paper.\n\nNote also that we are releasing samples from our method here: https://kuleshov.github.io/audio-super-res/\n\n----\n\nHere is our response to the reviewer's comments on the first version of our paper:\n\n- We included Zhang et al. (2017) to highlight that models over raw audio are an active area of research; we will certainly include the earlier work on raw audio as well.\n\n- We apologize for the errors in the formatting of the citations; we have already corrected this bug in the current version.\n\n- We thank the reviewer for catching this typo: the sentence should read \"R/T is the sampling rate of the signal\" (instead of \"R is the sampling rate\u2026\")\n\n- Note that we have updated our model architecture, hence this issue doesn't apply anymore. But we would like to point out that our residual model uses a popular version of a residual block that contains only one ReLU. See e.g. this blog post for a comparison of our residual block design to others (look at the third figure in particular): http://torch.ch/blog/2016/02/04/resnets.html\n\nSee also the reference tensorflow implementation of a ResNet, which uses the same design us us (look at the 2 sub layer version): https://github.com/tensorflow/models/blob/master/resnet/resnet_model.py\n\nAlso, note that we modeled our architecture based on the SRResNet of Ledig et al. (2016), which uses the same design for the residual block.\n\n- Our choice of a length-9 filters comes from the Resnet architecture for images, which uses 3x3 filters. Smaller sizes did not work as well, and larger sizes did not add improvements. We did not try dilated convolutions, but we have found that strided convolutions improved performance, and we are using them the latest version of the paper.\n\n- We applied a low-pass filter before subsampling the signal, and will make this clear in the final version. We also trained our method on low-resolution input without the low-pass filter, and the results were essentially identical. Interestingly, a method trained on filtered results seemed to introduce noise when it was run on non-filtered data (and vice versa).\n\n- Indeed, the 188 tags are not mutually exclusive, and we will make this more clear. Note that we do not use these tags in our experiments. We did not realize at first that there is an official split, hence we created our own. Note that we focus on a different music dataset in the latest version of the manuscript.\n\n- In the latest version of our paper, we use two objective metrics: SNR and log-distortion. We are happy to add any additional metrics as well.\n\n- We are using two baselines in the most current version of our manuscript: cubic splines and the deep neural network method of Li and Lee (2015), which seems to be among the most deep learning approaches in the bandwidth extension literature.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367464511, "tcdate": 1487367464511, "id": "ICLR.cc/2017/workshop/-/paper146/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper146/reviewers"], "reply": {"forum": "S1gNakBFx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367464511}}}, {"tddate": null, "tmdate": 1489567616111, "tcdate": 1489567616111, "number": 1, "id": "HJ_KJFUsl", "invitation": "ICLR.cc/2017/workshop/-/paper146/public/comment", "forum": "S1gNakBFx", "replyto": "ryRvsKbox", "signatures": ["~Volodymyr_Kuleshov1"], "readers": ["everyone"], "writers": ["~Volodymyr_Kuleshov1"], "content": {"title": "Samples from the method + Explanation for why L2 loss works in our case", "comment": "We thank the reviewer very much for their detailed and careful review.\n\nThe reviewer's main concern is that our loss function is not correlated with perceptual quality, and hence would not produce good results.\n\nFirst, we are releasing samples for our method and a cubic baseline (the DNN baseline will be added shortly). Our method achieves good reconstruction quality and outperforms the baseline.\n\nhttps://kuleshov.github.io/audio-super-res/\n\nWhy does our technique produce good quality samples? While we agree that the L2 loss is not perfectly correlated with audio quality, in practice, the correlation seems to be high enough to produce good results. Note also that most papers in the image super-resolution literature (including most papers published in the last year) report excellent results using the L2 loss, e.g.:\n\nDong et al., \"Image Super-Resolution Using Deep Convolutional Networks\" (2014)\nKim et al., \"Accurate Image Super-Resolution Using Very Deep Convolutional Neural Networks\" (2016)\nShi et al., \"Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network\" (2016)\n\nWhile the L2 loss may not allow us to recover unvoiced sounds, our receptive field (~0.25s) is large enough so that neighboring (voiced) phonemes may be used to recover these unvoiced sounds. Most importantly, many phonemes seem to retain enough low-frequency content to enable their recovery. \n\nConsider, for example, Sample 2 from the Single Speaker dataset. The \"K\" in \"RISK\" (the last word in the utterance) is mostly lost at r=6 and higher. Nonetheless, our method recovers the lost phoneme at r=6,8. See the very last sample on the page for an even more interesting example of this. Overall, the reconstructed samples still sound more \"dull\" than the original, lacking the full range of the true high frequencies. That is precisely due to the phenomenon described by the reviewer: the L2 loss \"smoothens\" the time-domain waveform, flattening some of the high frequencies. However, the phenomenon is not nearly as severe as described by the reviewer.\n\nWe tried to combine our method with perceptual losses based on features derived from randomly initialized neural networks, as well as GAN-based objectives. The perceptual losses did not significantly improve audio quality. The GANs added additional high frequencies, but also introduced many artifacts, which made the overall quality less pleasant than that of the L2-trained model. We plan to explore perceptual and adversarial losses for audio in follow-up work.\n\nThese are our responses to the remaining concerns of the reviewer:\n\n- In addition to SNR, our paper also reports log-distortion (LSD) values. We found that LSD correlates better with quality (e.g. the spline baseline obtains good SNR, but the output still sounds \"dull\", whereas LSD correlates much better with our perception of quality). We tried computing PESQ scores using the standard implementation from https://github.com/dennisguse/ITU-T_pesq, but found that it crashed on more than half of our samples. We will be very thankful if the reviewers can suggest better implementations or other important metrics besides SNR, LSD, and PESQ.\n\n- Our implementation of Li et al. is 2-10x faster than that of our method, depending on the sample. Note that the network of Li et al. operates over the full Fourier representation of the down-sampled signal, hence the dimensionality of its input is not significantly smaller than that of our method (i.e. less than an order of magnitude). Both methods can perform inference faster than real-time, hence speed is not the main constrain for real-world deployment. Both methods could also be significantly optimized for speed.\n\n- Our main reason for first up-sampling the signal is that it allows us to introduce a residual connection between the up-sampled input and the output of the model. This in turn makes training much faster (since the network effectively starts from the cubic interpolation), and this is very useful on larger datasets like VCTK."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367464511, "tcdate": 1487367464511, "id": "ICLR.cc/2017/workshop/-/paper146/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper146/reviewers"], "reply": {"forum": "S1gNakBFx", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367464511}}}, {"tddate": null, "tmdate": 1489197519199, "tcdate": 1489197519199, "number": 1, "id": "SyPAYAeil", "invitation": "ICLR.cc/2017/workshop/-/paper146/official/review", "forum": "S1gNakBFx", "replyto": "S1gNakBFx", "signatures": ["ICLR.cc/2017/workshop/paper146/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper146/AnonReviewer2"], "content": {"title": "significant flaws in model design and evaluation", "rating": "3: Clear rejection", "review": "The paper proposes a neural network approach to audio upsampling, or equivalently, reconstructing high-frequency signal components from low-frequency components. Most of the paper is pretty clearly written, but it has some significant flaws in model design and evaluation.\n\n- The introduction refers to Zhang et al. (2017) for speech recognition on raw audio, instead of much earlier work by e.g. Sainath et al., Jaitly et al. and Hoshen et al.\n\n- The statement in section 2 that \"very little work has been done on audio signals\" is unnecessarily broad and ignores a large body of prior work on using neural networks for e.g. source separation.\n\n- All citations seem to be in-text (using \\citet), but this is not appropriate in most cases.\n\n- The definition of R in section 2 is wrong. It is supposed to represent the sample rate, but following this definition it actually represents the total number of samples (which is only equal to the sample rate in Hz if the audio signal is exactly one second long). This is confusing.\n\n- The model described in section 3 / figure 1 has some unusual architectural properties that aren't really justified anywhere. The residual blocks only seem to contain a single ReLU activation layer, so that means the network has multiple linear layers directly following each other in many places, which is redundant. If this is not the case, the model should be described more clearly.\n\n- Why use filter size 9? The motivation for using large filters is not explained. It would also be useful to discuss the use of dilated convolutions instead (and why the authors chose not to use them).\n\n- The downsampling procedure (\"... produce corresponding low-resolution patches by recording every r-th position in j...\") is improper: an anti-aliasing low-pass filter needs to be applied before decimation. (Unless all input signals are expected to be obtained by downsampling high-resolution signals this way, but that would defeat the point.)\n\n- The description of magnatagatune in section 4 is wrong: the 188 tags do not correspond to genres, and they are not mutually exclusive. The validation split used is also non-standard and random, making reproduction more difficult.\n\n- Figure 2 is difficult to interpret. The spectrogram for the low-resolution signal seems to have been produced with an FFT that assumes the full samplerate, as it stands this shows the spectrogram of a signal that was sped up 4x, so it's unclear how this should actually be interpreted. Comparing the first and 3rd spectrogram, it looks like it features some spurious harmonics (e.g. around timestep 300). It would be useful to provide the corresponding audio samples somewhere.\n\n- PSNR in the time domain is a poor error measure for this task, because it does not take into account human perception: logarithmic loudness perception, frequency masking, frequency sensitivity (Fletcher-Munson), phase insensitivity etc. It is difficult to find the right way to measure this quantitatively, but providing more than one measure would be much more informative.\n\n- The paper is missing baselines. Even measurements for a few simple upsampling strategies (nearest-neighbour, bilinear, ...) would be very informative. As it stands the numbers are difficult to interpret.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489242982783, "id": "ICLR.cc/2017/workshop/-/paper146/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper146/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper146/AnonReviewer2", "ICLR.cc/2017/workshop/paper146/AnonReviewer1"], "reply": {"forum": "S1gNakBFx", "replyto": "S1gNakBFx", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper146/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489242982783}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1488448843438, "tcdate": 1487367463882, "number": 146, "id": "S1gNakBFx", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "S1gNakBFx", "signatures": ["~Volodymyr_Kuleshov1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Audio Super-Resolution using Neural Networks", "abstract": "We propose a neural network-based technique for enhancing the quality of audio signals such as speech or music by transforming inputs encoded at low sampling rates into higher-quality signals with an increased resolution in the time domain. This amounts to generating the missing samples within the low-resolution signal in a process akin to image super-resolution. On standard speech and music datasets, this approach outperforms baselines at 2x, 4x, and 6x upscaling ratios. The method has practical applications in telephony, compression, and text-to-speech generation; it can also be used to improve the scalability of recently-proposed generative models of audio.", "pdf": "/pdf/9c9a31f1c85665d30fa8fc378cf26c167a7012df.pdf", "paperhash": "kuleshov|audio_superresolution_using_neural_networks", "keywords": [], "conflicts": ["mcgill.ca", "cornell.edu", "berkeley.edu"], "authors": ["Volodymyr Kuleshov", "S. Zayd Enam", "Stefano Ermon"], "authorids": ["kuleshov@cs.stanford.edu", "zayd@stanford.edu", "ermon@cs.stanford.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 11}