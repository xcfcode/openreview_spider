{"notes": [{"id": "SkeuexBtDr", "original": "BJegi6yFvS", "number": 2105, "cdate": 1569439728361, "ddate": null, "tcdate": 1569439728361, "tmdate": 1589559040519, "tddate": null, "forum": "SkeuexBtDr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "WTOjR_bWd", "original": null, "number": 1, "cdate": 1576798740656, "ddate": null, "tcdate": 1576798740656, "tmdate": 1576800895595, "tddate": null, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Decision", "content": {"decision": "Accept (Spotlight)", "comment": "The paper addresses the problem of costly human supervision for training supervised learning methods.\nThe authors propose a joint approach for more effectively collecting supervision data from humans, by extracting rules and their exemplars, and a model for training on this data.\nThey demonstrate the effectiveness of their approach on multiple datasets by comparing to a range of baselines.\n\nBased on the reviews and my own reading I recommend to accept this paper.\nThe approach makes intuitively a lot of sense and is well explained.\nThe experimental results are convincing. ", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718223, "tmdate": 1576800268670, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Decision"}}}, {"id": "Bkxefw05KS", "original": null, "number": 1, "cdate": 1571641095870, "ddate": null, "tcdate": 1571641095870, "tmdate": 1574411521424, "tddate": null, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "In case of a lack of labeled data, human-designed rules can be used to label the unlabelled data. This paper proposes a better rule-based labeling method by restricting the coverage of the rule, which is based on the assumption that the rules can be applied to a local region but can not be 'over-generalized' to the whole sample space. The coverage of the rule is represented by a conditional distribution, which is parameterized as a neural network and jointly learned with the classifier network.\n\nI think this paper is tackling an important problem in machine learning, and the proposed idea is novel and interesting. I vote for weak acceptance because there are still some technical points that are not well-addressed enough:\n\nFirst, although the intuition of this model makes a lot of sense to me, the construction of the loss function is quite heuristic, with a lot of terms simply summing together, making it hard to judge which components are most important for the final results. A more principled and integrated framework like EM could be more convincing to me.\n\nSecond, it seems the unlabelled data is only used in the causal constraint term (the last term in Eqn 5) and it is controlled by a coefficient \\gamma. It is a bit unclear to me whether the unlabelled data is fully utilized while it only constraints the causal relation, as one can also use labeled data for constraining the causal relation. Also, why not include labeled data for this constraint regularization?\n\nAnother minor question is after the two networks are trained, will you only use the learned classifier for test data, or, do you also use the conditional distribution in the testing phase and compute an expectation of the predicted class? and why?\n\nAlso, what's the purpose of section 6 in the appendix?\n\nI general I think the idea of learning a conditional distribution to constrain the use of rules is an interesting and novel idea. The paper can be further improved if the algorithm can be more principled.\n\n\n---- after reading the response ---\n\nThanks for answering the questions. I believe some of these explanations can be added to the final version to improve clarity. My score does not change, but overall I advocate to accept this paper.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575335670118, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Reviewers"], "noninvitees": [], "tcdate": 1570237727646, "tmdate": 1575335670132, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Review"}}}, {"id": "BkxHJx_3ir", "original": null, "number": 6, "cdate": 1573842908931, "ddate": null, "tcdate": 1573842908931, "tmdate": 1573842908931, "tddate": null, "forum": "SkeuexBtDr", "replyto": "HklrTqcYir", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for answering the questions."}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeuexBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2105/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2105/Authors|ICLR.cc/2020/Conference/Paper2105/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146249, "tmdate": 1576860558520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment"}}}, {"id": "BygrBzoKsB", "original": null, "number": 5, "cdate": 1573659197136, "ddate": null, "tcdate": 1573659197136, "tmdate": 1573659197136, "tddate": null, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank the reviewers for constructive and insightful feedback on our work. We have addressed each reviewer\u2019s comments/questions individually.\n\nBased on the reviewers\u2019 feedback, we have made the following changes in the revised version:\n1. Added missing captions to figures and tables\n2. Modified section numbering in supplementary\n3. Fixed notational and typing errors\n4. Combined the tables of hyperparameters in supplementary for better appearance"}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeuexBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2105/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2105/Authors|ICLR.cc/2020/Conference/Paper2105/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146249, "tmdate": 1576860558520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment"}}}, {"id": "SygQVgiYoB", "original": null, "number": 4, "cdate": 1573658666850, "ddate": null, "tcdate": 1573658666850, "tmdate": 1573658666850, "tddate": null, "forum": "SkeuexBtDr", "replyto": "SyebtskccS", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment", "content": {"title": "Response to Reviewer #4", "comment": "Thank you for providing valuable feedback on our work. We have addressed your comments/questions below:\n\n> Some remarks on how the paper could become stronger: The type of problem that can be assessed with the proposed method seems to be fairly specific: most tasks studied are classification of natural language utterances. That is a natural class of tasks, since it is easy to imagine how labellers can formulate rules. However, it would have been very interesting if the authors had found ways to allow for more diversity here.  In general, I have the impression that there are more interesting ideas and results to be found in the direction explored by this paper - what about, e.g., allowing the classifier to add rules of its own?\n\nExtending this work to more diverse tasks is our goal in future research effort. Allowing the classifier to add rules of its own seems an interesting direction.\n\n> The paper would benefit from some general editing with regards to appearance; for example, the supplementary material sections continue the regular section numbering, instead of having their own; the images are missing captions, and sometimes have somewhat unorthodox axis tick labelling.\n\nWe have fixed these in the revision."}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeuexBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2105/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2105/Authors|ICLR.cc/2020/Conference/Paper2105/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146249, "tmdate": 1576860558520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment"}}}, {"id": "HklrTqcYir", "original": null, "number": 3, "cdate": 1573657277491, "ddate": null, "tcdate": 1573657277491, "tmdate": 1573657277491, "tddate": null, "forum": "SkeuexBtDr", "replyto": "Byg7vz-pFr", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for providing valuable feedback on our work. We have addressed your comments/questions below:\n\n> Minor Problems\n\nWe have fixed the notations and other typing errors in the revised version.\n\n> Since each rule can be regarded as experts or weak learners, how is this work related to learning strong learners from weak learners (boosting/ensemble)?\n\nOne big difference is that most rules cover only a small number of examples, and for a given example only a small number of rules cover them (Table 1). This is unlike the setting of typical \u201cweak to strong learners\u201d framework where all weak learners predict labels for all examples.    \n\n> Is it possible that the algorithm can incorporate more information of the rules, for example, the structure of the logical formulas?\n\nIn this work, we wanted to treat rules as black-box functions. Exploiting the logical formulas should help but will entail more application-specific encodings. \n\n> Is it possible to generalize the idea to RL?\n\nYes, our method can be extended to RL in the imitation learning setting in order to reduce the amount of human-generated data used in imitation learning. A rule in this scenario will be a partial policy, i.e., it will map some states to an action but may not cover other states. During training, the state space could either be sampled completely randomly or by following the policy formed by applying the current rule weights to each rule, or a combination.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeuexBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2105/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2105/Authors|ICLR.cc/2020/Conference/Paper2105/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146249, "tmdate": 1576860558520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment"}}}, {"id": "ByxNcIqtsB", "original": null, "number": 2, "cdate": 1573656204167, "ddate": null, "tcdate": 1573656204167, "tmdate": 1573656204167, "tddate": null, "forum": "SkeuexBtDr", "replyto": "Bkxefw05KS", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment", "content": {"title": "Response to Reviewer #1 ", "comment": "Thank you for providing valuable feedback on our work. We have addressed your comments/questions below:\n\n> First, although the intuition of this model makes a lot of sense to me, the construction of the loss function is quite heuristic, with a lot of terms simply summing together, making it hard to judge which components are most important for the final results. A more principled and integrated framework like EM could be more convincing to me.\n \nOur first attempt was indeed a principled EM framework which we called the Posterior regularization (PR) method that we derive in detail in the Supplementary.  Unfortunately, we found that this EM formulation performed worse than the only-L baseline in two of the five datasets (Table 2). That is what led us to our current non-EM formulation which provided higher accuracy while being (frustratingly :) simpler.  Also, it was more robust to hyper-parameter selection and initializations than the EM formulation. Finally, if you compare our ImplyLoss objective (Eq 5) with  EM\u2019s objective (Eq 14), the two labeled loss terms are identical.  The only difference is in the term involving unlabeled data.  In EM,  the KL term biases the model to match the estimated distribution by the teacher which is changing and could be incorrect.  In contrast, the ImplyLoss does not introduce any such bias as we discuss in Section 2.1 (below Table 1). \n\n> Second, it seems the unlabelled data is only used in the causal constraint term (the last term in Eqn 5) and it is controlled by a coefficient \\gamma. It is a bit unclear to me whether the unlabelled data is fully utilized while it only constraints the causal relation, as one can also use labeled data for constraining the causal relation. \n\nYes, unlabelled data is only used in the last term of Eqn 5. This term allows full utilization of the correctly generalized unlabelled data as follows: When P(r_j =1|x_i) is close to one, the causal term reduces to log-likelihood of labeling x_i as l_j, allowing us to treat x_i as a labeled instance. The same causal term allows us to ignore wrongly generalized instances as follows:  when P(r_j =1|x_i) is close to zero, the gradient on the label parameters (\\theta) is vanishingly small.\n \n> Also, why not include labeled data for this constraint regularization?\n\nEmpirically, we found no difference between including labeled data in the causal term or not.\nThe causal term when fitted with the clean labels for \u2018y_i\u2019 and r_{ji} (when available) reduce to the likelihood term LL(\\theta). We did not want to double count labeled instances and distort the training distribution.   \n\n> Another minor question is after the two networks are trained, will you only use the learned classifier for test data, or, do you also use the conditional distribution in the testing phase and compute an expectation of the predicted class? and why?\n\nWe only use the classification network (parametrized by \\theta) for test data. The conditional distribution is only applicable when a rule covers the instance. However, an example in the test set may not be covered by any of the rules (For one of our datasets coverage of rules is only 14% ).  We also tried to predict the 'y' that minimizes a joint score over y and r_ji conditionals. But in practice we did not see much difference.\n\n> Also, what's the purpose of section 6 in the appendix?\n\nSection 6 describes our attempt of an EM-based framework (Algorithm 1, Pg. 13) for this task. We compare implication loss with this alternative approach of jointly learning the classifier and rule network by imposing the same causal constraints in a different way.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeuexBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2105/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2105/Authors|ICLR.cc/2020/Conference/Paper2105/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146249, "tmdate": 1576860558520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment"}}}, {"id": "Byg7vz-pFr", "original": null, "number": 2, "cdate": 1571783258830, "ddate": null, "tcdate": 1571783258830, "tmdate": 1572972382433, "tddate": null, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes a novel semi-supervised learning paradigm where the algorithm learns from both clean instance-level labels and noisy rule-level labels, and also a simple but effective algorithm as solution. The proposed algorithm employs a set of latent coverage variables to bridge two kinds of supervisions and uses a soft causal constraint on the coverage variables to denoise the noisy labels. Empirically the paper demonstrates the effectiveness of the proposed algorithm with consistent improvements over several baselines on a wide range of classification tasks.\n\nThe idea of using macro-level noisy labels as part of the supervision is novel, and it could potentially trigger a paradigm shift on many research areas in machine learning. The proposed methodology is clean but effective, with extensive experimental support. Therefore I vote for accepting this submission.\n\nMinor problems\n\n(1) Abuse of notation \\phi in section 2.\n(2) \"... from traing the classifier ...\" in page 4.\n\n\nMore (further) questions\n\n(1) Since each rule can be regarded as experts or weak learners, how is this work related to learning strong learners from weak learners (boosting/ensemble)?\n(2) Is it possible that the algorithm can incorporate more information of the rules, for example, the structure of the logical formulas?\n(3) Is it possible to generalize the idea to RL?"}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575335670118, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Reviewers"], "noninvitees": [], "tcdate": 1570237727646, "tmdate": 1575335670132, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Review"}}}, {"id": "SyebtskccS", "original": null, "number": 3, "cdate": 1572629368763, "ddate": null, "tcdate": 1572629368763, "tmdate": 1572972382385, "tddate": null, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper addresses the problem that labelled data is often unavailable in the quantities required to train effective models. It deals with classification problems, and proposes a method to obtain more (but weaker) labels data with minimal involvement from human labellers, by asking them to generalize their labelling decisions into rules and then learning restrictions on those rules to avoid learning incorrectly generalized labels. The motivating observation is that human labellers are often able to make such generalizations in much less time than it would take them to apply that rule to a large dataset themselves. This is an interesting idea, especially for cases where labelling capacity is limited. The point being made about the labelling noise not being random in this situation is an interesting one - it might be worth exploring this notion further on its own also in contexts where the source of the noise is unknown.\n\nThe presentation of the implementation the authors choose for their proposed approach is clear, and the implementation is sensible. The experimental section includes comparisons to a number of alternative methods, and the authors find that their method outperforms all others, including recent methods for combining (noisy) rule-based labels and (clean) human-sourced labels.\n\nI would argue for accepting this paper. It studies an interesting question, which if answered has the potential to make access to machine learning solutions to certain types of problem significantly cheaper and therefore more widespread. The experiments are well-chosen and show that, depending on the data available and the task, significant gains can be made using the proposed method.\n\nSome remarks on how the paper could become stronger: The type of problem that can be assessed with the proposed method seems to be fairly specific: most tasks studied are classification of natural language utterances. That is a natural class of tasks, since it is easy to imagine how labellers can formulate rules. However, it would have been very interesting if the authors had found ways to allow for more diversity here.  In general, I have the impression that there are more interesting ideas and results to be found in the direction explored by this paper - what about, e.g., allowing the classifier to add rules of its own?\n\nThe paper would benefit from some general editing with regards to appearance; for example, the supplementary material sections continue the regular section numbering, instead of having their own; the images are missing captions, and sometimes have somewhat unorthodox axis tick labelling."}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575335670118, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Reviewers"], "noninvitees": [], "tcdate": 1570237727646, "tmdate": 1575335670132, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Review"}}}, {"id": "SkleMEdwtB", "original": null, "number": 1, "cdate": 1571419143878, "ddate": null, "tcdate": 1571419143878, "tmdate": 1571419143878, "tddate": null, "forum": "SkeuexBtDr", "replyto": "SkeuexBtDr", "invitation": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment", "content": {"comment": "https://github.com/iclrLFRGLE/iclrLFRGLE\n", "title": "Link to anonymized code"}, "signatures": ["ICLR.cc/2020/Conference/Paper2105/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["awasthi@cse.iitb.ac.in", "sghosh@cse.iitb.ac.in", "rasna.goyal66@gmail.com", "sunita@iitb.ac.in"], "title": "Learning from Rules Generalizing Labeled Exemplars", "authors": ["Abhijeet Awasthi", "Sabyasachi Ghosh", "Rasna Goyal", "Sunita Sarawagi"], "pdf": "/pdf/e4d3b0f4237ea03ce6b9b73bd796822f7f84a40c.pdf", "TL;DR": "Coupled rule-exemplar supervision and a implication loss helps to jointly learn to denoise rules and imply labels.", "abstract": "In many applications labeled data is not readily available, and needs to be collected via pain-staking human supervision. We propose a rule-exemplar method for collecting human supervision to combine the efficiency of rules with the quality of instance labels. The supervision is coupled such that it is both natural for humans and synergistic for learning. We propose a training algorithm that jointly denoises rules via latent coverage variables, and trains the model through a soft implication loss over the coverage and label variables. The denoised rules and trained model are used jointly for inference. Empirical evaluation on five different tasks shows that (1) our algorithm is more accurate than several existing methods of learning from a mix of clean and noisy supervision, and (2) the coupled rule-exemplar supervision is effective in denoising rules.", "keywords": ["Learning from Rules", "Learning from limited labeled data", "Weakly Supervised Learning"], "paperhash": "awasthi|learning_from_rules_generalizing_labeled_exemplars", "code": "https://github.com/awasthiabhijeet/Learning-From-Rules", "spotlight_video": "https://youtu.be/TQfq4YdqG3k", "_bibtex": "@inproceedings{\nAwasthi2020Learning,\ntitle={Learning from Rules Generalizing Labeled Exemplars},\nauthor={Abhijeet Awasthi and Sabyasachi Ghosh and Rasna Goyal and Sunita Sarawagi},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeuexBtDr}\n}", "original_pdf": "/attachment/f19f1266e6cb6ff4f421deaf69f3a74128afb581.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeuexBtDr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2105/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2105/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2105/Authors|ICLR.cc/2020/Conference/Paper2105/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146249, "tmdate": 1576860558520, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2105/Authors", "ICLR.cc/2020/Conference/Paper2105/Reviewers", "ICLR.cc/2020/Conference/Paper2105/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2105/-/Official_Comment"}}}], "count": 11}