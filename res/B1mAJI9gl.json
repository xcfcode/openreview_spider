{"notes": [{"id": "BJgcHjKGqX", "original": null, "number": 17, "cdate": 1538591553768, "ddate": null, "tcdate": 1538591553768, "tmdate": 1538591553768, "tddate": null, "forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "invitation": "ICLR.cc/2017/conference/-/paper282/public/comment", "content": {"title": "Explanation of the relation between z, x, h is not clear", "comment": "The hat{x} is reconstructed by hat{z}, and hat{z] is reconstructed from Wx. Here the x should be unknown, but the author seems to treated as knwon. The x goes through the convolution and activation to get h, then the maximal pooling is performed over h, i.e., max_pool(h). Here the problem is the x is unknown. "}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [""], "writers": ["(anonymous)"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642047, "id": "ICLR.cc/2017/conference/-/paper282/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1mAJI9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper282/reviewers", "ICLR.cc/2017/conference/paper282/areachairs"], "cdate": 1485287642047}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396478814, "tcdate": 1486396478814, "number": 1, "id": "S1PHnfIOl", "invitation": "ICLR.cc/2017/conference/-/paper282/acceptance", "forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "While the reviewers found some interest in this work, I'm afraid I have to agree with the critique that the model studied is too simple that its relevance for deep learning is questionable."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396479321, "id": "ICLR.cc/2017/conference/-/paper282/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396479321}}}, {"tddate": null, "tmdate": 1484929726780, "tcdate": 1481827244967, "number": 1, "id": "BkSnmveEl", "invitation": "ICLR.cc/2017/conference/-/paper282/official/review", "forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "signatures": ["ICLR.cc/2017/conference/paper282/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper282/AnonReviewer3"], "content": {"title": "Inconsistent notations with DL, and room for improvements in the presentation and the theory  ", "rating": "5: Marginally below acceptance threshold", "review": "\n\n\nSummary of the paper\n\nThe paper studies the invertiblity of convolutional neural network in the random model. A reconstruction algorithm similar to IHT is proposed for layer-wise inversion of the network.\n \n\nClarity:\n\n- The paper is confusing wrt to standard notations in deep learning.\n\nComments:\n\nThe paper makes two simplifications in the analysis of a CNN, that makes it map to a model based compressive sensing framework:\n\n1-  The non linearity (RELU) is dropped. This is a big simplification, for random gaussian weights for instance we know by JL that we can preserve L_2 distance, when RELU is applied the metric changes (see for instance the kernel for n=1 in  http://cseweb.ucsd.edu/~saul/papers/nips09_kernel.pdf).\n\n2- The pooling operation is modified to be shrinkage operator, that keeps the maximum value in a block  and sets to the zero other values, hence the dimensionality of the pooled representation is the same of the un-pooled representations. Note that in that cases the locations of where the maximum happens is known. This is not the case in the \u2019standard\u2019 max pooling definition. IHT does not map to a forward of a CNN as described in the paper. (see next point)\n\n3- It maybe that the notations  used in the paper are implying some confusions wrt to the standard notations in deep learning. \n\n  - Let z be the standard  pooled representation of dimension \u2018k\u2019. \n  - Define U as the unpooling  operation U(z, locations of maximum) :=  M(Wx,k).\nHence your model of inversion is assuming the knowledge of the \u2019standard\u2019 pooling representation and the switches (max locations). Referring to M as a pooling operation is misleading and confusing, it is the \u2018standard\u2019 unpooling operation.\n - Under this notations W^{\\top} U(z, locations of maximum) is a sensible reconstruction algorithm , with the simplification of ignoring the RELU.\n- Under these notations,  IHT is similar to a backward of the encoding neural network not  a forward. It is known if you take the derivative with respect to the input in a neural network, you get the transpose convolution also known as \u2018deconvolution\u2019 (which is not a correct naming) , hence this IHT iteration is nothing else then the well known transposed convolution.\n\n4- I suggest the authors to rewrite the paper taking into account standard definitions and notation in deep learning as discussed in point 3, and to clearly state that the recovery is done under the knowledge of the switches. \n\n=====\nAfter reading the authors rebuttal and the revisions , the reviewer maintains his main concerns with the paper. Many improvements are still needed for the paper to be ready to be published in ICLR, at the notation , presentation and the theory levels. \n===== \n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512637214, "id": "ICLR.cc/2017/conference/-/paper282/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper282/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper282/AnonReviewer3", "ICLR.cc/2017/conference/paper282/AnonReviewer2", "ICLR.cc/2017/conference/paper282/AnonReviewer1"], "reply": {"forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512637214}}}, {"tddate": null, "tmdate": 1484922122069, "tcdate": 1481901559685, "number": 2, "id": "B1eW8KW4l", "invitation": "ICLR.cc/2017/conference/-/paper282/official/review", "forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "signatures": ["ICLR.cc/2017/conference/paper282/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper282/AnonReviewer2"], "content": {"title": "Review", "rating": "7: Good paper, accept", "review": "\nThe authors propose a theoretical framework to analyze the recoverability of sparse activations in intermediate layers of deep networks, using theoretical tools from compressed sensing. They relate the computations that are performed by a CNN and a particular recovery algorithm (Iterative Hard Thresholding, IHT). They present proofs of necessary conditions for recoverability to hold, and also show detailed empirical evidence of how they hold in practice.\n\nThis is a well-written paper that presents a new angle on why the current CNN architectures work so well. The authors give a brief but sufficient review of the fundamentals of compressed sensing, present their main result relating feed-forward networks and IHT (a surprising result), and progress naturally to a detailed experimental section. The introductory analysis at the beginning of Section 3, in particular, delivers the gist of why the method should work with very approachable and simple math, which is not common in theoretical papers. The increasing complexity of the experiments, done in small steps, shows a nice progression from artificial distributions to a realistic experiment.\n\nA few aspects should be improved. First of all, although the treatment of ReLU non-linearities is sufficient, it is assumed with little discussion that max-pooling non-linearities shouldn't present a problem as well. A discussion of how this is inverted (e.g., with pooling switches) is needed.\n\nThe relationship between feed-forward nets and Algorithm 1 assumes tied weights. It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.\n\nAlthough it might be obvious, it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially, basing the activations of each layer on the one above it (in back-propagation order).\n\nFinally, the filter coherence measure must be defined either mathematically or with a proper reference.\n\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512637214, "id": "ICLR.cc/2017/conference/-/paper282/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper282/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper282/AnonReviewer3", "ICLR.cc/2017/conference/paper282/AnonReviewer2", "ICLR.cc/2017/conference/paper282/AnonReviewer1"], "reply": {"forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512637214}}}, {"tddate": null, "tmdate": 1484756646997, "tcdate": 1484756646997, "number": 16, "id": "SkJ28MpIe", "invitation": "ICLR.cc/2017/conference/-/paper282/public/comment", "forum": "B1mAJI9gl", "replyto": "BkSnmveEl", "signatures": ["~Kibok_Lee1"], "readers": ["everyone"], "writers": ["~Kibok_Lee1"], "content": {"title": "updated response", "comment": "Thank you for your helpful comments! We have updated our responses as follows.\n\nRe: The nonlinearity (ReLU) is dropped.\n- Our analysis does not simply ignore the nonlinearity (ReLU). In the beginning of Section 3, we discussed that \"the assumption that trained CNN filters come in positive-and-negative pair is validated by Shang et al. (2016).\" Due to this fact, we can cut down half of the convolution weights and remove the ReLU unit. We updated the equations to make the reasoning more clear. \n\nRe: The pooling operation is modified to be shrinkage operator. The locations of where the maximum happens (pooling switch) is known in the analysis.\n- We would like to clarify that our theoretical analysis does not require the information about the pooling switches (i.e., locations of where the maximum happens). We emphasize this point in the end of Section 3 (right after Theorem 3.3). \nWe also revise our submission to define {\\mathbb M} more explicitly in Eq. (2) (Section 2.3). In particular, {\\mathbb M} performs max-pooling and upsampling, The upsampling can place the non-zero value anywhere in the local region without affecting the validity of our reconstruction bound in Theorem 3.3. \nIn our experimental analysis, we particularly use the naive uniform upsampling so that the pooling switch has never been used for reconstruction with IHT. \nNote that we use pooling switches in Algorithm 2 to recover the true underlying sparse code in order to study the model-RIP property. However, Algorithm 2 is not for the IHT-based reconstruction, and the \\mathbb M_known is also used in Algorithm 2 just for notation convenience.\n\nRe: Notation\n- Thank you very much for your suggestion. We have revised the paper to clarify the notation. We highlight the changes below:\n\nc -> z: \"true\" underlying sparse code for generating the CNN input x\nh: hidden activations before pooling\nh -> \\hat{z}: hidden activations after pooling = reconstruction of original activations\n\nPlease let us know if you have more suggestions on the notations. We will be grateful to revise our submission to eliminate any confusions.\n\nRe: relation between IHT and deconvolution\n- You are right in that IHT (when \\Phi=W) involves convolution (line 4 of Alg. 1) and \"transposed convolution\" (or \"deconvolution\") for reconstruction (line 6 of Alg. 1). In fact, this was the exactly the same intuition that motivated our work. Our main contribution is to provide a novel theoretical analysis by exploiting the connection between IHT and convolution/deconvolution (or encoder/decoder) in CNNs."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642047, "id": "ICLR.cc/2017/conference/-/paper282/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1mAJI9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper282/reviewers", "ICLR.cc/2017/conference/paper282/areachairs"], "cdate": 1485287642047}}}, {"tddate": null, "tmdate": 1484756606079, "tcdate": 1484756606079, "number": 15, "id": "r1IY8GpUe", "invitation": "ICLR.cc/2017/conference/-/paper282/public/comment", "forum": "B1mAJI9gl", "replyto": "B1eW8KW4l", "signatures": ["~Kibok_Lee1"], "readers": ["everyone"], "writers": ["~Kibok_Lee1"], "content": {"title": "updated response", "comment": "Thank you for your helpful comments! We have updated our responses as follows.\n\nRe. A discussion of how this is inverted (e.g., with pooling switches) is needed.\n- The analysis of how CNN is inverted is separated in two sections. Our treatment on the ReLU nonlinearity is discussed in the beginning of the section 3. Pooling switches are discussed in the paragraph after Theorem 3.3 in the latest version. In particular, we do not need to know the pooling switch for upsampling, and the reconstruction bound in Theorem 3.3 applies to any valid upsampling algorithm (\"valid sampling algorithm\" means that a non-zero value is placed at one location in the upsampled activation map. See Footnote 6).\n\nRe. The relationship between feed-forward nets and Algorithm 1 assumes tied weights. It might be worthwhile to mention that the result is stronger for the case of RNNs, where this is the case by design.\n- We added footnote 7 on page 5 in the latest revision:\n\"Multiple iterations of IHT can improve the quality of signal recovery. However, it is rather equivalent to the recurrent version of CNNs and does not fit to the scope of this work.\"\n\n\nRe. it might help some readers to briefly note that the reconstruction algorithm is meant to be applied to each layer sequentially.\n- Footnote 3 on page 2 explicitly clarifies this:\n\"We can extend the equivalency on a single layer of CNNs to multiple layer CNNs simply by using the output on one layer as the input to another, still using the steps of the inner loop of IHT.\"\n\nRe. The filter coherence measure must be defined either mathematically or with a proper reference.\n- The filter coherence is defined in the end of Section 4.4 (where the table for coherence comparison is referred to). We added an equation for the definition in the latest version (footnote 12 at the bottom of the page 9)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642047, "id": "ICLR.cc/2017/conference/-/paper282/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1mAJI9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper282/reviewers", "ICLR.cc/2017/conference/paper282/areachairs"], "cdate": 1485287642047}}}, {"tddate": null, "tmdate": 1484756565426, "tcdate": 1484756565426, "number": 14, "id": "BypLUfa8g", "invitation": "ICLR.cc/2017/conference/-/paper282/public/comment", "forum": "B1mAJI9gl", "replyto": "SJKaeMfNx", "signatures": ["~Kibok_Lee1"], "readers": ["everyone"], "writers": ["~Kibok_Lee1"], "content": {"title": "updated response", "comment": "Thank you for your helpful comments! We have updated our responses as follows.\n\nWe agree that it is more difficult to theoretically analyze on multiple layers due to the error aggregation. As Theorem 3.3 implies, the reconstruction error bound becomes larger if we stack multiple layers. However, analyzing a single layer CNN is still important for understanding the invertibility of CNN. Even for a single layer CNN, although the invertibility may be intuitively plausible, there has been no nontrivial theoretical analysis in the literature. In addition, our analysis is nontrivial as the pooling operator is involved. we believe our work provides an interesting and novel connection between neural networks and compressed sensing and hope that it will stimulate further work."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642047, "id": "ICLR.cc/2017/conference/-/paper282/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1mAJI9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper282/reviewers", "ICLR.cc/2017/conference/paper282/areachairs"], "cdate": 1485287642047}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1484586336987, "tcdate": 1478283211075, "number": 282, "id": "B1mAJI9gl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "B1mAJI9gl", "signatures": ["~Kibok_Lee1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481937088789, "tcdate": 1481937088789, "number": 3, "id": "SJKaeMfNx", "invitation": "ICLR.cc/2017/conference/-/paper282/official/review", "forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "signatures": ["ICLR.cc/2017/conference/paper282/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper282/AnonReviewer1"], "content": {"title": "Official Review", "rating": "4: Ok but not good enough - rejection", "review": "The paper proposes to provide a theoretical explanation for why deep convolutional neural networks are invertible (at-least, when going back from certain intermediate layers to the image itself). It does so by considering the invertibility of a single layer, assuming the convolutional filters essentially correspond to incoherent measurements satisfying RIP.\n\nIn my opinion, while this is an interesting direction of research, the paper is not ready for publication. I feel the treatment does not go sufficiently towards explaining the phenomenon in deep neural networks. Even after reading the response from the authors, I feel the results are only a minor variation of the standard results from compressive sensing for sparse reconstruction with incoherent measurements.\n\nA deep neural network is fundamentally different from a single layer---it is the \"deep\" part that makes the forward task work. As the authors note, there is significant deterioration when IHT is applied recursively----therefore, at best the theory explains the partial invertibility of a single layer. That a single layer is approximately invertible isn't surprising, that a cascade of layers *is*.\n\nFor any theoretical analysis of this phenomenon to be useful, I believe it must go beyond analyzing a single compressive measurement-type layer, and try to explain how much of the same theory holds for a cascade. I say this because it's entirely possible that the sparse recovery theory breaks down beyond a single layer, and invertibility ends up being a property caused by correlations between the weights of different layers. In other words, there is no way to tell from the current results for individual layers whether they are in fact a step towards explaining the invertibility of whole networks.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512637214, "id": "ICLR.cc/2017/conference/-/paper282/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper282/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper282/AnonReviewer3", "ICLR.cc/2017/conference/paper282/AnonReviewer2", "ICLR.cc/2017/conference/paper282/AnonReviewer1"], "reply": {"forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512637214}}}, {"tddate": null, "tmdate": 1481918926002, "tcdate": 1481788409252, "number": 10, "id": "S1--h6JEx", "invitation": "ICLR.cc/2017/conference/-/paper282/public/comment", "forum": "B1mAJI9gl", "replyto": "S1gksS17x", "signatures": ["~Kibok_Lee1"], "readers": ["everyone"], "writers": ["~Kibok_Lee1"], "content": {"title": "answer", "comment": "Q. No pooling or non linearity?\n\nA. Our paper considers nonlinear activation and max pooling.\nRegarding nonlinear activation, we argue that if the filters come in positive and negative pairs, then one can disregard the ReLU or nonlinear operation as applying ReLU to the coefficients corresponding to both the positive and negative filters is equivalent to not applying ReLU to a vector of coefficients from the filters (not in pairs). See Section 3 for this argument.\nAbout max pooling, it is denoted as {\\mathbb M} (different from {\\mathcal M}, which is the block-sparse model), but we agree that the definition is somewhat hidden. We revised sentences in Section 2.2 and 2.3 to define the max pooling operator. (updated on Dec. 16)\n+: After applying the pooling operator {\\mathbb M}, we lose the information of the location where the maximum value was, because it technically performs unpooling after pooling to keep the dimension. Our reconstruction bound calculation starts from c and h, which are pooled feature maps, so the information of the location where the maximum was is not important.\n\nQ. Why not considering concatenated ReLU?\n\nA. ReLU is more commonly used in CNNs. We believe ReLU-based networks are more realistic and more broadly interesting. Moreover, the performance gain (coherence) with CReLU was not significant. We experimented with CReLU for conv(1,1) and conv(3,1): the coherence of ReLU-based network is 0.9427 and 0.5838, and that of CReLU-based network is 0.9476 and 0.5265.\n\nQ. What about the invertibility of the pooling operation in Bruna et al. (2014)?\n\nA. Our approach is different from Bruna et al.: we estimated || c - h ||_2 in Lemma B.4. (c is the true code and h is the reconstructed code from x.) \"h\" is the activation after the pooling operation. (h = {\\mathbb M}(Wx; k))\n\n\nQ. {\\mathcal M}^2_{k} versus {\\mathcal M}_{k}?\n\nA. We need to look at differences in model-k sparse signals which are vectors with at most 2k non-zero coefficients. Specifically, {\\mathcal M}_{k} is a set of model-k sparse vectors, (a vector can be divided into k blocks, each of which contains at most 1 non-zero value), and {\\mathcal M}^2_{k} is a set of the linear combinations of two elements in {\\mathcal M}_{k}. {\\mathcal M}^2_{k} is necessary in the middle of the proof, to represent the vector in the union of supports for c and h. See the proof of Lemma B.3 in detail."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642047, "id": "ICLR.cc/2017/conference/-/paper282/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1mAJI9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper282/reviewers", "ICLR.cc/2017/conference/paper282/areachairs"], "cdate": 1485287642047}}}, {"tddate": null, "tmdate": 1481822155939, "tcdate": 1481788364692, "number": 9, "id": "HkHAipkEg", "invitation": "ICLR.cc/2017/conference/-/paper282/public/comment", "forum": "B1mAJI9gl", "replyto": "rywPLJW7x", "signatures": ["~Kibok_Lee1"], "readers": ["everyone"], "writers": ["~Kibok_Lee1"], "content": {"title": "answer", "comment": "Q. What happens if you apply IHT recursively on all layers ?\n\nA. To apply IHT on all layers, we need to define W matrices for each layer and run a separate version of IHT (perhaps with just one iteration or perhaps with more than one iteration, as I just described). As long as the input to each layer remains sufficiently sparse and the number of filters, shifts, etc. are suitably balanced according to our Theorem 3.3, the entire process should, in theory, converge.\nIn addition, even on one layer (which is our current setup), we could apply IHT for more than one iteration to improve the reconstruction performance. That is, we could repeatedly refine our estimate for the reconstruction of one layer by iterating several times. In theory, this process will converge to an accurate reconstruction of the original image (with the error decreasing geometrically at each iteration).\nHowever, in practice, the practical parameters may not strictly conform with theoretical expectations. See Table 2 and 4 for how the sparsity/coherence deteriorates from one layer to the next.\n\n\nQ. What is the difference from the standard sparse recovery setup? Does the probability of failure blows up as you repeat the argument across multiple layers?\n\nA. The main difference from the standard sparse recovery setup would be three folds: we considered pooling as well, we applied a positive-negative pair assumption in our analysis (which turns out to be true), and we showed that the feedforward CNN is equivalent to the one step IHT under the model-RIP condition.\nIf we used fully random filters at each layer and set up the parameters within our bounds, then running IHT at each layer would yield an accurate reconstruction. Although, in this setting, fully random filters for the classification network do not give state-of-the-art classification performance, they are still reasonably good compared to learned filters (Table 1). For the learned filter case, the reconstruction can be less accurate by IHT, and the errors may be accumulated over layers. However, if we focus IHT on a single layer and train a decoder to compensate the error for the rest layers, we can get decent reconstruction results as shown in Figure 4 and 5."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287642047, "id": "ICLR.cc/2017/conference/-/paper282/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "B1mAJI9gl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper282/reviewers", "ICLR.cc/2017/conference/paper282/areachairs"], "cdate": 1485287642047}}}, {"tddate": null, "tmdate": 1480812126841, "tcdate": 1480812126835, "number": 2, "id": "rywPLJW7x", "invitation": "ICLR.cc/2017/conference/-/paper282/pre-review/question", "forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "signatures": ["ICLR.cc/2017/conference/paper282/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper282/AnonReviewer1"], "content": {"title": "single vs multiple layers", "question": "- The explanation of how IHT is applied in Fig 4c-d, since it only inverts a single layer. From what I understand, IHT is applied to invert a single layer (conv5_1 to get pool4 activations), and then a trained decoder network is applied on pool4. Is this true ? If so, what happens if you apply IHT recursively on all layers ? Does this work ?\n\n- The analysis seems to be about the invertibility of a single convolutional layer (+ non-linearity). In that sense, it's not very different from the standard sparse recovery setup.\n\nBut the surprising phenomenon is that deep neural \"networks\" are invertible, which consist of a cascade of such layers. To understand this invertibility, I feel one has to analyze whether the probability of failure blows up as you repeat the argument across multiple layers. Could you comment on how your analysis might extend to that ?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959363282, "id": "ICLR.cc/2017/conference/-/paper282/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper282/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper282/AnonReviewer3", "ICLR.cc/2017/conference/paper282/AnonReviewer1"], "reply": {"forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959363282}}}, {"tddate": null, "tmdate": 1480706775776, "tcdate": 1480706775771, "number": 1, "id": "S1gksS17x", "invitation": "ICLR.cc/2017/conference/-/paper282/pre-review/question", "forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "signatures": ["ICLR.cc/2017/conference/paper282/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper282/AnonReviewer3"], "content": {"title": "notations ", "question": "the paper presents theoretical results on inverting CNN in a random model .\n \nThere are several confusing notations in the paper, it is not clear at which level in the CNN the  invertibility is considered. Seems no pooling or non linearity is considered which is in fine a linear convolution invertibility in the sparse model.\n\n- Section 2.2 , is not clear about the model, the activation h is a linear model, followed by RELU , then by max pooling. The final activation has therefore a dimension K. \nSeems that invertibility is tackled only for the linear model (of dimension nK), no non linear activation considered and no pooling throughout the paper. Which is deceiving, since the purpose of the paper is the invertibility of a non linear model .\n\n-- In Section 3, ReLU is turned to a linear model with some motivation from concatenated RELU. why not consider concatenated RELU in section 2.2 and do the analysis based on that. \n\n-- what about the invertibility from the pooling operation as in showed in Bruna et al (2014)?\n\n-- What is the motivation behind using M^2_{k} versus M_{k}?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Towards Understanding the Invertibility of Convolutional Neural Networks", "abstract": "Several recent works have empirically observed that Convolutional Neural Nets (CNNs) are (approximately) invertible. To understand this approximate invertibility phenomenon and how to leverage it more effectively, we focus on a theoretical explanation and develop a mathematical model of sparse signal recovery that is consistent with CNNs with random weights. We give an exact connection to a particular model of model-based compressive sensing (and its recovery algorithms) and random-weight CNNs. We show empirically that several learned networks are consistent with our mathematical analysis and then demonstrate that with such a simple theoretical framework, we can obtain reasonable reconstruction results on real images. We also discuss gaps between our model assumptions and the CNN trained for classification in practical scenarios.", "pdf": "/pdf/abd15b9b7dbbd6f40102ed1d314281a90a8f46b8.pdf", "paperhash": "gilbert|towards_understanding_the_invertibility_of_convolutional_neural_networks", "keywords": ["Deep learning", "Theory"], "conflicts": ["umich.edu", "google.com"], "authors": ["Anna C. Gilbert", "Yi Zhang", "Kibok Lee", "Yuting Zhang", "Honglak Lee"], "authorids": ["annacg@umich.edu", "yeezhang@umich.edu", "kibok@umich.edu", "yutingzh@umich.edu", "honglak@umich.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959363282, "id": "ICLR.cc/2017/conference/-/paper282/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper282/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper282/AnonReviewer3", "ICLR.cc/2017/conference/paper282/AnonReviewer1"], "reply": {"forum": "B1mAJI9gl", "replyto": "B1mAJI9gl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper282/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959363282}}}], "count": 13}