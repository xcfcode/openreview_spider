{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1528124462247, "tcdate": 1518463338647, "number": 224, "cdate": 1518463338647, "id": "Sym_tDJwM", "invitation": "ICLR.cc/2018/Workshop/-/Submission", "forum": "Sym_tDJwM", "signatures": ["~Haque_Ishfaq1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop"], "content": {"title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "abstract": "Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.", "paperhash": "ishfaq|tvae_tripletbased_variational_autoencoder_using_metric_learning", "keywords": ["Metric Learning", "Variational Autoencoder", "Representation Learning", "Deep learning", "Semi-Supervised Learning"], "_bibtex": "@misc{\n  ishfaq2018tvae:,\n  title={TVAE: Triplet-Based Variational Autoencoder using Metric Learning},\n  author={Haque Ishfaq and Assaf Hoogi and Daniel Rubin},\n  year={2018},\n  url={https://openreview.net/forum?id=Sym_tDJwM}\n}", "authorids": ["hmishfaq@stanford.edu", "ahoogi@stanford.edu", "dlrubin@stanford.edu"], "authors": ["Haque Ishfaq", "Assaf Hoogi", "Daniel Rubin"], "TL;DR": "Learning latent embedding with fine grained information in VAE using triplet network and triplet loss from metric learning.", "pdf": "/pdf/4edf1bafd56e2cee960fe0a6d5356a06dd746eea.pdf"}, "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1518472800000, "tmdate": 1518474081690, "id": "ICLR.cc/2018/Workshop/-/Submission", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Workshop"]}, "signatures": {"values-regex": "~.*|ICLR.cc/2018/Workshop", "description": "Your authorized identity to be associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 9, "value-regex": "upload", "description": "Upload a PDF file that ends with .pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 8, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names. Please provide real names; identities will be anonymized."}, "keywords": {"order": 6, "values-regex": "(^$)|[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of keywords."}, "TL;DR": {"required": false, "order": 7, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,500}"}, "authorids": {"required": true, "order": 3, "values-regex": "([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,},){0,}([a-z0-9_\\-\\.]{2,}@[a-z0-9_\\-\\.]{2,}\\.[a-z]{2,})", "description": "Comma separated list of author email addresses, lowercased, in the same order as above. For authors with existing OpenReview accounts, please make sure that the provided email address(es) match those listed in the author's profile. Please provide real emails; identities will be anonymized."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1526248800000, "cdate": 1518474081690}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582691673, "tcdate": 1520713971589, "number": 1, "cdate": 1520713971589, "id": "B1hgWa-Fz", "invitation": "ICLR.cc/2018/Workshop/-/Paper224/Official_Review", "forum": "Sym_tDJwM", "replyto": "Sym_tDJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer3"], "content": {"title": "A possibly interesting addition to VAE. Experiments are insufficient. ", "rating": "3: Clear rejection", "review": "The paper proposes adding a supervised signal to VAE signal which is meant to improve the usability of the latent space vectors.  Specifically, the proposal is a triplet-based loss first used by Frome et al. (2007), which has since been widely used in metric learning (example Weinberger & Saul 2009). This loss relies on a weak supervision telling us \"A is more similar to B than it is to C\". The strength of this loss function is in cases where direct labels are unavailable, but in can be used in fully labeled datasets as well, as done in this paper. The paper would be improved by finding cases where direct supervision is not available but relative comparisons are, e.g. ranking- or preference-feedback tasks.\n\nPros:\n1. Possibly interesting idea for making the latent space vectors of a VAE more useful.\n\nCons:\n1. Supervised or semi-supervised VAEs could just as well create better representations of the classes. There is no comparison with them or in fact no mention of this possibility. \n2. Tying in with the previous point. the TVAE has strictly more information than VAE, since it has access to label information through the triplet loss, making comparisons less meaningful.\n3. The triplet loss in itself is not interesting at all. I would like to see experiments comparing performance of tasks for which a good metric on the latent space is useful. For example, NN classification using latent vectors, one-shot learning of new classes, or interpretability tasks.\n\n", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "abstract": "Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.", "paperhash": "ishfaq|tvae_tripletbased_variational_autoencoder_using_metric_learning", "keywords": ["Metric Learning", "Variational Autoencoder", "Representation Learning", "Deep learning", "Semi-Supervised Learning"], "_bibtex": "@misc{\n  ishfaq2018tvae:,\n  title={TVAE: Triplet-Based Variational Autoencoder using Metric Learning},\n  author={Haque Ishfaq and Assaf Hoogi and Daniel Rubin},\n  year={2018},\n  url={https://openreview.net/forum?id=Sym_tDJwM}\n}", "authorids": ["hmishfaq@stanford.edu", "ahoogi@stanford.edu", "dlrubin@stanford.edu"], "authors": ["Haque Ishfaq", "Assaf Hoogi", "Daniel Rubin"], "TL;DR": "Learning latent embedding with fine grained information in VAE using triplet network and triplet loss from metric learning.", "pdf": "/pdf/4edf1bafd56e2cee960fe0a6d5356a06dd746eea.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582691447, "id": "ICLR.cc/2018/Workshop/-/Paper224/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper224/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper224/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper224/AnonReviewer2"], "reply": {"forum": "Sym_tDJwM", "replyto": "Sym_tDJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper224/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper224/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582691447}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582624104, "tcdate": 1520839177840, "number": 2, "cdate": 1520839177840, "id": "HJGMcimKf", "invitation": "ICLR.cc/2018/Workshop/-/Paper224/Official_Review", "forum": "Sym_tDJwM", "replyto": "Sym_tDJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer1"], "content": {"title": "VAE + triplet margin loss", "rating": "5: Marginally below acceptance threshold", "review": "This paper proposes a deep embedding approach by minimizing the \ncombination of the loss of VAE and triplet-based large-martin hinge \nloss. Experimental results show that, based on triplet loss, the combined \napproach performs much better than VAE. \n\nHowever, minimizing triple-based large-margin loss on top of a deep neural \nnetwork (a deep autoencoder pertained with RBMs) for metric learning and \ndata visualization was proposed in a paper entitle A Deep Non-Linear \nFeature Mapping for Large-Margin kNN Classification about ten years ago, \nwhich was re-implemented using a modern Deep CNN architecture for image \nranking (Wang et al., 2014) and metric learning (Hoffer and Ailon, 2015).  \nThe contribution in this paper is very limited, although VAE is different \nfrom traditional autoencoder.\n\nMoreover, it is not surprising that the triplet-based methods in this paper and in \nall the three previous related papers perform much better than their respective \nunsupervised counterparts, because label information is employed during the \nprocess of triplet construction. It is better to elaborate how the supervised label \ninformation is used to improve the experiment part.  And it is better to show the \nstandard kNN classification error rate on the test dataset besides the triplet-based \nerror rate. The experimental results on MNIST don\u2019t seem to be convincing compared \nto previous metric learning results.\n\nIn summary, the proposed method is a simple combination of existing work and \nlacks novelty and insight.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "abstract": "Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.", "paperhash": "ishfaq|tvae_tripletbased_variational_autoencoder_using_metric_learning", "keywords": ["Metric Learning", "Variational Autoencoder", "Representation Learning", "Deep learning", "Semi-Supervised Learning"], "_bibtex": "@misc{\n  ishfaq2018tvae:,\n  title={TVAE: Triplet-Based Variational Autoencoder using Metric Learning},\n  author={Haque Ishfaq and Assaf Hoogi and Daniel Rubin},\n  year={2018},\n  url={https://openreview.net/forum?id=Sym_tDJwM}\n}", "authorids": ["hmishfaq@stanford.edu", "ahoogi@stanford.edu", "dlrubin@stanford.edu"], "authors": ["Haque Ishfaq", "Assaf Hoogi", "Daniel Rubin"], "TL;DR": "Learning latent embedding with fine grained information in VAE using triplet network and triplet loss from metric learning.", "pdf": "/pdf/4edf1bafd56e2cee960fe0a6d5356a06dd746eea.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582691447, "id": "ICLR.cc/2018/Workshop/-/Paper224/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper224/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper224/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper224/AnonReviewer2"], "reply": {"forum": "Sym_tDJwM", "replyto": "Sym_tDJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper224/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper224/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582691447}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521582602330, "tcdate": 1520960650527, "number": 3, "cdate": 1520960650527, "id": "rJMcNtSFf", "invitation": "ICLR.cc/2018/Workshop/-/Paper224/Official_Review", "forum": "Sym_tDJwM", "replyto": "Sym_tDJwM", "signatures": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer2"], "content": {"title": "Paper presents an interesting model for weakly supervised learning which has been propsoed before.", "rating": "2: Strong rejection", "review": "The authors propose to combine triplet learning with variational autoencoders in a reasonable framework and demonstrate it at MNIS toy examples.\n\nUnfortunately, this work is subsumed by the ICLR 2016 paper 'Bayesian Representation Learning With Oracle Constraints' by Karaletsos et al which has proposed the combination of tripelts with a VAE in a principled joint modeling framework.\n\nIn addition, the ICLR 16 paper proposed mechanisms to address shortcomings of the combination of a VAE with triplets related to disentangling, which this workshop paper fails to recognize.\n\nAs such, this paper unfortunately suffers from a lack of novelty.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "abstract": "Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.", "paperhash": "ishfaq|tvae_tripletbased_variational_autoencoder_using_metric_learning", "keywords": ["Metric Learning", "Variational Autoencoder", "Representation Learning", "Deep learning", "Semi-Supervised Learning"], "_bibtex": "@misc{\n  ishfaq2018tvae:,\n  title={TVAE: Triplet-Based Variational Autoencoder using Metric Learning},\n  author={Haque Ishfaq and Assaf Hoogi and Daniel Rubin},\n  year={2018},\n  url={https://openreview.net/forum?id=Sym_tDJwM}\n}", "authorids": ["hmishfaq@stanford.edu", "ahoogi@stanford.edu", "dlrubin@stanford.edu"], "authors": ["Haque Ishfaq", "Assaf Hoogi", "Daniel Rubin"], "TL;DR": "Learning latent embedding with fine grained information in VAE using triplet network and triplet loss from metric learning.", "pdf": "/pdf/4edf1bafd56e2cee960fe0a6d5356a06dd746eea.pdf"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1520657999000, "tmdate": 1521582691447, "id": "ICLR.cc/2018/Workshop/-/Paper224/Official_Review", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Paper224/Reviewers"], "noninvitees": ["ICLR.cc/2018/Workshop/Paper224/AnonReviewer3", "ICLR.cc/2018/Workshop/Paper224/AnonReviewer1", "ICLR.cc/2018/Workshop/Paper224/AnonReviewer2"], "reply": {"forum": "Sym_tDJwM", "replyto": "Sym_tDJwM", "writers": {"values-regex": "ICLR.cc/2018/Workshop/Paper224/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2018/Workshop/Paper224/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1528433999000, "cdate": 1521582691447}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1521573606286, "tcdate": 1521573606286, "number": 266, "cdate": 1521573605943, "id": "rk0JyJkqf", "invitation": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "forum": "Sym_tDJwM", "replyto": "Sym_tDJwM", "signatures": ["ICLR.cc/2018/Workshop/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Workshop/Program_Chairs"], "content": {"decision": "Reject", "title": "ICLR 2018 Workshop Acceptance Decision", "comment": "Based on the reviews, this paper has not been accepted for presentation at the ICLR workshop. However, the conversation and updates can continue to appear here on OpenReview."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "TVAE: Triplet-Based Variational Autoencoder using Metric Learning", "abstract": "Deep metric learning has been demonstrated to be highly effective in learning semantic representation and encoding information that can be used to measure data similarity, by relying on the embedding learned from metric learning. At the same time, variational autoencoder (VAE) has widely been used to approximate inference and proved to have a good performance for directed probabilistic models. However, for traditional VAE, the data label or feature information are intractable. Similarly, traditional representation learning approaches fail to represent many salient aspects of the data. In this project, we propose a novel integrated framework to learn latent embedding in VAE by incorporating deep metric learning. The features are learned by optimizing a triplet loss on the mean vectors of VAE in conjunction with standard evidence lower bound (ELBO) of VAE. This approach, which we call Triplet based Variational Autoencoder (TVAE), allows us to capture more fine-grained information in the latent embedding. Our model is tested on MNIST data set and achieves a high triplet accuracy of 95.60% while the traditional VAE (Kingma & Welling, 2013) achieves triplet accuracy of 75.08%.", "paperhash": "ishfaq|tvae_tripletbased_variational_autoencoder_using_metric_learning", "keywords": ["Metric Learning", "Variational Autoencoder", "Representation Learning", "Deep learning", "Semi-Supervised Learning"], "_bibtex": "@misc{\n  ishfaq2018tvae:,\n  title={TVAE: Triplet-Based Variational Autoencoder using Metric Learning},\n  author={Haque Ishfaq and Assaf Hoogi and Daniel Rubin},\n  year={2018},\n  url={https://openreview.net/forum?id=Sym_tDJwM}\n}", "authorids": ["hmishfaq@stanford.edu", "ahoogi@stanford.edu", "dlrubin@stanford.edu"], "authors": ["Haque Ishfaq", "Assaf Hoogi", "Daniel Rubin"], "TL;DR": "Learning latent embedding with fine grained information in VAE using triplet network and triplet loss from metric learning.", "pdf": "/pdf/4edf1bafd56e2cee960fe0a6d5356a06dd746eea.pdf"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1518629844880, "id": "ICLR.cc/2018/Workshop/-/Acceptance_Decision", "writers": ["ICLR.cc/2018/Workshop"], "signatures": ["ICLR.cc/2018/Workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Workshop/Program_Chairs"], "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Workshop/-/Submission", "writers": {"values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Workshop/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Workshop/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Workshop Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept", "Reject"]}}}, "nonreaders": [], "noninvitees": [], "cdate": 1518629844880}}}], "count": 5}