{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487717759379, "tcdate": 1478287952395, "number": 368, "id": "S1dIzvclg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "S1dIzvclg", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 16, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396543847, "tcdate": 1486396543847, "number": 1, "id": "SkuY2f8ue", "invitation": "ICLR.cc/2017/conference/-/paper368/acceptance", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers all enjoyed this paper and the analysis.\n \n pros:\n - novel new model\n - interesting insights into the design of model, through analysis of trajectories of hidden states of RNNs.\n \n cons:\n - results are worse than LSTMs.", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396544566, "id": "ICLR.cc/2017/conference/-/paper368/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "S1dIzvclg", "replyto": "S1dIzvclg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396544566}}}, {"tddate": null, "tmdate": 1483673536009, "tcdate": 1483673536009, "number": 9, "id": "rk_6Jqnrg", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "BJgm5CNSx", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "writers": ["~Thomas_Laurent1"], "content": {"title": "Authors' response", "comment": "Thanks for the interesting questions.\n\n1) A trained LSTM driven by a constant input different from zero can be chaotic. For example, the trained LSTM used to generate figure 3, if given an input sequence consisting of the word \u201cfall\u201d (or \u201ccool\u201d or \u201cairline\u201d) repeated infinitely, leads to a chaotic dynamical system. For most words however (including the word \u201cwhat\u201d) the system is not chaotic. \n\n2) Yes, we do see chaos after T in the case you describe, and the activations do not decay to zero.\n\n3) We have not tried yet, but certainly plan to do so.        "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1483168280537, "tcdate": 1483168280537, "number": 8, "id": "BJgm5CNSx", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["~Greg_Yang1"], "readers": ["everyone"], "writers": ["~Greg_Yang1"], "content": {"title": "Questions", "comment": "Thanks for a very interesting read.\n\nWhat happens if instead of driving the LSTMs with x_t = 0, you drive it with a fixed input, like the word \"What\"? Would that behave the same as in fig 3?\n\nIf you drive the LSTMs with some input and then fix x_t = 0 for t > T (as in fig 4), do you still see chaos? If there is gradual decay in the hidden units' activations, do you also see that the second layer forgets more slowly than the first?\n\nHave you tried training on the copy task as in the algorithmic learning literature (like NTM), to see whether there is a actual difference in how long memory is retained in CFN vs LSTM?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1482856050179, "tcdate": 1482856003105, "number": 7, "id": "B1oSIzeBx", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "B1OjV-f4l", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "writers": ["~Thomas_Laurent1"], "content": {"title": "Authors' response", "comment": "Thanks for the constructive review.\n\n1) Here are some additional statistics regarding the relaxation times of hidden units in the 1st and 2nd layers of the CFN. Following the experiment depicted in Figure 4, we run the CFN on PTB and set all inputs to zero for t>1000. We then define the relaxation time (or half-life) of the ith unit as the smallest T such that\n\n                                                                                           |h_i(1000+T)| <  0.5 |h_i(1000)|\n\nUsing this definition, the average relaxation times are 2.2 time steps for the 1st layer and 23.2 time steps for the 2nd layer. The average by itself does not show the full story. The standard deviations of these relaxation times over the whole layer are *very* large (1st layer: std=5, 2nd layer: std=75). For example, some units in the second layer have relaxation times of several hundred steps. If instead of averaging the relaxation time over the whole layer, we average it over the 25% units that decay the slowest we get 4.8 time steps (1st layer mean over the top quartile) and 85.6 time steps (2nd layer mean over the top quartile). In other words, by restricting attention to long-term units the difference between the 1st and 2nd layers becomes much more striking. We have added this information to the paper (middle of page 6). \n\n2) Developing a version of the CFN with multiple attractors is a very interesting direction in our opinion. We have not explored it yet, but certainly plan to do so. We also plan to explore more sophisticated tasks and training strategies, including batch normalization, as the referee suggests, in future work. These explorations, which lie beyond the scope of this work, are clearly necessary to understand the range of tasks that can be accomplished by dynamically simple networks, and how these networks differ from more traditional ones such as LSTMs and GRUs. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1482227837642, "tcdate": 1482227837642, "number": 3, "id": "H1LYxY84l", "invitation": "ICLR.cc/2017/conference/-/paper368/official/review", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["ICLR.cc/2017/conference/paper368/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper368/AnonReviewer1"], "content": {"title": "Nice investigation", "rating": "8: Top 50% of accepted papers, clear accept", "review": "The authors of the paper set out to answer the question whether chaotic behaviour is a necessary ingredient for RNNs to perform well on some tasks. For that question's sake, they propose an architecture which is designed to not have chaos. The subsequent experiments validate the claim that chaos is not necessary.\n\nThis paper is refreshing. Instead of proposing another incremental improvement, the authors start out with a clear hypothesis and test it. This might set the base for future design principles of RNNs.\n\nThe only downside is that the experiments are only conducted on tasks which are known to be not that demanding from a dynamical systems perspective; it would have been nice if the authors had traversed the set of data sets more to find data where chaos is actually necessary.", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512607746, "id": "ICLR.cc/2017/conference/-/paper368/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper368/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper368/AnonReviewer2", "ICLR.cc/2017/conference/paper368/AnonReviewer3", "ICLR.cc/2017/conference/paper368/AnonReviewer1"], "reply": {"forum": "S1dIzvclg", "replyto": "S1dIzvclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512607746}}}, {"tddate": null, "tmdate": 1482093364020, "tcdate": 1481933984542, "number": 1, "id": "B1OjV-f4l", "invitation": "ICLR.cc/2017/conference/-/paper368/official/review", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["ICLR.cc/2017/conference/paper368/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper368/AnonReviewer2"], "content": {"title": "Cool paper", "rating": "7: Good paper, accept", "review": "This paper poses an interesting idea: removing chaotic behavior or RNNs.\nWhile many other papers on new RNN architecture usually focus too much on the performance improvement and leave the analysis part on their success as a black-box, this paper does a good job on presenting why its method may work well.\n\nAlthough, the paper shows lots of comparison between the chaotic systems (GRUs & LSTMs) and the stable system (proposed CFN model), the reviewer is not fully convinced by the main claim of this paper, the nuance that chaotic behaviour makes dynamic system to have rich representation power but makes the system too unstable. In the paper, the LSTM shows a very sensitive behaviour, even when a very small amount of noise is added to the input. However, it still performs surprisingly well with this chaotic behaviour. \n\nMeasuring the model complexity is a very difficult task, therefore, many papers manage to use either same number of hidden units or choose approximately close model sizes. In this paper, the experiments were carried by using the same amount of parameters for both the LSTM and CFN. However, I think the CFN may have much more simpler computational graph. Taking the idea of this work, can we develop a stable dynamic system, but which does not only have one attractor?\n\nIt is also interesting to see that the layers of CFNs are updated in different timescales in a sense that the decaying speed decreases when the layer gets higher. Could you provide more statistics on this? For example, what is the average relaxation time of the whole hidden units at each layer?\n\nBatch normalization and layer normalization can be helpful to make the training of RNNs become more stable. How would the behaviour of batch normalized or perhaps layer normalized LSTM look like? Also, it is often not trivial to make batch normalization or layer normalization to work on a new architecture. I think it may be useful to compare batch normalized or layer normalized versions of the LSTM and CFN.\n\nThe quality of the work is good, explanation is clear enough along with nice analyses and proofs. Overall, the performance is not any better than LSTMs, but it is still interesting when thinking of simplicity of this model. I am a bit concerned if this model might not work that well in more harder task, e.g., translation. Figure 4 of this paper is very interesting, where the proposed architecture shows that the hidden units at the second layer tends to keep its information longer than the first layer ones.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512607746, "id": "ICLR.cc/2017/conference/-/paper368/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper368/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper368/AnonReviewer2", "ICLR.cc/2017/conference/paper368/AnonReviewer3", "ICLR.cc/2017/conference/paper368/AnonReviewer1"], "reply": {"forum": "S1dIzvclg", "replyto": "S1dIzvclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512607746}}}, {"tddate": null, "tmdate": 1482059546097, "tcdate": 1482059546097, "number": 2, "id": "HyMmkxNVg", "invitation": "ICLR.cc/2017/conference/-/paper368/official/review", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["ICLR.cc/2017/conference/paper368/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper368/AnonReviewer3"], "content": {"title": "interesting starting point", "rating": "7: Good paper, accept", "review": "I think the authors provide an interesting direction for understanding and maybe constructing recurrent models that are easier to interpret. Is not clear where such direction will lead but I think it could be an interesting starting point for future work, one that worth exploring. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512607746, "id": "ICLR.cc/2017/conference/-/paper368/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper368/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper368/AnonReviewer2", "ICLR.cc/2017/conference/paper368/AnonReviewer3", "ICLR.cc/2017/conference/paper368/AnonReviewer1"], "reply": {"forum": "S1dIzvclg", "replyto": "S1dIzvclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512607746}}}, {"tddate": null, "tmdate": 1481852066340, "tcdate": 1481852066340, "number": 6, "id": "SJcsN6x4g", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "writers": ["~Thomas_Laurent1"], "content": {"title": "Authors\u2019 comment: Conclusion added", "comment": "We added a short conclusion reflecting some of the discussions with the reviewers. "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1481513084227, "tcdate": 1481513084221, "number": 5, "id": "ry4F_qs7l", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "ryoj8-97l", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "writers": ["~Thomas_Laurent1"], "content": {"title": "Authors' response", "comment": "No Chaos is not necessarily bad! The ability of a network to get into chaotic regime might actually be important for some tasks. We are arguing neither for nor against chaotic models. We simply asked the question whether this ability to generate complex and chaotic trajectories is *necessary* in order to perform well on an important task, language modeling. And the answer seems to be that it is not. \n\nThe problem with chaos is that it makes networks very hard to interpret. The mechanism by which the network solves the given task become quite mysterious if the underlying dynamical system is chaotic. For example, in the paper you pointed out (\u201cOpening the black box\u201d), the authors uncover the mechanism by which a continuous vanilla RNN solves various simple tasks. To do this they study the underlying dynamical systems without input data (by the way, thanks for pointing out this very nice paper!). These dynamical systems are certainly not chaotic: if they were, the authors would not have been able to study the relevant trajectories that transition the network from one stable state to the other. There would be no interpretation.\n\nTo us it was a surprise that a trained LSTM can be chaotic in the absence of input data.  \n\nWhat we like about the proposed network is the fact that it has a quite transparent mechanism.  We wanted to propose a simple model that has the potential of being mathematically well understood, with the hope that such an understanding would drive subsequent improvements.\n\nAnswer to question (1):\nYou are correct the CFN is not designed to save information for an unlimited amount of time. The model was designed so that units get activated when presented the correct feature, then relax to zero at a rate controlled by the forget gate. The rate of relaxation  gives an indication of the time scale(s) over which the proposed network can save information. On Penn Treebank for example, it takes hundreds of time steps for the slowest units to relax to zero (see the recently added figure 4 in the paper). As you point out in your comment, this is typically in practice what is expected from other network. \n\nWe have posted a general comment that addresses the issue of long term dependencies in more details.\n\nAnswer to question (2):\nIt depends entirely on the system. Trajectories of some chaotic systems can diverge in very few, say under a dozen, time steps. Sometimes this divergence takes much longer. For the dynamical system induced by a trained LSTM shown in the paper, divergence occurs between 100-500 steps. This isn\u2019t an issue in practice, though, since the same model *with* input data is not chaotic, by which we mean that initially perturbed trajectories instead converge toward the same trajectory after a few steps."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1481512581492, "tcdate": 1481512321285, "number": 3, "id": "SkFFr9jQg", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "writers": ["~Thomas_Laurent1"], "content": {"title": "Authors\u2019 comment: Added experiments on long term dependencies.", "comment": "Several reviewers have posted comments asking about the capability of the proposed model to capture long-term dependencies. This is a natural question since the model was designed so that units get activated when presented the correct feature, then relax to zero at a rate controlled by the forget gate. At a first glance it is unclear that such a simple mechanism could capture long term dependencies (the relaxing rates might be too fast).\n\nWe added a simple experiment in the paper showing that long term dependencies can be obtained by stacking multiple layers of the basic architecture (see Figure 4). We took a 2-layer, 224-unit CFN network trained on Penn Treebank and ran it with the following input data: The first 1000 inputs x_t are the first 1000 words of the test set of PTB; All subsequent inputs are set to zero, so that x_t=0 if t>1000. For each layer we then select the 10 units that decay the slowest after t>1000 and plotted them on Figure 4. The first layer retains information for about 10 time steps, whereas the second layer retains information for about 100 steps. Adding a third or fourth layer would then allow the architecture to retain information for even longer periods. We have not yet implemented a multi-layer network to handle tasks (other than language modeling) where such longer-term dependencies are needed, but we believe the main obstacle here is one of proper initialization and training rather than a shortcoming of the architecture itself.\n\nImportantly, this behavior (i.e. higher layers decay more slowly) can be explained analytically, see equation (11).\n\nOverall, we find it interesting that complexity and long-term dependencies can plausibly be obtained in a classical way (i.e. stacking layers) rather than relying on the intricate and hard to interpret dynamics of an LSTM."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1481512537180, "tcdate": 1481512537174, "number": 4, "id": "BJWwL5oQl", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "SJQONDDXe", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "writers": ["~Thomas_Laurent1"], "content": {"title": "Authors\u2019 response", "comment": "We only have rigorous experimental results for the time being on word-level language modeling. We certainly plan to develop experiments that probe the capability of the model to capture long-term dependencies in other context, but we do not yet have methodologically sound results to present.\n\nWe have added a language modeling experiment in the paper that shows that long-term dependencies are obtained by vertically stacking multiple layers. See the comment ``Added experiments on long term dependencies'' that we posted for more details.\n\nOverall, the goal of the paper is to see if it is possible to obtain an RNN that performs well while avoiding the intricate and uninterpretable dynamics that are typical of LSTMs or GRUs. We did not aim for a model that competes with LSTMs on a wide variety of tasks as of yet. Rather, we wanted to propose a simple model that has the potential of being mathematically well understood, with the hope that such an understanding would drive subsequent improvements. That our model can succeed at LSTM levels on language modeling makes it worth studying in our opinion."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1481410211527, "tcdate": 1481410211522, "number": 3, "id": "ryoj8-97l", "invitation": "ICLR.cc/2017/conference/-/paper368/pre-review/question", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["ICLR.cc/2017/conference/paper368/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper368/AnonReviewer1"], "content": {"title": "Is chaos bad?", "question": "1) To my understanding, the network proposed by the authors is aimed to not have chaos by explicitly constructing the origin as an attractor.\n[1] features a nice analysis of how RNNs could use attractors to save information. It is my impression, that this implies that the CFN is not able to save information for an unlimited amount of time. What do you think?\n\n2) Currently, RNNs are practical on problems where the maximum sequence length is less than, say, 1000 due to memory and speed constraints. Time steps of 50 or 100 are more common. Is this a time scale where chaotic behaviour is usually a problem?\n\n\n\n[1] Sussillo, David, and Omri Barak. \"Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks.\" Neural computation 25.3 (2013): 626-649."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481410212039, "id": "ICLR.cc/2017/conference/-/paper368/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper368/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper368/AnonReviewer3", "ICLR.cc/2017/conference/paper368/AnonReviewer2", "ICLR.cc/2017/conference/paper368/AnonReviewer1"], "reply": {"forum": "S1dIzvclg", "replyto": "S1dIzvclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481410212039}}}, {"tddate": null, "tmdate": 1481389666993, "tcdate": 1481389286917, "number": 2, "id": "SJkxSnKXl", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "HJkSybEXe", "signatures": ["~Thomas_Laurent1"], "readers": ["everyone"], "writers": ["~Thomas_Laurent1"], "content": {"title": "Authors' response", "comment": "Thanks for the thoughtful comments. \n\n1) Yes, a trained LSTM with input data is not chaotic (if it were chaotic, it would not be able to generalize from train to test set). We have added an experiment in the paper to illustrate this point (see Figure 3b). \n\nThe point of section 2 is to draw a distinction between our model and an LSTM at the dynamical level. The fact that \"the computational mechanism by which RNNs achieve their goals remains an open question\" [1] motivated us to understand the dynamics of RNN hidden states, and moreover, to see whether an understanding of these dynamics offer insight into the solution mechanisms implemented by these networks. The point of chaos is that this investigation immediately runs into a wall with LSTMs; Even in a simple scenario these models can have complex and unpredictable dynamics. Developing an understanding of LSTM solution mechanisms via its hidden state dynamics is therefore intractable.\n\nThe proposed model provides an LSTM-like network where such an investigation is possible. The simplicity of the proposed network makes it more amenable to analysis.  We initiate such an analysis by proving a few properties in the paper, in both the presence and absence of input data, but obviously much more work is needed before a complete picture can emerge. We simply hope that this investigation opens up new avenues of inquiry. For instance, a model with well-understood dynamics can drive the development of new networks that are not based off of continual modifications and complexifications of LSTMs. That our model can succeed at LSTM levels on language modeling makes it worth studying, at least in the sense that a well-understood but poorly performing network is rather useless.\n\nWe will add a conclusion to the paper that makes the motivation clearer.\n\n2) It is possible that the proposed model might not perform as well on more complex tasks due to the simplicity of its dynamics. However, one could also envision that long-term dependencies and complexity can be obtained by stacking many layers of networks with simple and interpretable dynamics -- in other words, complexity could come from the vertical depth of the network rather than intricate dynamics driven by hidden states. Note that the proposed network reduces to a classical feedforward architecture when the forget gate is closed and the input gate is open.\n\nMoreover, we do not aim for a model that competes with LSTMs on a wide variety of tasks as of yet. Rather, we wanted to propose a simple model that has the potential of being mathematically well understood, and that such an understanding drives subsequent improvements.\n\n3) We originally started with the \u201cmoving average\u201d version of the network (with no tanh on the hidden state) due to its interpretability. Adding a tanh on the hidden state makes the network easier to train and leads to significantly better results.  \n\n\n[1] Intelligible Language Modeling with Input Switched Affine Networks\nJ. Foerster, J. Gilmer, J. Chorowski, J. Sohl-Dickstein, D. Sussillo\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}, {"tddate": null, "tmdate": 1481237611059, "tcdate": 1481237611053, "number": 2, "id": "SJQONDDXe", "invitation": "ICLR.cc/2017/conference/-/paper368/pre-review/question", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["ICLR.cc/2017/conference/paper368/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper368/AnonReviewer2"], "content": {"title": "Long-term dependencies", "question": "I wonder how this model can do well when it becomes more important to capture the long-term dependencies?\nHave you plan to try extending the experiments on other tasks as well?\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481410212039, "id": "ICLR.cc/2017/conference/-/paper368/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper368/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper368/AnonReviewer3", "ICLR.cc/2017/conference/paper368/AnonReviewer2", "ICLR.cc/2017/conference/paper368/AnonReviewer1"], "reply": {"forum": "S1dIzvclg", "replyto": "S1dIzvclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481410212039}}}, {"tddate": null, "tmdate": 1481015094948, "tcdate": 1481015094940, "number": 1, "id": "HJkSybEXe", "invitation": "ICLR.cc/2017/conference/-/paper368/pre-review/question", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["ICLR.cc/2017/conference/paper368/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper368/AnonReviewer3"], "content": {"title": "Pre-review questions", "question": "Hi, thanks for an interesting read. And sorry for the delay in posting the questions.\n\nQuestion: \n\n (1) While the LSTM might have a chaotic behaviour (trained or untrained), when you add the input in the equation (e.g. for the trained model) its behaviour is not that chaotic anymore I think. E.g. if I take a string, and add a bit of noise (1e-7) to one character at the beginning, and unroll this sequences I don't think I get divergent behaviors. Did the authors try this? Also is not clear to me that dynamics are simple either. I guess the bigger problem here is that we might not have the right mathematical tools to even understand this. I know Herbert Jaeger and others attempted to look at this kind of settings (Manjunath, G., Jaeger, H. (2014): The Dynamics of Random Difference Equations is Remodeled by Closed Relations. SIAM Journal on Mathematical Analysis 46(1), 459-483), though I'm not closely familiar to how useful those tools are either. Does it make sense to study the model in a very different settings than the one used?\n\n (2) Do authors have an intuition why you don't need a rich dynamics to model complex data? It feels to me like we are giving away a lot of representational power by not being able to have a chaotic behavior. Can we do that based on a single anecdotal evidence? I'm especially worried since language modelling is not known to necessary require long term memory. You can get quite good results by just modeling recent events. Can the authors somehow split these things apart? Is their new model much worse than say an LSTM to deal with long term correlations ? \n\n (3) What happens if you remove the tanh as well ? Then you are just doing a moving average of the previous hidden activations. Would that perform well? I'm not convinced tanh buys you much in the particular model you are proposing. \n\nThanks"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1481410212039, "id": "ICLR.cc/2017/conference/-/paper368/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper368/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper368/AnonReviewer3", "ICLR.cc/2017/conference/paper368/AnonReviewer2", "ICLR.cc/2017/conference/paper368/AnonReviewer1"], "reply": {"forum": "S1dIzvclg", "replyto": "S1dIzvclg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper368/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1481410212039}}}, {"tddate": null, "tmdate": 1478510646135, "tcdate": 1478510646128, "number": 1, "id": "ryCEua6lx", "invitation": "ICLR.cc/2017/conference/-/paper368/public/comment", "forum": "S1dIzvclg", "replyto": "S1dIzvclg", "signatures": ["~Heikki_Arponen1"], "readers": ["everyone"], "writers": ["~Heikki_Arponen1"], "content": {"title": "Edge of chaos?", "comment": "I think it would be useful to discuss the concept of *edge* of chaos here (see e.g. Bertschinger, Nachschlager - Real-Time Computation at the Edge of Chaos in Recurrent Neural Networks), i.e. the hypothesis that RNNs are optimal (in some sense) at the boundary between chaotic and deterministic regimes. Specifically, it would be nice to see if your network gets closer to this edge during training (I think it will).\n\nIt wasn't clear to me if you studied the chaoticity in the case *with* input... the \"epsilon-activation\" thing seems very nonstandard. Why didn't you just compute the mean Lyapunov exponent? You can do that with or without input. I think you might find that the RNN with input will approach the edge of chaos during training (Lyapunov exp gets closer to zero, probably starting from negative values in your case).\n\nThe LSTM phase space diagram in Fig. 2 looks pretty bad... I think that particular unit is not behaving well at all. What you should get in properly trained models is something like in Fig. 1 (a), but more noisy because there's effects from the input.\n\nAnyway, overall a very interesting paper! I'm glad to see RNNs studied from a chaotic dynamics perspective."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "A recurrent neural network without chaos", "abstract": "We introduce an exceptionally simple  gated recurrent neural network (RNN)  that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.", "pdf": "/pdf/90ed26d8d9ccbd87ae3e0019930730bbbdb93556.pdf", "paperhash": "laurent|a_recurrent_neural_network_without_chaos", "keywords": [], "conflicts": ["lmu.edu", "csulb.edu"], "authors": ["Thomas Laurent", "James von Brecht"], "authorids": ["tlaurent@lmu.edu", "james.vonbrecht@csulb.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287605155, "id": "ICLR.cc/2017/conference/-/paper368/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "S1dIzvclg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper368/reviewers", "ICLR.cc/2017/conference/paper368/areachairs"], "cdate": 1485287605155}}}], "count": 17}