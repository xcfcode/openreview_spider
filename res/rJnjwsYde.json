{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028552615, "tcdate": 1490028552615, "number": 1, "id": "r1ZMuFaog", "invitation": "ICLR.cc/2017/workshop/-/paper12/acceptance", "forum": "rJnjwsYde", "replyto": "rJnjwsYde", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Reference Priors", "abstract": "In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are chosen with the hopes that they are reasonably uninformative.  Yet, objective priors such as the Jeffreys and Reference would often be preferred over flat priors if deriving them was generally tractable.  We overcome this problem by proposing a black-box learning algorithm for Reference prior approximations.  We derive a lower bound on the mutual information between data and parameters and describe how its optimization can be made derivation-free and scalable via differentiable Monte Carlo expectations.  We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's Reference prior.     ", "pdf": "/pdf/b77f2ea887d3de3dd62976de07c86014fb35fdce.pdf", "TL;DR": "A method for learning Reference prior approximations.", "paperhash": "nalisnick|variational_reference_priors", "conflicts": ["uci.edu"], "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": [], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028553204, "id": "ICLR.cc/2017/workshop/-/paper12/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJnjwsYde", "replyto": "rJnjwsYde", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028553204}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489437498756, "tcdate": 1489437398715, "number": 2, "id": "HJkyXKNjx", "invitation": "ICLR.cc/2017/workshop/-/paper12/public/comment", "forum": "rJnjwsYde", "replyto": "SJG51Nfsl", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Author Response", "comment": "Thanks for your attentive comments, Reviewer #2.  \n\nOn estimator variance:  the estimator does have high variance, but it is not as bad as the harmonic mean estimator's, to which I believe you're referring.  When using a finite negative value for alpha, the estimator becomes very similar to the harmonic mean (but exponentiated), and this is why we use the VR-max estimate instead.  We found learning to be stable when in less that 20 dimensions. \n\nDoes the reference prior yield a better density model than the spherical Gaussian one?:  preliminary experiments were inconclusive.  The reference prior resulted in a better model for 25d but worse in 2d, but in each case the difference was slight, <0.1 .\n\nIn the VAE experiment, what keeps the prior from expanding to be infinitely broad?:  firstly, the neural network sampler must have finite weights, resulting in the prior having finite domain.  Secondly, if the decoder network uses units that can saturate, the prior will stop expanding once the downstream activations becomes sufficiently large.\n\nIs the true reference prior guaranteed to be proper?  What happens if it is improper?:  it most likely won't be proper, which is a benefit of our methodology since it allows us to find an approximation that is proper (or has other properties the user desires).  Yet, MCMC usually still works for improper posteriors though: http://stats.stackexchange.com/questions/211917/sampling-from-an-improper-distribution-using-mcmc-and-otherwise\n\nThanks again,\nEric"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Reference Priors", "abstract": "In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are chosen with the hopes that they are reasonably uninformative.  Yet, objective priors such as the Jeffreys and Reference would often be preferred over flat priors if deriving them was generally tractable.  We overcome this problem by proposing a black-box learning algorithm for Reference prior approximations.  We derive a lower bound on the mutual information between data and parameters and describe how its optimization can be made derivation-free and scalable via differentiable Monte Carlo expectations.  We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's Reference prior.     ", "pdf": "/pdf/b77f2ea887d3de3dd62976de07c86014fb35fdce.pdf", "TL;DR": "A method for learning Reference prior approximations.", "paperhash": "nalisnick|variational_reference_priors", "conflicts": ["uci.edu"], "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": [], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486628772632, "tcdate": 1486628772632, "id": "ICLR.cc/2017/workshop/-/paper12/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper12/reviewers"], "reply": {"forum": "rJnjwsYde", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486628772632}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489384510040, "tcdate": 1489384437793, "number": 1, "id": "rk0lV27il", "invitation": "ICLR.cc/2017/workshop/-/paper12/public/comment", "forum": "rJnjwsYde", "replyto": "rkEy9ugix", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "writers": ["~Eric_Nalisnick1"], "content": {"title": "Author Response", "comment": "Thanks for your thoughtful comments, Reviewer #1.  We, in general, agree with your assessment.  Below are a few responses and comments.\n\n1.  Indeed, the step from the 1d models to the VAE is large.  We left out discussion of some intermediate models (ex: Gaussian mixtures) because we wanted to include the VAE result, which we thought would be of more interest to the ICLR community.\n\n2.  On whether the VAE result is trustworthy: assuming a euclidean latent space, the VAE's true reference prior is a function that approaches infinity at the domain's extremes.  Our reference prior approximation clearly exhibits these characteristics, and therefore we think it's extremely unlikely that optimization is finding some pathological or unrepresentative solution.\n\n3.  On scale-invariance: reference priors are usually identifiable only up to proportionality; so yes, they are scale invariant.  Actually, our method allows the user to sidestep these problems with the reference prior (ex: inability to be normalized) because we can learn an approximation that is well-behaved, a proper distribution, etc.   \n\nThanks again,\nEric"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Reference Priors", "abstract": "In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are chosen with the hopes that they are reasonably uninformative.  Yet, objective priors such as the Jeffreys and Reference would often be preferred over flat priors if deriving them was generally tractable.  We overcome this problem by proposing a black-box learning algorithm for Reference prior approximations.  We derive a lower bound on the mutual information between data and parameters and describe how its optimization can be made derivation-free and scalable via differentiable Monte Carlo expectations.  We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's Reference prior.     ", "pdf": "/pdf/b77f2ea887d3de3dd62976de07c86014fb35fdce.pdf", "TL;DR": "A method for learning Reference prior approximations.", "paperhash": "nalisnick|variational_reference_priors", "conflicts": ["uci.edu"], "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": [], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1486628772632, "tcdate": 1486628772632, "id": "ICLR.cc/2017/workshop/-/paper12/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper12/reviewers"], "reply": {"forum": "rJnjwsYde", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1486628772632}}}, {"tddate": null, "tmdate": 1489285002328, "tcdate": 1489285002328, "number": 2, "id": "SJG51Nfsl", "invitation": "ICLR.cc/2017/workshop/-/paper12/official/review", "forum": "rJnjwsYde", "replyto": "rJnjwsYde", "signatures": ["ICLR.cc/2017/workshop/paper12/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper12/AnonReviewer2"], "content": {"title": "Interesting approach to learning priors for generative models", "rating": "7: Good paper, accept", "review": "This extended abstract proposes an interesting method to learn a reference prior distribution using a variational formulation with the reparameterization trick. There is a need for this sort of work, since the generic prior distributions commonly used in VAEs and GANs are somewhat unsatisfying. The idea of learning a reference prior is interesting, and I haven\u2019t seen it discussed in the context of deep generative models.\n\nThe contributions and experiments seem sufficient for a workshop paper, so I would recommend acceptance. \n\nI\u2019m a little concerned about the variance of the estimates in Eqn. (3); this resembles the likelihood weighting based estimate of p(D), which can have extremely large, or even infinite variance. Are the estimates stable?\n\nThe VAE example is interesting. What can we learn from the shape of the learned prior?  Does the bimodal structure imply the distribution is multimodal?  Does the reference prior yield a better density model than the spherical Gaussian one?\n\nIn the VAE experiment, what keeps the prior from expanding to be infinitely broad?  Is the true reference prior guaranteed to be proper?  What happens if it is improper?\n\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Reference Priors", "abstract": "In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are chosen with the hopes that they are reasonably uninformative.  Yet, objective priors such as the Jeffreys and Reference would often be preferred over flat priors if deriving them was generally tractable.  We overcome this problem by proposing a black-box learning algorithm for Reference prior approximations.  We derive a lower bound on the mutual information between data and parameters and describe how its optimization can be made derivation-free and scalable via differentiable Monte Carlo expectations.  We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's Reference prior.     ", "pdf": "/pdf/b77f2ea887d3de3dd62976de07c86014fb35fdce.pdf", "TL;DR": "A method for learning Reference prior approximations.", "paperhash": "nalisnick|variational_reference_priors", "conflicts": ["uci.edu"], "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": [], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489285003040, "id": "ICLR.cc/2017/workshop/-/paper12/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper12/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper12/AnonReviewer1", "ICLR.cc/2017/workshop/paper12/AnonReviewer2"], "reply": {"forum": "rJnjwsYde", "replyto": "rJnjwsYde", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper12/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489285003040}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489239932678, "tcdate": 1489172956366, "number": 1, "id": "rkEy9ugix", "invitation": "ICLR.cc/2017/workshop/-/paper12/official/review", "forum": "rJnjwsYde", "replyto": "rJnjwsYde", "signatures": ["ICLR.cc/2017/workshop/paper12/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper12/AnonReviewer1"], "content": {"title": "An interesting and original proposal to bring (uninformative) reference priors to deep generative models. This work might contribute an important and original argument for the discussion of how to choose priors for deep generative models.", "rating": "7: Good paper, accept", "review": "The paper is well written and presents a novel variational approach to find approximate reference priors for arbitrary models. The derivation of their method is clear and easy to follow. In the experimental section, the authors first show that their method recovers the well known Jeffreys prior for 1-dimensional toy models with high accuracy. They then show that they can also find an approximate reference prior for a VAE model with 2-dimensional latent space: This prior is significantly different from the widely used isotropic Gaussian, e.g., is is multimodal. This could be a significant result and might contribute important arguments for the discussion of how to choose priors and how to choose the model structure for deep generative models. Unfortunately, and probably due to the 3 page constraint for this workshop, I\u2019m not convinced that these results are 100% trustworthy: The step from 1d models, where the proposed method works as expected, to VAE-style latent variable models seems rather big and I can imagine various ways how the optimization might fail and produce misleading results. Additional results for models of intermediate complexity and more details/diagnostics could greatly enhance this paper (but would probably break the 3 page limit). I\u2019m also wondering whether there is a scale-invariance / degeneracy in the model: Scaling the mean/stddev. of the prior and posterior by a constant factor should result in an equivalent model.    \n\n\n\nNevertheless, I think this is very interesting work which has the potential to initiate a new discussion about priors for generative models.\n\nPro:\n- original approach; well motivated \n- experiments show the method works on 1d toy models\n- the result for the 2-dimensional VAE is surprising and might form some kind of argument for future work on latent variable models -> potential high impact. \n\nCon:\n- weak experimental section: I\u2019m not convinced that the result for the 2d VAE is trustworthy.  ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Variational Reference Priors", "abstract": "In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are chosen with the hopes that they are reasonably uninformative.  Yet, objective priors such as the Jeffreys and Reference would often be preferred over flat priors if deriving them was generally tractable.  We overcome this problem by proposing a black-box learning algorithm for Reference prior approximations.  We derive a lower bound on the mutual information between data and parameters and describe how its optimization can be made derivation-free and scalable via differentiable Monte Carlo expectations.  We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's Reference prior.     ", "pdf": "/pdf/b77f2ea887d3de3dd62976de07c86014fb35fdce.pdf", "TL;DR": "A method for learning Reference prior approximations.", "paperhash": "nalisnick|variational_reference_priors", "conflicts": ["uci.edu"], "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": [], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489285003040, "id": "ICLR.cc/2017/workshop/-/paper12/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper12/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper12/AnonReviewer1", "ICLR.cc/2017/workshop/paper12/AnonReviewer2"], "reply": {"forum": "rJnjwsYde", "replyto": "rJnjwsYde", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper12/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper12/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489285003040}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1488582702234, "tcdate": 1486628772083, "number": 12, "id": "rJnjwsYde", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "rJnjwsYde", "signatures": ["~Eric_Nalisnick1"], "readers": ["everyone"], "content": {"title": "Variational Reference Priors", "abstract": "In modern probabilistic learning, we often wish to perform automatic inference for Bayesian models.  However, informative priors are often costly to elicit, and in consequence, flat priors are chosen with the hopes that they are reasonably uninformative.  Yet, objective priors such as the Jeffreys and Reference would often be preferred over flat priors if deriving them was generally tractable.  We overcome this problem by proposing a black-box learning algorithm for Reference prior approximations.  We derive a lower bound on the mutual information between data and parameters and describe how its optimization can be made derivation-free and scalable via differentiable Monte Carlo expectations.  We experimentally demonstrate the method's effectiveness by recovering Jeffreys priors and learning the Variational Autoencoder's Reference prior.     ", "pdf": "/pdf/b77f2ea887d3de3dd62976de07c86014fb35fdce.pdf", "TL;DR": "A method for learning Reference prior approximations.", "paperhash": "nalisnick|variational_reference_priors", "conflicts": ["uci.edu"], "authors": ["Eric Nalisnick", "Padhraic Smyth"], "keywords": [], "authorids": ["enalisni@uci.edu", "smyth@ics.uci.edu"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 6}