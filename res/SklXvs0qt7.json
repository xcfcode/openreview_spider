{"notes": [{"id": "SklXvs0qt7", "original": "S1xPrgoStX", "number": 247, "cdate": 1538087770605, "ddate": null, "tcdate": 1538087770605, "tmdate": 1545355393739, "tddate": null, "forum": "SklXvs0qt7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "HJlD0CKexE", "original": null, "number": 1, "cdate": 1544752846983, "ddate": null, "tcdate": 1544752846983, "tmdate": 1545354517528, "tddate": null, "forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Meta_Review", "content": {"metareview": "The manuscript describes a procedure for prioritizing the contents of an experience replay buffer in a UVFA setting based on a density model of the trajectory of the achieved goal states. A rank-based transformation of densities is used to stochastically prioritize the replay memory.\n\nReducing the sample complexity of RL is a worthy goal and reviewers found the overall approach is interesting, if somewhat arbitrary in the implementation details. Concerns were raised about clarity and justification, and the restriction of experiments to fully deterministic environments.\n\nAfter personally reading the updated manuscript I found clarity to still be lacking. Statements like \"... uses the ranking number (starting from zero) directly as the probability for sampling\" -- this is not true (it is normalized, as confusingly laid out in equation 2 with the same symbol used for the unnormalized and normalized densities), and also implies that the least likely trajectory under the model is never sampled, which doesn't seem like a desirable property. Schaul's \"prioritized experience replay\" is cited for the choice of rank-based distribution, but the distribution employed in that work has rather different form. The related work section is also very poor given the existing body of literature on curiosity in a reinforcement learning context, and the new \"importance sampling perspective\" section serves little explicatory purpose given that an importance re-weighting is not performed. \n\nOverall, I concur most strongly with AnonReviewer1 that more work is needed to motivate the method and prove its robustness applicability, as well as to polish the presentation.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Insufficient clarity and detail, reviewer concerns not addressed."}, "signatures": ["ICLR.cc/2019/Conference/Paper247/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper247/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353283023, "tddate": null, "super": null, "final": null, "reply": {"forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper247/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper247/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper247/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353283023}}}, {"id": "SyeblhC2pQ", "original": null, "number": 5, "cdate": 1542413288886, "ddate": null, "tcdate": 1542413288886, "tmdate": 1542564153884, "tddate": null, "forum": "SklXvs0qt7", "replyto": "Byxla5U9h7", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "content": {"title": "To Reviewer", "comment": "Thank you for the valuable feedback!\nWe uploaded a revised version of the paper based on the comments.\n\n- The reason behind using V-GMM is that V-GMM is much faster than KDE in inference and has a better generalization ability compared to GMM. We use V-GMM as a proof of concept for the idea \u201cCuriosity-Driven Experience Prioritization via Density Estimation\u201d. Other density estimation methods can also be applied. We now clarify these reasons in Section \u201c2.3 Density Estimation Methods\u201d of the revised paper.\n\n- We concatenate the goals and estimate the trajectory density instead of state density because HER needs to sample a future state in the trajectory as a virtual goal for training.\n\n- For episodes of different length, we can pad or truncate the trajectories into same lengths and apply V-GMM. Another method is to use PCA or auto-encoder to reduce the dimension into a fixed size and then apply CDP.\n\n- Similarly, to handle scaling issues, for very high dimension vectors, we can first apply dimension reduction methods, such as PCA and auto-encoder, and then use CDP.\n\n- The reference for \"It is known that PER can become very expensive in computational time\u201d is actually the \u201cPrioritized Experience Replay\u201d paper itself. On page three of the PER paper, it writes \u201cImplementation: To scale to large memory sizes N , we use a binary heap data structure for the priority queue, for which finding the maximum priority transition when sampling is O(1) and updating priorities (with the new TD-error after a learning step) is O(log N). See Appendix B.2.1 for more details. \u201c\n\nIn their Atari case, the memory size N is of 1e4 transitions.\nIn our hand manipulation environment cases, the memory size N is of 1e6 trajectories, and each trajectory has 100 transitions. Thus, the memory size is 1e4 (theirs) vs 1e8 (ours). The complexity of updating priorities is O(log N). Therefore, PER is very expensive in computational time, at least in our case.\n\nThe memory buffer size N can be found in OpenAI Baselines link: https://github.com/openai/baselines"}, "signatures": ["ICLR.cc/2019/Conference/Paper247/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618707, "tddate": null, "super": null, "final": null, "reply": {"forum": "SklXvs0qt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper247/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper247/Authors|ICLR.cc/2019/Conference/Paper247/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618707}}}, {"id": "Byl5tiR3pX", "original": null, "number": 4, "cdate": 1542413186075, "ddate": null, "tcdate": 1542413186075, "tmdate": 1542413703172, "tddate": null, "forum": "SklXvs0qt7", "replyto": "H1g5Um_92m", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "content": {"title": "To Reviewer", "comment": "Thank you for the valuable feedback!\nWe uploaded a revised version of the paper based on the comments.\n\n- We added a mathematical justification paragraph in Section 3.3 \u201cAn Importance Sampling Perspective\u201d. We argue that to estimate the integral of the loss function L(\u03c4) of the RL agent efficiently, we need to draw samples \u03c4 from the buffer in regions which have a high probability, p(\u03c4), but also where L|(\u03c4)| is large. Since, p(\u03c4) is a uniform distribution, i.e., the agent replays trajectories at random, we only need to draw samples which has large errors L|(\u03c4)|. The result can be highly efficient, meaning the agent needs less samples than sampling from the uniform distribution p(\u03c4). The CDP framework finds the samples that have large errors based on the \u2018surprise\u2019 of the trajectory. \n\nAny density estimation method that can approximate the trajectory density can provide a more efficient proposal distribution q(\u03c4) than the uniform distribution p(\u03c4). The sampling mechanism should have a property of oversampling trajectories with larger errors/\u2018surprise\u2019.\n\n- To mitigate the influence of very unusual stochastic transitions, we use the ranking instead of the density directly. The reason is that the rank-based variant is more robust because it is not affected by outliers nor by density magnitudes. Furthermore, its heavy-tail property also guarantees that samples will be diverse (Schaul et al., 2015b). \n\n- Yes, the experiments are mostly in deterministic domains.\n\n- In the FetchSlide environment, the best-learned policy of CDP outperforms the baselines and PER, as shown in Table 1. \nYes, we did not use the last set of parameters but used the best one encountered during training, as described in Section 4 \u201cExperiments\u201d: \u201cAfter training, we use the best-learned policy as the final policy and test it in the environment. The testing results are the final mean success rates.\u201c\n\n- Our implementation is based on \u201cOpenAI Baselines\u201d, which provides HER. We combined HER with PER in \u201cOpenAI Baselines\u201d.\nOpenAI Baselines link: https://github.com/openai/baselines\n\n- To improve the clarity of the paper, we move the exact set-up into the earlier section, Section 2.1 \u201cEnvironments\u201d. \nIn this section, we also redefine the \u201cstate\u201d based on your suggestions.\nWe delete the \u201ctroublesome\u201d sentence and also clarify what the goal actually is in Section 2.1.\nFor more detail, please read the revised paper, Section 2.1."}, "signatures": ["ICLR.cc/2019/Conference/Paper247/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618707, "tddate": null, "super": null, "final": null, "reply": {"forum": "SklXvs0qt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper247/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper247/Authors|ICLR.cc/2019/Conference/Paper247/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618707}}}, {"id": "HylYBoAhTm", "original": null, "number": 3, "cdate": 1542413120613, "ddate": null, "tcdate": 1542413120613, "tmdate": 1542413120613, "tddate": null, "forum": "SklXvs0qt7", "replyto": "rJgNuJFq37", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "content": {"title": "To Reviewer", "comment": "Thank you for the valuable feedback!\nWe uploaded a revised version of the paper based on the comments.\n\n- To improve the clarity, we clarify why we chose to use V-GMM, among the three basic density estimation methods, including KDE, GMM, and V-GMM.  (in the revised version of the paper Section \u201c2.3 Density Estimation Methods\u201d) \nThe reasons are the following:\n1. GMM can be trained reasonably fast for RL agents. GMM is also much faster in inference compared to Kernel Density Estimate (KDE) (Rosenblatt, 1956). \n2. Compared to GMM,  V-GMM has a natural tendency to set some mixing coefficients close to zero and generalizes better. \n3. We only use a basic density estimation method, such as V-GMM, in our framework as a proof of concept for the idea \u201cCuriosity-Driven Experience Prioritization via Density Estimation\u201d. Other destiny estimation methods can also be applied in this framework. \n\n- We move the exact setup (Section \u201c2.1 Environments\u201d in the new version) in early sections to improve the clarity of the paper.\n\n- We are glad that you like the idea of the paper. Yes, indeed the curiosity mechanism in our context is related to surprise. The idea of our method is also related to neuroscience (Gruber et al., 2014).\n\n- Yes, the entire trajectories need to stored in the replay buffer and the memory size increases as the trajectory length increases. However, this is a general issue with off-policy RL methods which uses experience replay, such as DQN and DDPG. Our method CDP only uses the trajectories that are already in the memory, so CDP does not introduce additional memory usage."}, "signatures": ["ICLR.cc/2019/Conference/Paper247/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618707, "tddate": null, "super": null, "final": null, "reply": {"forum": "SklXvs0qt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper247/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper247/Authors|ICLR.cc/2019/Conference/Paper247/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618707}}}, {"id": "rJxr2qAnp7", "original": null, "number": 2, "cdate": 1542412972581, "ddate": null, "tcdate": 1542412972581, "tmdate": 1542413005241, "tddate": null, "forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "content": {"title": "Revision", "comment": "\n- Move the exact set-up in early section (Section 2.1 Environments).\n- Clarify the reason for using V-GMM in Section 2.3\n- Define the \u201cstate\u201d more formally in Section 2.1\n- Add mathematical justification in Section 3.3 An Importance Sampling Perspective\n- Other small fixes to improve the clarity of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper247/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618707, "tddate": null, "super": null, "final": null, "reply": {"forum": "SklXvs0qt7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper247/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper247/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper247/Authors|ICLR.cc/2019/Conference/Paper247/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper247/Reviewers", "ICLR.cc/2019/Conference/Paper247/Authors", "ICLR.cc/2019/Conference/Paper247/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618707}}}, {"id": "rJgNuJFq37", "original": null, "number": 3, "cdate": 1541209964491, "ddate": null, "tcdate": 1541209964491, "tmdate": 1541534160374, "tddate": null, "forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Official_Review", "content": {"title": "review for Curiosity-Driven Experience Prioritization via Density Estimation", "review": "The paper proposes a novel method for sampling examples for experience replay. It addresses the problem of having inbalanced data (in the experience buffer during training). The authors trained a density model and replay the trajectories that has a low density under the model. \n\nNovelty:\n\nThe approach is related to prioritized experience replay, PER is computational expensive because of the TD error update, in comparison, CDR only updates trajectory density once per trajectory.\n\nClarity:\n\nThe paper seems to lack clarity on certain design/ architecture/ model decisions.  For example, the authors did not justify why VGMM model was chosen and how does it compare to other density estimators.  Also, I had to go through a large chunk of the paper before coming across the exact setup. I think the paper could benefit from having this in the earlier sections.\n\n\nOther comments about the paper:\n\n-  I do like the idea of the paper. It also seems that curiosity in this context seems to be very related to surprise? There are neuroscience evidence indicating that humans turns to remember (putting more weights) on events that are more surprising.\n\n- The entire trajectory needs to be stored, so the memory wold grow with episode length. I could see this being an issue when episode length is too long.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper247/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Official_Review", "cdate": 1542234505473, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper247/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335681484, "tmdate": 1552335681484, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper247/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1g5Um_92m", "original": null, "number": 2, "cdate": 1541206866078, "ddate": null, "tcdate": 1541206866078, "tmdate": 1541534160173, "tddate": null, "forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Official_Review", "content": {"title": "Handling unbalanced target distributions when conditioning on goal in RL", "review": "This paper addresses a problem that arises in \"universal\" value-function approximation (that is, reinforcement-learning when a current goal is included as part of the input);  when doing experience replay, the experience buffer might have much more representation of some goals than others, and it's important to keep the training appropriately balanced over goals.\n\nSo, the idea is to a kind of importance weighting of the trajectory memory, by doing a density estimation on the goal distribution represented in the memory and then sample them for training in a way that is inversely related to their densities.  This method results in a moderate improvement in the effectiveness of DDPG, compared to the previous method for hindsight experience replay.\n\nThe idea is intuitively sensible, but I believe this paper falls short of being ready for publication for three major reasons.\n\nFirst, the mechanism provided has no mathematical justification--it seems fairly arbitrary.   Even if it's not possible to prove something about this strategy, it would be useful to just state a desirable property that the sampling mechanism should have and then argue informally that this mechanism has that property.  As it is, it's just one point in a large space of possible mechanisms.\n\nI have a substantial concern that this method might end up assigning a high likelihood of resampling trajectories where something unusual happened, not because of the agent's actions, but because of the world having made a very unusual stochastic transition.   If that's true, then this distribution would be very bad for training a value function, which is supposed to involve an expectation over \"nature\"'s choices in the MDP.\n\nSecond, the experiments are (as I understand it, but I may be wrong) in deterministic domains, which definitely does not constitute a general test of a proposed RL  method.  \n- I'm not sure we can conclude much from the results on fetchSlide (and it would make sense not to use the last set of parameters but the best one encountered during training)\n- What implementation of the other algorithms did you use?\n\nThird, the writing in the paper has some significant lapses in clarity.  I was a substantial way through the paper before understanding exactly what the set-up was;  in particular, exactly what \"state\" meant was not clear.  I would suggest saying something like s = ((x^g, x^c), g) where s is a state from the perspective of value iteration, (x^g, x^c) is a state of the system, which is a vector of values divided into two sub-vectors, x^g is the part of the system state that involves the state variables that are specified in the goal, x^c (for 'context') is the rest of the system state, and g is the goal.  The dimensions of x^g and g should line up.\n- This sentence  was particularly troublesome:  \"Each  state s_t also includes the state of the achieved goal, meaning the goal state is a subset of the normal state.  Here, we overwrite the notation s_t  as the achieved goal state, i.e., the state of the object.\"\n- Also, it's important to say what the goal actually is, since it doesn't make sense for it to be a point in a continuous space.  (You do say this later, but it would be helpful to the reader to say it earlier.)\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper247/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Official_Review", "cdate": 1542234505473, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper247/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335681484, "tmdate": 1552335681484, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper247/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Byxla5U9h7", "original": null, "number": 1, "cdate": 1541200567556, "ddate": null, "tcdate": 1541200567556, "tmdate": 1541534159934, "tddate": null, "forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "invitation": "ICLR.cc/2019/Conference/-/Paper247/Official_Review", "content": {"title": "The method proposed in the paper may have poor generalization and scaling performance", "review": "This work considers a version of importance sampling of states from the replay buffer.  Each trajectory is assigned a rank, inversely proportional to its probability according to a GMM. The trajectories with lower rank are preferred at sampling.\n\nMain issues:\n\n1. Estimating rank from a density estimator\n\n- the reasoning behind picking VGMM as the density estimator is not fully convincing and (dis)advantages of other candidate density estimators are almost not highlighted.\n\n- it is unclear and possibly could be better explained why one needs to concatenate the goals (what would change if we would not concatenate but estimate state densities rather than trajectories?)\n\n2. Generalization issues\n\n- the method is not applicable to episodes of different length\n- the approach assumes existence of a state to goal function f(s)->g\n- although the paper does not expose this point (it is discussed the HER paper)\n\n3. Scaling issues\n\n- length of the vector grows linearly with the episode length\n- length of the vector grows linearly with the size of the goal vector\n\nFor long episodes or episodes with large goal vectors it is quite possible that there will not be enough data to fit the GMM model or one would need to collect many samples prior.\n\n4. Minor issues\n\n- 3.3 \"It is known that PER can become very expensive in computational time\" - please supply a reference\n \n\n- 3.3 \"After each update of the model, the agent needs to update the priorities of the transitions in the replay buffer with the new TD-errors\" - However the method only renews priorities of randomly selected transitions (why would there be a large overhead?). Here is from the PER paper \"Our final implementation for rank-based prioritization produced an additional 2%-4% increase in running time and negligible additional memory usage\"\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper247/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Curiosity-Driven Experience Prioritization via Density Estimation", "abstract": "In Reinforcement Learning (RL), an agent explores the environment and collects trajectories into the memory buffer for later learning. However, the collected trajectories can easily be imbalanced with respect to the achieved goal states. The problem of learning from imbalanced data is a well-known problem in supervised learning, but has not yet been thoroughly researched in RL. To address this problem, we propose a novel Curiosity-Driven Prioritization (CDP) framework to encourage the agent to over-sample those trajectories that have rare achieved goal states. The CDP framework mimics the human learning process and focuses more on relatively uncommon events. We evaluate our methods using the robotic environment provided by OpenAI Gym. The environment contains six robot manipulation tasks. In our experiments, we combined CDP with Deep Deterministic Policy Gradient (DDPG) with or without Hindsight Experience Replay (HER). The experimental results show that CDP improves both performance and sample-efficiency of reinforcement learning agents, compared to state-of-the-art methods.", "keywords": ["Curiosity-Driven", "Experience Prioritization", "Hindsight Experience", "Reinforcement Learning"], "authorids": ["zhaorui.in.germany@gmail.com", "volker.tresp@siemens.com"], "authors": ["Rui Zhao", "Volker Tresp"], "TL;DR": "Our paper proposes a curiosity-driven prioritization framework for RL agents, which improves both performance and sample-efficiency.", "pdf": "/pdf/2e0745004fb12144624e78f1eda529cb7268702a.pdf", "paperhash": "zhao|curiositydriven_experience_prioritization_via_density_estimation", "_bibtex": "@misc{\nzhao2019curiositydriven,\ntitle={Curiosity-Driven Experience Prioritization via Density Estimation},\nauthor={Rui Zhao and Volker Tresp},\nyear={2019},\nurl={https://openreview.net/forum?id=SklXvs0qt7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper247/Official_Review", "cdate": 1542234505473, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SklXvs0qt7", "replyto": "SklXvs0qt7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper247/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335681484, "tmdate": 1552335681484, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper247/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 9}