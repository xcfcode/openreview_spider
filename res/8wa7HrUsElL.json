{"notes": [{"id": "8wa7HrUsElL", "original": "H8MwOlxGlIj", "number": 2116, "cdate": 1601308233115, "ddate": null, "tcdate": 1601308233115, "tmdate": 1614985711643, "tddate": null, "forum": "8wa7HrUsElL", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "FRLrsJ1oMmA", "original": null, "number": 1, "cdate": 1610040431192, "ddate": null, "tcdate": 1610040431192, "tmdate": 1610474031214, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper proposes a method of decentralized mechanism design to reduce the price of anarchy. Based on the detailed responses of the authors, all reviewers were satisfied by the technical contribution after the rebuttal period. \n\nThere was, however, a heavily engaged and lengthy discussion between most reviewers regarding the applicability of the method and how it links to the motivation given in the paper. The paper could be improved by (1) highlighting an exemplar real world use case in the paper motivation (there are a couple mentioned briefly in the introduction but one of these could be emphasized more); and (2) connecting the choices made in the design of the approach to the opening motivation sections and exemplar use case.\n\nThe level of engagement from most reviewers demonstrates a good level of interest from a representative sample of the ICLR community but demonstrated that their remained work outstanding to clarify the core message and significance of the contribution."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040431179, "tmdate": 1610474031198, "id": "ICLR.cc/2021/Conference/Paper2116/-/Decision"}}}, {"id": "EbZqAXVsDTN", "original": null, "number": 2, "cdate": 1603815578395, "ddate": null, "tcdate": 1603815578395, "tmdate": 1606585476509, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review", "content": {"title": "Nice idea, but not explored deeply enough", "review": "The paper details a method by which it attempts a method to change the utility function for agents so they incorporate the utilities of other agents, resulting in more cooperating agents, so that the price of anarchy is minimized. It examines such trained agents in several problem settings, seeing them performing quite well.\n\nThe attempt to change the utility is quite interesting, but, sadly, it is not really explored enough. That is, while agents improve, no attempt has been made to examine the new \"mixing\" matrix, and see what changed to make the agents collaborate better (or not). It is quite a pity, as in this case, unlike many other learning cases, the result is quite interpretable for us.\n\nAs more a game-theorist than a ML expert, I did struggle a bit with the technical bits in section 2. The definition of \"local price of anarchy\" seems sometime overlapping, but not really the same as the definition of that of Ben-Zwi and Ronen from 2011 (\"Local and global price of anarchy of graphical games\", TCS 412, 1196-1207). I had some difficulty following algorithm 1, as \\Delta t_b seems always negative, and it was not clear to me what G is (and in particular, for negative values).\n\nIn the experimental section, as noted above, an interpretation of the A matrix would have helped. At least in some of the examples, it seems a matrix A with each value being 1/n seems to satisfy all that's needed (e.g. the Braess paradox). I could not follow the setting of the \"zero-sum election\", and what is the described game. Moreover, the introduction promises an incentive-compatible setting. What is intended to be an IC mechanism?\n\nUPDATE POST-REBUTTAL\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\nMany of my questions have been answered, though I do think reviewers should explicitly note Ben-Zwi and Ronen's paper, and change their use of \"incentive compatible\". I think a deeper and more systematic analysis of the A matrix is also warranted, but I do now feel the paper has better scientific merit.", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538103711, "tmdate": 1606915781241, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2116/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review"}}}, {"id": "o4d9oGl3tb4", "original": null, "number": 3, "cdate": 1603874279403, "ddate": null, "tcdate": 1603874279403, "tmdate": 1606505769561, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review", "content": {"title": "recommendation to accept", "review": "##########################################################################\n\nSummary:\n\nThis paper proposes a differentiable, local estimator of multi game inefficiency, as measured\nby price of anarchy. We then present two instantiations of a single decentralized meta-algorithm, one\n1st order (gradient-feedback) and one 0th order (bandit-feedback), that reduce this inefficiency. Experiments conducted in three domains (traffic network, Coins and Cleanup) show that agents minimizing local estimates of price of anarchy achieve lower loss on\naverage than selfish agents without the ability to compromise.\n\n##########################################################################\n\nReasons for score: \n\nOverall the paper is well organized and provided clear motivation of the problem. The proposed approach is well-presented and the idea of mixing losses is interesting. An upper bound for the optimized price of anarchy is presented.\n\n##########################################################################Pros: \n\nDetailed comments and concerns:\n\n1. What is the implication of \"mixing losses\" in practical systems? What are the incentives for agents to adjust their original incentives? I would like to see more discussions addressing the incentive compatibility issue in adjusting for a mixing losses.\n\n2. The authors show that if each agent\u2019s original loss is convex with diagonally dominant Hessian and the strategy space is unconstrained, the the unique, globally stable fixed point of the game defined with mixed losses is a Nash. I would be willing to see more discussions on if these restrictions could be relaxed to achieve Nash under the mixed losses.\n\n3. What are the computation times for running D3C, and how does that compare to the baselines?\n\n----- Post Discussion ---- Updates to the paper have helped make the motivation/technical parts of the paper more clear. The authors' response are helpful w.r.t the questions I raised in the review.\n\n ", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538103711, "tmdate": 1606915781241, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2116/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review"}}}, {"id": "pgREdlfY7ub", "original": null, "number": 12, "cdate": 1606152799908, "ddate": null, "tcdate": 1606152799908, "tmdate": 1606223994251, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "39gJnMiCWi", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Re \"Zero Sum and IC\"", "comment": "1.\nEach player\u2019s strategy, $x_i$, is a 1-d scalar variable constrained to [0, 1], so it can be interpreted as a probability if that is most comfortable, but in truth it is just a scalar belonging to a convex set. We did not create them with any semantics in mind. We construct the losses by adding the losses from the prisoner\u2019s dilemma game (see Figure 8 on p. 23 in the supplementary) and a canonical zero-sum game ($\\min_{x_1, x_2} \\max_{x_3, x_4} [x_1, x_2]^\\top [x_3, x_4]$). The losses are defined for each \"candidate\" as follows:\n- $f_1(x) = x_1^2 + (x_2 - 1)^2 + x_1 x_3 + x_2 x_4$\n- $f_2(x) = x_2^2 + (x_1 - 1)^2 + x_1 x_3 + x_2 x_4$\n- $f_3(x) = x_3^2 + (x_4 - 1)^2 - x_1 x_3 - x_2 x_4$.\n- $f_4(x) = x_4^2 + (x_3 - 1)^2 - x_1 x_3 - x_2 x_4$.\n\nDespite the fact that agents only see the sum of the losses of these two games, D3C was able to tease out the correct teams (players 1 and 2 vs 3 and 4). By the way, we fixed the makeshift diagram for this game in our previous response. We did not realize the first time that the formatting did not work as planned.\n\n2.\nIndeed, given your comment we agree that we have extended the \"incentive compatible\" term beyond its standard meaning in the mechanism design literature. Agents could manipulate by sharing or behaving according to false loss functions. Our result is that if all agents opt into the mechanism (with their true loss functions), they achieve good outcomes. And if some agents opt into the mechanism, in some settings (e.g., see Appendix E.2.2), the best thing an agent can do is also opt in. In other words, opting into the mechanism and acting according to the true loss function and the proposed system is a Nash equilibrium.\n\nRegarding the specific conditions for agents to reach a Nash, in Section 2.7 \u201cAssessment\u201d we state \u201c If each agent\u2019s original loss is convex with diagonally dominant Hessian and the strategy space is unconstrained, the unique, globally stable fixed point of the game defined with mixed losses is a Nash (see Appendix H.4).\u201d Note this is just to say that the resulting joint strategy $x^*$ is a Nash equilibrium given a fixed $A$, not that the pair $(x^*, A^*)$ is a Nash."}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "9iZZL3vBA9y", "original": null, "number": 4, "cdate": 1605528445305, "ddate": null, "tcdate": 1605528445305, "tmdate": 1606223894194, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "EbZqAXVsDTN", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "Thank you for reviewing our paper. From our experience, the bulk of recent work in social dilemmas in deep (neural nets) multi-agent RL fails to explain its contributions in light of longstanding issues already raised by game theory, and so we made a concerted effort here to tie our work to game theoretic concepts and terminology (e.g., price of anarchy, Myerson-Satterthwaite theorem, etc.). These fields are evolving somewhat independently, so tying them together requires finesse. As someone with a game theory background, we very much appreciate your viewpoint here.\n  Also, we believe that a few of your major concerns, for example, regarding the lack of analysis of the \u201cmixing matrix\u201d and the overlap of local PoA with other work, can be easily cleared up. Please see our responses below. We hope that in light of these clarifications (specifically regarding the analysis of the mixing matrix), you will consider reassessing your score.\n\n\u201cMixing\u201d Matrix -- This is a simple misunderstanding. We completely agree with you. The mixing matrix is very interesting to analyze, and we do in fact examine the \"mixing\" matrix in 4 cases, although maybe it was not obvious to all reviewers.\n  Figure 4b \"Relative Reward Attention\" plots $\\log(A_{ii} / A_{ji})$ for each player in the Coin dilemma. Note that for a $2 \\times 2$ matrix, each player's mixing weight can be uniquely represented by 1 number because $A_{ij} = 1 - A_{ii}$ by the simplex constraint. The relative reward attention plot shows the agents start off selfish (positive values) and then become cooperative (zero), then altruistic (negative), and then oscillate.\n  This same type of plot is included for the Prisoner\u2019s Dilemma experiment in the appendix. Figure 10 \"Relative Loss Attention\" (RLA) indeed shows the agents converge to the cooperative solution of $\\frac{1}{n}$ as you expected! We\u2019ll make it more clear that the RLA asymptote of $0$ for $n=2$ agents corresponds to $A = \\frac{1}{n}$ here. The fact that the trivial solution of $A = \\frac{1}{n}$ always exists is discussed in the Intro (paragraph 3), the last paragraph of Section 2.5, and the second paragraph of Section 3.\n  \u201cRelative Loss Attention\u201d is also plotted for the \u201cTrust-Your-Brother\u201d domain in Figure 12.\n  We did not include this same plot for Mini-Cleanup (Figure 14) or Harvest Patch (Figure 15), but we have these plots ready for the revision. The oscillation of relative reward attention is what gives rise to the alternating of roles in the cleanup domain (Figure 5) and allows agents to achieve similar levels of reward. This is in contrast to a stark division of labor seen in other work (see F.2 which discusses Yang et al. '20).\n  We also explicitly discuss the resulting \"mixing\" matrix in Section 3.4 for the election example. That section is specifically included to emphasize the value of analyzing the \"mixing\" matrix. We can include a diagram that better illustrates the hierarchical game being played here. Abstractly, it looks like\n\n                                       Party A vs Party B (Zero-Sum)\n\n                           //                                                \\\\\n\n          Party A Primary (Social Dilemma)                 Party B Primary (Social Dilemma)\n\n         //                             \\\\                  //                         \\\\\n      Candidate 1                Candidate 2          Candidate 3                Candidate 4\n\nand mathematically it is created by summing the utilities defining a zero-sum game over all 4 agents with the utilities defining two social dilemmas (specifically the prisoner\u2019s dilemma utilities defined in Section E.2) defined over the two separate parties A and B.\n\nLocal PoA [Ben-Zwi and Ronen \u201811] -- We agree. The definition of local in the paper you reference appears to be overlapping with ours. Their notion of local considers a subset of the strategy space that only contains a subset of the players (specifically the neighboring nodes in the graphical game). This would equate to considering only subspaces (specifically axis aligned projections) of the joint strategy space $\\mathcal{X}$ in our formulation whereas we do not make this restriction. On the other hand, we do not consider the entire strategy space either. Local, for us, generally considers all players in the game, but restricts the pure strategies (actions) allowed by each player. So, in summary, these two definitions look at two ways of defining \"local\".\n  For concreteness consider a two player, two action game. Each player $i$\u2019s mixed strategy can be represented by a single probability scalar $p_i$. The joint strategy space can be visualized as a unit square in the positive quadrant. The paper you reference considers a local game to be either constrained to the x-axis or y-axis. Our definition considers any small line segment within the unit square. We can include such a diagram and explanation in the appendix."}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "TJFZTnRen5U", "original": null, "number": 13, "cdate": 1606156256257, "ddate": null, "tcdate": 1606156256257, "tmdate": 1606156256257, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Changes to the Paper", "comment": "We have made a few changes to the paper in line with some of our responses below. Specifically, we\n\n- Fixed the typos noticed by the reviewers and made a few minor edits to the writing,\n- Added the explicit dependence of $x$ on $t$, i.e., $x=x(t)$, to the notation in Section 2.1,\n- Fixed / removed the statement regarding PPAD-complete below Equation 1,\n- Added a footnote regarding the dependence of $\\Delta t$ on $\\beta$ in Theorem 1,\n- Added additional \"Relative Reward Attention\" plots to Figure 6,\n- Included a separate related work section in Section 4.\n\nWe are very grateful for the reviewers' engagement in the discussion period. This was extremely helpful for improving the paper and we appreciate the time you took to thoughtfully consider and respond to our rebuttals."}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "39gJnMiCWi", "original": null, "number": 11, "cdate": 1605993679906, "ddate": null, "tcdate": 1605993679906, "tmdate": 1605993679906, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "NGn8yqlcm9t", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Zero Sum and IC", "comment": "Thank you \u2014 you answered a lot of my questions. However, I'd like some clarification on two points:\n\n1. I could not follow the description of the zero-sum election. What are the player's strategies?\n\n2. You claim, in the introduction, \"We also provide specific, albeit narrow, conditions under which agents may achieve a Nash equilibrium, i.e. the mechanism is incentive compatible\". First, a mechanism with a NE is not necessarily incentive compatible. Second, where are these specific conditions? The words \"incentive compatible\" do not occur in the paper beyond the introduction."}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "4gSV5FK-eB9", "original": null, "number": 6, "cdate": 1605528914745, "ddate": null, "tcdate": 1605528914745, "tmdate": 1605528957684, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "i0gnl7khcAs", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Thank you for reviewing our paper. We will do our best to clear up the points of confusion you note above. Please take these into consideration when determining how to adjust your score. We will respond bullet by bullet.\n\n1.\na) Game Theoretic Domains -- Do you mean, for example, $2 \\times 2$ games like chicken or matching pennies? We explore a game of this form in Section F.4. These classic normal form games are also known as linear bimatrix games and can be written as $\\max_{x \\in \\Delta} x^T A y = u_1(x,y)$ and $\\max_{y \\in \\Delta} x^T B y = u_2(x,y)$ where $\\Delta$ denotes the probability simplex. That is to say they are no different in form than the differentiable prisoner's dilemma game we explore in Section E.2. Utility functions just need to be negated to be reinterpreted as losses.\nb) Reviewer 4 made a similar comment. Please take a second look at the introduction where we discuss how \"We appeal to individual rationality in three ways.\" Also look at Section 2.7 starting with \"We justify the agents learning weight vectors...\" which explains how the proposed meta-algorithm addresses these concerns. We also point out in Appendix E.2.2 that D3C agents are robust to \u201cmavericks\u201d that choose not to mix losses in the prisoner\u2019s dilemma domain. Please let us know any specific questions you have after reviewing these.\n\n2.\nThe $KL$ term does not have to do with the communication concern, but rather with the rationality concern you raise above. An agent that learns to be altruistic (i.e., one that gives its reward to another agent) might be easily taken advantage of. Adding a $KL$ penalty encourages the agent to selfishly attend to its own loss so that it avoids altruistic behaviors if unnecessary.\n\n3.\nYour understanding of Equation 1 is correct. The denominator is a constant given total loss is conserved (i.e., budget balanced). If we could just minimize the numerator, we could directly minimize global PoA. However, minimizing the numerator requires computing the Nash equilibrium which is intractable. Instead, we introduce local PoA where we constrain the joint strategy space to a line segment. For example, consider playing rock paper scissors and then being told you must play rock, paper, and scissors such that $a \\cdot p_{rock} + b \\cdot p_{paper} + c \\cdot p_{scissors} = d$. We have now constrained your strategy space. By constraining the strategy space and leveraging theory from smooth games, we're able to construct an upper bound on this local PoA that circumvents directly computing a Nash. This is not necessarily the only way to simplify the game such that local PoA can be efficiently approximated. Alternative approaches are work for future research.\n\n4.\nLet $x=x(t)$ be a function that returns the joint strategy $x$ along any point in its trajectory. Gradient descent dynamics state that $\\dot{x} = - \\frac{df_i^A(x)}{dt}$. These are the continuous time dynamics of $x$. $f^A_i(x)$ then depends on $t$ through $x(t)$. We will add text to make this dependence more clear.\n\n5.\nYes, there is some dependence of Equation 4 on $\\beta$ which is resolved by using a smaller $\\Delta t$ (i.e., policy learning rate). The larger the $\\beta$, the smaller the $\\Delta t$ that is required for the bound to hold. This is a technical detail that is identified more clearly in the derivation in the appendix. For example, check out Equation 63 in Lemma 7 of Section D and note that $\\xi$ contains a $\\beta$ term. If $\\beta_i$ is very large, then $\\Delta t$ has to be very small to ensure $a_i$ is greater than $0$.\n\n6.\nOur experiments are meant to support the \u201cOur Contribution\u201d statement made in the Intro. You should conclude that the agents are able to minimize the price of anarchy (thereby e.g., resolving social dilemmas) in a decentralized fashion using only access to the other agents\u2019 scalar reward signals. Section 3.4 is a counterargument to \"completely mix the agents' utilities\". Uniform mixing is not the answer to all multiagent problems. We present an algorithm that automatically learns how to mix depending on the multiagent problem domain. This is also summarized in our Conclusion.\n\nMisc:\n\n(A) Thank you. That's a great point. We'll make the change.\n\n(B) Redefine the game with player strategies restricted to the line segment $\\mathcal{X}_{\\tau}$. All other components of the game (utilities, players, etc.) stay the same. We give a more detailed response above in bullet 3.\n\n(C) The utilitarian / egalitarian distinction is not central to the paper. This is more of a footnote to point the reader to Appendix D.2 if they are interested."}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "NGn8yqlcm9t", "original": null, "number": 5, "cdate": 1605528620086, "ddate": null, "tcdate": 1605528620086, "tmdate": 1605528620086, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "9iZZL3vBA9y", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (continued)", "comment": "Algorithm 1 --  We break this response into 3 parts.\na) $\\Delta t_b$ is, in fact, always positive. $t_b$ is returned by the function $trial$ at the end of the for-loop which sets $t_b$ to the current time step $t$. The quantity $t-t_b$ evaluated at the start of the subsequent loop then represents the elapsed time, a positive value.\nb) $G$ tracks the mean of $g$. $g$ is the return (sum of discounted rewards). This scalar can be positive or negative and depends on the environment. We use $G$ as it is sometimes the variable used for returns in the RL literature.\nc) To give you a high level understanding of the algorithm, it may be helpful to read the orange comments in the algorithm block. Each agent draws a mixing vector (from trial) and tries optimizing their policy with respect to their reward mixed according to all the agents' current mixing weights. Think of this as \"testing out an alliance\". If their reward signal (i.e., return or $G$ or negative loss or utility) decreases on average over the span of the trial, the agent adjusts their mixing weight in the opposite direction (because that alliance made them worse off). This loop is repeated over training.\n\nIncentive Compatibility -- The meta-algorithm that adjusts the mixing weights is the mechanism just as Vickrey-Clarke-Groves (VCG) is a mechanism, however, in contrast to VCG, our meta-algorithm is distributed over agents and can be viewed as the agents negotiating their own compromise. If you mean that we do not explore settings where agents are capable of explicitly \"lying\", that is true. However, an agent may mislead another by adjusting its mixing weights and use that to its advantage. With this in mind, agents converge to a stable equilibrium in the Braess' paradox example with $A=\\frac{1}{n}$ for which any strategy that attempts to mislead (lie) by deviating is immediately punished. Therefore, truthfulness is ultimately incentivized in that example."}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "EnovW4pF9Ju", "original": null, "number": 3, "cdate": 1605528011914, "ddate": null, "tcdate": 1605528011914, "tmdate": 1605528011914, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "o4d9oGl3tb4", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for reviewing our paper. We\u2019re glad to hear that you found it well motivated and presented. We\u2019ll respond to your comments bullet by bullet.\n\n1.\na) Practically speaking, \u201cmixing losses\u201d means that after every episode (or policy learning update loop) each agent must broadcast their return (discounted sum of rewards, i.e., a single float), $G_i$, to a central master that then mixes them (i.e., performs the matrix vector product $A^T G$ where $A$ is the mixing matrix and $G = [G_1, \\ldots, G_N]^\\top$). Every few episodes, an agent updates their mixing weights and sends them to the master to overwrite the row of $A$ corresponding to that agent. Note that instead of broadcasting to a central master, the agents could broadcast the necessary weights directly to each other and mixing could be performed by each individual agent -- this is technically equivalent but maybe a philosophically interesting distinction.\nb) Please take a second look at the introduction where we discuss how \"We appeal to individual rationality in three ways.\" Also look at Section 2.7 starting with \"We justify the agents learning weight vectors...\" which explains how the proposed meta-algorithm addresses these concerns. Please let us know any specific questions you have after reviewing these.\n\n2.\nWe don't currently see any route to relaxing this restriction. The fact that the mixture weights and strategies are adjusted concurrently results in nonlinear dynamics and proving these continuous time dynamics (let alone discrete updates) converge to a Nash is very difficult. It may be possible to prove local convergence as is done with GANs where the game is also highly nonlinear.\n\n3.\nMaybe the answer to this question is more clear after reading our answer to 1a. The computation required for mixing losses is negligible in our experiments (computing a single matrix-vector product $A^T$ [vector_of_losses]) and so the runtime is the same. If we were to scale up to 1000s of agents, runtime may be affected, but would likely still be dominated by the environment step and querying the neural network policy. Note that our experiments plot the losses and returns of the agent after every strategy (policy) update, not every mixture weight update so the comparison to a vanilla selfish agent really is \u201capples to apples\u201d. In other words, if a loss-sharing agent tries out a mixing weight for 5 steps during which it makes 5 policy updates, we plot the returns for each policy update. A vanilla selfish agent does the same; it just does not ever adjust its mixing weight (it is fixed to a onehot)."}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "-VM0JswXzi-", "original": null, "number": 2, "cdate": 1605527753182, "ddate": null, "tcdate": 1605527753182, "tmdate": 1605527753182, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "T9ChW_VUW-_", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Thank you for taking the time to carefully read our paper. We\u2019re pleased to hear that the major message resonates with you.\n\nPros:\n\nWe have a longer version of the paper prepared with a distinct related work section that touches on connections to recent work on learning loss functions, learning loss mixtures, gifting rewards to other agents, and more general connections to social psychology, neuroscience, and evolutionary biology, so we will include this in our revision. FYI, there is another submission to ICLR titled \u201cLearning to Share in Multi-agent Reinforcement Learning\u201d which also looks at mixing losses, but it is motivated from Markov games rather than from a classical game theoretic perspective like ours (price of anarchy, Nash, Myerson-Satterthwaite theorem). We have no relation to that paper; we just noticed it and thought you might find it interesting. Their related work section highlights the lack of decentralized approaches and so has a similar motivation to our work.\n  Our work is most similar to \u201cInducing Cooperation through Reward Reshaping based on Peer Evaluations in Deep Multi-Agent Reinforcement Learning\u201c [AAMAS 2020]. This work also provides a decentralized approach that transforms the game by modifying rewards, however, it does not guarantee \u201cbudget-balance\u201d nor does it derive its proposed algorithm from any principle (e.g., price of anarchy); the proposed algorithm is a heuristic supported by experiments. Other work, \u201cGifting in Multi-Agent Reinforcement Learning\u201c [AAMAS 2020] explores gifting rewards to agents as well, but it does so by simply expanding the action space of agents to include a gifting action. It is also not budget balanced.\n\nCons:\n\nAs you say, our proposed approach is not ideal for all settings. There are cases where sharing reward scalars might be prohibitively expensive (or not permitted) and agents may have to learn to cooperate and circumvent social dilemmas using only access to their own environment observations. We will also compare this assumption to others made in the literature (observing other agent actions or access to other agent policies is common, e.g., \u201cLearning to Incentivize Others (LIO)\u201d / \u201dLearning with Opponent-Learning Awareness (LOLA)\u201d). Note that among the possible bits of information to communicate-- policy weights ($w_i \\in \\mathbb{R}^d$), observations ($o_i \\in \\mathbb{R}^m$), actions/strategies ($x_i \\in \\mathbb{R}^{q}$ for continuous actions), rewards ($r_i \\in \\mathbb{R}$)-- we think rewards are possibly the cheapest. Note, communicating actions is cheap for discrete action spaces, but expensive for continuous (vector) spaces. On the other hand, rewards are always scalars. It\u2019s amazing how much a human can communicate with a frown (-1) or smile (+1) ;)\n\nOther notes:\n\nThanks for raising this. We should have said \"all agents\" instead of \"all other agents\". That was a mistake.\n\nThe appendix can be found in the supplementary material. There should be a \u201cdownload arrow\u201d next to the word \u201czip\u201d at the top of the OpenReview page.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "8wa7HrUsElL", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2116/Authors|ICLR.cc/2021/Conference/Paper2116/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923852091, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Comment"}}}, {"id": "i0gnl7khcAs", "original": null, "number": 1, "cdate": 1603179780514, "ddate": null, "tcdate": 1603179780514, "tmdate": 1605024285568, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review", "content": {"title": "I'm confused about too many things with this paper", "review": "The authors propose to change agents' incentives in order to lead to a better Nash equilibrium outcome. \nThey propose a practical decentralized approach to computing the \"optimal\" change of incentives.\nI'm confused by many aspects of this paper:\n1. How would you implement the change of incentives in a any of the game theoretic domains? I.e. in what context would agents who lose money agree to participate without a centralized authority ordering them to?\n2. Why do you consider changes of incentives with bounded KL-divergence? What does that have to do with your motivating concern around communication (3rd paragraph of intro)?\n3. Why do you optimize the price of anarchy instead of social welfare at worst Nash equilibrium? These should be the same when you don't change the total loss, but then I don't understand the denominator of (3).\n4. What is $t$ in (4)? How does f^A_i depend on t?\n5. Shouldn't \\beta_i appear in (4)? Or does (4) hold for all \\beta_i (e.g. approaching infinity)?\n6. What should I conclude from your experiments? Of course if you completely mix the agents' utilities the Nash equilibrium would be locally optimal. The Nash equilibrium utilities at the modified games seem better, but it's not clear how much the games were modified.\n\nMisc:\n(A) Computing the Price of Anarchy in general is NP-complete rather than PPAD-complete because PPAD corresponds to finding *any* Nash equilibrium rather than the worst one.\n(B) \"set of the equilibria of the game restricted to the line\" - I don't understand what this sentence means.\n(C) Why don't you define utilitarian/egalitarian before Theorem 1?\n(D) Typos: \"one-hot\" \"the the\"\n\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538103711, "tmdate": 1606915781241, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2116/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review"}}}, {"id": "T9ChW_VUW-_", "original": null, "number": 4, "cdate": 1604019911006, "ddate": null, "tcdate": 1604019911006, "tmdate": 1605024285369, "tddate": null, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "invitation": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review", "content": {"title": "Review", "review": "This paper proposes a (decentralized) method for online adjustment of agent incentives in multi-agent learning scenarios, as a means to obtain higher outcomes for each agent and for the group as a whole. The paper uses the \u201cprice of anarchy\u201d (the worst value of an equilibrium divided by the best value in the game) as a proxy for the efficiency of the game outcome, and derive an upper bound on a local price of anarchy that agents can differentiate. In several experiments (a traffic network, the coin game, Cleanup), their method leads to improved individual agent and group outcomes relative to baselines, while avoiding cases of stark division of labor that sometimes emerges when agents directly optimize the sum of all agent rewards. \n\nPros:\nOverall, I thought this paper was quite strong. I agree with the claim in the paper that having agents simply optimize the cooperative return is not always realistic or interesting. The framing of compromise as the mixing of agent incentives is particularly interesting, and it makes intuitive sense to me. I\u2019m not familiar enough with the literature to know if this formulation is novel (and unfortunately the paper does not have a Related Work section). The paper makes significant contributions towards making this idea practical, including relaxing the requirement that the agents can observe or directly differentiate with respect to the other agent strategies. \n\nThe paper is also pretty well written and communicated, though I did not dive into the proofs and skimmed some of the technical details. I appreciated the frankness with which the paper describes their method, e.g. in the following excerpts:\n\u201cIdeally, one meta-algorithm would allow a multi-agent system to perform sufficiently well in all these scenarios. The approach we propose, D3C (Sec. 2), is not that meta-algorithm, but it represents a holistic effort to combine critical ingredients that we hope takes a step in the right direction\u201d\n\u201cWe also provide specific, albeit narrow, conditions under which agents may achieve a Nash equilibrium\u201d\n\nCons:\nOf course, the \u2018mixing of losses\u2019 strategy proposed in the paper requires that agents be able to observe the losses of other agents, which is not always feasible in practice.\n\nOther notes:\nI was a bit confused by the sentence: \u201cOur agents reconstruct their losses using the losses of all other agents as a basis\u201d, which seems to imply that the agents\u2019 own loss function is not used as part of this basis. \n\nThe Appendix also seems to be missing from the paper, although there are references to it in the text (e.g. reference to F.4). \n\nOverall:\nOn the whole, I think this paper is quite good and worthy of acceptance. \n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2116/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2116/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "D3C: Reducing the Price of Anarchy in Multi-Agent Learning", "authorids": ["~Ian_Gemp1", "kevinrmckee@google.com", "~Richard_Everett1", "~Edgar_Alfredo_Duenez-Guzman1", "~Yoram_Bachrach2", "~David_Balduzzi1", "~Andrea_Tacchetti1"], "authors": ["Ian Gemp", "Kevin McKee", "Richard Everett", "Edgar Alfredo Duenez-Guzman", "Yoram Bachrach", "David Balduzzi", "Andrea Tacchetti"], "keywords": ["multiagent", "social dilemma", "reinforcement learning"], "abstract": "Even in simple multi-agent systems, fixed incentives can lead to outcomes that are poor for the group and each individual agent. We propose a method, D3C, for online adjustment of agent incentives that reduces the loss incurred at a Nash equilibrium. Agents adjust their incentives by learning to mix their incentive with that of other agents, until a compromise is reached in a distributed fashion. We show that D3C improves outcomes for each agent and the group as a whole in several social dilemmas including a traffic network with Braess\u2019s paradox, a prisoner\u2019s dilemma, and several reinforcement learning domains.", "one-sentence_summary": "We propose a decentralized, gradient-based meta-algorithm to adapt the losses of agents in a multi-agent system such that the price of anarchy is reduced.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "gemp|d3c_reducing_the_price_of_anarchy_in_multiagent_learning", "supplementary_material": "/attachment/40924c19c5afc5cb9687f36468897af613c11944.zip", "pdf": "/pdf/cb0ca3cefad4eee6e9c22f209b9927bf44bb5692.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=YRtyTv3BAg", "_bibtex": "@misc{\ngemp2021dc,\ntitle={D3C: Reducing the Price of Anarchy in Multi-Agent Learning},\nauthor={Ian Gemp and Kevin McKee and Richard Everett and Edgar Alfredo Duenez-Guzman and Yoram Bachrach and David Balduzzi and Andrea Tacchetti},\nyear={2021},\nurl={https://openreview.net/forum?id=8wa7HrUsElL}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "8wa7HrUsElL", "replyto": "8wa7HrUsElL", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2116/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538103711, "tmdate": 1606915781241, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2116/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2116/-/Official_Review"}}}], "count": 14}