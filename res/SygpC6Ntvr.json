{"notes": [{"id": "SygpC6Ntvr", "original": "Byg81GfdPr", "number": 873, "cdate": 1569439189066, "ddate": null, "tcdate": 1569439189066, "tmdate": 1586715134823, "tddate": null, "forum": "SygpC6Ntvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "4kE-ALhpG", "original": null, "number": 1, "cdate": 1576798708381, "ddate": null, "tcdate": 1576798708381, "tmdate": 1576800927995, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper studies methods for using weight sparsification to reduce the computational load of network inference.  While there is not absolute consensus on whether this paper should be accepted, one of the main criticisms of this paper is that sparse compute is not always realistic or efficient on a GPU.  While this may be true of the current SOTA in hardware, emerging computing platforms and CPU libraries may handle sparse networks quite well.  For this reason, I am willing to down-weight this criticism. Based on the remaining comments, this paper has the merit to be accepted, even if it is a bit forward looking in terms of the hardware platforms it targets.\n\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705076, "tmdate": 1576800252782, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper873/-/Decision"}}}, {"id": "r1xAylF2ir", "original": null, "number": 7, "cdate": 1573847014126, "ddate": null, "tcdate": 1573847014126, "tmdate": 1573849597691, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "HyeUOAxaYH", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment", "content": {"title": "Further updates", "comment": "Further updates:\n\n4. We have now added results on the Cifar-100 dataset in Appendix C, reporting the precision. Our results indicate that our models use less than $50\\%$ computation compared to SDH, however with a slightly lower precision."}, "signatures": ["ICLR.cc/2020/Conference/Paper873/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygpC6Ntvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper873/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper873/Authors|ICLR.cc/2020/Conference/Paper873/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164894, "tmdate": 1576860541316, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment"}}}, {"id": "H1xy8gYnoB", "original": null, "number": 8, "cdate": 1573847110604, "ddate": null, "tcdate": 1573847110604, "tmdate": 1573849570174, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "BJejmeERKS", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment", "content": {"title": "Further updates", "comment": "Further updates:\n\n4. We have now added results on the Cifar-100 dataset in Appendix C, reporting the precision without any re-ranking. Our results indicate that our models use less than $50\\%$ of the computation compared to SDH,  however with a slightly lower precision.\n\n5. We have also added results comparing reranking and no reranking in Appendix C. We notice that there is a significant dip in the performance without re-ranking with the gap being smaller for ResNet with FLOPs regularization. We also notice that the FLOPs regularizers has a better trade-off curve even for the no re-ranking setting."}, "signatures": ["ICLR.cc/2020/Conference/Paper873/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygpC6Ntvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper873/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper873/Authors|ICLR.cc/2020/Conference/Paper873/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164894, "tmdate": 1576860541316, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment"}}}, {"id": "ByeykJtnsH", "original": null, "number": 6, "cdate": 1573846743392, "ddate": null, "tcdate": 1573846743392, "tmdate": 1573847443539, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment", "content": {"title": "Second revision for rebuttal", "comment": "Here are the changes that we made in second revision before the rebuttal deadline:\n\n1. A comparison of reranking vs no reranking has been added in Appendix C.\n2. Added experiments on Cifar-100 dataset in Appendix C, reporting the precision."}, "signatures": ["ICLR.cc/2020/Conference/Paper873/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygpC6Ntvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper873/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper873/Authors|ICLR.cc/2020/Conference/Paper873/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164894, "tmdate": 1576860541316, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment"}}}, {"id": "HJlnoe_PoH", "original": null, "number": 4, "cdate": 1573515427955, "ddate": null, "tcdate": 1573515427955, "tmdate": 1573516084793, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "HyeUOAxaYH", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment", "content": {"title": "Response to review #4", "comment": "Thank you for the feedback and suggestions. We have now added some more discussion on figure 2b in section 4. Responses to other issues:\n\n1. Literature review on learning sparse representations: Thank you for pointing us to the literature on sparse AE. We have now expanded the literature review with approaches for learning sparse representations.\n\n2. More metrics: we have now added TPR-FPR curves in appendix C of our first revision. Our plots show that for similar probability of activation $p$, the FLOPs regularizer performs better compared to $\\ell_1$. This demonstrates the efficient utilization of all the dimensions in the case of the FLOPs regularizer that helps in learning richer representations for the same sparsity.\n\n3. Confidence thresholds: Both the evaluation metric and the retrieval time are not so sensitive to the choice of the confidence threshold. The threshold is chosen such that the size of the shortlist is large enough (around 1000 in our experiments) so that it contains the top-k examples (where k is what appears in recall@k ), so as to not affect the metric. Also the chosen threshold is small enough that the time is dominated by the sparse multiplication time, so it does not significantly affect the total time either. Without a threshold however, the time for sorting the scores dominate, resulting in poor speedup.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper873/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygpC6Ntvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper873/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper873/Authors|ICLR.cc/2020/Conference/Paper873/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164894, "tmdate": 1576860541316, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment"}}}, {"id": "HJgnKfuvsS", "original": null, "number": 5, "cdate": 1573515907527, "ddate": null, "tcdate": 1573515907527, "tmdate": 1573516057353, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "BJejmeERKS", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment", "content": {"title": "Response to review #3", "comment": "Thank you for the feedback and suggestions to improve our paper. Response to comments:\n\n1. Flops vs. speedup: We have now added a paragraph on cache efficiency. While FLOPs reduction is a reasonable measure of speedup on primitive processors of limited parallelization and cache memory. FLOPs is not an accurate measure of actual speedup when it comes to mainstream commercial processors such as Intel\u2019s CPUs and Nvidia\u2019sGPUs, as the latter have cache and SIMD (Single-Instruction Multiple Data) mechanisms highly optimized for dense matrix multiplication, while sparse matrix multiplication are inherently less tailored to their cache and SIMD design (Sohoni et al., 2019). On the other hand, there have been threads of research on hardwares with cache and parallelization tailored to sparse operations that show speedup proportional to the FLOPs reduction (Han et al., 2016; Parashar et al., 2017). Modeling the cache and other hardware aspects can potentially lead to better performance but less generality and is left to our future works.\n\n2. Need both dense and sparse embeddings: Our method does need both the embeddings to achieve good performance. The accuracy drops without the re-ranking step.\n\n3. HNSW: One of the technical difficulties we face while evaluating using HNSW is that it does not support deleting elements. Deleting elements is essential for our particular evaluation framework. Our evaluation framework involves having only 1 facescrub target mixed with the megaface distractors in the database. Thus after testing each query-target pair, we require to delete the target from the database. Since deleting is not supported, the other option is to re-create the HNSW index each time, which is not a feasible option wrt the computation time. We are considering other evaluation metrics for which HNSW is a feasible option.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper873/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygpC6Ntvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper873/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper873/Authors|ICLR.cc/2020/Conference/Paper873/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164894, "tmdate": 1576860541316, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment"}}}, {"id": "rke37kOvsB", "original": null, "number": 3, "cdate": 1573515044177, "ddate": null, "tcdate": 1573515044177, "tmdate": 1573516028438, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "BkxR8nw95S", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment", "content": {"title": "Response to review #1", "comment": "Thank you for the feedback, and pointing us to prior work using sparse computations. We have now added a paragraph on the literature for SpMV computations. Response to specific comments:\n\n1. Dependency of mu and sigma: In practice mu and sigma depend on the activations produced by the dataset. The activations are further affected by the main loss function. For simplicity in this analysis, we only consider the effect of the regularizer and ignore the loss function. In such a setting, due to the representational capacity of the neural network, the mean and the variance can be controlled almost independently. Hence the simplifying assumption. In reality however, they are dependent on the dataset and the parameters of the network.\n\n2. SThresh: Thanks for pointing this out. It is not necessary for SThresh to follow the same gradient pattern. We have now removed the line from the updated PDF.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper873/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygpC6Ntvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper873/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper873/Authors|ICLR.cc/2020/Conference/Paper873/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164894, "tmdate": 1576860541316, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment"}}}, {"id": "Bygin0DvoB", "original": null, "number": 2, "cdate": 1573514931084, "ddate": null, "tcdate": 1573514931084, "tmdate": 1573514931084, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment", "content": {"title": "First revision for rebuttal", "comment": "We thank the reviewers for their very helpful feedback. We have incorporated some of the suggested changes in the first version of our revision. Here is a summary of the changes (also marked in blue in the updated pdf):\n\n1. Added more literature on sparse multiplication approaches (section 2).\n2. Added a broader review of learning sparse representations (section 2).\n3. Added more discussion on FLOPs vs speedup (section 3).\n4. Added more explanation on figure 2b in the analysis in section 4.\n5. Added more results: TPR FPR curves (Appendix C).\n\nTime permitting, we might be able to add some of the remaining suggestions before the rebuttal deadline.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper873/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SygpC6Ntvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper873/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper873/Authors|ICLR.cc/2020/Conference/Paper873/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504164894, "tmdate": 1576860541316, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper873/Authors", "ICLR.cc/2020/Conference/Paper873/Reviewers", "ICLR.cc/2020/Conference/Paper873/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Comment"}}}, {"id": "HyeUOAxaYH", "original": null, "number": 1, "cdate": 1571782254176, "ddate": null, "tcdate": 1571782254176, "tmdate": 1572972541650, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "Summary\n \nThis paper focuses on learning a representation that facilitates efficient content-based retrieval. Although the representations that are learned from deep neural networks can contain rich information, it is computationally expensive to use those representations to perform a search for the best match. In particular, computing the Euclidean distance between a query and an instance scales linearly with the size of the representation. Prior approaches to this problem have focused either on: (1) compactifying the learned representations into another form, such as a Hamming code, in a way that preserves the identifiability of an instance; (2) resorting to approximate methods that sacrifice accurate search for efficiency.\n \nTo address these inconvenient trade-offs, this paper proposes an algorithm to learn a high-dimensional, sparse representation that is directly used in retrieval, instead of learning a separate, more compact representation. The novelty of this algorithm is its focus on minimizing the number of FLOPs in computing queries of instances, taking note as well of the role of the distribution of non-zero values in determining the number of FLOPs. A continuous relaxation of the equations thus derived provides the proposed algorithm. The paper notes differences between the algorithm and SDH, a recent candidate that learns a sparse, high-high dimensional hash. Based on experiments, the paper claims that the proposed algorithm yields a similar or better speed vs. recall tradeoff compared to baselines. The paper also provides additional experiments demonstrating the sparsity of the representations and the even distribution of non-zero values of the proposed regularizer.\n \nDecision: accept\n\nThe algorithm is clearly and succinctly motivated from the standpoint of reducing the number of FLOPs. The presentation of the distribution that minimizes FLOPs is convincing, and there is easy-to-follow buildup into the continuous relaxation of the FLOPs minimization problem. \n\nThe proposed algorithm is itself relatively simple (just an additional regularizer term that can be optimized with any SGD-based optimizer), compared to other methods that learn a separate representation or that use approximate nearest-neighbour search, and directly tries to address aforementioned trade-offs between efficiency of retrieval and richness of the representation. \n\nI found helpful the comparison both to the nearest competitor method (SDH) and the unrelaxed regularizer. The additional experiments comparing the continuous relaxation and the unrelaxed regularizer were interesting, but I found Figure 2b a little hard to understand. I also appreciated the intuition developed at the end of section 4 for how the regularizer promotos orthogonality.\n\nThe recall/time trade-off curves in figure 3 support the main empirical claim of the paper. The sparsity plots in figure 3 contributed to a broader understanding of the algorithm besides based on metrics other than accuracy. \n\nNevertheless, there were some experimental presentation issues. Are errors bars possible for figs 3ac? Some evaluation metrics that are present in other papers are also missing, which might provide a more complete understanding of algorithm's advantages and disadvantages. Specifically, precision@k (Jeong and Song, 2018) and true-positive/false-positive curves (Kemelmacher-Shlizerman et. al., 2016) could help readers in making an informed algorithmic choice. It also seems that the paper could have included more comparisons to other ways of learning sparse representations (e.g., L2, top-k autoencoders, dropout), and included a broader literature review of learning sparse representations (see here: https://arxiv.org/pdf/1505.05561.pdf). \n\nQuestions\n1. Did confidence thresholds in ranking (Appendix B, re-ranking) affect the results in any way? Did it depend on the algorithm used?\n\nMinor comments that did not affect the rating\n1. Typo in figure 3; the second sentence should say \"curves are produced by\"\n2. Typos in Appendix A\na. Lemmas 2, 5 should have X_+ instead of X in the statements\nb. Second last line of proof of lemma 1 should have X_+\n3. Would be helpful to have whole procedure from learning to retrieval in an algorithm box\n4. Include a summary of the problem setting before section 3. It is at first a bit unclear what the problem is for newcomers\n5. Add algorithm header to the retrieval algorithm presented in fig 1\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper873/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574986714456, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper873/Reviewers"], "noninvitees": [], "tcdate": 1570237745742, "tmdate": 1574986714469, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Review"}}}, {"id": "BJejmeERKS", "original": null, "number": 2, "cdate": 1571860515425, "ddate": null, "tcdate": 1571860515425, "tmdate": 1572972541606, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper proposes to learn sparse representation in neural networks for retrieval in large database of vectors. Such sparse representation, when the fraction of non-zeros is high, can be computed using sparse matrix multiplication, or variants of inverted index scoring and lead to potentially lower FLOPs needed. This paper proposes to induce sparsity by adding a regularization term, which counts the expected number of FLOPs needed for sparse scoring.\n\nThe final experiments were done on Megaface dataset, where there are 1M distrators and accuracy is measured in Recall@1. The sparse embedding approach is combined with a dense re-ranking stage, and the speed-recall trade is compared to the pure dense approaches (such as using baselines such as FAISS\u2019s IVF-PQ, LSH or SDH). The authors\u2019 evaluation showed that, the performance of FLOPs regularized embedding is better than L1 regularized sparse embedding, or applying IVF-PQ directly to dense embeddings.\n\nPros:\n- The topic of learning sparse embeddings is of great interests. As far as I know, many researchers attempted and there was not an agreement. For example, https://arxiv.org/pdf/1904.10631.pdf [1] claimed \"Using sparse operations reduces the number of FLOPs, as the zero entries can be ignored. However, in general, sparsity leads to irregular memory access patterns, so it may not actually result in a speedup;\" This has been my experience as well. The authors\u2019 embedding seems to be much more sparse (<5% non-zeros) than the sparse embeddings from other approaches and thus works better.\n\n- The formulation is simple and intuitive - it appears easy enough to plug into a variety of embedding architectures (potentially all embeddings, NLP, CV, Audio).\n\nCons:\n- The learned sparse embedding system still requires training a separate dense embedding for re-ranking, so essentially is a hybrid approach. One cannot simply \"get rid of the dense\". Ideally one would hope to not need two inference runs (for sparse and dense) and keep two database (sparse and dense). Maybe the author can report how sparse embedding performs on its own.\n\n- It is widely known that inverted index is not FLOPs bound, as its FLOPs utilization in inverted index is typically low. Inverted index is almost always dominated by random memory accesses and thus ideally the regularizer should be modelling after cache miss / memory access pattern instead of FLOPs. I\u2019d like to see authors gave more discussion on these topics instead of taking a FLOPs centric view (which is not true for inverted index).\n\n- For comparison dense ANN, Faiss\u2019s IVF-PQ is a relatively dated pipeline. It would be good to see how what the curve would look like for other dense technique such as HNSW [2] which performs better than IVF-PQ on http://ann-benchmarks.com/. Also dense ANN can also greatly benefit from the use of batching, which is not considered for this paper. \n\n- Finally, I recommend the authors perform additional experiments on other datasets. As the authors suggested, sometimes sparse embedding learning risks collapse to predicting the classes. The unseen face queries avoid this problem to some extent. But I still worry that the Megaface task somehow allow representation to be much more sparse than other NLP/CV tasks. For example, authors can try BERT tasks with sparse embedding. It would be much more convincing to see this approach generalizes across many tasks.\n\n===\nOverall, I think this is a nice paper which takes a good step to improve the effectiveness of sparse embeddings over existing methods such as simple L1 regularization. However, with the need of training separate dense embeddings, and the fact that experiments were conducted only on 1 tasks with relatively weak ANN baseline, I\u2019d lean towards rejection.\n\n[1] Low-Memory Neural Network Training: A Technical Report\n[2] Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper873/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574986714456, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper873/Reviewers"], "noninvitees": [], "tcdate": 1570237745742, "tmdate": 1574986714469, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Review"}}}, {"id": "BkxR8nw95S", "original": null, "number": 3, "cdate": 1572662358115, "ddate": null, "tcdate": 1572662358115, "tmdate": 1572972541564, "tddate": null, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "invitation": "ICLR.cc/2020/Conference/Paper873/-/Official_Review", "content": {"rating": "8: Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper reads well, with intriguing ideas that challenge the \u2018dense\u2019 approach to DNNs, excellent thought experiments and convincing experiments.\n\nThe reviewer is neither an expert in face verification and in K-NN retrieval algorithms but has a solid experience in sparse ML algorithms and group lasso algorithms.\n\nIn this respect, the algorithms proposed in this paper represent an excellent extension of existing sparse algorithms that go against the current trend of focusing on compact dense representations because this is what GPUs handle best.\n\nClarity: an excellent introduction (appreciated by a reviewer not up-to-date in the topic) introduces representation learning for retrieval, though says little about the sparse multiplication state-of-the-art or face verification.  The rest of the paper reads very well (I had to dig deep to find some clarifications in detailed comments). For lack of space, one has to through quite few references to fully understand the experiments.\n\nQuality: there are only few equations in this paper, but they rely on excellent notation. The algorithm is also well formulated. What strikes me as very good are the \u2018thought experiments\u2019 that suggest (rather than prove) that the approach is well grounded: the end of section 4 is excellent\n\nOriginality and significance: working on sparse representations is quite \u2018original\u2019 now, especially as they are so GPU-unfriendly (I note the sparse algorithm is implemented in C++, and am looking forward to the code release). I hope the excellent results reported in this paper will incite others to revisit them. 15 years ago, such as paper would have been less original. The sparse vector sparse matrix product algorithm is well known and has been used in ML publications before, for instance:\n-\tHaffner, ICML 2006, \u201cFast Transpose Methods for Kernel Learning on Sparse Data\u201d \n-\tKudo and Matsumoto 2003 \u201cFast methods for kernel-based text analysis\u201d\nWhat seems to be original is the algorithm to balance sparsity probabilities. The reviewer is well aware that a single non sparse column can kill the performance of the sparse matrix multiplication algorithm, and this is probably the main reason this algorithm has not found broad usage. The derivation and the connection to group and exclusive Lasso are excellent.\n\nDetailed comments:\n-\tpage 5 \u201cwhere we suppress the dependency of Mu and Sigma\u201d. How do you do that?\n-\tPage 7 \u201cThe analysis in figure 2 follows similarly for Stresh\u201d.  Probably more explanation is needed here. Replacing Relu with a sum of Relus over affine transforms of the Gaussian variable is a complex operation:  how does it keep the same curves?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper873/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper873/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Minimizing FLOPs to Learn Efficient Sparse Representations", "authors": ["Biswajit Paria", "Chih-Kuan Yeh", "Ian E.H. Yen", "Ning Xu", "Pradeep Ravikumar", "Barnab\u00e1s P\u00f3czos"], "authorids": ["bparia@cs.cmu.edu", "cjyeh@cs.cmu.edu", "a061105@gmail.com", "ningxu01@gmail.com", "pradeepr@cs.cmu.edu", "bapoczos@cs.cmu.edu"], "keywords": ["sparse embeddings", "deep representations", "metric learning", "regularization"], "TL;DR": "We propose an approach to learn sparse high dimensional representations that are fast to search, by incorporating a surrogate of the number of operations directly into the loss function.", "abstract": "Deep representation learning has become one of the most widely adopted approaches for visual search, recommendation, and identification. Retrieval of such  representations from a large database is however computationally challenging. Approximate methods based on learning compact representations, have been widely explored for this problem, such as locality sensitive hashing, product quantization, and PCA. In this work, in contrast to learning compact representations, we propose to learn high dimensional and sparse representations that have similar representational capacity as dense embeddings while being more efficient due to sparse matrix multiplication operations which can be much faster than dense multiplication. Following the key insight that the number of operations decreases quadratically with the sparsity of embeddings provided the non-zero entries are distributed uniformly across dimensions, we propose a novel approach to learn such distributed sparse embeddings via the use of a carefully constructed regularization function that directly minimizes a continuous relaxation of the number of floating-point operations (FLOPs) incurred during retrieval. Our experiments show that our approach is competitive to the other baselines and yields a similar or better speed-vs-accuracy tradeoff on practical datasets.", "pdf": "/pdf/a4828a25256072c99f7232b0756d6310ac4c4d02.pdf", "paperhash": "paria|minimizing_flops_to_learn_efficient_sparse_representations", "code": "https://github.com/biswajitsc/sparse-embed", "_bibtex": "@inproceedings{\nParia2020Minimizing,\ntitle={Minimizing FLOPs to Learn Efficient Sparse Representations},\nauthor={Biswajit Paria and Chih-Kuan Yeh and Ian E.H. Yen and Ning Xu and Pradeep Ravikumar and Barnab\u00e1s P\u00f3czos},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SygpC6Ntvr}\n}", "original_pdf": "/attachment/55832982cb98e24d7307fd6f476267ae9bff1045.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SygpC6Ntvr", "replyto": "SygpC6Ntvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper873/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574986714456, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper873/Reviewers"], "noninvitees": [], "tcdate": 1570237745742, "tmdate": 1574986714469, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper873/-/Official_Review"}}}], "count": 12}