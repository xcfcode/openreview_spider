{"notes": [{"id": "Mf4ZSXMZP7", "original": "T72jz5cgvqs", "number": 1562, "cdate": 1601308173260, "ddate": null, "tcdate": 1601308173260, "tmdate": 1614985741428, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 15, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2pU6UgZqyn5", "original": null, "number": 1, "cdate": 1610040396363, "ddate": null, "tcdate": 1610040396363, "tmdate": 1610473991564, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper received mixed reviews, 3 positives (7, 6, 6) and 2 negatives (4, 4). Due to the divergence of the reviews, I carefully read the paper and made my best efforts to understand the paper and the review comments. This paper proposes to learn a quantization network using a small calibration set given a network trained with the full precision. The combination of AdaQuant, integer programming, and batch-norm tuning makes sense although they do not have substantial novelty. The three components are reasonably tightly-coupled and comprise a complete algorithm. However, the sequential-AdaQuant distracts the main claim of this work significantly. This is probably added during the review process but looks ad-hoc to me. Sequential AdaQuant seems to be effective to improve accuracy, but cannot be applied before the bit allocation was set, which makes it require integer programming no more. Because of this issue, the overall presentation becomes confusing and the argument sometimes sounds unfair (please refer to the last posting by R5.). \n\nIn addition, the presentation of this paper could be improved, especially for the details of the integer programming formulation. It is not clear how to define some variables mathematically. The discussion about the size of the calibration set together with the overfitting issue is lacking, and rigorous discussion and analysis would make the paper much stronger. The reviewers are not convinced of the novelty of this paper, and they rather believe that this is an engineering-oriented work. Considering this fact,  the evaluation of this paper is not very comprehensive. The ablation study with respect to the size of the calibration set should be conducted more intensively. The experiment fails to show the benefit of mixed precision quantization effectively and it is limited to presenting the compression ratio in Figure 3. The authors used a small calibration set taken from the training dataset, which looks weird because they claim that the post-training quantization requires only a small \"unlabeled\" calibration set at the beginning of the abstract; it is more desirable to use arbitrary examples in the same domain.\n\nDespite the interesting aspects, I believe that this paper needs a focus and substantial improvement for publication, and, consequently, recommend rejection."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040396350, "tmdate": 1610473991548, "id": "ICLR.cc/2021/Conference/Paper1562/-/Decision"}}}, {"id": "29f6RlrgQKc", "original": null, "number": 2, "cdate": 1603818870614, "ddate": null, "tcdate": 1603818870614, "tmdate": 1606296011511, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review", "content": {"title": "Collection of steps to deal with quantization-induced error in post-training quantization", "review": "This work presents a quite comprehensive multi-step scheme for post-training neural quantization that does not rely on large datasets or large computational resources.  \n\nThe work is has significance in the domain of post-training neural quantization, especially in cases where only a small calibration set or limited resources are available. It would be interesting to also think about accumulator quantization.\n\n\nPros:\n\nThe empirical results are relatively strong in this method; 4-bit quantization is a good achievement in the models considered here.\n\nThe quantization process covers nicely the various different parts of the errors that  post-training quantization induces and propose somewhat original solutions to them. \n\nCons:\n\nThe framework is relatively complex and consists of multiple steps.\n\nWhat is the detailed difference of computational resource use between light and advanced pipeline?\n\nAdaQuant seems like a rather straight-forward step from AdaRound by combining with it some related works.\n\nIt is not clear how the BN error compensation differs exactly from related work.\n\nSome details were missing, for example, it is up to the reader to guess how many bits were used in the accumulators.\n\nSome spelling mistakes, e.g., \u201cOptimizing Quantization Pipline\u201d\n\n\nOverall: An engineering oriented paper with some lack of testable hypotheses and analysis of some parts of the methods, but the 4-bit results justify publication. Edit: I have not seen author reply and further reading of the paper has not clarified the main issues found by all reviewers. I have to lower the score. Edit2: new version of the paper and the author reply cleared some concerns, score raised accordingly.\n\n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115906, "tmdate": 1606915771570, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review"}}}, {"id": "4ByFF1n0oB", "original": null, "number": 11, "cdate": 1606123462059, "ddate": null, "tcdate": 1606123462059, "tmdate": 1606123462059, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "mQUDoVjTXEX", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Author response  AnonReviewer2 - published Nov 21st ", "comment": "We noticed  #reviewer 2 recently edited the review: \"I have not seen author reply and further reading of the paper has not clarified the main issues found by all reviewers.\"\nPerhaps the reviewer missed we did reply to each reviewer in a separate reply (after we posted the general reply explaining the changes in the manuscript).  \nWe kindly ask the reviewer to take a look at our response and the revised manuscript (published Nov 21st). We believe we have addressed all comments (if not, we would like to understand what is missing).  "}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "Em63tUvdvms", "original": null, "number": 3, "cdate": 1605987722926, "ddate": null, "tcdate": 1605987722926, "tmdate": 1606122676477, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "General Response", "comment": "We thank the reviewers for their positive feedback and insightful suggestions. The feedback helped us improve the manuscript and boost the suggested method performance.\nWe updated the manuscript as follows:\n1. In section 3.1 we added a new flavor of AdaQuant, named sequential-AdaQunat, targeted for the common fixed bit allocation configuration (i.e., not mixed-precision setting)\n2. In section 4.1 we added a table that clarifies the difference between the advanced and light pipelines.\n2. In section 5.1, we updated baselines and added new results of sequential-AdaQuant - we believe they are quite impressive.\n3. Several clarifications to the appendix including (a) the required size of calibration set; and (b) the additive assumption at the core of our integer-programming formulation.\n\nWe updated the code to include sequential-AdaQunat\n\nWe kindly ask the reviewers to consider our clarifications and improvements to the manuscript in their final score. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "vXy2_e6kF9A", "original": null, "number": 4, "cdate": 1605988348599, "ddate": null, "tcdate": 1605988348599, "tmdate": 1606121159400, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "gKjagp4qWvK", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Reply to AnonReviewer5 - Part 1", "comment": "We thank the reviewer for his feedback. Please see our answers below:\n\nQ: It is straightforward to think that the joint optimization of quantization step size for weight and activation would result in better quantization results. But this joint optimization would also increase the search space (at least) quadratically, resulting in significant computational cost. Note that the biggest merit of post-training quantization is its simplicity (cf., QAT incurs full-blown training epochs); thus increased cost for post-training quantization is not desirable. Since AdaQuant is the major claim, the authors should provide more discussion on how they dealt with this increased complexity\n\nA: We note that Eq.2 was optimized with Adam, a variant of SGD, as stated in section D of the appendix.  Such gradient-based optimization methods are known to work well even with very large parameter spaces (e.g. deep neural networks).  Thus, we are sure what is the concern regarding increased search space. We wish to stress that AdaQuant is extremely fast: in less than two minutes (using GTX 1080Ti) we obtain an optimized ResNet50 model. Over CPU it takes less than an hour. Furthermore, our current code can be easily improved by executing Adaquant on all layers in parallel. Per-layer Adaquant takes less than three minutes on CPU and 5 sec on GTX 1020Ti. Results were obtained with a calibration size of 100 samples. Additionally, we ran experiments with sequential AdaQuant which accounts for the quantization error from previous quantized layers and thereby improves results drastically. For instance, for ResNet50 we have been able to achieve 75.08\\% top-1 accuracy when the entire model is in int4 (except the first and last layers, which were kept in 8-bit) as opposed to 73.7\\% when running AdaQuant in parallel.\n\nQ: There is no clear explanation of how AdaQuant increases the generality of the quantized model, and the discussion about the sample size (B) is hard to understand (why there's infinite solution when $B << N$? how $B>= C_i k^2/(HW)$ is derived for the convolution case?)\n\nA: We added a detailed explanation in both the manuscript (section 3.1 ) and in section A of the appendix. For a Fully Connected layer with an input size of $B\\times N$ and a weight matrix of size $M\\times B$, the output size would be $B\\times M$. Since we try to minimize the MSE over all the outputs we essentially have $BM$ equations with $NM$ parameters. In a generic linear system, if we have more parameters than equations, then we have an infinite number of solutions. However, some of those degrees of freedom can lead to over-fitting the data. Similarly, if we have many more equations than parameters we might result in under-fitting the data. To avoid this we wish to have roughly the same number of equations and parameters. A toy example can be found in section A of the Appendix. Similar derivation for convolution results with $BC_oHW$ equations and the total number of parameters is $C_oC_ik^2$.\n\nQ: It seems that the \"per-channel\" quantization method is utilized in this work, but the formulation in (2) seems to be for \"per-layer\" optimization. Why they are different?\n\nA: By per-layer optimization, we mean that each layer is optimized separately. Thus we do not have to run backpropagation on the entire model. As stated above this also implies that we can run AdaQuant in parallel on all layers. The paper applies per-channel quantization on the weights (i.e., a different scale for each kernel) and per-tensor quantization on the activation (i.e., one scale for all the activation tensors), as is commonly supported by hardware [1].\n\nQ: What is the formulation of the penalty function, \"deltaL\"? The authors described it simply as \"Loss\", but it is not clear what the exact method it is calculated. In fact, deltaL can be pretty complex functions, which might not be independent terms for each layer; thus the formulation like (3) might not be correct. Note that the impact of quantization in the earlier layers affect the quantization impact in the current layer. Without a clear explanation and justification about it, the proposed IP formulation does not make sense.\n\nA: You are right, for IP to work we had to assume that both the performance degradation and the loss degradation are additive. Clearly, for the loss, this is only a first-order approximation of its Taylor expansion. Similar to previous works that examined this approximation [2,3], we found it to be quite accurate for most models.\n\n[1] Jain, Animesh, et al. \"Efficient execution of quantized deep learning models: A compiler approach.\" arXiv preprint arXiv:2006.10226 (2020).\n\n[2] Choukroun, Yoni, et al. \"Low-bit quantization of neural networks for efficient inference.\" 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW). IEEE, 2019.\n\n[3] Lin, Darryl, et al., \"Fixed point quantization of deep convolutional networks.\" International conference on machine learning. 2016.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "qwhQbPqkhV7", "original": null, "number": 5, "cdate": 1605988869646, "ddate": null, "tcdate": 1605988869646, "tmdate": 1606120945818, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "vXy2_e6kF9A", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Reply to AnonReviewer5 - Part 2", "comment": "Q: The authors mentioned that deltaP should be additive and sum up to the total benefit. How can one guarantee it?\n\nA: The Performance gain, deltaP, is slightly more tricky to evaluate, as it might depend on the hardware and software at hand (e.g., latency and throughput). Yet for compression (reducing model footprint) it is obviously additive.  We added an explanation about this assumption in section C of the Appendix.\n\nQ: It seems that the complexity of the IP optimization increases as the number of layers increases. How much computation time increases if the number of layers is large?\n\nA: The computation increase linearly with the number of layers. Yet, evaluating the performance gain and loss degradation can be done in parallel for all layers. Additionally, our integer-programming exploits the well-optimized pulp library [4] thus requires only a few seconds to derive an optimal solution.\n\nClaim: Batch normalization tuning: unfortunately, there is a very similar idea proposed by [Sun et al., NeurIPS 19]. Cf. \"Sec.3 Trans-Precision Inference in FP8\".\n\nA: Thank you for referring us to this paper. We agree that the fact that re-tuning BN-statistics improves performance is not surprising, however, it is not our novelty here. First, we describe how to reconstruct BN layers, which commonly do not exist in a post-training setting, due to folding. Without these reconstructed BN layers we could not have tuned the statistics. Second, we are the first to show that BN folding after BN tuning can be done in per-channel weight quantization (this is enabled by the per-channel weight scales). We believe that these findings and the detailed code would be appreciated by practitioners.\n\nQ: Currently, the ablation study looks very confusing. It is not clear which of the pipeline options (light, advanced?) include what kinds of techniques. Please do specify (maybe in a separate table) the list of techniques covered by different pipeline options.\n\nA: In section 4 we detail the exact stages of each of the pipelines: \"Our \\textbf{light pipeline} requires three steps: (1) Fuse layers and define quantization parameters; (2)Find optimal mixed-precision configuration using IP; and (3) Use BN tuning to correct the internal statistics. ... in the \\textbf{advanced pipeline} we apply AdaQuant to reduce each layer\u2019s output distortion from its full precision counterpart before invoking the IP algorithm\". We also added a table in section 4 to further explain the differences.\n\nClaim: The proposed method is not much evaluated by various neural networks. It would be desirable to expand the coverage of neural nets as much as the prior work did.\n\nA: Thank you for your feedback we added additional models (ResNet 34/101, ResNext50, and Inception V-3) in section 5.1.\n\nQ: Currently, the proposed methods only utilized \"per-channel\" quantization. How much accuracy the proposed methods can maintain if they adopt \"per-layer\" quantization?\n\nA: In this work we have a per-channel scale for the weights, while the activations have only one scale for the entire tensor (i.e. layer), as is commonly done [1]. However, our method can help per-tensor quantization as well. For instance on BERT-large with per-tensor quantization, we achieved 80.4\\% exact-match vs. 79.6\\% without traditional PTQ methods.  Our code supports per-tensor quantization (just change perC to False). Finally, please note that BN-tuning as a second stage after mixed-precision is not applicable for per-tensor quantization.\n\nQ: What is the definition of \"compression ratio\"? (typically compression RATIO is like 12:1, and compression rate is like 2X, 3X...)\n\nA: Compression ratio is relative to 32bit. Thus for 8bit (marked as 0.25), it is 1:4 and for 4-bit (marked as 0.125) it is 1:8. The rest is in between. We clarify this in the revised paper.\n\n[4] https://github.com/coin-or/pulp"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "uIQGtlotTI", "original": null, "number": 6, "cdate": 1605989317504, "ddate": null, "tcdate": 1605989317504, "tmdate": 1606120899226, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "LGgSIt4lRt0", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Reply to AnonReviewer3", "comment": "We thank the reviewer for his feedback, please see our answers below:\n\nQ: \" However, the problem is, are these four components all original or just adapted from existing works?\"\n\nA: All three components (AdaQuant, Integer programming, Batch-norm tuning) are original, and lead to significant improvements over previous works, as we discuss below.\n\nClaim: \" The authors are failed to present the relation of the proposed components with the existing ones. \"\n\nA: These relations are discussed in section 2 \"Related works\".\n\nClaim: \"For example, in my understanding, the AdaQuant is only an adapted version of AdaRound, but the discussion about it (in Section 3.1) is too superficial. \"\n\nA: Indeed, both AdaQuant and AdaRound minimize the per-layer reconstruction error. However, AdaQuant is much less restrictive, allowing the weights to have large changes, while AdaRound\u2019s forces the quantized weights to be within 1 of their round-to-nearest value.  In addition, AdaRound only quantized the weights and not the activation's thus it is not optimized the quantization parameters.\n\nClaim: \"Furthermore, the experiment part is not convincing and I think can not back up the claim. With so many \"proposed components\", the missing of thorough ablation study is a big problem.\"\n\nA: Please note figure 1, where we show AdaQuant (without any other method) leads to major improvements in test accuracy over AdaRound --- even when optimized over very small calibration data sets. We find this to be quite remarkable and unusual since fine-tuning the entire model over a small calibration set is known to overfit the data. Additionally, in figure 3 we show how the addition of Integer programming and BN tuning each improves performance. Finally, we suggest sequential AdaQuant as described in section 5.1 and suggest light and advanced pipelines for the mixed-precision setting. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "O7Q946pkuZG", "original": null, "number": 7, "cdate": 1605989972022, "ddate": null, "tcdate": 1605989972022, "tmdate": 1606120873937, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "3PQxXLv3lW6", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Reply to AnonReviewer4", "comment": "We thank the reviewer for his positive and thorough feedback; please see our answers below:\n\nQ: Very basic details, such as the metric used in some tables, are omitted.\n\nA: Thank you for your feedback we fixed this in the revised manuscript.\n\nClaim: On the flow of the paper: Fig. 1 is barely introduced. Fig. 2 is introduced much earlier than it is first referred to in the text, in section 5.2, where it is not explained either. The labels in the figure are not defined either at this point.\n\nA: Thank you for your comment - we added more information about fig-1 in both section D of the appendix and manuscript. Figure-2 was moved to section 5 and additional details about the labels were added to the caption.\n\nQ: The experiment of Fig.1 should be introduced with some level of detail (there is none outside the caption). The caption is a bit confusing: what are the \"the last layers after\"? \n\nWhy are variance bars always red? They are also hard to see. The labels are not descriptive (light pipeline, advanced pipeline, relaxed advanced pipeline) and hard to find in the text. \n\nA: Thank you for your feedback - we added details in the caption and fixed the \"after\" typo. The standard deviation was very small for calibration size larger than 100, thus we marked it in red.\n\nClaim: MN-V2 and B-SQuad1.1 just drop in Table 1 without any context. It can be inferred from earlier sections what this refers to but it should be explained clearly. Same for \"min-max\" in Fig. 1 and Table 1.\n\nA: Thank you for your feedback we fixed this in the revised manuscript.\n\nClaim: Baselines are barely discussed. Also, I am not very familiar with quantization papers, so I might have missed relevant baselines, but they seem hard to compare. For instance, the authors omit Nagel et al 2020, which seems to do better at similar quantization levels, but I am not sure the results are directly comparable. The authors should discuss this better.\n\nA: Thank you for your comment, Negal et al 2020 (AdaRound) only quantized the weights as opposed to AdaQuant which quantize both the weights and activations. For a fair comparison, we used our own implemented AdaRound based optimization and used it to quantize both weights and activation. As can be seen in Figure 1, Adaquant results with a significantly improved accuracy compared to AdaRound (Nagel et al 2020) for all considered calibration set sizes. \n\nA: Thanks for your detailed feedback- all typos where fixed in the manuscript and section 4 title was changed to \"Quantization flow\"."}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "mQUDoVjTXEX", "original": null, "number": 8, "cdate": 1605990334092, "ddate": null, "tcdate": 1605990334092, "tmdate": 1606120819838, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "29f6RlrgQKc", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Reply to AnonReviewer2", "comment": "We thank the reviewer for his positive feedback. Please refer to our answers below\n\nClaim: The framework is relatively complex and consists of multiple steps.\n\nA: The framework contains several components, and each can be applied separately. All components have almost no hyperparameters and require very little time to use. We believe that the code we created is easy to use and run and thus reproducing the results and extending it should be easy. For instance, upon request of Reviewer #5 we add experiments over different flavors of ResNet (R-34/R101) and ResNext101 as well as inception-v3.\n\nQ: What is the detailed difference of computational resource use between light and advanced pipeline?\n\nA: The advanced pipeline as opposed to the light pipeline starts with AdaQuant, which applies per-layer optimization for the weights and quantization parameters and ends with bias correction as detailed in table-1 in section 4. We measure the time it takes to run AdaQuant and it does not exceed 3m on CPU and 30sec on GPU. Since AdaQuant can be performed in parallel we believe this is a reasonable time. Bias correction requires approximately 10 epochs.\n\nClaim: AdaQuant seems like a rather straightforward step from AdaRound by combining with it some related works.\n\nA: While AdaRound performs a quite complex optimization to find the perfect rounding for each weight (by using soft and hard sigmoids), we use a much simpler and stronger approach to tune the weights. We used AdaQuant as an optimization step before mixed-precision. Since most works just use a fixed configuration (e.g., keep first and last layers in higher precision)  we added experiments with sequential AdaQuant (section 5.1 of the revised paper). Sequential AdaQuant fixed an inherent error of AdaRound as it enables correcting quantization errors obtain by quantizing predecessor layers. We kindly ask the reviewer to read this section - we believe the results are quite impressive.\n\nClaim: It is not clear how the BN error compensation differs exactly from related work.\n\nA: We agree that the fact that re-tuning BN-statistics improves performance is not surprising, however, it is not our novelty here. First, we describe how to reconstruct BN layers, which commonly do not exist in a post-training setting, due to folding. Without these reconstructed BN layers we could not have tuned the statistics. Second, we are the first to show that BN folding after BN tuning can be done in per-channel weight quantization (this is enabled by the per-channel weight scales). We believe that these findings and the detailed code would be appreciated by practitioners.\n\nQ: Some details were missing, for example, it is up to the reader to guess how many bits were used in the accumulators.\n\nA: We used 32bit accumulators but it is interesting to check 16bit as well.\n\nClaim: Some spelling mistakes, e.g., \u201cOptimizing Quantization Pipeline\u201d\n\nA: Thank you for your feedback we fixed all typos in the revised manuscript."}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "mHbL4_hmJnF", "original": null, "number": 9, "cdate": 1605990897912, "ddate": null, "tcdate": 1605990897912, "tmdate": 1606120776533, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "VH858vlzD7x", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Reply to AnonReviewer1", "comment": "We thank the reviewer for his positive and encouraging feedback. Please refer to our answers below\n\nQ: Page 3 last paragraph: it seems there are errors in the results for calibration data. (1) it is stated that if \"B>> M then we might underfit the data.\". Do the authors mean \"B>>N\"? Similarly it seems the result given for the convolution (i.e. $B> cK^2/HW$) needs to be revised. (2) The analysis requires specifying the rank of W and in particular the relationship between M and N. In particular, note that the matrix W can at most have a rank of min(M,N) and as a result the rest of the linear equations for calibrating the data would be redundant. This needs to be taken into account in your result.\n\nA: (1) Thanks for catching this typo, corrected. \n(2) We respectfully disagree. First, let's consider for example the case of single output ($M=1$). Here we are in scalar linear regression with $N$ parameters and $B$ equations. So we are over-parameterized if and only if $B<N$. However, if we replace $N$ with $\\min(N, M)$, as implied by the reviewer suggestion (if we understood it correctly), will results in $B< \\min(M,N) = 1$,  thus we will never be over-parameterized, which is clearly incorrect. Next, when $M>1$, we can just look at each equation independently (i.e., each equation contains different variables), so the same conclusion remains. We added a more detailed discussion of this in Appendix A to clarify these issues.\n\nQ: Figure 4 (a,b): Please provide the BOPS for the mixed-precision results. BOPS proposed by (https://arxiv.org/pdf/2005.07093.pdf) is a good metric to measure the total reduction in computations for mixed-precision quantization.\n\nA: In this work we compared the accuracy to compression ratio. Changing the performance acceleration matrix to BOPS is relatively easy, but require to rerun all the experiments. We hope to add those to the appendix soon.\n\nQ: The results section uses a weak FP32 baseline. For instance, the baseline accuracy for ResNet50 is $77.2\\%$ and MobileNetV2 is $73\\%$, while the paper uses $76.1\\%$ and $71.8\\%$.\n\nA: We used the standard torchvision [1] models as given by pytorch but open your request we updated the models in section 5.1 to pytorchcv [2] models.\n\nQ: It is stated, \"on the extensively studies 8bit MobileNet-V2 topology we achieved 71.6\\% top-1 accuracy\". This is a good result but please note that other work in the literature (arxiv:2001.00281) reports 72.91\\% for INT8 quantization of MobileNetV2 (this comparison is actually missing from the paper). It is immediately not clear if the lower reported accuracy is due to the weaker FP32 baseline used or if it is an inherent problem with the method (most probably it is the former but it would be to show this).\n\nA: Thank you for pointing us to that result. We used Pytorch native models as given by torchvision, but upon your request, we update the results for pytorchcv model used by ZeroQ. AdaQuant results in full accuracy of 73.03\\% top-1. We added the result to the paper in section 5.1 table-2.\nMinor Comments:\n\nClaim: There were several grammatical/spelling mistakes in the paper. Please proofread the paper thoroughly.\n\nA: Thanks for the feedback we fixed all the typos in the revised manuscript.\n\n[1] https://pytorch.org/docs/stable/torchvision/models.html\n[2] https://pypi.org/project/pytorchcv/"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "r34FANQGLp", "original": null, "number": 10, "cdate": 1606035173807, "ddate": null, "tcdate": 1606035173807, "tmdate": 1606035173807, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "mHbL4_hmJnF", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment", "content": {"title": "Rebuttal Addresses My Concerns", "comment": "I would like to thank the authors for the detailed rebuttal. The comments address my concerns. I think this is a very good paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Mf4ZSXMZP7", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1562/Authors|ICLR.cc/2021/Conference/Paper1562/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923858327, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Comment"}}}, {"id": "3PQxXLv3lW6", "original": null, "number": 3, "cdate": 1603896776746, "ddate": null, "tcdate": 1603896776746, "tmdate": 1605024414327, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review", "content": {"title": "See below", "review": "The paper introduces a series of techniques to quantize neural networks, and how to combine them:\n* Layer by layer quantization where weights can change as needed (rather than to the nearest quantization error).\n* Integer programming to determine the precision required at every layer.\n* Tuning batch norm weights by re-computing statistics.\n\nPROS\n\n* Main text is well written (see below for other issues though).\n* All components in the proposed method are straightforward.\n* Strong results, with very little performance loss after very aggressive quantization (~4 bits).\n\nCONS\n\n* The organization of the paper can and should be easily improved (see below).\n* Very basic details, such as the metric used in some tables, are omitted.\n* Few experiments/baselines.\n\n---\n\nOn the flow of the paper: Fig. 1 is barely introduced. Fig. 2 is introduced much earlier than it is first referred to in the text, in section 5.2, where it is not explained either. The labels in the figure are not defined either at this point (and ip should be IP). Additionally, the colors for the last two lines are too similar.\n\nThe experiment of Fig.1  should be introduced with some level of detail (there is none outside the caption). The caption is a bit confusing: what are the \"the last layers after\"? Why are variance bars always red? They are also hard to see. The labels are not descriptive (light pipeline, advanced pipeline, relaxed advanced pipeline) and hard to find in the text. MN-V2 and B-SQuad1.1 just drop in Table 1 without any context. It can be inferred from earlier sections what this refers to but it should be explained clearly. Same for \"min-max\" in Fig. 1 and Table 1.\n\nBaselines are barely discussed. Also, I am not very familiar with quantization papers, so I might have missed relevant baselines, but they seem hard to compare. For instance, the authors omit [Nagel et al 2020](https://arxiv.org/pdf/2004.10568.pdf), which seems to do better at similar quantization levels, but I am not sure the results are directly comparable. The authors should discuss this better.\n\nI think section 4 can be titled simply \"Quantization flow\".\n\nCompression ratio in the plots should indicate %?\n\n---\n\nTypos/grammar:\n\n* \"Mobilsnet-V2\" -> \"Mobilenet-V2\n* \"we suggest [an] integer-linear programming\"\n* \"AdaRound['s] implicit constraint\"\n* \"MSE distance\": shouldn't this just be MSE?\n* \"Early work by Lin et al. (2016) used [a] convex optimization formulation which results ~~with~~ [in] a simple greedy compression scheme.\"\n* \"3. OPTIMIZING [THE] QUANTIZATION PIP[E]LINE\"\n* \"model['s] internal statistic[s]\" -> found twice\n* \"often result with an inferior solution\" -> \"often result in an inferior solution\"\n* \"Accordingly, researches suggested\" -> researchers?\n* \"where V is a continuous variable V\" -> redundant\n* \"thus enjoy[ing] some of the flexibility\"? -> \"benefitting from\" might be a better phrasing?\n* \"Quantization-Aware- Training\" -> extra space\n* \"and [is] much less prone to over-fitting\"\n* \"[A] Similar derivation\"\n* \"Even optimizing on [a] single image\"\n* \"MAC operations.\" -> acronym not previously introduced (unless mistaken)\n* Section 4: IP acronym should be introduced in the integer programming section.\n* \"we investigate [a] mixture\"\n* \"results with high degradation\" -> in high degradation\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115906, "tmdate": 1606915771570, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review"}}}, {"id": "LGgSIt4lRt0", "original": null, "number": 4, "cdate": 1603902878908, "ddate": null, "tcdate": 1603902878908, "tmdate": 1605024414265, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review", "content": {"title": "Overall, the authors propose to use a combination of different twisted techniques for better neural quantization, which lacks meaningful insights.", "review": "The authors propose to use many techniques to push the limit of neural quantization, which shows reasonable improvements in some datasets.\n\n+ good performance.\n+ clear presentation.\n+ easy to read and follow\n\nMy main complaint is that this type of combination is better for a technical report rather than a standalone paper. In more detail, the authors claim four proposed components: AdaQuant, Integer programming, Batch-norm tuning, and two pipelines for neural quantization. However, the problem is, are these four components all original or just adapted from existing works? The authors are failed to present the relation of the proposed components with the exitinging ones. For example, in my understanding, the AdaQuant is only an adapted version of AdaRound, but the discussion about it (in Section 3.1) is too superficial. Furthermore, the experiment part is not convincing and I think can not back up the claim. With so many \"proposed components\", the missing of thorough ablation study is a big problem.", "rating": "4: Ok but not good enough - rejection", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115906, "tmdate": 1606915771570, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review"}}}, {"id": "VH858vlzD7x", "original": null, "number": 1, "cdate": 1603769574907, "ddate": null, "tcdate": 1603769574907, "tmdate": 1605024414199, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review", "content": {"title": "Interesting Approach To Quantization", "review": "\n\nSummary: The paper studies the problem of Post-Training Quantization of NNs, where no fine-tuning is performed to quantize the model. In particular, the authors focus on sub-8 bit quantization and propose a novel integer linear programming formulation to find the optimal bit width for a given model size. Additional approaches are proposed to minimize accuracy degradation after quantization. These include\n(i) AdaQuant in which the parameters are quantized layer-by-layer to match the full precision output,\n(ii) a batch norm tuning approach to re-adjust the statistics to the quantized model, and\n(iii) an advanced pipeline for cases where backpropogation can be performed.\n\nExperiments are performed on ResNet18/50, MobileNetV2, and BERT-SQuaD1.1 showing the feasibility of the proposed method. \n\nI think the approach in the paper is pretty interesting, and specially the integer linear programming solution.\nOverall the paper is strong however, please note the following:\n\n\n- Page 3 last paragraph: it seems there are errors in the results for calibration data.\n1/ it is stated that if \"B>> M then we might underfit the data.\". Do the authors mean \"B>>N\"? Similarly it seems the result given for the convolution (i.e. B> cK^2/HW) needs to be revised.\n2/ The analysis requires specifying the rank of W and in particular the relationship between M and N. In particular, note that the matrix W can at most have a rank of min(M,N) and as a result the rest of the linear equations for calibrating the data would be redundant. This needs to be taken into account in your result.\n\n- Page 4: \"Depending on the needs, our performance metrics P would be either the execution time of the network or its power consumption.\" This is good but no result on either latency or power consumption is provided in the paper.\n\n- Figure 4 (a,b): Please provide the BOPS for the mixed-precision results. BOPS proposed by (https://arxiv.org/pdf/2005.07093.pdf) is a good metric to measure the total reduction in computations for mixed precision quantization.\n\n- The results section uses weak FP32 baseline. For instance, the baseline accuracy for ResNet50 is 77.2\\% and MobileNetV2 is 73\\%, while the paper uses 76.1\\% and 71.8\\%.\n\n- Related to the above, it is stated \"on the extensively studies 8bit MobileNet-V2 topology we achieved 71.6% top-1 accuracy\". This is a good result but please note that other work in the literature (arxiv:2001.00281) reports 72.91\\% for INT8 quantization of MobileNetV2 (this comparison is actually missing from the paper). It is immediately not clear if the lower reported accuracy is due to the weaker FP32 baseline used or if it is an inherent problem with the method (most probably it is the former but it would be to show this).\n\n\n\n\nMinor Comments:\n\n- There were several grammatical/spelling mistakes in the paper. Please proofread the paper thoroughly. Below are some of the errors that I caught:\n\n- 3 OPTIMIZING QUANTIZATION PIPLINE -> PIPELINE\n\n- Page 6:  the our method robustness -> the robustness of our method\n\n- Page 6: manged -> managed\n\n- Page 7: an significant advantage -> a significant advantage\n\n- Page 8: we managed to switched  -> we managed to switch\n\n- Page 8: For instance, on the extensively studies -> For instance, on the extensively studied", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115906, "tmdate": 1606915771570, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review"}}}, {"id": "gKjagp4qWvK", "original": null, "number": 5, "cdate": 1604961195559, "ddate": null, "tcdate": 1604961195559, "tmdate": 1605024414138, "tddate": null, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "invitation": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review", "content": {"title": "Confusing claims with lack of details and limited novelty", "review": "\nThis paper proposed a set of methods for post-training quantization of dnns. The methods include AdaQuant (which jointly optimizes quantization steps for weight and activation per output activation of each layer), Integer Programming (which determines bit-precision for all the layers), and the batchnorm tuning. The authors presented promising experimental results on various neural networks to support the proposed methods. \n\nHowever, there are serious concerns about these claims as follows:\n1) AdaQuant\n- It is straightforward to think that the joint optimization of quantization step size for weight and activation would result in better quantization results. But this joint optimization would also increase the search space (at least) quadratically, resulting in significant computational cost. Note that the biggest merit of post-training quantization is its simplicity (cf., QAT incurs full-blown training epochs); thus increased cost for post-training quantization is not desirable. Since AdaQuant is the major claim, the authors should provide more discussion on how they dealt with this increased complexity.\n\n- The authors claim that AdaQuant avoids overfitting, but the reason does not seem to be clear. There is no clear explanation of how AdaQuant increases the generality of the quantized model, and the discussion about the sample size (B) is hard to understand (why there's infinite solution when B << N? how B>= Ck^2/(HW) is derived for the convolution case?)\n\n- Also, it seems that the \"per-channel\" quantization method is utilized in this work, but the formulation in (2) seems to be for \"per-layer\" optimization. Why they are different?\n\n- How much time does it take to solve this joint optimization?\n\n\n2) Integer Programming\n- The authors proposed an Integer Programming formulation, but there seem to be missing information: what is the formulation of the penalty function, \"deltaL\"? The authors described it simply as \"Loss\", but it is not clear what the exact method it is calculated. In fact, deltaL can be pretty complex functions, which might not be independent terms for each layer; thus the formulation like (3) might not be correct. Note that the impact of quantization in the earlier layers affect the quantization impact in the current layer. Without clear explanation and justification about it, the proposed IP formulation does not make sense.\n\n- The authors mentioned that deltaP should be additive and sum up to the total benefit. How can one guarantee it? \n\n- Also, it seems that the complexity of the IP optimization increases as the number of layers increases. How much computation time increases if the number of layers are large? \n\n\n3) Batch normalization tuning\n- Unfortunately, there is a very similar idea proposed by [Sun et al., NeurIPS 19]. Cf. \"Sec.3 Trans-Precision Inference in FP8\".\n\n\nAlso, there are several suggestions to improve understanding of readers.\n- Currently, the ablation study looks very confusing. It is not clear which of the pipeline options (light, advanced?) include what kinds of techniques. Please do specify (maybe in a separate table) the list of techniques covered by different pipeline options. \n\n- The proposed method is not much evaluated by various neural networks. It would be desirable to expand the coverage of neural nets as much as the prior work did. \n\n- Currently, the proposed methods only utilized \"per-channel\" quantization. How much accuracy the proposed methods can maintain if they adopt \"per-layer\" quantization?\n\n- What is the definition of \"compression ratio\"? (typically compration RATIO is like 12:1, and compression rate is like 2X, 3X...)\n\n- \n\n\n", "rating": "4: Ok but not good enough - rejection", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2021/Conference/Paper1562/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1562/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming", "authorids": ["~Itay_Hubara1", "~Yury_Nahshan1", "~Yair_Hanani1", "~Ron_Banner1", "~Daniel_Soudry1"], "authors": ["Itay Hubara", "Yury Nahshan", "Yair Hanani", "Ron Banner", "Daniel Soudry"], "keywords": ["Efficient Deep Learning", "Quantization", "Compression"], "abstract": "Lately, post-training quantization methods have gained considerable attention, as they are simple to use, and require only a small unlabeled calibration set. This small dataset cannot be used to fine-tune the model without significant over-fitting. Instead, these methods only use the calibration set to set the activations' dynamic ranges. However, such methods always resulted in significant accuracy degradation, when used below 8-bits (except on small datasets). Here we aim to break the 8-bit barrier. To this end, we minimize the quantization errors of each layer separately by optimizing its parameters over the calibration set.  We empirically demonstrate that this approach is: (1) much less susceptible to over-fitting than the standard fine-tuning approaches, and can be used even on a very small calibration set; and (2) more powerful than previous methods, which only set the activations' dynamic ranges. Furthermore, we demonstrate how to optimally allocate the bit-widths for each layer, while constraining accuracy degradation or model compression by proposing a novel integer programming formulation. Finally, we suggest model global statistics tuning, to correct biases introduced during quantization. Together, these methods yield state-of-the-art results for both vision and text models. For instance, on ResNet50, we obtain less than 1\\% accuracy degradation --- with 4-bit weights and activations in all layers, but the smallest two. Our code is available at, https://github.com/papers-submission/CalibTIP", "one-sentence_summary": "State-of-the-art results using advanced method for post training per channel quantization - squeezing all the information from the calibration set.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hubara|improving_post_training_neural_quantization_layerwise_calibration_and_integer_programming", "pdf": "/pdf/ce0e4b560de85ec662b936d3b2e452d8b848ce88.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=o9VxHXzxYP", "_bibtex": "@misc{\nhubara2021improving,\ntitle={Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming},\nauthor={Itay Hubara and Yury Nahshan and Yair Hanani and Ron Banner and Daniel Soudry},\nyear={2021},\nurl={https://openreview.net/forum?id=Mf4ZSXMZP7}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Mf4ZSXMZP7", "replyto": "Mf4ZSXMZP7", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1562/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538115906, "tmdate": 1606915771570, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1562/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1562/-/Official_Review"}}}], "count": 16}