{"notes": [{"id": "SyxWOQTKZN", "original": null, "number": 2, "cdate": 1546404712583, "ddate": null, "tcdate": 1546404712583, "tmdate": 1546404712583, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "BklH98B9TX", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Public_Comment", "content": {"comment": "I may have misunderstood what exactly you meant when you wrote \" McDonnell (2018) ... changed the model structure and training\", but I want to clarify that there is *no difference* between architecture and training methods for baseline and 1-bit networks in McDonnell (2018). You appear to me to be saying this is not the case.  See point 2 on the strategy outlined on page 2: \"Make minimal changes when training for 1-bit-per-weight\" and Figure 2 in McDonnell (2018).", "title": "\"changed the model structure and training\""}, "signatures": ["~Mark_D_McDonnell1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Mark_D_McDonnell1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Public_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1542311872199, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed."}, "nonreaders": {"values": []}, "forum": "SyxnvsAqFm", "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "replyto": null, "content": {"comment": {"value-regex": "[\\S\\s]{1,5000}", "required": true, "order": 1, "description": "Your comment or reply (max 5000 characters)."}, "title": {"value-regex": ".{1,500}", "required": true, "order": 0, "description": "Brief summary of your comment."}}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": ["ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1542311872199}}}, {"id": "SyxnvsAqFm", "original": "HJxjwX_qtm", "number": 300, "cdate": 1538087780014, "ddate": null, "tcdate": 1538087780014, "tmdate": 1545355393621, "tddate": null, "forum": "SyxnvsAqFm", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 13, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SkxCoecggE", "original": null, "number": 1, "cdate": 1544753317941, "ddate": null, "tcdate": 1544753317941, "tmdate": 1545354517654, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Meta_Review", "content": {"metareview": "The authors propose a technique for quantizing neural networks, which consist of repeated quantization/de-quantization operations during training, and the second step learns scale factors. The method is simple, clearly presented, and requires no change in the training procedure.\nHowever, the authors noted that the work is somewhat incremental, and is similar to previously proposed approaches. As noted by the reviewers, the AC agrees that the work would be significantly strengthened by additional analysis of complexity in terms of computational time and memory relative to the other techniques. \n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Work would be strengthened by additional analyses"}, "signatures": ["ICLR.cc/2019/Conference/Paper300/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper300/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353264092, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353264092}}}, {"id": "r1l2A103JV", "original": null, "number": 10, "cdate": 1544507347927, "ddate": null, "tcdate": 1544507347927, "tmdate": 1544507347927, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "rylSuuHqpm", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "Thanks for the response.", "comment": "After reading the responses and the revised manuscript, the reviewer still did not find complexity analysis in Sec 5.5. The complexity analysis is important since the claimed contribution is to improve training quantitative models. There are still no comparison with other quantization methods in terms of computational time and memory.\n\nIn addition, the explanation to the comparison to Zhou et al. is not convincing.  There is a significant gap between the results.\n\nThe reviewer will keep the rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "SJgHXAwh14", "original": null, "number": 9, "cdate": 1544482332567, "ddate": null, "tcdate": 1544482332567, "tmdate": 1544482332567, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SJxdMvH9pX", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "thanks for the additional info", "comment": "Thanks authors for providing more information. Even with these considered, the work still provided limited contribution, and therefore I would maintain the same rating."}, "signatures": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer2"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer2", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "rkgHiQOsyN", "original": null, "number": 8, "cdate": 1544418204753, "ddate": null, "tcdate": 1544418204753, "tmdate": 1544418204753, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SJxhAc3dJ4", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "reply", "comment": "Thank you for the post-rebuttal.\n\nPlease note that the convergence of \\alpha after 80k (be very small updates) is solely responsible for over 4% increase in quantized accuracy as can be seen in Figure 1.b.\n\nI would also like to address the simplicity of the algorithm:\n1. Proposed quantization training method requires almost no change in the existing training procedure.\n2. Phase1 performs quantization every few iterations. Phase2 directly trains \\alpha using the quantized network.\n3. As a result, the training time is also significantly reduced. Section 5.5 shows that our training method reduces quantization overhead from 40x to 10% of the training time in our experiments.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper300/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "Ske7pUNoy4", "original": null, "number": 7, "cdate": 1544402619133, "ddate": null, "tcdate": 1544402619133, "tmdate": 1544402850376, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SJxhAc3dJ4", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "Our response", "comment": "\nThank you for the post-rebuttal.\n\nFigure 2. (left) shows how often B values are flipped while Figure 2. (right) describes that 'alpha' still needs to be updated even after 80 iterations (when B values are not updated any more). Both alpha and 'B' values are updated during the exploration phase with high learning rate while 'alpha' updating without updating 'B' values is done with large learning rate at later phase as a fine-tuning procedure. \nAdmittedly, even normal optimization process takes its most of time for fine-tuning within a local optimum space and it gives us the motivation of updating 'B' only for reduced quantization computation overhead when the training procedure gets into the fine-tuning process (without exploring multiple local minima).\nPlease note that we observe similar frequencies of bit flipping in Figure 2 even without quantization or different quantization methods.\n\nCompared to BinaryConnect/Xnor-Net, we have the following advantages.\n1) We do not perform the quantization process at every mini-batch, instead ours quantize the model occasionally according to the quantization step size.\n2) After entering fine-tuning stage, we simplify the quantization model using 'alpha' only.\n\nOn top of efficient computation method, our techniques achieve better accuracy (especially with RNN models and wide CNN models)."}, "signatures": ["ICLR.cc/2019/Conference/Paper300/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "SJxhAc3dJ4", "original": null, "number": 6, "cdate": 1544239827834, "ddate": null, "tcdate": 1544239827834, "tmdate": 1544239827834, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "BklH98B9TX", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "post-rebuttal", "comment": "I thank the authors for their response. However, the reply does not fully address my concerns. First, the authors claimed in their rebuttal that the convergence difference of B and \\alpha is due to the different spaces they lie in (i.e., B in the binary space and \\alpha in the continuous space). Empirically, B and \\alpha indeed have similar convergence speed as shown in Figure 2. While I understand that training \\alpha is efficient, but the authors do not directly answer why it is necessary to train \\alpha additionally. I also do not see why the proposed method is computationally more advanced than BinaryConnect/Xnor-Net from the rebuttal, as one only needs to do a simple and efficient binarization before the forward propagation for BinaryConnect/Xnor-Net, while the proposed method requires several alternating training phases and is more complicated.\n\nHence, I keep my rating unchanged."}, "signatures": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer1"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer1", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "rylSuuHqpm", "original": null, "number": 4, "cdate": 1542244460597, "ddate": null, "tcdate": 1542244460597, "tmdate": 1542244638112, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "BkxDCAl43Q", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "reply", "comment": "Thank you for the review.\n\nWe have updated the manuscript as per your suggestion to provide an introduction to the motivation for the section 3.3 and section 4.\n\nOn cifar10 and cifar100 experiments with wide-resnet, our method recovers full accuracy with its baseline (baseline is taken from Tensorflow models implementation - https://github.com/tensorflow/models) as given in Table 2. With resnet32, our accuracy loss is 0.11%, with final accuracy matching that from TTQ. However, unlike TTQ, our occasional distortion reduces amount of computation overhead from quantization and does not require any intrusive change in the training procedure. Also refer to our response to AnonReviewer2 for more details on this.\n\nThe greedy quantization has low overhead to the original training time (10% and 15% of the training time for 1bit and 2bit quantization). Using more sophisticated techniques like refined, alternating quantization leads more overhead (over 5x, 40x compared to the training time respectively). As table 3 shows the benefit of these quantization methods, our training method, unlike existing methods, performs quantization once every 500 iterations, nullifying the overheads of these quantization to less than 10% of the original training time. We have updated our manuscript accordingly as well.\n1 bit quantization leads to almost 32x reduction in parameter size, while ternary and 2 bit quantization reduce parameters by almost 16x.\n\nPlease refer to our response for AnonReviewer1 for our imagenet results in comparison with Zhou et al. (2018)."}, "signatures": ["ICLR.cc/2019/Conference/Paper300/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "BklH98B9TX", "original": null, "number": 2, "cdate": 1542243980789, "ddate": null, "tcdate": 1542243980789, "tmdate": 1542244628154, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SkgUjCCFhX", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "reply", "comment": "Thank you for the review.\n \nB and \\alpha both are learned greedily. B represents the sign of the weights and \\alpha gives the value. As the training converges, the updates to the weights get smaller and smaller. This makes it more difficult for the weight to flip their sign bit, resulting in faster convergence of B. \nAs \\alpha belongs to floating point space, small updates are important for the convergence of \\alpha. However, as B belongs to binary space, small updates do not affect B due its coarse-grained binary space, leading to faster convergence of B.\n\nPhase II (just train \\alpha) reduces the computation overhead significantly. Phase II reduces the number of trainable parameters (approx. 1/1000 of the total parameters are trained) resulting in fewer parameter updates. Further, phase II does not need to perform the quantized-distortion step. Also, the forward-backward propagation are faster as the weights are represented in binary format (B, \\alpha) instead of full precision weights.\n\nThe proposed method differs from BinaryConnect/Xnor-net in the back-propagation step. Xnor-net evaluate the loss using binary weights and applies the updates to full-precision weights. Xnor-net performs quantization every step, which requires them to do binary backward propagation (needing a major change in training procedure).\n\nOn cifar10 and cifar100 experiments with WideResNet, our method recovers full accuracy with its baseline (baseline is taken from Tensorflow models implementation given in https://github.com/tensorflow/models) as given in Table 2. McDonnell (2018) are unable to reach their baseline (although they have a higher baseline because they changed the model structure and training).\n\nOn imagenet, our method gets comparable results with Xnor-net. Zhou et al. (2018) performs training for each layer separately requiring them to increase the number of forward propagation to be done. This scale the effective number of epochs by the number of layers. For resnet32, this scales the effective epochs by 32x. In contrast, the proposed method performs the quantization training in the original training time. As the training time is too large with layer-by-layer training for large networks, we have targeted feasibility of our method over improved results."}, "signatures": ["ICLR.cc/2019/Conference/Paper300/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "SJxdMvH9pX", "original": null, "number": 3, "cdate": 1542244112515, "ddate": null, "tcdate": 1542244112515, "tmdate": 1542244615959, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SJl-NOn7TQ", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "content": {"title": "reply", "comment": "Thank you for the review.\n\nIn \"Retraining-Based Iterative Weight Quantization for Deep Neural Networks\", the concept of \"exploration vs exploitation\" was missing leading to longer training time (up to 6x of the original training time). Further, it also led to very low accuracy as we analysed the sensitivity of the quantization step size to the accuracy. We introduced a key hyper-parameter of the quantization step size, which determines the amount of weight distortion and the amount of retraining time (amount of convergence until the next step). Quantization step size was the key to make the technique work for CNNs.\n\nThe L_KM loss function proposed helps reduce the variance of the weights compared to L2 loss, resulting in better approximation of \\alpha. The biggest effect of L_KM loss is presented in \"One-step Quantization\" in section 5.3, where quantization is performed without any retraining. Using L_KM doubles accuracy compared to just using L2 loss (84.51% with L_KM compared to 44.33% with L2 loss) due to reduced variance of the weights sharing the same \\alpha.\n\nThere are many good quantization methods for CNN to use only 1bit or 2bit formats (including ternary). For CNN, our goal was to show our advantage in the computation complexity while training the quantized model (Our training method performs fewer quantization operations as analysed in section 5.5). However, for RNNs, it is still challenging to quantize with 1-2 bits (Table 3), where we perform better than the existing methods.  \n\nPlease refer to our response for AnonReviewer1 for our imagenet results in comparison with Zhou et al. (2018).\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper300/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621619859, "tddate": null, "super": null, "final": null, "reply": {"forum": "SyxnvsAqFm", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper300/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper300/Authors|ICLR.cc/2019/Conference/Paper300/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers", "ICLR.cc/2019/Conference/Paper300/Authors", "ICLR.cc/2019/Conference/Paper300/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621619859}}}, {"id": "SJl-NOn7TQ", "original": null, "number": 3, "cdate": 1541814313486, "ddate": null, "tcdate": 1541814313486, "tmdate": 1541814313486, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Review", "content": {"title": "Limited novelty and does not provide significant improvement compared to existing approaches", "review": "This work addresses the issue of quantization for neural network, and in particular focus on Ternary weight networks. The proposed approach has two phases, the first phase performs quantization and de-quantization at certain iterations during training, where the schedule of these operations are hyperparameters specified a priori. The second phase focuses on training the scaling factor. The first phase is similar to the iterative quantization method proposed in \u201cRetraining-Based Iterative Weight Quantization for Deep Neural Networks\u201d, and differs in that this work performs the quantization and de-quantization operations more frequently.\nThis work also proposed a modified version of L2 regularization, but it\u2019s not clear how much benefit it provides compared to a regular L2 regularization. There is also a shuffling approach, but seems to provide limited improvement.\nThe experiment results in general does not provide convincing evidence that the proposed method outperforms existing approaches. For example, the ResNet-32 on CIFAR-10 result does not perform better than the one reported in \u201cTrained ternary quantization\u201d, and the ImageNet result is also worse than some existing works.\nThe work is lack of novelty and the results do not show significant improvement over existing approaches.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Review", "cdate": 1542234493087, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335693567, "tmdate": 1552335693567, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "SkgUjCCFhX", "original": null, "number": 2, "cdate": 1541168797715, "ddate": null, "tcdate": 1541168797715, "tmdate": 1541534111125, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Review", "content": {"title": "This paper considers the problem of training weight quantized deep neural networks. An iterative method is proposed where weight quantization and full-precision weight retraining are performed iteratively, and the gap between the full-precision network and quantized network is supposed to diminish during the iterative process. Experiments are performed on both CNNs and LSTMs on some benchmark data sets.", "review": "The paper is a little hard to follow and some parts are poorly written. While the authors claim that they use the greedy approach (in sec 2) for quantization where both B and \\alpha are learned in a greedy way, it is not clear why there is convergence difference between the two as claimed by the authors in section 3.1. Moreover, the authors claimed faster convergence of B than \\alpha because fewer bit clips are observed from the left subplot of Figure 2. However, this conclusion is not quite convincing because 1) on the right subplot of Figure 2, it seems that \\alpha also becomes more stable after 80k iterations; 2) the fewer bit clips may comes from using a stepwise learning rate decay scheme. Thus, the motivation for using another phase to train the \\alpha is not strong.\n\nThe iterative quantization approach has limited novelty. It is similar to many quantization methods like BinaryConnect and Xnor-Net, except that the quantization step is not done immediately after the BP and model updates, but after some iterations of full-precision training. Moreover, these methods also use full-precision weights for update during training.\n\nClarity in the experiments section is a little better than the previous sections. However,\n- The proposed method only performs comparably with TTQ, and shows significant accuracy drop on the Cifar-10 and Cifar-100 datasets (especially on Cifar-100)\n- On the ImageNet dataset, there is a large accuracy drop of the proposed method compared to Zhou et al. (2018). Though the authors said that they believe their proposed model can reach a higher accuracy by using layer-by-layer quantization as in Zhou et al. (2018), it is hard to verify this claim due to lack of the corresponding results. Thus, efficacy of the proposed method on large datasets or models are hard to evaluate.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Review", "cdate": 1542234493087, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335693567, "tmdate": 1552335693567, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "BkxDCAl43Q", "original": null, "number": 1, "cdate": 1540783823305, "ddate": null, "tcdate": 1540783823305, "tmdate": 1541534110920, "tddate": null, "forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "invitation": "ICLR.cc/2019/Conference/-/Paper300/Official_Review", "content": {"title": "An effective training methods for network quantization  was proposed, however, the claimed contribution has been well justified.", "review": "This paper proposes a method based on re-training the full-precision model and then optimizing the corresponding binary model. It consists of two phases: (1)  the full-precision model training where the quantization step is introduced through QSS to train the network, and (2) fine tuning of quantized networks, where  the trained network was converted into a binary model. In addition, using the skewed matrix for quantization improves the accuracy. Then a loss function based on the k means form is used to normalize the weight for reducing the quantization error. Quantization experiments for CNNs or LSTMs have been conducted on CIFAR10, CIFAR100, IMAGENET, and WikiText-2 dataset. \n\nThis paper has been presented clearly. However, it can be improved by introducing the motivation of the tricks(e.g. skewed matrix and loss related to k-means ) used for quantization.\n\nIn the experiments, the precision improvement on the CIFAR and ImageNet dataset performs worse than some competitors. For example, the precision of the proposed method was significantly worse than Zhou et al, 2018 on ImageNet. It is better to  analyze the reason. \n\nIn addition, as claimed from the introduction, the contribution of this paper was  to reduce the overhead of expensive quantization. However, no experimental results on computation time  and parameter size have been shown. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper300/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Computation-Efficient Quantization Method for Deep Neural Networks", "abstract": "Deep Neural Networks, being memory and computation intensive, are a challenge to deploy in smaller devices. Numerous quantization techniques have been proposed to reduce the inference latency/memory consumption. However, these techniques impose a large overhead on the training procedure or need to change the training process. We present a non-intrusive quantization technique based on re-training the full precision model, followed by directly optimizing the corresponding binary model. The quantization training process takes no longer than the original training process. We also propose a new loss function to regularize the weights, resulting in reduced quantization error. Combining both help us achieve full precision accuracy on CIFAR dataset using binary quantization. We also achieve full precision accuracy on WikiText-2 using 2 bit quantization. Comparable results are also shown for ImageNet. We also present a 1.5 bits hybrid model exceeding the performance of TWN LSTM model for WikiText-2.", "keywords": ["quantization", "binary", "ternary", "flat minima", "model compression", "deep learning"], "authorids": ["kparichay@gmail.com", "dslee3@gmail.com", "guddnr145@gmail.com", "halo8218@gmail.com"], "authors": ["Parichay Kapoor", "Dongsoo Lee", "Byeongwook Kim", "Saehyung Lee"], "TL;DR": "A simple computation-efficient quantization training method for CNNs and RNNs.", "pdf": "/pdf/99466b7d179e8405a7694b22535fde948327a16f.pdf", "paperhash": "kapoor|computationefficient_quantization_method_for_deep_neural_networks", "_bibtex": "@misc{\nkapoor2019computationefficient,\ntitle={Computation-Efficient Quantization Method for Deep Neural Networks},\nauthor={Parichay Kapoor and Dongsoo Lee and Byeongwook Kim and Saehyung Lee},\nyear={2019},\nurl={https://openreview.net/forum?id=SyxnvsAqFm},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper300/Official_Review", "cdate": 1542234493087, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "SyxnvsAqFm", "replyto": "SyxnvsAqFm", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper300/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335693567, "tmdate": 1552335693567, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper300/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 14}