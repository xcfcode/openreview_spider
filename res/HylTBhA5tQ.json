{"notes": [{"id": "HylTBhA5tQ", "original": "ByxzSCs5K7", "number": 1584, "cdate": 1538088004802, "ddate": null, "tcdate": 1538088004802, "tmdate": 1546589777103, "tddate": null, "forum": "HylTBhA5tQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 14, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ryl8ns2-l4", "original": null, "number": 1, "cdate": 1544829870060, "ddate": null, "tcdate": 1544829870060, "tmdate": 1545354530680, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Meta_Review", "content": {"metareview": "Reviewers are in a consensus and recommended to accept after engaging with the authors. Please take reviewers' comments into consideration to improve your submission for the camera ready.\n", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Accept (Poster)", "title": "Paper decision"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1584/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352783861, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352783861}}}, {"id": "SylEXOI9AX", "original": null, "number": 11, "cdate": 1543297052219, "ddate": null, "tcdate": 1543297052219, "tmdate": 1543297052219, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Additional small edits before the revision period closes", "comment": "\nWe have addressed all the concerns of AnonReviewer3. During the discussion with AnonReviewer3, we found that there might be some confusions on how we generate adversarial examples from blind-spot images, and how we calculate the $\\ell_p$ distortions for adversarial examples. Thus we slightly revise Section 3.3 and 4.4 to make things clear. We hope this will make our paper easier to follow.\n\nAgain we thank all the reviewers for the encouraging and constructive comments!\n\nThanks,\nPaper1584 Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "HkgrM_e9R7", "original": null, "number": 10, "cdate": 1543272460846, "ddate": null, "tcdate": 1543272460846, "tmdate": 1543272460846, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "SylN6rJ5Cm", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Thank you so much for your comments and considerations", "comment": "We really appreciate the reviewer's fruitful suggestions, and we see where the confusion is. \n\nIn Sec. 3.3, blind-spot attack uses scaling and shifting to generate new natural reference images x' = \\alpha * x + \\beta. We still apply C\\&W L_inf attacks on x\u2019 to generate adversarial images x'_adv for all \\alpha and \\beta. We will revise our paper to make this clearer.\n\nThank you again for your comments and we will make the writing better.\n\nThank you!\nPaper 1584 Authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "SylN6rJ5Cm", "original": null, "number": 9, "cdate": 1543267771855, "ddate": null, "tcdate": 1543267771855, "tmdate": 1543268202360, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "B1gGnv0Y07", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Further discussion on 'blind-spot' attack", "comment": "Thanks for the clarification. \n\n\"We use alpha and beta to obtain new natural reference images instead of adversarial images.\"\n\nThis is a key point which makes reviewer confusing, since in Sec. 3.3, blind-spot attacks seem to generate adversarial images only using scaling and shifting. However, in experiments x\u2019 = \\alpha * x + \\beta is used to generate natural reference image. I am Okay with that only if the experiment is consistent, e.g., applying C\\&W attack on x\u2019 = \\alpha * x + \\beta for all \\alpha and \\beta discussed in this paper. \n\nPlease carefully revise Sec. 3.3. and experiment section to make the aforementioned point clearer. \n\nBased on the authors's current response, I increase my score to 6.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "H1gVYFVS3X", "original": null, "number": 1, "cdate": 1540864380131, "ddate": null, "tcdate": 1540864380131, "tmdate": 1543267789793, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Review", "content": {"title": "Reviewer's summery: interesting idea/findings but with questions", "review": "In this paper, the authors associated with the generalization gap of robust adversarial training with the distance between the test point and the manifold of training data. A so-called 'blind-spot attack' is proposed to show the weakness of robust adversarial training.  Although the paper contains interesting ideas and empirical results, I have several concerns about the current version. \n\na) In the paper, the authors mentioned that \"This simple metric is non-parametric and we found that the results are not sensitive to the selection of k\". Can authors provide more details, e.g., empirical results, about it? What is its rationale?\n\nb) In the paper, \"We find that these blind-spots are prevalent and can be easily found without resorting to complex\ngenerative models like in Song et al. (2018). For the MNIST dataset which Madry et al. (2018) demonstrate the strongest defense results so far, we propose a simple transformation to find the blind-spots in this model.\" Can authors provide empirical comparison between blind-spot attacks and the work by Song et al. (2018), e.g., attack success rate & distortion? \n\nc) The linear transformation x^\\prime = \\alpha x + \\beta yields a blind-spot attack which can defeat robust adversarial training. However, given the linear transformation, one can further modify the inner maximization (adv. example generation) in robust training framework so that the $\\ell_infty$ attack satisfies  max_{\\alpha, \\beta} f(\\alpha x + \\beta) subject to \\| \\alpha x + \\beta \\|\\leq \\epsilon. In this case, robust training framework can defend blind-spot attacks, right? I agree with the authors that the generalization error is due to the mismatch between training data and test data distribution, however, I am not convinced that blind-spot attacks are effective enough to robust training. \n\nd) \"Because we scale the image by a factor of \\alpha, we also set a stricter criterion of success, ..., perturbation must be less\nthan \\alpha \\epsilon to be counted as a successful attack.\" I did not get the point. Even if you have a scaling factor in x^\\prime = \\alpha x + \\beta, the universal perturbation rule should still be | x - x^\\prime  |_\\infty \\leq \\epsilon. The metric the authors used would result in a higher attack success rate, right? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Review", "cdate": 1542234198385, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977924, "tmdate": 1552335977924, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HkeeRxhmAX", "original": null, "number": 5, "cdate": 1542861000453, "ddate": null, "tcdate": 1542861000453, "tmdate": 1543264219222, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "BylRCJCM6m", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "We will really appreciate it if you could provide us more feedback before the revision period ends", "comment": "Dear AnonReviewer3,\n\nThank you again for your insightful and constructive comment!\n\nWe hope that we have addressed your questions. We understand you may be discussing our paper with other reviewers and you can take your time. As the revision period is closing soon, we will really appreciate it if you could let us know if you find anything unclear in our response, or have any further concerns about our paper. We will try our best to revise our paper based on your suggestions before the revision period ends.\n\nThank you!\nPaper 1584 Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "B1gGnv0Y07", "original": null, "number": 7, "cdate": 1543264170331, "ddate": null, "tcdate": 1543264170331, "tmdate": 1543264170331, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "BylEr05YAX", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Thank you for the questions! Here are our further clarifications.", "comment": "Dear AnonReviewer3,\n\nThank you for your response and further questions. We would like to answer them as below:\n\n\u201cI assumed that the distortion condition will be examined as $| \\alpha x + \\beta |_infty \\leq \\eps$, right?\u201d\nNo, this is not how we examine the Linf distortion success condition in Table 2.\n\nWe use alpha and beta to obtain new natural reference images instead of adversarial images. For example, for an original image x from the test set, we scale and shift this image to obtain a new natural reference image x\u2019 = \\alpha * x + \\beta. Then we run C&W attack on x\u2019 to obtain its adversarial image x\u2019_adv. Note that x\u2019 = \\alpha * x + \\beta is not considered as an adversarial image but as a natural image since in the blind-spot attack we are finding the blind-spots (where the model do not have good robustness) in the natural data distribution.\n\nThe distortion condition is examined as the distance between x\u2019 and x\u2019_adv: $|x\u2019 - x\u2019_adv|_\\infty \\leq \\eps$, but not $| \\alpha x + \\beta |_infty \\leq \\eps$. We will try to make this clearer in our revision.\n\n\u201cIn the last column of Table 2, alpha = 0.7 & beta = 0.15, I wonder why ASRs under thr = 0.3 and thr = 0.21 are the same.\u201d\nThe reason is that most adversarial examples generated from blind-spot images with alpha=0.7 and beta=0.15 have small distortions, less than both 0.3 and 0.21. So they are considered successful in both criteria. \n\n\u201cit quite surprising that ASRs for the two cases (alpha = 0.7, beta = 0, thr = 0.21) and  (alpha = 0.7, beta = 0.15, thr = 0.21) have a large gap. Any rationale behind that?\u201d\nThe ASR for the case with non-zero beta is much higher than beta=0 case indicates that scaling+shifting is more effective than scaling alone to reduce the robustness of the model under attack. Scaling+shifting is a more powerful blind-spot attack.\n\nWe are glad to discuss further with you if you have any additional questions. Thanks again for the constructive feedback!\n\nThank you!\nPaper 1584 Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "BylEr05YAX", "original": null, "number": 6, "cdate": 1543249468153, "ddate": null, "tcdate": 1543249468153, "tmdate": 1543257796955, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "HkeeRxhmAX", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Responses clarified the reviewer's previous questions", "comment": "\"We want to emphasize that the \u201cblind-spot attack\u201d is a class of attacks, which exploits the gap between training and test data distributions (see our definition in Section 3.3). The linear transformation used in our paper is one of the simplest attacks in this class. If we know the details of this specific attack before training, it is possible defend against this specific simple attack.\"\n\nOk, I agree with the authors at this point. \n\n\"The stricter criterion actually makes our attack success rates *lower* rather than higher. Finding adversarial examples with smaller distortions is harder than finding adversarial examples with large distortions. As an extreme case, if the criterion is distortion<=0, the attack success rate will always be zero, since we cannot fool the model using unmodified natural images. In Table 2, the success rates under the column 0.27 are strictly lower than the numbers under the column 0.3. We consider this additional stricter criterion because images after scaling are within a smaller range, so we also restrict the noise to be smaller, to keep the same signal-to-noise ratio and make an absolutely fair comparison. If we don\u2019t use this stricter criterion, our attack success rates will look even better.\n\"\n\nYes, the authors are correct that finding adversarial examples with smaller distortions is harder than finding adversarial examples with large distortions, thus $\\alpha \\epsilon$ will make attack success rate (ASR) LOWER. Based on that, I checked Table 2, which is still unclear to me. \n\nIn the last column of Table 2, alpha = 0.7 & beta = 0.15, I wonder why ASRs under thr = 0.3 and thr = 0.21 are the same. Since an attack is considered as successful if its Linf distortion is less than given thrs, I assumed that the distortion condition will be examined as $| \\alpha x + \\beta - x |_infty \\leq \\eps$, right? If so, it quite surprising that ASRs for the two cases (alpha = 0.7, beta = 0, thr = 0.21) and  (alpha = 0.7, beta = 0.15, thr = 0.21) have a large gap. Any rationale behind that?\n\n\nI will adjust my score based on the authors' further clarification."}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "rkeMtGiyCQ", "original": null, "number": 4, "cdate": 1542595193964, "ddate": null, "tcdate": 1542595193964, "tmdate": 1542595374058, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Reply to All Reviewers", "comment": "During the rebuttal period, we further enhanced our experiments by conducting blind-spot attacks on two certified, state-of-the-art adversarial training methods, including (Wong & Kolter 2018) and (Singha et al. 2018). Surprisingly, although they can provably increase robustness on the training set, they still suffer from blind-spot attacks by slightly transforming the test set images. See Tables 4, and 5 in the Appendix. The attack success rates go significantly higher after a slight scale and shift on both MNIST and Fashion MNIST test sets, for both two defense models.\n\nAdditionally, we also add results for a relatively larger dataset, GTS (german traffic sign) in Appendix (Section 6.2). The results (in histograms) we observed are similar to the ones we observed on CIFAR.\n\nWith these new results, our conclusion is not limited to the adversarial training method proposed by (Madry et al. 2018). Our paper uncovers the weakness of many state-of-the-art adversarial training methods, even including those with theoretical guarantees on the training dataset. By identifying a new class of adversarial attacks, even in its simplest form (small shift + scale), many good defense methods become vulnerable again. \n\nIn conclusion, we show that many state-of-the-art strong adversarial defense methods, even including those with robustness certificates on training datasets, cannot well generalize their robustness on unseen test data from a very slightly changed domain. This partially explains the difficulty in applying adversarial training on larger datasets like CIFAR and ImageNet. We believe that our results are significant. We also think these experiments are important to further understanding adversarial examples and proposing better defenses.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "BJlit-i1Rm", "original": null, "number": 3, "cdate": 1542594947226, "ddate": null, "tcdate": 1542594947226, "tmdate": 1542594947226, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "rJe1e3jj3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Thank you for the questions! We have updated our paper and answered your questions below.", "comment": "Thank you for the encouraging comments. First of all, we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods, and observe that they are also vulnerable to blind-spot attacks. Please see our reply to all reviewers.\n\nWe agree that the K-L based method is complicated and computationally extensive. Fortunately, we only need to compute it once per dataset. To the best of our knowledge, currently, there is no perfect metric to measure the distance between a training set and a test set. Ordinary statistical methods (like kernel two-sample tests) do not work well due to the high dimensionality and the complex nature of image data. So the measurement we proposed is a best-effort attempt that can hopefully give us some insights into this problem. \n\nAs suggested by the reviewer, we added a new metric based on the mean of \\ell_2 distance on the histogram in Section 4.3. The results are shown in Table 1 (under column \u201cAvg. normalized l2 Distance\u201d). The results align well with our conclusion: the dataset with significant better attack success rates has noticeably larger distance. It further supports the conclusion of our paper and indicates that our conclusion is distance metric agnostic.\n\nWe hope that we have made everything clear, and we again appreciate your comments. Let us know if you have any additional questions.\n\nThank you!\nPaper 1584 Authors\n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "rkxhfWj1C7", "original": null, "number": 2, "cdate": 1542594836151, "ddate": null, "tcdate": 1542594836151, "tmdate": 1542594836151, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "Hygsieas2X", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Thank you for the questions! We have updated our paper and answered your questions below. ", "comment": "Thank you for your insightful comments to help us improve our paper. First of all, we would like to mention that we add more experiments on two additional state-of-the-art strong and certified defense methods, and observe that they are also vulnerable to our proposed attacks. Please see our reply to all reviewers.\n\nHere are our responses to your concerns in \u201cCons\u201d and \u201cMinor comments\u201d.\n\nAlthough we were not able to provide theoretical analysis in this paper, our proposed attacks are very effective on state-of-the-art adversarial training methods, and we believe our conclusions\nCurrently, there is relatively few theoretical analysis in this field in general, and many analysis makes unpractical assumptions. We believe our results can inspire other researcher\u2019s theoretical research.\n\nRegarding the \u201cblind-spot attack\u201d phrase, we are open to suggestions from the reviewers. Other phrases we considered including \u201cevasion attack\u201d, \u201cgeneralization gap attack\u201d and \u201cscaling attack\u201d. Which one do you think is a better option?\n\nRegarding the distances in Figure 3:\nThanks for raising this concern. We have added a note to clarify this issue. The difference in distance can be partially explained by the sparsity in an adversarially trained model. As suggested in [1], the adversarially trained model by Madry et al. tends to find sparse features (see Figure 5 in [1]), where many components are zero. Thus, the distances tend to be overall smaller.\n\nRegarding the results in Table 1:\nIn our old version, we only used the adversarially trained network. In our revision, we added K-L divergence computed from both adversarially trained and naturally trained networks. Additionally, we also add a new distance metric proposed by AnonReviewer1. The K-L divergences by both networks, as well as the newly added distance metric, show similar observations.\n\nRegarding adding more visualizations:\nWe added some more visualizations in Fig 10 in the appendix. It is worth noting that the Linf distortion metric used in adversarial training is sometimes not a good metric to reflect visual differences. However, the test images under our proposed attack indeed have much smaller Linf distortions.\n\nWe hope that we have answered all your questions, and we are glad to discuss with you if you have any further concerns about our paper.\n\n[1] Tsipras, Dimitris, et al. \"Robustness may be at odds with accuracy.\" arXiv preprint arXiv:1805.12152 (2018).\n\nThank you!\nPaper 1584 Authors"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "BylRCJCM6m", "original": null, "number": 1, "cdate": 1541754837757, "ddate": null, "tcdate": 1541754837757, "tmdate": 1541754837757, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "H1gVYFVS3X", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "content": {"title": "Thank you for the questions! We have updated our paper and answered your questions below.", "comment": "Dear AnonReviewer3,\n\nThank you for your insightful questions. They are very helpful for us to improve the paper. We would like to answer your 4 questions as below.\n\na) We added more figures with k=10, 100, 1000 in the appendix (in main text, we used k=5). Our main conclusion does not change regardless the value of k: there is a strong correlation between attack success rate and the distance between test examples to training dataset. A larger distance usually implies a higher attack success rate. The rational to use this metric is that it is simple, and nearest neighbour based methods are usually robust to hyper-parameter selection. We don\u2019t want our observations depend on hyper-parameters during distance measurement.\n\nb) Song et al. (2018) does not have ordinary metrics like distortion or (ordinary) attack success rates to compare with. In their attack, the input is a random noise for GAN, and they generate  adversarial images from scratch. In typical adversarial attacks, people start from a specific reference (natural) image x and add adversarial distortion to obtain x_adv. In their paper, adversarial images are generated by GANs directly and there is no reference images at all, so distortion cannot be calculated (see definitions 1 and 2 in their paper). They have to conduct user study to determine what is the true class label for a generated image, and see if the model will misclassify it. The success rate is the model\u2019s misclassification rate from user study.\n\nIn our paper, our attacks first conduct slight transformations on a natural test image x to obtain x\u2019, and then run ordinary gradient based adversarial attacks on x\u2019 to obtain x\u2019_adv. We have a reference image x\u2019, so we can compute the distortion between x\u2019 and x\u2019_adv, and determine the success by a certain criterion on distortion. This setting is different from Song et al. (2018) so we cannot directly compare distortion and success rates with them.\n\nc) We want to emphasize that the \u201cblind-spot attack\u201d is a class of attacks, which exploits the gap between training and test data distributions (see our definition in Section 3.3). The linear transformation used in our paper is one of the simplest attacks in this class. If we know the details of this specific attack before training, it is possible defend against this specific simple attack. However, it is always possible to find some different blind-spot attacks (for example, by using a generative model). Rather than starting a new arm race between attacks and defenses, our argument here is to show the fundamental limitations of adversarial training -- it is hard to cover all the blind-spots during training time because it is impossible to eliminate the gap between training and test data especially when data dimension is high. \n\nd) The stricter criterion actually makes our attack success rates *lower* rather than higher. Finding adversarial examples with smaller distortions is harder than finding adversarial examples with large distortions. As an extreme case, if the criterion is distortion<=0, the attack success rate will always be zero, since we cannot fool the model using unmodified natural images. In Table 2, the success rates under the column 0.27 are strictly lower than the numbers under the column 0.3. We consider this additional stricter criterion because images after scaling are within a smaller range, so we also restrict the noise to be smaller, to keep the same signal-to-noise ratio and make an absolutely fair comparison. If we don\u2019t use this stricter criterion, our attack success rates will look even better.\n\n\nIn our updated revision, we also include additional experiments on GTS dataset, as long as two other state-of-the-art adversarial training methods by Wong et al. and Sinha et al.. We observe very similar results on all these methods and datasets, further confirming the conclusion of our paper.\n\nWe hope our answers resolve all the doubts you had with our paper. We would like to further discuss with you if you have any unclear things or additional questions, and hope you can reconsider the rating of our paper. \n\nThank you!\nPaper 1584 Authors\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621605874, "tddate": null, "super": null, "final": null, "reply": {"forum": "HylTBhA5tQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1584/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1584/Authors|ICLR.cc/2019/Conference/Paper1584/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers", "ICLR.cc/2019/Conference/Paper1584/Authors", "ICLR.cc/2019/Conference/Paper1584/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621605874}}}, {"id": "Hygsieas2X", "original": null, "number": 3, "cdate": 1541292194596, "ddate": null, "tcdate": 1541292194596, "tmdate": 1541533013720, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Review", "content": {"title": "An interesting paper analyzing the effect of the distance between training and test set on robustness of adversarial training", "review": "This paper provides some insights on influence of data distribution on robustness of adversarial training. The paper demonstrates through a number of analysis that the distance between the training an test data sets plays an important role on the effectiveness of adversarial training. To show the latter, the paper proposes an approach to measure the distance between the two data sets using combination of nonlinear projection (e.g. t-SNE), KDE, and K-L divergence. The paper also shows that under simple transformation to the test dataset (e.g. scaling), performance of adversarial training reduces significantly due to the large gap between training and test data set. This tends to impact high dimensional data sets more than low dimensional data sets since it is much harder to cover the whole ground truth data distribution in the training dataset.\n\nPros:\n- Provides insights on why adversarial training is less effective on some datasets.\n- Proposes a metric that seems to strongly correlate with the effectiveness of adversarial training.\n\nCons:\n- Lack of theoretical analysis. It could have been nice if the authors could show the observed phenomenon analytically on some simple distribution.\n- The marketing phrase \"the blind-spot attach\" falls short in delivering what one may expect from the paper after reading it. The paper would read much better if the authors better describe the phenomena based on the gap between the two distribution than using bling-spot. For some dataset, this is beyond a spot, it could actually be huge portion of the input space!\n\nMinor comments:\n- I believe one should not compare the distance shown between the left and right columns of Figure 3 as they are obtained from two different models. Though the paper is not suggesting that, it would help to clarify it in the paper. Furthermore, it would help if the paper elaborates why the distance between the test and training dataset is smaller in an adversarially trained network compared to a naturally trained network.\n- Are the results in Table 1 for an adversarially trained network or a naturally trained network? Either way, it could be also interesting to see the average K-L divergence between an adversarially and a naturally trained network on the same dataset.\n- Please provide more visualization similarly to those shown in Fig 4.\n", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Review", "cdate": 1542234198385, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977924, "tmdate": 1552335977924, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJe1e3jj3X", "original": null, "number": 2, "cdate": 1541286887357, "ddate": null, "tcdate": 1541286887357, "tmdate": 1541533013445, "tddate": null, "forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1584/Official_Review", "content": {"title": "Clear and simple idea, insightful experiments.", "review": "The paper is well written and the main contribution, a methodology to find \u201cblind-spot attacks\u201d well motivated and differences to prior work stated clearly.\n\nThe empirical results presented in Figure 1 and 2 are very convincing. The gain of using a sufficiently more complicated approach to assess the overall distance between the test and training dataset is not clear, comparing it to the very insightful histograms. Why for example not using a simple score based on the histogram, or even the mean distance? Of course providing a single measure would allow to leverage that information during training. However, in its current form this seems rather complicated and computationally expensive (KL-based). As stated later in the paper the histograms themselves are not informative enough to detect such blind-spot transformation. Intuitively this makes a lot of sense given that the distance is based on the network embedding and is therefore also susceptible to this kind of data. However, it is not further discussed how the overall KL-based data similarity measure would help in this case since it seems likely that it would also exhibit the same issue.\n", "rating": "7: Good paper, accept", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2019/Conference/Paper1584/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Limitations of Adversarial Training and the Blind-Spot Attack", "abstract": "The adversarial training procedure proposed by Madry et al. (2018) is one of the most effective methods to defend against adversarial examples in deep neural net- works (DNNs). In our paper, we shed some lights on the practicality and the hardness of adversarial training by showing that the effectiveness (robustness on test set) of adversarial training has a strong correlation with the distance between a test point and the manifold of training data embedded by the network. Test examples that are relatively far away from this manifold are more likely to be vulnerable to adversarial attacks. Consequentially, an adversarial training based defense is susceptible to a new class of attacks, the \u201cblind-spot attack\u201d, where the input images reside in \u201cblind-spots\u201d (low density regions) of the empirical distri- bution of training data but is still on the ground-truth data manifold. For MNIST, we found that these blind-spots can be easily found by simply scaling and shifting image pixel values. Most importantly, for large datasets with high dimensional and complex data manifold (CIFAR, ImageNet, etc), the existence of blind-spots in adversarial training makes defending on any valid test examples difficult due to the curse of dimensionality and the scarcity of training data. Additionally, we find that blind-spots also exist on provable defenses including (Kolter & Wong, 2018) and (Sinha et al., 2018) because these trainable robustness certificates can only be practically optimized on a limited set of training data.", "keywords": ["Adversarial Examples", "Adversarial Training", "Blind-Spot Attack"], "authorids": ["huan@huan-zhang.com", "chenhg@mit.edu", "zhaos@utexas.edu", "boning@mtl.mit.edu", "inderjit@cs.utexas.edu", "chohsieh@cs.ucla.edu"], "authors": ["Huan Zhang*", "Hongge Chen*", "Zhao Song", "Duane Boning", "Inderjit S. Dhillon", "Cho-Jui Hsieh"], "TL;DR": "We show that even the strongest adversarial training methods cannot defend against adversarial examples crafted on slightly scaled and shifted test images.", "pdf": "/pdf/8a378790c2b538af164f53d751d30d33811f0018.pdf", "paperhash": "zhang|the_limitations_of_adversarial_training_and_the_blindspot_attack", "_bibtex": "@inproceedings{\nzhang2018the,\ntitle={The Limitations of Adversarial Training and the Blind-Spot Attack},\nauthor={Huan Zhang and Hongge Chen and Zhao Song and Duane Boning and inderjit dhillon and Cho-Jui Hsieh},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HylTBhA5tQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1584/Official_Review", "cdate": 1542234198385, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HylTBhA5tQ", "replyto": "HylTBhA5tQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1584/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335977924, "tmdate": 1552335977924, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1584/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 15}