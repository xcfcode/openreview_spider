{"notes": [{"id": "SkeP3yBFDS", "original": "rkgja-1tvr", "number": 1955, "cdate": 1569439663026, "ddate": null, "tcdate": 1569439663026, "tmdate": 1577168213694, "tddate": null, "forum": "SkeP3yBFDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "BvbXerpLuY", "original": null, "number": 1, "cdate": 1576798736782, "ddate": null, "tcdate": 1576798736782, "tmdate": 1576800899562, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Decision", "content": {"decision": "Reject", "comment": "This paper introduces a new RNN architecture which uses a small network to decide which cells get updated at each time step, with the goal of reducing computational cost.  The idea makes sense, although it requires the use of a heuristic gradient estimator because of the non-differentiability of the update gate.\n\nThe main problem with this paper in my view is that the reduction in FLOPS was not demonstrated to correspond to a reduction in wallclock time, and I don't expect it would, since the sparse updates are different for each example in each batch, and only affect one hidden unit at a time.  The only discussion of this problem is \"we compute the FLOPs for each method as a surrogate for wall-clock time, which is hardware-dependent and often fluctuates dramatically in practice.\"  Because this method reduces predictive accuracy, the reduction in FLOPS should be worth it!\n\nMinor criticism:\n1) Figure 1 is confusing, showing not the proposed architecture in general but instead the connections remaining after computing the sparse updates.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795713140, "tmdate": 1576800262693, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Decision"}}}, {"id": "B1xwoW0njB", "original": null, "number": 4, "cdate": 1573867935089, "ddate": null, "tcdate": 1573867935089, "tmdate": 1573868312421, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "BklWehDioS", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment", "content": {"title": "thanks for changes", "comment": "Thanks for the additional clarification and experiment, it helped to contextualize the difficulty of the problem and value added of the method. I've revised my score accordingly."}, "signatures": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer2", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeP3yBFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1955/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1955/Authors|ICLR.cc/2020/Conference/Paper1955/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148435, "tmdate": 1576860561639, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment"}}}, {"id": "Byxm0iP5KH", "original": null, "number": 1, "cdate": 1571613643017, "ddate": null, "tcdate": 1571613643017, "tmdate": 1573867874640, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "Summary: This paper proposes selective activation RNN (SA-RNN), by using an update coordinator to determine which subset of the RNN\u2019s hidden state dimensions should be updated at a given timestep. The proposed loss term is then a sum of the original objective (e.g. classification) and a weighted sum of the probability that each dimension will be updated for each timestep. The method is evaluated on 3 time series datasets: Seizures, TwitterBuzz, Yahoo. \n\nDecision: Weak Reject. Although the authors tackle a challenging problem, their empirical results are lacking to provably demonstrate that their approach outperforms existing baselines.\n\nSupporting Arguments/Feedback: The authors compare SA-RNN to 5 baselines: random updates, clockwork RNN, phased LSTM, Skip RNN, and VC-GRU. Although I appreciated the authors\u2019 comparison across the suite of methods with respect to various metrics (e.g. # FLOPS, proportion of neurons that weren\u2019t updated, etc.), the experiments were conducted on datasets that were relatively simple. For example, in prior work, the empirical evaluations were on much larger-scale datasets such as Wikipedia [Shen et. al 2019], real clinical data sources [Liu et. al 2018], and Charades videos [Campos et. al 2018], among others. I would be very interested to see how this training procedure fairs when evaluated on much more complex tasks, and would make the results about computational speedups at train/test time much more convincing.\n\nQuestions:\n- I\u2019m curious if you tried different types of gradient estimators to get around the non-differentiability rather than the straight-through estimator. Also how was the slope-annealing conducted (e.g. annealing schedule)?\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575786219878, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1955/Reviewers"], "noninvitees": [], "tcdate": 1570237729867, "tmdate": 1575786219893, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Official_Review"}}}, {"id": "BklWehDioS", "original": null, "number": 3, "cdate": 1573776361492, "ddate": null, "tcdate": 1573776361492, "tmdate": 1573776361492, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "Byxm0iP5KH", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment", "content": {"title": "Reply to reviewer 2 - Thank you for thoughtful review", "comment": "Thank you very much for your constructive review. \n\nPer your suggestion, we have run further experiments on more complicated data that we hope will convince you of both the merit of our proposed method along with this line of research with respect to previous approaches. We revised the paper, adding a new section to the beginning of the appendix that describes the \u201cAdding\u201d task, which asks a network to learn to sum two values in a long sequence of sampled values given a mask indicating the indices to be summed, along with our results. We invite you to take a look at the new section but we highlight our main findings as follows:\n\nTo add a new perspective on complexity in our experiments we tested long-term dependencies as part of this new experiment, following the lead of the literature [1-3]. As shown in our Appendix, we found that even in the presence of extremely long-term dependencies (up to 500 timesteps), our proposed SA-RNN solves the task perfectly with a very low number of FLOPs and very few state updates compared to the other data-reactive method SkipRNN. As expected, the standard GRU also solves the task while Random Skips does not.\n\nRegarding your Gradient Estimation and Slope-Annealing question:\nIn our experiments, we used the straight-through estimator to be comparable to the literature, including [3] and [4].  Plus, we were pleased to have achieved our empirically-good results with these basic settings. However, we agree that this is an interesting question, as it is unlikely that the same estimator is the absolute best for all possible tasks. Thus, there is potentially room for further tuning by designing update pattern-specific gradient estimation. For slope-annealing we used the parameters and setting as described in [4], gradually increasing the slope of the hard sigma function as the model trains, starting at slope $\\alpha=1$ and increasing according to the schedule $a = \\min(5, 1+0.04*N_{epoch})$. We have added information about both of these settings to our experimental description in our paper to assure full reproducibility.\n\n[1] Henaff, M., Szlam, A., LeCun, Y. \u201cRecurrent Orthogonal Networks and Long-Memory Tasks\u201d, ICML 2015.\n[2] Neil, D., Pfeiffer, M., Liu, S.-C., \u201cPhased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences\u201d, NeurIPS 2016.\n[3] Campos, V., Jou, B., Giro-i-Nieto, X., Torres, J., Chang, S.-F. \u201cSkipRNN: Learning to Skip State Updates in Recurrent Neural Networks\u201d, ICLR 2018.\n[4] Chung, J., Ahn, S., Bengio, Y. \u201cHierarchical Multiscale Recurrent Neural Networks\u201d, ICLR 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper1955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeP3yBFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1955/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1955/Authors|ICLR.cc/2020/Conference/Paper1955/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148435, "tmdate": 1576860561639, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment"}}}, {"id": "rJgFtz44iB", "original": null, "number": 2, "cdate": 1573302913028, "ddate": null, "tcdate": 1573302913028, "tmdate": 1573302913028, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "BJecY0BnFr", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment", "content": {"title": "Response to Reviewer 3 - Thank you for positive feedback + expression improvement", "comment": "We thank you for your time and effort in reviewing our work and your positive response to our proposed method and experimental results, especially given your expertise in the area.\n\nConcerning your suggestions to improve the presentation itself, we have undertaken a careful round of proof-reading to improve the readability of the manuscript. In addition, we had a colleague in the English department provide additional editing suggestions. We have now uploaded a new version of the paper addressing both your specific edits as well as this general round of proof-reading. We invite you to take a look at the revised presentation.\n\nTiming comparisons: Metrics such as wall-clock time greatly depend on factors outside the model such as implementation strategy, machine learning framework, and hardware specifics. To target the methodological differences between our compared methods, we compute and report the FLOPs instead. This is independent of hardware and implementation. Instead, it directly compares the computational requirements of the update-mechanisms, as described in [1] for a fairer comparison.\n\n[1] Campos et. al, \u201cSkipRNN: Learning to Skip State Updates in Recurrent Neural Networks\u201d, ICLR 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeP3yBFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1955/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1955/Authors|ICLR.cc/2020/Conference/Paper1955/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148435, "tmdate": 1576860561639, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment"}}}, {"id": "B1lnoIMQiB", "original": null, "number": 1, "cdate": 1573230243609, "ddate": null, "tcdate": 1573230243609, "tmdate": 1573230243609, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "S1xB6zspFr", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment", "content": {"title": "Respose to Reviewer 1 - Thank you for positive feedback.", "comment": "We greatly appreciate your positive feedback on our method, presentation, and experimental results, especially given your expertise in the area."}, "signatures": ["ICLR.cc/2020/Conference/Paper1955/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkeP3yBFDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1955/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1955/Authors|ICLR.cc/2020/Conference/Paper1955/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504148435, "tmdate": 1576860561639, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1955/Authors", "ICLR.cc/2020/Conference/Paper1955/Reviewers", "ICLR.cc/2020/Conference/Paper1955/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Official_Comment"}}}, {"id": "BJecY0BnFr", "original": null, "number": 2, "cdate": 1571737218131, "ddate": null, "tcdate": 1571737218131, "tmdate": 1572972401933, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This paper attempts to reduce computation in recurrent neural networks. Instead of artificially determining the update pattern for updating the states, the authors propose SA-RNN to predict discrete update patterns automatically through optimization driven entirely by the input data. Experiments on publicly-available datasets show that the proposed method has competitive performance with even fewer updates.\n\nPros:\nOverall, I think the idea of this paper is clear and the whole paper is easy to follow. The experiments clearly show the advantage of the proposed method claimed by the authors.\n\nCons:\n1.\tSome expressions need to be improved. For example, in \u201cThis way, representations can be learned while solving a sequential learning task while minimizing the number of updates, subsequently reducing compute time.\u201d two \u201cwhile\u201ds are not elegant and there should be an \u201cIn\u201d before \u201cthis way\u201d. In \u201cWe augment an RNN with an update coordinator that adaptively controls the coordinate directions in which to update the hidden state on the fly\u201d, the usage of \u201cin which to\u201d is not right. I suggest the authors to thoroughly proofread the whole paper and improve the presentation.\n2.\tSince this paper focuses on the efficiency of RNN, I suggest the authors could provide the time complexity comparisons. Merely the comparisons on skip of neurons cannot show the advantage on the efficiency.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575786219878, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1955/Reviewers"], "noninvitees": [], "tcdate": 1570237729867, "tmdate": 1575786219893, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Official_Review"}}}, {"id": "S1xB6zspFr", "original": null, "number": 3, "cdate": 1571824317427, "ddate": null, "tcdate": 1571824317427, "tmdate": 1572972401896, "tddate": null, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "invitation": "ICLR.cc/2020/Conference/Paper1955/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "A main problem with RNN is to update all hidden dimensions in each time step. The authors proposed selective-activation RNN (SA-RNN), which modifies each state of RNN by adding an update coordinator which is modeled as a lightweight neural network. The coordinator, based on the incoming data, makes a discrete decision to update or not update each individual hidden dimension. A multi-objective optimization problem is defined to both solving a sequential learning task and minimizing the number of updates in each time step. The authors evaluated their networks on three public benchmark datasets and achieved good results compared to the state-of-the-art ones.\nThe papers is well-written. The idea proposed in this paper is interesting and it is presented very well. There is also an extensive evaluation.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1955/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Reducing Computation in Recurrent Networks by Selectively Updating State Neurons", "authors": ["Thomas Hartvigsen", "Cansu Sen", "Xiangnan Kong", "Elke Rundensteiner"], "authorids": ["twhartvigsen@wpi.edu", "csen@wpi.edu", "xkong@wpi.edu", "rundenst@wpi.edu"], "keywords": ["recurrent neural networks", "conditional computation", "representation learning"], "TL;DR": "We show that conditionally computing individual dimensions of an RNN's hidden state depending on input data at each time step from scratch with no assumptions leads to higher accuracy with far fewer computations than state-of-the-art approach.", "abstract": "Recurrent Neural Networks (RNN) are the state-of-the-art approach to sequential learning. However, standard RNNs use the same amount of computation at each timestep, regardless of the input data. As a result, even for high-dimensional hidden states, all dimensions are updated at each timestep regardless of the recurrent memory cell. Reducing this rigid assumption could allow for models with large hidden states to perform inference more quickly. Intuitively, not all hidden state dimensions need to be recomputed from scratch at each timestep. Thus, recent methods have begun studying this problem by imposing mainly a priori-determined patterns for updating the state. In contrast, we now design a fully-learned approach, SA-RNN, that augments any RNN by predicting discrete update patterns at the fine granularity of independent hidden state dimensions through the parameterization of a distribution of update-likelihoods driven entirely by the input data. We achieve this without imposing assumptions on the structure of the update pattern. Better yet, our method adapts the update patterns online, allowing different dimensions to be updated conditional to the input. To learn which to update, the model solves a multi-objective optimization problem, maximizing accuracy while minimizing the number of updates based on a unified control. Using publicly-available datasets we demonstrate that our method consistently achieves higher accuracy with fewer updates compared to state-of-the-art alternatives. Additionally, our method can be directly applied to a wide variety of models containing RNN architectures.", "pdf": "/pdf/08cf4380ea9c107d75980378e8b7059b3bb275e3.pdf", "paperhash": "hartvigsen|reducing_computation_in_recurrent_networks_by_selectively_updating_state_neurons", "original_pdf": "/attachment/e603c75fed6145b5c36452d5b51a6ff32dd4c86d.pdf", "_bibtex": "@misc{\nhartvigsen2020reducing,\ntitle={Reducing Computation in Recurrent Networks by Selectively Updating State Neurons},\nauthor={Thomas Hartvigsen and Cansu Sen and Xiangnan Kong and Elke Rundensteiner},\nyear={2020},\nurl={https://openreview.net/forum?id=SkeP3yBFDS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkeP3yBFDS", "replyto": "SkeP3yBFDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1955/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575786219878, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1955/Reviewers"], "noninvitees": [], "tcdate": 1570237729867, "tmdate": 1575786219893, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1955/-/Official_Review"}}}], "count": 9}