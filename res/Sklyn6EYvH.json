{"notes": [{"id": "Sklyn6EYvH", "original": "BJeDco1OvB", "number": 767, "cdate": 1569439143429, "ddate": null, "tcdate": 1569439143429, "tmdate": 1577168271285, "tddate": null, "forum": "Sklyn6EYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 7, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "jbc0bHlS-R", "original": null, "number": 1, "cdate": 1576798705485, "ddate": null, "tcdate": 1576798705485, "tmdate": 1576800930636, "tddate": null, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "invitation": "ICLR.cc/2020/Conference/Paper767/-/Decision", "content": {"decision": "Reject", "comment": "This paper that defines a \u201cResidual learning\u201d mechanism as the training regime for variational autoencoder. The method gradually activates individual latent variables to reconstruct residuals.\n\nThere are two main concerns from the reviewers. First, residual learning is a common trick now, hence authors should provide insights on why residual learning works for VAE. The other problem is computational complexity. Currently, reviews argue that it seems not really fair to compare to a bruteforce parameter search. The authors\u2019 rebuttal partially addresses these problems but meet the standard of the reviewers.\n\nBased on the reviewers\u2019 comments, I choose to reject the paper.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795728579, "tmdate": 1576800281012, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper767/-/Decision"}}}, {"id": "HJgdDxAatS", "original": null, "number": 2, "cdate": 1571836000306, "ddate": null, "tcdate": 1571836000306, "tmdate": 1574244595317, "tddate": null, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "invitation": "ICLR.cc/2020/Conference/Paper767/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "Overview:\nAuthors introduce a new VAE-based method for learning disentangled representations.\nThe main idea is to apply a \u201cresidual learning mechanism\u201d, which resembles an autoregressive model, but here conditioning between sequential steps is done in both latent and input spaces. Namely, for each input, the method involves making several (one per *each* embedding dimension) sequential passes, where each pass takes the residual between the input and the current accumulated output (i.e. pixel-wise difference between two images) as well as the values of the sampled embeddings so far. \nAuthors report quantitative and qualitative evaluation with a set of reasonable baselines (FactorVAE, beta-VAE), and the results seem to be slightly better.\n\nDecision:\nThe writing quality is not great: there are frequent typos and not-so-precise formulations, and is at times hard to follow (see list below).\nThe method itself is not really well-motivated: there seems to be no formal justification provided, and the informal one is not very clearly explained in the paper. \nScalability of the method is clearly an issue. Many applications might require the size of embeddings to have hundreds of dimensions, which would mean that the given method cannot really be applied for non-toy problems. \nWith all those considerations in mind, I cannot recommend to accept this paper as is, thus the final rating, \u201creject\u201d.\n\nAdditional comments / typos:\n* It would be worth seeing how this approach relates to auto-regressive models.\n* p2. \u201cencoder maps \u2026 x to a latent representation q\u201d - this sentence is not very strict. \n* p2. \u201c.. is defined as following\u201d: as follows?\n* p2. \u201c.. in addition of b\u201d\n* p3. \u201c.. this reguralizer encourage\u201d\n* p7: \u201c\u2026 it reduces the solution space and improve training stability\u201c: is there any sort of theoretical reasoning that could support this?\n* The reference format is not very readable, probably better to change to (Names, Year).\n\n<update>\nAuthors addressed some of my concerns (thanks!), thus I increased my rating slightly.\nThere is still a large concern wrt to the computational complexity. I kind of understand the argument about the hyperparameter tuning, but it seems not really fair to compare to a bruteforce parameter search (one can probably do some sort of bayesian optimization).\n</update>\n\n\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper767/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper767/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575656753230, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper767/Reviewers"], "noninvitees": [], "tcdate": 1570237747388, "tmdate": 1575656753243, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper767/-/Official_Review"}}}, {"id": "rJxUWI5xsB", "original": null, "number": 3, "cdate": 1573066238178, "ddate": null, "tcdate": 1573066238178, "tmdate": 1573066238178, "tddate": null, "forum": "Sklyn6EYvH", "replyto": "rylCNq2htS", "invitation": "ICLR.cc/2020/Conference/Paper767/-/Official_Comment", "content": {"title": "Authors Response to Review #2", "comment": "We understand the challenges of reviewing large volume of ICLR submissions and greatly appreciate the reviewer's time and effort to review our submission. \n\nWe thank the reviewer for the positive feedback and acknowledging the novelty of the proposed approach to address an important yet challenging task of disentangled representation learning.  \n\nThe computation complexity of SR-VAE in a single training process is longer comparing to other approaches. However, as responded to another reviewer, the current state-of-art approaches relies heavily on hyperparameter tuning that requires hundreds runs, potential years of GPU times to figure out the optimal solution. The proposed approach eliminate this search process, thus reduce the total computation. More importantly, the lack of quantitative metric for unsupervised disentanglement learning makes it hard to understand the effect of hyperparameter tuning (the current common evaluation for unsupervised disentanglement learning is human visual inspection). We believe the proposed approach significantly reduce this effort by eliminating the hyperparameter search.   \n\nAs the reviewer suggested, we added bracket of the reference to increase the readability. "}, "signatures": ["ICLR.cc/2020/Conference/Paper767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sklyn6EYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference/Paper767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper767/Reviewers", "ICLR.cc/2020/Conference/Paper767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper767/Authors|ICLR.cc/2020/Conference/Paper767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166533, "tmdate": 1576860539181, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference/Paper767/Reviewers", "ICLR.cc/2020/Conference/Paper767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper767/-/Official_Comment"}}}, {"id": "HJggpH9xiH", "original": null, "number": 2, "cdate": 1573066168261, "ddate": null, "tcdate": 1573066168261, "tmdate": 1573066168261, "tddate": null, "forum": "Sklyn6EYvH", "replyto": "HJgdDxAatS", "invitation": "ICLR.cc/2020/Conference/Paper767/-/Official_Comment", "content": {"title": "Authors Response to Review #3", "comment": "We understand the challenges of reviewing large volume of ICLR submissions and greatly appreciate the reviewer's time and effort to review our submission. \n\nMotivation ---> We aimed to motivate our study in Section 2 by discussing the issues of disentanglement learning with VAE. The formal justification of these issues was carefully discussed in theory among the cited prior works including: Section 2 of \u201cDisentangling by Factorising\u201d [Kim & Mnih (2018)] and Section 4 of \u201cUnderstanding disentangling in \u03b2-VAE\u201d [Burgess et al.(2017)]. In addition, we would like to clarify the main issue of learning of disentangled representation with VAE lies in the trade-off between the information bottleneck and the input reconstruction. Current approaches use augmented objective to address this problem. We propose to address this issue with a different training regime. We\u2019ve revised the manuscript in section 2 to cite the formal justification more clearly.  \n\nComputational Complexity ---> We agree with reviewer that the computation complexity of a single training process is longer comparing to other approaches. However, as discussed in Section 3 of our paper and highlighted in [Locatello et al, 2018], the current state-of-art approaches relies heavily on hyperparameter tuning that requires hundreds runs, potential years of GPU times to figure out the optimal solution. The proposed approach eliminates this search process, thus reduces the total computation. More importantly, the lack of quantitative metric for unsupervised disentanglement learning makes it hard to understand the effect of hyperparameter tuning (the current common evaluation is human visual inspection). We believe the proposed approach significantly reduces this effort by eliminating the hyperparameter search.   \nRegarding the comments \u201cMany applications might require the size of embeddings to have hundreds of dimensions\u201d, we agree that different applications require different latent space dimension. However, the idea of disentangled representation is to search for a compact (low dimension) representation space for real-world objects. This resembles human\u2019s capability to understand an object in a concise manner. For applications that involves complicated components, many recent work (e.g. Multi-Object Representation Learning with Iterative Variational Inference,  MONet: Unsupervised Scene Decomposition and Representation, GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations) focus on decomposing the scene into different components/objects/parts where each individual component can be modeled by a low-dimension disentangled representation. \n\nAdditional comments / typos --->\n* It would be worth seeing how this approach relates to auto-regressive models. \nThe proposed SR-VAE does resemble the non-linear autoregressive model for inference with the latent representation. It can be understood as the future z values are predicted based on the past z values along with the accumulated reconstruction. We have added this discussion to the related work section in the paper. \n\n* p2. \u201cencoder maps \u2026 x to a latent representation q\u201d - this sentence is not very strict.  \nIt would be great if the reviewer could provide details on the confusion and we are happy to address it accordingly.   \n\n* p7: \u201c\u2026 it reduces the solution space and improve training stability\u201c: is there any sort of theoretical reasoning that could support this? \nIn general limiting the search space does not necessarily improve the optimization results unless better local optimal solutions are in the constrained space. Since the optimal solution of the original VAE or beta-VAE satisfies the dependency constraints introduced in SR-VAE, we believe SR-VAE could help with reaching a better local optimal solution. Empirically, we observed that SR-VAE improves training stability by comparing the worst 10 runs out of 30 for 2D Shape in Fig 2(j)(k). We consistently observe better performance and smaller variances by SR-VAE. \n\nWe thank the reviewer for pointing out the typos and grammar errors. We have corrected these mistakes and carefully proof-read the paper. As another reviewer suggested, we also added bracket of the reference to increase the readability. "}, "signatures": ["ICLR.cc/2020/Conference/Paper767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sklyn6EYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference/Paper767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper767/Reviewers", "ICLR.cc/2020/Conference/Paper767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper767/Authors|ICLR.cc/2020/Conference/Paper767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166533, "tmdate": 1576860539181, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference/Paper767/Reviewers", "ICLR.cc/2020/Conference/Paper767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper767/-/Official_Comment"}}}, {"id": "Hkx6kS9gjr", "original": null, "number": 1, "cdate": 1573065957140, "ddate": null, "tcdate": 1573065957140, "tmdate": 1573065957140, "tddate": null, "forum": "Sklyn6EYvH", "replyto": "SyxsGxuG5r", "invitation": "ICLR.cc/2020/Conference/Paper767/-/Official_Comment", "content": {"title": "Authors Response to Review #1", "comment": "We understand the challenges of reviewing large volume of ICLR submissions and greatly appreciate the reviewer's time and effort to review our submission. \n\n--- Thank you for pointing out the reference paper which improves the image generation with Laplacian pyramid in GAN. As a general learning method, residual learning has been applied to different domains ranging from discriminative models (ResNet), generative model (GAN), and reinforcement learning (relatively recent Residual RL). While the suggested reference by the reviewer carries the idea of iterative learning process, we believe the closest study to our work is the framework of \u201cDRAW\u201d [Gregor et al. (2015)]. Notice that \"DRAW\" is also cited in the recommended paper. We have discussed in details the connection between our work and \"DRAW\" in section 4.  \n\n--- We are glad that the reviewer is familiar with the work by Locatello et. al. in ICML 2019.  This work is heavily cited in our manuscript with its earlier arxiv version ([Locatello et al, 2018] in our paper). One of the key insights in this paper is: \u201ctheoretically prove that (perhaps unsurprisingly) the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases both on the considered learning approaches and the data sets.\u201d. This is indeed the motivation of our work. The proposed \u201cResidual learning\u201d forward pass serves as an inductive bias on the model to facilitate disentangled learning in VAE. It combines two important components of: 1) explicit dependency and 2) decomposition of reconstruction, to address the information bottleneck of disentanglement learning with VAE framework. To reduce the confusion, we updated our citation for  the work of Locatello et al from the arxiv version to the ICML version. We also modified Section 2 to emphasize this point. "}, "signatures": ["ICLR.cc/2020/Conference/Paper767/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Sklyn6EYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference/Paper767/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper767/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper767/Reviewers", "ICLR.cc/2020/Conference/Paper767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper767/Authors|ICLR.cc/2020/Conference/Paper767/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504166533, "tmdate": 1576860539181, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper767/Authors", "ICLR.cc/2020/Conference/Paper767/Reviewers", "ICLR.cc/2020/Conference/Paper767/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper767/-/Official_Comment"}}}, {"id": "rylCNq2htS", "original": null, "number": 1, "cdate": 1571764790079, "ddate": null, "tcdate": 1571764790079, "tmdate": 1572972554824, "tddate": null, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "invitation": "ICLR.cc/2020/Conference/Paper767/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "The authors of this paper present a novel that for unsupervised disentangled representation learning. The model, named sequential residual VAE (SR-VAE), gradually activates individual latent variables to reconstruct residuals. Quantitative and qualitative experiments show that the proposed model outperforms beta-VAE and Factor-VAE.  Since the training involves a sequence of model training,  SR-VAE certainly consumes more time than other VAEs. Minors: citations in the main text should be put in brackets. "}, "signatures": ["ICLR.cc/2020/Conference/Paper767/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper767/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575656753230, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper767/Reviewers"], "noninvitees": [], "tcdate": 1570237747388, "tmdate": 1575656753243, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper767/-/Official_Review"}}}, {"id": "SyxsGxuG5r", "original": null, "number": 3, "cdate": 1572139027509, "ddate": null, "tcdate": 1572139027509, "tmdate": 1572972554736, "tddate": null, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "invitation": "ICLR.cc/2020/Conference/Paper767/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The authors applied residual learning machenism in VAE learning, which I have seen such methods in Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks (https://arxiv.org/abs/1506.05751), which basically applied the residual learning method in GAN. But the authors fail to discuss the relationship and difference with this paper.\nAlso, the best paper from ICML 2019 claimed that unsupervised learning method can not really disentangle the features. They claim \\beta-VAE, factor VAE is not good. The authors shall all discuess this point. Otherwise, it is not convincing to the readers.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper767/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper767/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Disentangled Representation Learning with Sequential Residual Variational Autoencoder", "authors": ["Nanxiang Li", "Shabnam Ghaffarzadegan", "Liu Ren"], "authorids": ["nanxiang.li@us.bosch.com", "shabnam.ghaffarzadegan@us.bosch.com", "liu.ren@us.bosch.com"], "keywords": ["Disentangled Representation Learning", "Variational Autoencoder", "Residual Learning"], "abstract": "Recent advancements in unsupervised disentangled representation learning focus on extending the variational autoencoder (VAE) with an augmented objective function to balance the trade-off between disentanglement and reconstruction. We propose Sequential Residual Variational Autoencoder (SR-VAE) that defines a \"Residual learning\" mechanism as the training regime instead of the augmented objective function. Our proposed solution deploys two important ideas in a single framework: (1) learning from the residual between the input data and the accumulated reconstruction of sequentially added latent variables; (2) decomposing the reconstruction into decoder output and a residual term. This formulation encourages the disentanglement in the latent space by inducing explicit dependency structure, and reduces the bottleneck of VAE by adding the residual term to facilitate reconstruction. More importantly, SR-VAE eliminates the hyperparameter tuning, a crucial step for the prior state-of-the-art  performance using the objective function augmentation approach.  We demonstrate both qualitatively and quantitatively that SR-VAE improves the state-of-the-art  unsupervised disentangled representation learning on a variety of complex datasets.", "pdf": "/pdf/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "code": "https://www.dropbox.com/s/5hkfn8xy5r8w5sz/Code.zip?dl=0", "paperhash": "li|disentangled_representation_learning_with_sequential_residual_variational_autoencoder", "original_pdf": "/attachment/91a50a320b87bb6064a5e9b1082596e94d045440.pdf", "_bibtex": "@misc{\nli2020disentangled,\ntitle={Disentangled Representation Learning with Sequential Residual Variational Autoencoder},\nauthor={Nanxiang Li and Shabnam Ghaffarzadegan and Liu Ren},\nyear={2020},\nurl={https://openreview.net/forum?id=Sklyn6EYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Sklyn6EYvH", "replyto": "Sklyn6EYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper767/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575656753230, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper767/Reviewers"], "noninvitees": [], "tcdate": 1570237747388, "tmdate": 1575656753243, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper767/-/Official_Review"}}}], "count": 8}