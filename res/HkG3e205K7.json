{"notes": [{"id": "r1lc8Cd-ZN", "original": null, "number": 8, "cdate": 1545862737798, "ddate": null, "tcdate": 1545862737798, "tmdate": 1545862737798, "tddate": null, "forum": "HkG3e205K7", "replyto": "Bye_rmJWlV", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "content": {"title": "RE:", "comment": "Thank you for pointing out this source of confusion. Indeed, the identity has been used extensively in previous publications and we are not claiming it as an original contribution of the paper. We introduce the identity in Eq. 5 as a \"well-known equivalence\".  As the AC notes, our contribution is in the application of the identity, the experimental evaluation, and the theoretical asymptotic analysis."}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkG3e205K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1114/Authors|ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618692}}}, {"id": "HJe9Sh_--N", "original": null, "number": 7, "cdate": 1545862210457, "ddate": null, "tcdate": 1545862210457, "tmdate": 1545862233637, "tddate": null, "forum": "HkG3e205K7", "replyto": "rJlMEiAnam", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "content": {"title": "Camera ready update", "comment": "We have uploaded the final version with a link to the source code used for the experiments ( https://sites.google.com/view/dregs )."}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkG3e205K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1114/Authors|ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618692}}}, {"id": "HkG3e205K7", "original": "HyxmiuYYY7", "number": 1114, "cdate": 1538087924006, "ddate": null, "tcdate": 1538087924006, "tmdate": 1545862077373, "tddate": null, "forum": "HkG3e205K7", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "Bye_rmJWlV", "original": null, "number": 1, "cdate": 1544774463663, "ddate": null, "tcdate": 1544774463663, "tmdate": 1545354522428, "tddate": null, "forum": "HkG3e205K7", "replyto": "HkG3e205K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Meta_Review", "content": {"metareview": "The paper is well written and easy to follow. The experiments are adequate to justify the usefulness of an identity for improving existing multi-Monte-Carlo-sample based gradient estimators for deep generative models. The originality and significance are acceptable, as discussed below.\n\nThe proposed doubly reparameterized gradient estimators are built on an important identity shown in Equation (5). This identity appears straightforward to derive by applying both score-function gradient and reparameterization gradient to the same objective function, which is expressed as an expectation. The AC suspects that this identity might have already appeared in previous publications / implementations, though not being claimed as an important contribution / being explicitly discussed. While that identity may not be claimed as the original contribution of the paper if that suspicion is true, the paper makes another useful contribution in applying that identity to the right problem: improving three distinct training algorithms for deep generative models. The doubly reparameterized versions of IWAE and reweighted wake-sleep (RWS) further show how IWAE and RWS are related to each other and how they can be combined for potentially further improved performance. \n\nThe AC believes that the paper makes enough contributions by well presenting the identity in (5) and applying it to the right problems. ", "confidence": "5: The area chair is absolutely certain", "recommendation": "Accept (Poster)", "title": "An useful identity that helps improve existing training algorithms for deep generative models"}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1114/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352961201, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkG3e205K7", "replyto": "HkG3e205K7", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352961201}}}, {"id": "rJlMEiAnam", "original": null, "number": 5, "cdate": 1542413098280, "ddate": null, "tcdate": 1542413098280, "tmdate": 1542413098280, "tddate": null, "forum": "HkG3e205K7", "replyto": "HkG3e205K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "content": {"title": "Updated manuscript", "comment": "We have updated the manuscript based on reviewer feedback. Apart from clarifying edits, we have rewritten the derivation in Appendix 8.1 and included a plot of variance for several values of K as Appendix Figure 8."}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkG3e205K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1114/Authors|ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618692}}}, {"id": "rygSDcRhaQ", "original": null, "number": 4, "cdate": 1542412892702, "ddate": null, "tcdate": 1542412892702, "tmdate": 1542412892702, "tddate": null, "forum": "HkG3e205K7", "replyto": "S1gF1q36j7", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "content": {"title": "Author response", "comment": "Recent work on reparameterizing mixture distributions has shown that the necessary gradients can be computed with the implicit reparameterization trick (Graves 2016, Jankowiak & Obermeyer 2018; Jankowiak & Karaletsos 2018; Figurnov et al. 2018).  Using this approach to reparameterize the mixture, DReGs readily apply when q is a Gaussian mixture model. We mention this explicitly in the text now.\n\nEq. 6 explicitly characterizes the bias in STL. There is no reason to believe this term analytically vanishes, and we confirm numerically that it is non-zero in the toy Gaussian example. We believe this is sufficient to support our claim of bias.\n\nWe present the K ELBO results in these plots to be consistent with previous work (Rainforth et al. 2018). We agree that it can be misleading for the reasons you indicated, so we now explicitly call this out in the maintext.\n\nYes, the color assignment is the same. We note this in the caption for both figures now."}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkG3e205K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1114/Authors|ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618692}}}, {"id": "SJluWc0npX", "original": null, "number": 3, "cdate": 1542412799600, "ddate": null, "tcdate": 1542412799600, "tmdate": 1542412799600, "tddate": null, "forum": "HkG3e205K7", "replyto": "Hyle7iIF27", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for the helpful suggestions.\n\n1. Thank you for pointing out this source of confusion. The correctness of the proof is related to the fact that \\frac{\\partial}{\\partial \\phi} g(\\phi, \\tilde{\\phi}) |_{\\tilde{\\phi} = \\phi} != \\frac{\\partial}{\\partial \\phi} g(\\phi, \\phi). On the left hand side the derivative is taken first, which results in a function of \\phi and \\tilde{\\phi}, which we then evaluate. As you note, this is not equivalent to setting \\tilde{\\phi} = \\phi, and then taking the derivative. We want the former. Following your suggestion, we have completely rewritten the proof to avoid this confusing step.\n\n2. We used the trace of the Covariance matrix (normalized by the number of parameters) to summarize the variance, and we implemented this by maintaining exponential moving average statistics. SNR was computed as the mean of the estimator divided by the standard deviation (as in Rainforth et al. 2018). We added this information as footnotes in the maintext.\n\n3. We have added a plot of the variance of the gradient estimator as K changes (Appendix Fig. 8). We found that as K increases, for IWAE and JVI, the variance of the doubly reparameterized gradient estimator slowly decreases relative to the variance of the original gradient estimator. On the other hand for RWS, we found that as K increases, the variance of the doubly reparameterized gradient estimator gradually increases relative to the variance of the original gradient estimator. However, we emphasize that in all cases, the variance of the doubly reparameterized gradient estimator was less than the variance of the original gradient estimator.\n\n4. Yes, intuitively, the right hand side directly takes advantage of the gradient of f whereas the left hand side ends up computing something akin to finite differences. We have added a sentence explaining this intuition in the maintext.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkG3e205K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1114/Authors|ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618692}}}, {"id": "HyelqKC2pX", "original": null, "number": 2, "cdate": 1542412679758, "ddate": null, "tcdate": 1542412679758, "tmdate": 1542412679758, "tddate": null, "forum": "HkG3e205K7", "replyto": "Hkx5PKC3hX", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "content": {"title": "Author response", "comment": "Thank you for checking the derivations. We appreciate the positive comments."}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621618692, "tddate": null, "super": null, "final": null, "reply": {"forum": "HkG3e205K7", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1114/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1114/Authors|ICLR.cc/2019/Conference/Paper1114/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers", "ICLR.cc/2019/Conference/Paper1114/Authors", "ICLR.cc/2019/Conference/Paper1114/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621618692}}}, {"id": "Hkx5PKC3hX", "original": null, "number": 3, "cdate": 1541364066387, "ddate": null, "tcdate": 1541364066387, "tmdate": 1541533410434, "tddate": null, "forum": "HkG3e205K7", "replyto": "HkG3e205K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Review", "content": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "review": "This paper applies a reparameterization trick to estimate the gradients objectives encountered in variational autoencoder based frameworks with continuous latent variables.  Especially the authors use this double reparameterization trick on Importance Weighted Auto-Encoder (IWAE) and Reweighted Wake-Sleep (RWS)  methods. Compared to IWAE, the developed method's SNR does not go to zero with increasing the number of particles.\n\nOverall, I think the idea is nice and the results are encouraging. I checked all the derivations, and they seem to be correct. Thus I recommend this paper to be accepted in its current form.", "rating": "7: Good paper, accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Review", "cdate": 1542234302937, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkG3e205K7", "replyto": "HkG3e205K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875697, "tmdate": 1552335875697, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "Hyle7iIF27", "original": null, "number": 2, "cdate": 1541135128060, "ddate": null, "tcdate": 1541135128060, "tmdate": 1541533410228, "tddate": null, "forum": "HkG3e205K7", "replyto": "HkG3e205K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Review", "content": {"title": "Good paper", "review": "The paper observes the gradient of multiple objective such as IWAE, RWS, JVI are in the form of some \u201creward\u201d multiplied with score function which can be calculated with one more reparameterization step to reduce the variance. The whole paper is written in a clean way and the method is effective.\n\nI have following comments/questions:\n\n1. The conclusion in Eq(5) is correct but the derivation in Sec. 8.1. may be arguable. Writing \\phi and \\tilde{\\phi} at the first place sets the partial derivative of \\tilde{\\phi}  to \\phi as 0. But the choice of \\tilde{\\phi} in the end is chosen as \\phi. If plugging  \\phi to \\tilde{\\phi}, the derivation will change. The better way may be calculating both the reparameterization and reinforce gradient without redefining a \\tilde{\\phi}.\n\n2. How does the variance of gradient calculated where the gradient is a vector? And how does the SNR defined in the experiments?\n\n3. How does the variance reduction from DReG changes with different value of K?\n\n4. Is there any more detailed analysis or intuition why the right hand side of Eq(5) has lower variance than the left hand side?", "rating": "7: Good paper, accept", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Review", "cdate": 1542234302937, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkG3e205K7", "replyto": "HkG3e205K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875697, "tmdate": 1552335875697, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "S1gF1q36j7", "original": null, "number": 1, "cdate": 1540372960534, "ddate": null, "tcdate": 1540372960534, "tmdate": 1541533409966, "tddate": null, "forum": "HkG3e205K7", "replyto": "HkG3e205K7", "invitation": "ICLR.cc/2019/Conference/-/Paper1114/Official_Review", "content": {"title": "Proposed method is interesting but additional experiments are needed", "review": "Overall:\nThis paper works on improving the gradient estimator of the ELBO. Author experimentally found that the estimator of the existing work(STL) is biased and proposed to reduce the bias by using the technique like  REINFORCE.\nThe problem author focused on is unique and the solution is simple, experiments show that proposed method seems promising.\n\nClarity:\nThe paper is clearly written in the sense that the motivation of research is clear, the derivation of the proposed method is easy to understand.\n\nSignificance:\nI think this kind of research makes the variational inference more useful, so this work is significant. But I cannot tell the proposed method is really useful, so I gave this score.\nThe reason I doubt the reason is that as I written in the below, the original STL can handle the mixture of Gaussians as the latent variable but the proposed method cannot. So I do not know which is better and whether I should use this method or use the original STL with flexible posterior distribution to tighten the evidence lower bound. I think additional experiments are needed. I know that motivation is a bit different for STL and proposed method but some comparisons are needed.\n\nQuestion and minor comments:\nIn the original paper of STL, the author pointed out that by freezing the gradient of variational parameters to drop the score function term, we can utilize the flexible variational families like the mixture of Gaussians.\nIn this work, since we do not freeze the variational parameters, we cannot utilize the mixture of Gaussians as in the STL. IWAE improves the lower bound by increasing the samples, but we can also improve the bound by specifying the flexible posteriors like the mixture of Gaussians in STL.\nFaced on this, I wonder which strategy is better to tighten the lower bound, should we use the STL with the mixture of Gaussians or use the proposed method?  \nTo clarify the usefulness of this method, I think the additional experimental comparisons are needed.\n\nAbout the motivation of the paper, I think it might be better to move the Fig.1 about the Bias to the introduction and clearly state that the author found that the STL is biased \"experimentally\".\n\nFollowings are minor comments.\nIn experiment 6.1, I'm not sure why the author present the result of K ELBO estimator in the plot of Bias and Variance.\nI think author want to point that when K=1, STL is unbiased with respect to the 1 ELBO, but when k>1, it is biased with respect to IWAE estimator.\nHowever, the objective of K ELBO and IWAE are different, it may be misleading. So this should be noted in the paper.\n\nIn Figure 3, the left figure, what each color means? Is the color assignment is the same with the middle figure?\n(Same for Figure 4)", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1114/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives", "abstract": "Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma & Welling 2013, Rezende et al. 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al. 2018, Le et al. 2018). Roeder et a. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein & Bengio 2014), and the jackknife variational inference (JVI) gradient (Nowozin 2018). Finally, we show that this computationally efficient, drop-in estimator translates to improved performance for all three objectives on several modeling tasks.", "keywords": ["variational autoencoder", "reparameterization trick", "IWAE", "VAE", "RWS", "JVI"], "authorids": ["gjt@google.com", "jdl404@nyu.edu", "shanegu@google.com", "cmaddis@google.com"], "authors": ["George Tucker", "Dieterich Lawson", "Shixiang Gu", "Chris J. Maddison"], "TL;DR": "Doubly reparameterized gradient estimators provide unbiased variance reduction which leads to improved performance.", "pdf": "/pdf/19726e292215ba5feaede515a28e98368ebfc974.pdf", "paperhash": "tucker|doubly_reparameterized_gradient_estimators_for_monte_carlo_objectives", "_bibtex": "@inproceedings{\ntucker2018doubly,\ntitle={Doubly Reparameterized Gradient Estimators for Monte Carlo Objectives},\nauthor={George Tucker and Dieterich Lawson and Shixiang Gu and Chris J. Maddison},\nbooktitle={International Conference on Learning Representations},\nyear={2019},\nurl={https://openreview.net/forum?id=HkG3e205K7},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1114/Official_Review", "cdate": 1542234302937, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "HkG3e205K7", "replyto": "HkG3e205K7", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1114/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335875697, "tmdate": 1552335875697, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1114/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 11}