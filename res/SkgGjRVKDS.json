{"notes": [{"id": "SkgGjRVKDS", "original": "ryxcnlYODB", "number": 1310, "cdate": 1569439385742, "ddate": null, "tcdate": 1569439385742, "tmdate": 1586338693176, "tddate": null, "forum": "SkgGjRVKDS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "38ji3hend1", "original": null, "number": 8, "cdate": 1579422171359, "ddate": null, "tcdate": 1579422171359, "tmdate": 1579422171359, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment", "content": {"title": "The camera ready version has been released.", "comment": "Hi all,\n\nWe have uploaded the camera ready version of the paper, and the official code has been released. We added the results of mask-rcnn on COCO with pre-trained weights in the appendix. \n\nAny question or discussion is welcome!\n\nBest, \nRuosi Wan"}, "signatures": ["ICLR.cc/2020/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgGjRVKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1310/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1310/Authors|ICLR.cc/2020/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157961, "tmdate": 1576860554526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment"}}}, {"id": "D3waFvsOgg", "original": null, "number": 1, "cdate": 1576798720083, "ddate": null, "tcdate": 1576798720083, "tmdate": 1576800916477, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This work introduces Moving Average Batch Normalization (MABN) method to address performance issues of batch normalization in small batch cases. The method is theoretically analyzed and empirically verified on ImageNet and COCO.\nSome issues were raised by the reviewers, such as restrictive nature of some of the assumptions in the analysis as well as performance degradation due lack of centralizing feature maps. Nevertheless, all the reviewers found the contributions of this paper interesting and important, and they all recommended accept.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795709094, "tmdate": 1576800257747, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Decision"}}}, {"id": "BkeYyKYnKS", "original": null, "number": 1, "cdate": 1571752160530, "ddate": null, "tcdate": 1571752160530, "tmdate": 1574669512333, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "title": "Official Blind Review #3", "review": "This paper provides a new method to deal with the small batch size problem of BN, called MABN. Compared to BRN, MABN has two main contributions: 1) find two statistics, g and \u03c8, in BP to apply moving average operation without introducing too much overhead; 2) reduce the number of statistics of BN via centralizing weight for better stability.\n\nThough, I still have several concerns:\n1.\tYou mentioned that \u201ccentralizing weights ... to satisfy the zero mean assumption\u201d in Section 4.2. In other words, given E(W-mean(W))=0, E(X_{output})=E(W-mean(W))E(X_{input})=0. This equation holds only when W-mean(W) and X_{input} are irrelevant. However, model weights are updated based on the input and the loss function during training. Do you have any proof to ensure W-mean(W) and X_{input} are irrelevant?\n\n2.\tYour \u201cModified Structure\u201d contains two operations, centralizing weights and reducing BN\u2019s statistics. As much as I know, weight standardization can significantly improve BN\u2019s performance, but the effect of weight centralization, i.e. \u201cpart\u201d of weight standardization, is unclear yet. Therefore, I think a vanilla BN with weight centralization should also be included in ablation study. This experiment can help us better understand the pure effectiveness of reducing BN\u2019s statistics.\n\n3.\tI think the results on COCO should be discussed in detail since object detection is an important task in computer vision and suffers from the small batch size regime. I wonder if it\u2019s possible to compare MABN with SyncBN/GN on a higher baseline, such as finetuning from ImageNet pretrained model or training from scratch with at least 6x scheduler.\n\n4.\tWhat about the FLOPS, memory footprint and practical speed? These results would affect the application value of MABN. It would also be great if you can release the code, which would upgrade my rating a lot.\n\n5.\tWhat is the value of m for SMA? Is this value changed in different datasets? This would affect the performance of the proposed method. \n\n\nAlthough there are several issues not addressed by the authors (the assumption of this paper  \"W-mean(W) and X_{input} are irrelevant\"; why not using ImageNet pre-trained model on COCO), I keep my initial rating of weak accept. ", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1310/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1310/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139066663, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1310/Reviewers"], "noninvitees": [], "tcdate": 1570237739244, "tmdate": 1576139066675, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Official_Review"}}}, {"id": "rJxHNwjTKr", "original": null, "number": 3, "cdate": 1571825453058, "ddate": null, "tcdate": 1571825453058, "tmdate": 1574436134561, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #2", "review": "The paper extends recently proposed BatchRenormalization (BRN) technique which uses exponential moving average (EMA) statistics in forward and backward passes of BatchNorm (BN) instead of vanilla batch statistics. Motivation of the work is to stabilize training neural networks on small batch size setup. Authors propose to replace EMA in backward pass by simple moving average (SMA) and show that under some assumptions such replacement reduces variance. Also they consider slightly different way of normalization without centralizing features X, but centralizing convolutional kernels according to Qiao et al. (2019).\n\nConcerns:\nChanging batch statistics with moving averages in Eq. (4) and Eq. (16) may introduce bias in stochastic gradients. Authors do not reflect this problem in the work.\nAssumption (3) from Theorem 3 does not hold in practice since authors centralize weights not features. Authors do not study the influence of this on the performance of the method.\nAuthors\u2019 main motivation is to study small batch size experimental setup which is important in segmentation and detection, but they don\u2019t include main competitor (BRN) in these experiments.\n\nOverall, the proposed method has a lack novelty and thorough comparison against BRN. Therefore, I would suggest rejecting the current version.\n--------------------------------------------------------\nUpdate after author rebuttal\n\nThank you for your clarification. Below I provide an updated review for the paper.\n\nThe paper proposes the improvement of batch normalization techniques for the case of small batch size. Authors reveal new statistics used in gradient calculation of original BatchNorm and show their instability in case of small batch size. Thereafter they improve the method by reducing the number of statistics in backward pass resulting in more stability and performance increase. The authors provide a good experimental study on the influence of individual parts of the method. However, I still have concerns about the strictnesses of theorems assumptions and interpretability of their results in practice (i.e., o(1) biases in Theorem 1, o(1) assumptions in Theorem 2).\n\nOverall, almost all of my concerns were justified. Therefore I increase my score to 6.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1310/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1310/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139066663, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1310/Reviewers"], "noninvitees": [], "tcdate": 1570237739244, "tmdate": 1576139066675, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Official_Review"}}}, {"id": "BJeCWt3KiH", "original": null, "number": 1, "cdate": 1573665029880, "ddate": null, "tcdate": 1573665029880, "tmdate": 1573832486456, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "rJxHNwjTKr", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #2  ", "comment": "We\u2019re thankful to the reviewer for the comments and suggestions. The reviewer's main concern was on the lack novelty and thorough comparison against, which we address below one-by-one:\n\nMajor concerns:\n1. \u201cthe proposed method has a lack novelty\u201d.\n\nFirst of all, we have to point out some misunderstanding about our work in reviewer\u2019s comment: \u201cAuthors propose to replace EMA in backward pass by simple moving average (SMA) and show that under some assumptions such replacement reduces variance.\u201d Combining with preceding context, the reviewer think MABN just replace SMAS of statistics associated with feature maps (mean, variance) used by BRN during backward passes. It\u2019s totally wrong! MABN remains to use EMAS of feature maps as BRN, but replace extra batch statistics associated with gradients (g, \\phi defined in equation (5) in the paper) of BN by SMAS during backward passes. We humbly suggest the reviewer to read our paper again.\n\nOur major contribution is to reveal the existence of batch statistics (g and \\phi) associated with gradients from BN during backward propagation (BP), and prove they will severely affect the training process of neural network in small batch cases. To our best knowledge, the existence and influence of g and \\phi in BN had never been mentioned or discussed by others even so far. \n\nAnother major contribution is, we showed replacing batch statistics of g and \\phi with SMAS make it possible to completely make up the defect of vanilla BN in small batch cases, while BRN has been widely known not useful to address small batch issues in practice, our experiments presented in last and revised version (table1, 3 in main body, table 4, 5, 7 in appendix) also proves it. Though MABN and BRN indeed have something in common, MABN solves the small batch problem in essence, while BRN do not remedy the primary problem. \n\nBesides, we do not purely take use of weight centralization (WC) as a modification of Weight Standardization (WS) to improve our method. Our real contribution is to modify the structure of normalization (from (x \u2013mean)/std to x/E(x^2)) to reduce the number of batch statistics. We prove the effectiveness of modified normalization by both theoretical proof (Theorem 2 in section 4.1) and experiments (see 1, 6 in Table 2 in section 5.1). We admit modified normalization can worsen the performance due to lack of centralization. Thus we remedy the defect by centralizing weights. To clarify the effectiveness of WC, we added the ablation study on WC (see 10, 11, in Table 4 in appendix B.2), it can be shown WC don't significantly improve vanilla BN, but it can improve the performance of MABN with a clear margin (+1.87%). \n\n2. \u201clack thorough comparison against BRN\u201d, \u201cAuthors\u2019main motivation is to study small batch size experimental setup which is important in segmentation and detection, but they don\u2019t include main competitor (BRN) in these experiments.\u201d\n\nBRN is not a main competitor of MABN on detection and segmentation tasks, for its bad performance. Our main competitor is SyncBN. For integrity, we added the experiments with BRN in detection and segmentation tasks (see Table 3, Table5, Table 7) in the revised version. All experiment results shows BRN can slightly improve the performance of vanilla BN in small batch cases, but still remains a huge gap from MABN and SyncBN.\n\n3. \u201cChanging batch statistics with moving averages in Eq. (4) and Eq. (16) may introduce bias in stochastic gradients. Authors do not reflect this problem in the work.\u201d\n\nWe have theoretically demonstrated the bias of moving averages statistics is extremely small compared with mean and variance by the equation (11) in Theorem 2, we also have illustrated the result by experiment, which is shown in Figure 4 in appendix B.1 in last version. We highlight the result in the main body in the revised version.\n\n4. \u201cAssumption (3) from Theorem 3 does not hold in practice since authors centralize weights not features. Authors do not study the influence of this on the performance of the method.\u201d\n\nWe think you mean theorem 2. Since we can\u2019t strictly prove assumption (3) in theorem 2 can be satisfied by centralizing weights, we have pointed out WC is just a practical compensation, and carefully studied its influence by experiments, we presented the experiment result in Table 4 in last version. WC can improve the performance of MABN with a clear margin (1.87%), but without WC, MABN still significantly outperform its counterpart BRN by 4.84%. We highlight the result in the revised version.\n\nIn addition, we have to point out some mistakes in reviewer\u2019s comment: BRN is proposed in Feb. 2017. It has been almost 3 years since then, which is not a short time in deep learning fields, it seems not appropriate to call it \u201crecently proposed techniques\u201d, more importantly, BRN just use EMAS of mean and variance of feature maps as renormalize factors, but still use batch statistics (mean and variance) and their gradients during backward pass. \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgGjRVKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1310/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1310/Authors|ICLR.cc/2020/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157961, "tmdate": 1576860554526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment"}}}, {"id": "Syx9qFhtjB", "original": null, "number": 3, "cdate": 1573665170091, "ddate": null, "tcdate": 1573665170091, "tmdate": 1573665170091, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "BkeYyKYnKS", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #3", "comment": "We\u2019re thankful to the reviewer\u2019s comments and conducted suggestions. We will response his/her concerns one by one.\n\n1. \u201cYou mentioned that \u201ccentralizing weights ... to satisfy the zero mean assumption\u201d in Section 4.2. In other words, given E(W-mean(W))=0, E(X_{output})=E(W-mean(W))E(X_{input})=0. This equation holds only when W-mean(W) and X_{input} are irrelevant. However, model weights are updated based on the input and the loss function during training. Do you have any proof to ensure W-mean(W) and X_{input} are irrelevant?\u201d\n\nWe admit modified normalization can worsen the performance of the model due to the lack of centralizing feature maps, and we have emphasized weight centralization (WC) is only a practical compensation without any theoretical guarantee. We clarify the effectiveness of WC by ablation study, the experiment results is shown in Table 4 in appendix. It can be seen WC indeed improve the performance of MABN with a clear margin (compare 1 and 6 in table 4). More accurate theoretical analysis remains as a future work. \n\n2. \u201cYour \u201cModified Structure\u201d contains two operations, centralizing weights and reducing BN\u2019s statistics. As much as I know, weight standardization can significantly improve BN\u2019s performance, but the effect of weight centralization, i.e. \u201cpart\u201d of weight standardization, is unclear yet. Therefore, I think a vanilla BN with weight centralization should also be included in ablation study. This experiment can help us better understand the pure effectiveness of reducing BN\u2019s statistics.\u201d\n\nWe added the experiment of vanilla BN with WC in revised version (10, 11 in Table 4 in appendix B.2) as the reviewer\u2019s request. In worst case (vanilla BN, small batch size), WC can improve the performance of vanilla BN from 35.22% to 34.27% with a clear margin, in best cases (vanilla BN, regular batch size), WC can only improve vanilla BN slightly, from 23.41% to 23.35%. While WC can significantly improve MABN even close to the best case, from 25.45% to 23.58%. We think WC can significantly improve the performance of MABN by compensating for the lack of centralizing feature maps.\n\n3. \u201cI think the results on COCO should be discussed in detail since object detection is an important task in computer vision and suffers from the small batch size regime. I wonder if it\u2019s possible to compare MABN with SyncBN/GN on a higher baseline, such as finetuning from ImageNet pretrained model or training from scratch with at least 6x scheduler.\u201d\n\nWe are working on comparing MABN with SyncBN/GN in pretrained and 6x settings. However it\u2019s more time-consuming than we thought, so there\u2019s no guarantee we can add the experiment result by the due of rebuttal. But we will add the further experiment results in the final version if this work is accepted.\n\n4. \u201cWhat about the FLOPS, memory footprint and practical speed? These results would affect the application value of MABN. It would also be great if you can release the code, which would upgrade my rating a lot.\u201d\n\nWe added the results of FLOPS, memory footprint and practical speed in appendix B.5. Notice we haven\u2019t done any computation optimization on the implementation of MABN, so MABN seems a little slow. We decide to release the code to verify the implementation of MABN, the dropbox link https://www.dropbox.com/sh/5jrhc6knovqecls/AAA5asCVvte945ulT5PtECA_a?dl=0 It includes the codes and checkpoint of MABN on Imagenet and COCO. The code is still messy, we are working on cleaning it up and releasing an official version to include all experiment code mentioned in the paper.\n\n5. \u201cWhat is the value of m for SMA? Is this value changed in different datasets? This would affect the performance of the proposed method.\u201d\n\nIn all experiment, the buffer size (m) of MABN is set as 16. The buffer size (m) depends on what the size of batch the user want MABN to simulate. We add the annotation in the revised version. We found using very large buffer size of MABN can\u2019t improve the performance: on the one hand, the bias of SMAS of g and \\phi rapidly increase as buffer size grows; on the other hand, it has been proven both in experiments and theoretical analysis that appropriate randomness of batch statistics indeed help improve the generalization performance of neural network[1, 2].\n\n[1] You Y, Gitman I, Ginsburg B. Large batch training of convolutional networks[J]. arXiv preprint arXiv:1708.03888, 2017.\n[2] Zhou A, Ma Y, Li Y, et al. Towards Improving Generalization of Deep Networks via Consistent Normalization[J]. arXiv preprint arXiv:1909.00182, 2019."}, "signatures": ["ICLR.cc/2020/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgGjRVKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1310/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1310/Authors|ICLR.cc/2020/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157961, "tmdate": 1576860554526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment"}}}, {"id": "ByehHF2KjS", "original": null, "number": 2, "cdate": 1573665091829, "ddate": null, "tcdate": 1573665091829, "tmdate": 1573665091829, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "Bke_CWyaFH", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment", "content": {"title": "Author Response to Official Blind Review #1", "comment": "We really appreciate for the reviewer\u2019s praise and encouragement. We are trying our best to refine our work and extend its usage in deep learning field. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1310/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SkgGjRVKDS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1310/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1310/Authors|ICLR.cc/2020/Conference/Paper1310/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504157961, "tmdate": 1576860554526, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1310/Authors", "ICLR.cc/2020/Conference/Paper1310/Reviewers", "ICLR.cc/2020/Conference/Paper1310/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Official_Comment"}}}, {"id": "Bke_CWyaFH", "original": null, "number": 2, "cdate": 1571774927704, "ddate": null, "tcdate": 1571774927704, "tmdate": 1572972485537, "tddate": null, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "invitation": "ICLR.cc/2020/Conference/Paper1310/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new approach for batch-normalization. Standard approaches are sensitive to the batch size, because small batches will lead to unstable statistics. So when the mini-batch is small, the performance can drop significantly. The paper addresses this issue by analyzing extra statistics in the batch normalization and introducing moving average statistics, weights centralization and a slightly modified normalization. The proposed method does not require large batch sizes and nonlinear operations, but still maintain the robustness. The theoretical analysis and guarantees are provided as well. Experiments on typical datasets demonstrate the effectiveness of the proposed trick.\n\nOverall, the idea is interesting to me. The work is solid in both theory and practice. Hopefully the proposed scheme has the potential to fundamentally enhance the training of deep neural networks. "}, "signatures": ["ICLR.cc/2020/Conference/Paper1310/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1310/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["jjyan17@fudan.edu.cn", "wanruosi@megvii.com", "zhangxiangyu@megvii.com", "weizh@fudan.edu.cn", "weiyichen@megvii.com", "sunjian@megvii.com"], "title": "Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization", "authors": ["Junjie Yan", "Ruosi Wan", "Xiangyu Zhang", "Wei Zhang", "Yichen Wei", "Jian Sun"], "pdf": "/pdf/ac7753d0910e2d52269ca0364490b6f569ae6431.pdf", "TL;DR": "We propose a novel normalization method to handle small batch size cases.", "abstract": "Batch Normalization (BN) is one of the most widely used techniques in Deep Learning field. But its performance can awfully degrade with insufficient batch size. This weakness limits the usage of BN on many computer vision tasks like detection or segmentation, where batch size is usually small due to the constraint of memory consumption. Therefore many modified normalization techniques have been proposed, which either fail to restore the performance of BN completely, or have to introduce additional nonlinear operations in inference procedure and increase huge consumption. In this paper, we reveal that there are two extra batch statistics involved in backward propagation of BN, on which has never been well discussed before. The extra batch statistics associated with gradients also can severely affect the training of deep neural network. Based on our analysis, we propose a novel normalization method, named Moving Average Batch Normalization (MABN). MABN can completely restore the performance of vanilla BN in small batch cases, without introducing any additional nonlinear operations in inference procedure. We prove the benefits of MABN by both theoretical analysis and experiments. Our experiments demonstrate the effectiveness of MABN in multiple computer vision tasks including ImageNet and COCO. The code has been released in https://github.com/megvii-model/MABN.", "keywords": ["batch normalization", "small batch size", "backward propagation"], "paperhash": "yan|towards_stabilizing_batch_statistics_in_backward_propagation_of_batch_normalization", "code": "https://github.com/megvii-model/MABN", "_bibtex": "@inproceedings{\nYan2020Towards,\ntitle={Towards Stabilizing Batch Statistics in Backward Propagation of Batch Normalization},\nauthor={Junjie Yan and Ruosi Wan and Xiangyu Zhang and Wei Zhang and Yichen Wei and Jian Sun},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=SkgGjRVKDS}\n}", "original_pdf": "/attachment/84fde2e0acfd48d63351a96df8f71ee91203d0a1.pdf"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkgGjRVKDS", "replyto": "SkgGjRVKDS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1310/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576139066663, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1310/Reviewers"], "noninvitees": [], "tcdate": 1570237739244, "tmdate": 1576139066675, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1310/-/Official_Review"}}}], "count": 9}