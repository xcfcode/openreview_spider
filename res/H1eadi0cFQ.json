{"notes": [{"id": "H1eadi0cFQ", "original": "Hke7FFKctX", "number": 395, "cdate": 1538087796821, "ddate": null, "tcdate": 1538087796821, "tmdate": 1545355407815, "tddate": null, "forum": "H1eadi0cFQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications", "abstract": "Hierarchically embedding smaller networks in larger networks, e.g.~by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying  how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.~closing the tunnel later, or permanently, i.e.~iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers.   Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.", "keywords": ["deep learning", "cnn", "structural modification", "optimization", "saddle point"], "authorids": ["yannic.kilcher@inf.ethz.ch", "garybecigneul06@gmail.com", "thomas.hofmann@inf.ethz.ch"], "authors": ["Yannic Kilcher", "Gary B\u00e9cigneul", "Thomas Hofmann"], "TL;DR": "If optimization gets stuck in a saddle, we add a filter to a CNN in a specific way in order to escape the saddle.", "pdf": "/pdf/1c704fa0c25e9106af3f1a343e7884d234b1e240.pdf", "paperhash": "kilcher|escaping_flat_areas_via_functionpreserving_structural_network_modifications", "_bibtex": "@misc{\nkilcher2019escaping,\ntitle={Escaping Flat Areas via Function-Preserving Structural Network Modifications},\nauthor={Yannic Kilcher and Gary B\u00e9cigneul and Thomas Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eadi0cFQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 6, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "BJex8Z_JgN", "original": null, "number": 1, "cdate": 1544679752155, "ddate": null, "tcdate": 1544679752155, "tmdate": 1545354506100, "tddate": null, "forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper395/Meta_Review", "content": {"metareview": "The paper proposes a method to escape saddle points by adding and removing units during training. The method does so by preserving the function when the unit is added while increasing the gradient norm to move away from the critical point. The experimental evaluation shows that the proposed method does escape when positioned at a saddle point - as found by the Newton method. The reviewers find the theoretical ideas interesting and novel, but they raised concerns about the method's applicability for typical initializations, the experimental setup, as well as the terminology used in the paper. The title and terminology were improved with the revision, but the other issues were not sufficiently addressed.", "confidence": "3: The area chair is somewhat confident", "recommendation": "Reject", "title": "Meta-review"}, "signatures": ["ICLR.cc/2019/Conference/Paper395/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper395/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications", "abstract": "Hierarchically embedding smaller networks in larger networks, e.g.~by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying  how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.~closing the tunnel later, or permanently, i.e.~iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers.   Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.", "keywords": ["deep learning", "cnn", "structural modification", "optimization", "saddle point"], "authorids": ["yannic.kilcher@inf.ethz.ch", "garybecigneul06@gmail.com", "thomas.hofmann@inf.ethz.ch"], "authors": ["Yannic Kilcher", "Gary B\u00e9cigneul", "Thomas Hofmann"], "TL;DR": "If optimization gets stuck in a saddle, we add a filter to a CNN in a specific way in order to escape the saddle.", "pdf": "/pdf/1c704fa0c25e9106af3f1a343e7884d234b1e240.pdf", "paperhash": "kilcher|escaping_flat_areas_via_functionpreserving_structural_network_modifications", "_bibtex": "@misc{\nkilcher2019escaping,\ntitle={Escaping Flat Areas via Function-Preserving Structural Network Modifications},\nauthor={Yannic Kilcher and Gary B\u00e9cigneul and Thomas Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eadi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper395/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545353232280, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper395/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper395/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper395/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545353232280}}}, {"id": "Skg1pk8KhQ", "original": null, "number": 1, "cdate": 1541132215430, "ddate": null, "tcdate": 1541132215430, "tmdate": 1543722721175, "tddate": null, "forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper395/Official_Review", "content": {"title": "review", "review": "The paper addresses the problem of increasing and decreasing the number of hidden nodes (aka, dimensionality) in the network such that the optimization will not enter the plateaus of saddle points. The opening or closing of tunnels (filters) guarantee the existence of \u201cnew escape directions\u201d and faster convergence. \n\nStrengths:\n+ provide a new perspective of designing the shape/dimensionality of a network in a dynamic manner. \n+ provide theoretic proof of CNNs and FCs on the contribution to the gradient after cloning. \n\nWeakness:\n- Experiments are very weak to verify the theory.\n\nDetailed comments:\n\n- Eqn. (6) seems to provide a unified evaluation on the contribution of two units to the gradients. How does it relate to the experiments? It gives me a sense that the manuscript is isolated between theory (Section 2 and 3) and verification (experiments).\n- Why does the blue curve get stuck in a flat area? A better staring learning rate could alleviate the plateau bottleneck. \n- The experiment settings are a little bit simple, even for the most complicated one in Section 4.4, where there are five conv layers and the tunnel opening only involves one single filter. Do authors conduct more filters opening in more layers? How about the closing case? There is no result/analysis in the experiments. \n- Why authors claim the blue curve in the left figure 2, a \u201cflat area\u201d? It seems working as the orange one (loss decreases normally). \n- Another big concern is that the proposed method is supposed to prevent network from saddle points and faster convergence, which is verified. And yet, the ultimate goal is to improve the performance. I am surprised that there is no such result at all in the manuscript (for example, error rate goes down on cifar/mnist/etc). \n\nIn summary, I do recognize the theoretical effort the paper has provided; however, the experiments seem not to verify the proposed method in a professional manner. \n\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper395/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications", "abstract": "Hierarchically embedding smaller networks in larger networks, e.g.~by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying  how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.~closing the tunnel later, or permanently, i.e.~iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers.   Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.", "keywords": ["deep learning", "cnn", "structural modification", "optimization", "saddle point"], "authorids": ["yannic.kilcher@inf.ethz.ch", "garybecigneul06@gmail.com", "thomas.hofmann@inf.ethz.ch"], "authors": ["Yannic Kilcher", "Gary B\u00e9cigneul", "Thomas Hofmann"], "TL;DR": "If optimization gets stuck in a saddle, we add a filter to a CNN in a specific way in order to escape the saddle.", "pdf": "/pdf/1c704fa0c25e9106af3f1a343e7884d234b1e240.pdf", "paperhash": "kilcher|escaping_flat_areas_via_functionpreserving_structural_network_modifications", "_bibtex": "@misc{\nkilcher2019escaping,\ntitle={Escaping Flat Areas via Function-Preserving Structural Network Modifications},\nauthor={Yannic Kilcher and Gary B\u00e9cigneul and Thomas Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eadi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper395/Official_Review", "cdate": 1542234471169, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper395/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714421, "tmdate": 1552335714421, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper395/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJxUDp2s27", "original": null, "number": 3, "cdate": 1541291357734, "ddate": null, "tcdate": 1541291357734, "tmdate": 1543229568199, "tddate": null, "forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper395/Official_Review", "content": {"title": "Interesting work, though not sure how symmetry breaking happens", "review": "This is an interesting submission and I appreciate especially connecting to a body of literature which is not normally well known in our community (e.g. Fukumizu&Amari). I think the perspective is definitely new and probably quite relevant not only for practical approaches to escape saddles but also to understand learning in deep learning.  I have a few notes and suggestions:\n\n1) Name of the paper: \nI think is not descriptive of the approach and actually the words \u201cmagic\u201d makes it sound strange. I think this will reduce the amount of people reading the work. Please consider something more descriptive like: \u201cEscaping saddle points by increasing capacity\u201d. Or something else more inspired, but that also hints what the work is about. \n\n2) Notation: \nThe notation used is not ML friendly (or generically) to the average reader. I strongly suggest to use b_v for bias, W_uv for weights, and not theta_v and theta_uv which is not typical notation. \u2018u\u2019 and \u2018v\u2019 are somewhat non-typical choices either, though I understand that they come from the graph notation. Transfer functions are usual sigma. In the text you explain the process by starting with a u\u2019 and then add the clone which is u. Normally you should have started with u and add the clone that is u\u2019. x_u for the value of unit u (assuming this is in the middle of a deep net) is also quite a strange notation. I can guess the authors might be from a slightly different community, but I\u2019m worried about people from the target audience (ICLR) being turned away from the work or even worse confused because of notations.\n\n3) Related work\nThere is the Net2Net work that is related to what is going on here that is not cited (https://arxiv.org/pdf/1511.05641.pdf). I think there was some follow-up work after this.\n\n4) Symmetry breaking\nI do not understand how symmetry is broken. If I clone a unit, and have a new variant of it u\u2019 that now has the same outbound connections but multiplied by alpha (while the original unit by 1-alpha) then while the norm of the gradients differ, their direction does not. Wouldn\u2019t this mean that the units will track each other and hence no tunnel is open? In Net2Net dropout was used to break symmetry (i.e. a source of noise that would pick one path over the other). There is no source of noise here to break symmetry. \n\n5) Diagrams and analysis\nConnected to this, I feel like this could have been represented clearly with a diagram showing the net before and after. There could be some analysis, a more extended discussion of where the symmetry breaking comes from, empirical evidence that it does. I\u2019m not necessarily worried that experiments are not scaled up, I\u2019m more concerned that the hypothesis and solution is only tested by means of change in performance. What is this tunnel doing? How does it change the Hessian at the saddle? Any visualization to reinforce the intuition of what the approach is doing? \n\n\n6) Closing tunnels & re-organizing\nI don\u2019t understand the mechanism for reorganizing weights and closing tunnels. It seems first of all to confirm my intuition that there is no symmetry breaking since we can \u201cclose\u201d the tunnel by simple algebraic manipulation. So if those two units always stay in sync how do you actually change the error surface? How do you take advantage of this extra capacity to solve anything. Regardless, when it comes to re-organizing, it seems you pick two of these units that are in sync (previous tunnel I guess) and collapse them to open a new tunnel, right? How does this change anything? Which unit needs to be cloned? Any? So then why is the previous tunnel not efficient anymore? \n", "rating": "6: Marginally above acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper395/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": true, "forumContent": {"title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications", "abstract": "Hierarchically embedding smaller networks in larger networks, e.g.~by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying  how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.~closing the tunnel later, or permanently, i.e.~iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers.   Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.", "keywords": ["deep learning", "cnn", "structural modification", "optimization", "saddle point"], "authorids": ["yannic.kilcher@inf.ethz.ch", "garybecigneul06@gmail.com", "thomas.hofmann@inf.ethz.ch"], "authors": ["Yannic Kilcher", "Gary B\u00e9cigneul", "Thomas Hofmann"], "TL;DR": "If optimization gets stuck in a saddle, we add a filter to a CNN in a specific way in order to escape the saddle.", "pdf": "/pdf/1c704fa0c25e9106af3f1a343e7884d234b1e240.pdf", "paperhash": "kilcher|escaping_flat_areas_via_functionpreserving_structural_network_modifications", "_bibtex": "@misc{\nkilcher2019escaping,\ntitle={Escaping Flat Areas via Function-Preserving Structural Network Modifications},\nauthor={Yannic Kilcher and Gary B\u00e9cigneul and Thomas Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eadi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper395/Official_Review", "cdate": 1542234471169, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper395/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714421, "tmdate": 1552335714421, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper395/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "HJggSEsEC7", "original": null, "number": 3, "cdate": 1542923320484, "ddate": null, "tcdate": 1542923320484, "tmdate": 1542923320484, "tddate": null, "forum": "H1eadi0cFQ", "replyto": "SJgrbqOinQ", "invitation": "ICLR.cc/2019/Conference/-/Paper395/Official_Comment", "content": {"title": "Thank You", "comment": "Thank you very much for your constructive comments. We address them in order as they appear in the feedback.\n\nFirst, we recognize that using Newton's method and random restarts (we did ~150 and chose the \"flattest\" one) in order to explicitly find saddle points puts us in a constructed scenario and is somewhat dissociated from actual practice. However, being stuck in a flat area is a pre-requirement for our analysis and thus, since our experiments should serve to support our theory, we chose to match the requirements as close as possible. Almost all published research on escaping saddle points does pose being stuck in a saddle point as e pre-requirement.\nSecond, we are aware that there is an ongoing, unresolved discussion in the community about how important the problem of saddle points is in deep learning. Our goal is not to add to this discussion, but to present a solution in one possible case.\nThat said, we do report in the appendix what happens if the method is applied to models trained under a standard protocol (nothing happens, because they seem to reach minima instead of saddles), and we have added a table to the appendix reporting the wall clock times of all experiments. From these, one can see that the time overhead caused by our method is insignificant compared to the time variance over experimental repetitions (the computational bottleneck of our method is dominated by memory).\n\nAs for your comments on comparison to other methods, we agree and thus we have implemented two baseline methods and included them in the experimental sections. Note that while these baselines require changing the gradient update over multiple iterations, our method is done within a single step and still outperforms them in terms of how fast the flat area is escaped from.\n\nWe changed the title to be more informative. We've also removed the word \"magic\" from our text in order to make it more concise.\n\nYour observation that closing the tunnel does hurt the optimization procedure is correct and does not have to be done in practice. Our motivation for closing the tunnel is twofold: First, we wanted to establish a method that does not change the final size of the network. Second, closing the tunnel is a half-way step to our practical weight reorganization method, which does not change the network architecture at all and therefore might be much more implementable in current deep learning libraries.\n\nLastly, thank you for the relevant related work. We have included citations to the papers.\n\nBesides the changes resulting from your feedback, we have made several other changes, motivated by the other reviewers. Most notable are the change in notation of network weights in order to better conform to the community's practices, the introduction of experiments on symmetry breaking in the appendix and the drop of the additional experiments on CIFAR10 from the appendix, since the network was arguably not stuck in a true flat area."}, "signatures": ["ICLR.cc/2019/Conference/Paper395/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper395/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper395/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications", "abstract": "Hierarchically embedding smaller networks in larger networks, e.g.~by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying  how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.~closing the tunnel later, or permanently, i.e.~iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers.   Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.", "keywords": ["deep learning", "cnn", "structural modification", "optimization", "saddle point"], "authorids": ["yannic.kilcher@inf.ethz.ch", "garybecigneul06@gmail.com", "thomas.hofmann@inf.ethz.ch"], "authors": ["Yannic Kilcher", "Gary B\u00e9cigneul", "Thomas Hofmann"], "TL;DR": "If optimization gets stuck in a saddle, we add a filter to a CNN in a specific way in order to escape the saddle.", "pdf": "/pdf/1c704fa0c25e9106af3f1a343e7884d234b1e240.pdf", "paperhash": "kilcher|escaping_flat_areas_via_functionpreserving_structural_network_modifications", "_bibtex": "@misc{\nkilcher2019escaping,\ntitle={Escaping Flat Areas via Function-Preserving Structural Network Modifications},\nauthor={Yannic Kilcher and Gary B\u00e9cigneul and Thomas Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eadi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper395/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615343, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eadi0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper395/Authors", "ICLR.cc/2019/Conference/Paper395/Reviewers", "ICLR.cc/2019/Conference/Paper395/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper395/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper395/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper395/Authors|ICLR.cc/2019/Conference/Paper395/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper395/Reviewers", "ICLR.cc/2019/Conference/Paper395/Authors", "ICLR.cc/2019/Conference/Paper395/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615343}}}, {"id": "ryeOGVsE0Q", "original": null, "number": 2, "cdate": 1542923280482, "ddate": null, "tcdate": 1542923280482, "tmdate": 1542923280482, "tddate": null, "forum": "H1eadi0cFQ", "replyto": "HJxUDp2s27", "invitation": "ICLR.cc/2019/Conference/-/Paper395/Official_Comment", "content": {"title": "Thank You", "comment": "Thank you for these extensive and helpful comments. We'll address them in order:\n\n1.) Name of the Paper\nWe agree and we have changed the paper's name to a more descriptive one. Also we've dropped the word \"Magic\" from the paper to achieve more clarity.\n\n2.) Notation\nWe agree that the notation stood to be improved and we've changed all thetas to \"w\" and \"b\" for weights and biases. Further, we've swapped \"u\" and \"u'\", where now u refers to the original node to be cloned from and u' refers to the newly created, copied node. We believe these changes should make the work more understandable for the community.\n\n3.) Related Work\nWe've changed our related work section to include Net2Net and similar work and describe the main difference: This work is done in the context of transfer learning and architecture search, not in the context of escaping flat areas. While these works also do preserve the network's input-output function, our main contribution is that we provide an explicit way to control the gradient after the architecture change. This can be seen most evidently in hidden unit cloning, where these works clone units with weights of 0.5 equally (which leads to remaining stuck in a flat area as well as symmetry problems), where as we use \\lambda^t with direct relation to the resulting gradient.\n\n4.) Symmetry breaking\nAs said before, it is true that when nodes are cloned with weights 0.5 equally, then the resulting network will have a symmetry that cannot be broken. Net2Net solves this by introducing noise (losing theoretical guarantees). However, this is only the case when the shares are exactly 0.5, where as we use weights of \\lambda and (1-\\lambda) where lambda is not only a number, but an entire vector. It's true that at first this just results in differently scaled, but otherwise same, activations from both units, but through the nonlinearities in the network, even this directional symmetry will be broken. Over multiple steps, the units diverge from each other.\nIn order to show this, we've performed a series of experiments and made a section in the appendix (called \"symmetry breaking\") where we show the results: We've observed the L2 distance between the two units u and u' over the number of steps after we perform tunnel opening. What is visible is that only when \\lambda is exactly 0.5 everywhere, there is a symmetry that can't be broken. Any other \\lambda, be this another fixed number, a randomly sampled vector or our proposed \\lambda^t will lead to symmetry being broken over time. Note that the experimental results suggest that using our proposed method, symmetry is broken significantly faster than using any other method. We hope this answers your question.\n\n5.) Diagrams and analysis\nWe would have loved to include visual diagrams of the network before and after, but ultimately had to prioritize other things for the limited space. In terms of analysis, we hope that the newly added experiments on symmetry breaking serve this purpose.\n\n6.) Closing tunnels & re-organizing\nClosing the tunnel without effect can only be done when there have been no gradient update steps since opening, as we state in the paper. When gradient update steps are performed, the cloned nodes get out of sync and that's how the extra capacity is used. This also means that when closing the tunnel after a number of steps, it becomes a heuristic, i.e. we cannot guarantee that the input-output function is preserved. In fact, it almost certainly isn't. We can only give such a guarantee for opening the tunnel. Our goal was to make a method that doesn't change the final capacity of the network and this heuristic is a half-way step towards our other heuristic of practical re-organization, which does not change the architecture at all. We have amended the section on tunnel closing to hopefully clarify this further.\n\nBesides the changes resulting from your feedback, we have made several other changes, motivated by the other reviewers. Most notable are the addition of two baseline methods as well as the drop of the additional experiments on CIFAR10 from the appendix, since the network was arguably not stuck in a true flat area."}, "signatures": ["ICLR.cc/2019/Conference/Paper395/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper395/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper395/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications", "abstract": "Hierarchically embedding smaller networks in larger networks, e.g.~by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying  how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.~closing the tunnel later, or permanently, i.e.~iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers.   Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.", "keywords": ["deep learning", "cnn", "structural modification", "optimization", "saddle point"], "authorids": ["yannic.kilcher@inf.ethz.ch", "garybecigneul06@gmail.com", "thomas.hofmann@inf.ethz.ch"], "authors": ["Yannic Kilcher", "Gary B\u00e9cigneul", "Thomas Hofmann"], "TL;DR": "If optimization gets stuck in a saddle, we add a filter to a CNN in a specific way in order to escape the saddle.", "pdf": "/pdf/1c704fa0c25e9106af3f1a343e7884d234b1e240.pdf", "paperhash": "kilcher|escaping_flat_areas_via_functionpreserving_structural_network_modifications", "_bibtex": "@misc{\nkilcher2019escaping,\ntitle={Escaping Flat Areas via Function-Preserving Structural Network Modifications},\nauthor={Yannic Kilcher and Gary B\u00e9cigneul and Thomas Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eadi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper395/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621615343, "tddate": null, "super": null, "final": null, "reply": {"forum": "H1eadi0cFQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper395/Authors", "ICLR.cc/2019/Conference/Paper395/Reviewers", "ICLR.cc/2019/Conference/Paper395/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper395/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper395/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper395/Authors|ICLR.cc/2019/Conference/Paper395/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper395/Reviewers", "ICLR.cc/2019/Conference/Paper395/Authors", "ICLR.cc/2019/Conference/Paper395/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621615343}}}, {"id": "SJgrbqOinQ", "original": null, "number": 2, "cdate": 1541274109244, "ddate": null, "tcdate": 1541274109244, "tmdate": 1541534031585, "tddate": null, "forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "invitation": "ICLR.cc/2019/Conference/-/Paper395/Official_Review", "content": {"title": "Interesting ideas for expanding/contracting a network's size to escape saddles, but would benefit from further experiments", "review": "Summary:\n\nThis paper presents a new strategy for escaping saddle points by adding and removing hidden units during training. The method essentially finds conditions where adding a hidden unit does not change the overall input-output map, and uses these as constraints to add a hidden unit that maximally increases the gradient norm (thus potentially getting learning unstuck). Experiments show that the method can improve training speed relative to the same network with randomly added new units.\n\nMajor comments:\n\nThis paper presents interesting theoretical ideas and clearly separates the opportunities for adding/removing hidden units without changing the input-output map from the impact on the gradient due to the change in parametrization. \n\nThe experiments show that the proposed method can speed up learning when a network is genuinely stuck at a saddle point. Importantly however, the experimental evaluation intentionally seeks saddle points using Newton\u2019s method, such that learning is genuinely stuck, before adding the additional units. It is therefore less clear whether this method can offer speedups to network training in practice. Do NNs come close enough to saddle points to benefit from the method when beginning from typical initializations? Experiments on ImageNet begin from a specifically chosen random seed that happens to enter a very flat region. How many random seeds were tried before finding this one? This would speak to the importance of these findings in general. The paper would benefit greatly from applying the proposed method to networks trained under standard protocols, to identify the speed up (if any) it can confer for the average case. It would also be important to account for wall clock time, as the proposed method involves potentially expensive steps (at least in its straightforward form).\n\nThe paper notes several other strategies for expelling from saddle points. The experimental evaluation could be improved greatly by including comparisons to these alternatives. Does the proposed method escape more quickly, or have other merits relative to these alternatives?\n\nThe clarity of the paper is good overall but the title could be improved to be more informative of the content of the method.\n\nOverall the significance of the paper is not clearly established because the evaluations mostly consist of internal comparisons, in somewhat unnatural settings (where networks are initialized right next to saddle points). The theoretical observations, however, seem promising.\n\nMinor comments:\n\nI could not understand the motivation for closing the tunnel\u2014it seems as though optimization proceeds more quickly if it remains open. \n\nThe paper discusses a range of relevant work but would benefit from citing other incremental learning work in neural networks, in particular:\n\nY. Bengio et al., Convex Neural Networks, NIPS 2006\n\nF. Bach. Breaking the curse of dimensionality with convex neural networks. JMLR 2017\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper395/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Escaping Flat Areas via Function-Preserving Structural Network Modifications", "abstract": "Hierarchically embedding smaller networks in larger networks, e.g.~by increasing the number of hidden units, has been studied since the 1990s. The main interest was in understanding possible redundancies in the parameterization, as well as in studying  how such embeddings affect critical points. We take these results as a point of departure to devise a novel strategy for escaping from flat regions of the error surface and to address the slow-down of gradient-based methods experienced in plateaus of saddle points. The idea is to expand the dimensionality of a network in a way that guarantees the existence of new escape directions. We call this operation the opening of a tunnel. One may then continue with the larger network either temporarily, i.e.~closing the tunnel later, or permanently, i.e.~iteratively growing the network, whenever needed. We develop our method for fully-connected as well as convolutional layers.   Moreover, we present a practical version of our algorithm that requires no network structure modification and can be deployed as plug-and-play into any current deep learning framework. Experimentally, our method shows significant speed-ups.", "keywords": ["deep learning", "cnn", "structural modification", "optimization", "saddle point"], "authorids": ["yannic.kilcher@inf.ethz.ch", "garybecigneul06@gmail.com", "thomas.hofmann@inf.ethz.ch"], "authors": ["Yannic Kilcher", "Gary B\u00e9cigneul", "Thomas Hofmann"], "TL;DR": "If optimization gets stuck in a saddle, we add a filter to a CNN in a specific way in order to escape the saddle.", "pdf": "/pdf/1c704fa0c25e9106af3f1a343e7884d234b1e240.pdf", "paperhash": "kilcher|escaping_flat_areas_via_functionpreserving_structural_network_modifications", "_bibtex": "@misc{\nkilcher2019escaping,\ntitle={Escaping Flat Areas via Function-Preserving Structural Network Modifications},\nauthor={Yannic Kilcher and Gary B\u00e9cigneul and Thomas Hofmann},\nyear={2019},\nurl={https://openreview.net/forum?id=H1eadi0cFQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper395/Official_Review", "cdate": 1542234471169, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "H1eadi0cFQ", "replyto": "H1eadi0cFQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper395/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335714421, "tmdate": 1552335714421, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper395/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 7}