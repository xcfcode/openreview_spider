{"notes": [{"id": "BklsagBYPS", "original": "rklsc--Yvr", "number": 2586, "cdate": 1569439939100, "ddate": null, "tcdate": 1569439939100, "tmdate": 1577168246727, "tddate": null, "forum": "BklsagBYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "paDqAd9O-j", "original": null, "number": 1, "cdate": 1576798752762, "ddate": null, "tcdate": 1576798752762, "tmdate": 1576800882767, "tddate": null, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Decision", "content": {"decision": "Reject", "comment": "This paper proposes to measure the distance of the generator manifold to the training data. The proposed approach bears significant similarity to past studies that also sought to analyze the behavior of generative models that define a low-dimensional manifold (e.g. Webster 2019, and in particular, Xiang 2017). I recommend that the authors perform a broader literature search to better contextualize the claims and experiments put forth in the paper.\n\nThe proposed method also suffers from some limitations that are not made clear in the paper. First, the measure depends only on the support of the generator, but not the density. For models that have support everywhere (exact likelihood models tend to have this property by construction), the measure is no longer meaningful. Even for VAEs, the measure is only easily applicable if the decoder is non-autoregressive so that the procedure can be applied only to the mean decoding. \n\nIn this current state, I do not recommend the paper for submission.\n\nXiang (2017). On the Effects of Batch and Weight Normalization in Generative Adversarial Networks\nWebster (2019). Detecting Overfitting of Deep Generative Networks via Latent Recovery\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795724437, "tmdate": 1576800276087, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Decision"}}}, {"id": "SygOTfanYH", "original": null, "number": 3, "cdate": 1571766976508, "ddate": null, "tcdate": 1571766976508, "tmdate": 1574342786060, "tddate": null, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "This paper defines a goodness of fit measure F for generative networks, that reflects how well a model can generate the training data. F allows to detect mode collapse: as long as it is strictly positive, mode collapse is observed as parts of the training data have not been memorized. It aims at providing an alternative to the Fr\u00e9chet Inception Distance and the Inception Score that rely on pretrained neural networks (whereas this new measure does not). It also provides insight into the DCGAN and WGAN networks in that regard, observing for instance that data subsampling helps decrease F, which motivates the use of a mixture of GANs.\n\nThis paper brings an interesting contribution to the evaluation of generative networks. However:\n\n1.\tThe use of the square distance in the image space is not obvious and not justified.\n2.\tComputation of this metric is not straightforward: there is no theoretical guarantee and it is computationally expensive.\n3.\tThe theoretical properties of this measure and its robustness are not investigated.\n4.\tTypos are obscuring the reading of the paper.\n\n- Post rebuttal: I have read the authors' response and am maintaining my weak reject rating.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576450692795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2586/Reviewers"], "noninvitees": [], "tcdate": 1570237720721, "tmdate": 1576450692815, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Official_Review"}}}, {"id": "Sklvl9H5sB", "original": null, "number": 4, "cdate": 1573702126983, "ddate": null, "tcdate": 1573702126983, "tmdate": 1573702126983, "tddate": null, "forum": "BklsagBYPS", "replyto": "H1x3_8ztjH", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment", "content": {"title": "Thanks for the response", "comment": "Thanks for the detailed response. \n\nI think it is debatable to claim \"generative models should have F = 0\", especially when the generative models have covered the high-density areas. \n\nOn the other hand, it is crucial to distinguish models which only memorize the training data but generate randomly beyond that and models which cover the data manifold well and generate reasonably. The proposed metric seems to favor the first than the latter, which is counter-intuitive. Therefore, metrics that cover both precision and recall would be more sensible in evaluation."}, "signatures": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklsagBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2586/Authors|ICLR.cc/2020/Conference/Paper2586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139087, "tmdate": 1576860548453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment"}}}, {"id": "SyeYCUzKsB", "original": null, "number": 3, "cdate": 1573623504753, "ddate": null, "tcdate": 1573623504753, "tmdate": 1573623504753, "tddate": null, "forum": "BklsagBYPS", "replyto": "Bkx7i9i1KH", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment", "content": {"title": "Response to Reviewer #2", "comment": "Thank you for the feedback on our paper. We address your concerns as follows:\n\n- For generative networks that map a latent space of \u201clow\u201d dimension to a space of \u201chigh\u201d dimension, the range of the network does not cover the entire space, because two spaces are of differing dimensions. Hence, we have that p(x) = 0 can occur for training points. This is true for VAEs as well as GANs as we mentioned in the paper.\n\n- If generative models can generate high-quality images but cannot generate the training set, then the learned distribution is likely only focused on a few modes of the true data distribution. This means that the learned distribution might cover a very small part of the data distribution well, which is not desirable since we want to learn the entire data distribution. Our main contribution is the definition of a metric that enables us to measure how well a generative model memorizes the training set.\n\n- Mode collapse can be defined with respect to different distributions. With respect to the empirical data distribution, F being 0 is necessary and sufficient to avoid mode collapse. As you mention, with respect to some true data distribution that was sampled to obtain an empirical distribution, F = 0 is only necessary to avoid mode collapse.\n\n- If the latent space is much smaller than the output space, then it is not trivial to have F = 0 because the generator cannot reproduce any output image. If F = 0, then you can compare probabilities for generated images against an oracle data distribution. In many toy 2D examples, you have oracle access to the true distributions because the distributions are simulated. However, we are concerned with image datasets where we only have access to an empirical distribution. Hence, mode collapse must be compared with the empirical distribution because we do not have access to an oracle distribution.\n\n- The results with the mixture model are not obvious because they imply that using less data can lead to better performance. Specifically, this implies that DCGAN-type architectures are underparameterized.\n\nLet us know if you have any further concerns about our paper, and thank you for the helpful feedback.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2586/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklsagBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2586/Authors|ICLR.cc/2020/Conference/Paper2586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139087, "tmdate": 1576860548453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment"}}}, {"id": "H1x3_8ztjH", "original": null, "number": 2, "cdate": 1573623412133, "ddate": null, "tcdate": 1573623412133, "tmdate": 1573623412133, "tddate": null, "forum": "BklsagBYPS", "replyto": "HJlxTZnitB", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment", "content": {"title": "Response to Reviewer #1", "comment": "Thank you for the feedback on our paper. We address your concerns as follows:\n\n- It is true that if F>0 then we can still have memorization, because we are taking an average. In practice, this does not happen, as we captured in the histograms of Figure 4 and Figure 8 in the paper. We observe that the distribution of distance is relatively symmetric and unimodal, making the average a very informative measure of memorization. In addition, we are mostly concerned with having complete memorization of the data instead of just partial memorization. For partial memorization scenarios, we agree that variations on our metric (such as minimum distance) could be very useful as well.\n\n- We consider generating the training set as a first step toward understanding important issues like mode collapse. Of course, our measure alone will not be used to directly evaluate the fidelity of generated samples. High fidelity samples are desirable, but if F > 0 for these models, then that means that they are not learning the simplest distribution of all: the empirical distribution. Hence, generative models should have F = 0, which implies that there is no mode collapse with the empirical distribution.\n\n- We are comparing the support of G to the training set only (and not the probability densities), because we are focusing on the simpler, yet necessary, topic of memorization in generative networks. If a generative network cannot learn the training set, then there exists an image x such that the probability of generating x is equal to 0. Thus, a probabilistic distance of x from the distribution of Imag(G) is related to the distance between the image of G and x, which is our approach.\n\nLet us know if you have any further concerns about our paper, and thank you for the helpful feedback."}, "signatures": ["ICLR.cc/2020/Conference/Paper2586/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklsagBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2586/Authors|ICLR.cc/2020/Conference/Paper2586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139087, "tmdate": 1576860548453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment"}}}, {"id": "r1lF7UfFsS", "original": null, "number": 1, "cdate": 1573623328993, "ddate": null, "tcdate": 1573623328993, "tmdate": 1573623328993, "tddate": null, "forum": "BklsagBYPS", "replyto": "SygOTfanYH", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment", "content": {"title": "Response to Reviewer #3", "comment": "Thank you for the feedback on our paper. We address your concerns as follows:\n\n1. Defining a metric between images is a long-standing problem in image processing, because standard p-norms do not capture image structure well. We use squared distance (2-norm) because it has a long history in not only signal and image processing but beyond. Nevertheless, other metrics can be used within our framework if there is prior information that implies that a particular metric would be superior to others. For this reason, we do not claim that the 2-norm is optimal for images.\n\n2. We follow standard methods for optimizing over the latent space of a generative network such those from the GLO paper. This enables us to have a certain confidence in the optimization problem because this method \u201c... recovers the true latent vector 100% of the time to arbitrary precision.\u201d [1] Although, this is not theoretical guarantees, their empirical performance is a convincing argument for why they should be used for calculating F in practice. Developing theoretical guarantees for this nonconvex optimization problem is beyond the scope of this paper and would be an interesting paper by itself. \n\n3. The robustness of calculating F is measured with different initializations as shown in Figure 6. If we optimize over the latent space with different initial distributions of z, we find that the statistics of the solution z* are the same. Hence, the calculation of F is robust to different initial distributions of z, which means that even unlikely z\u2019s will converge to a z* that has typical statistics. \n\nLet us know if you have any further concerns about our paper, and thank you for the helpful feedback.\n\n[1] Zachary C Lipton and Subarna Tripathi.  Precise recovery of latent vectors from generative adversarial networks. arXiv preprint arXiv:1702.04782, 2017."}, "signatures": ["ICLR.cc/2020/Conference/Paper2586/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklsagBYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2586/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2586/Authors|ICLR.cc/2020/Conference/Paper2586/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139087, "tmdate": 1576860548453, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2586/Authors", "ICLR.cc/2020/Conference/Paper2586/Reviewers", "ICLR.cc/2020/Conference/Paper2586/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Official_Comment"}}}, {"id": "Bkx7i9i1KH", "original": null, "number": 1, "cdate": 1570908826631, "ddate": null, "tcdate": 1570908826631, "tmdate": 1572972318854, "tddate": null, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The paper proposes a new goodness of fit measure for generative models, and uses it to get insight into GAN's. While this is an important topic and a novel approach, I do not think the paper delivers on what it promises.\n\nI think this paper should be rejected. First, while it claims to be a general method for generative models it, it is limited to only  GANs and even for GANs it is limited. Second, most of the observations are nice but trivial, e.g. larger latent space leads to larger image.\n\nDetailed remarks:\n- The main point that the training set points x must have p(x)>0 under the model is naturally satisfied for almost all models except GANs such as VAEs, autoregressive model and flow models with standard implementations as the support is the whole space. This is in contrast to the claim in the paper that \"its applications can be extended to other generative networks such as Variational Autoencoders.\".\n- Even for GANs as this measure only looks at the support and not the distribution it is not clear if this measure does more then evaluate mode collapse. While this is an important task, it falls short of the promises the authors claim.\n- The authors claim that \"We demonstrate that our measure being minimized is a necessary and sufficient condition to detect mode collapse.\" but only show that it is necessary.\n- Proposition 1 is a trivial statement.\n- The authors claim that \" mode collapse happens if P(x) > 0 but minz ||G(z) \u2212 x|| > 0\". This is a main point by the authors, but it ignores the probability and only looks at the support. It has been shown that mode collapse happens even in 2d distributions, e.g. veegan paper, where it is easy to get the support to be the whole distribution.\n- The results in sec. 5 are quiet obvious, with a larger latent space you can naturally get a larger support, same as with a mixture model.\n\n\nIn general the method only looks at the support, ignoring the distribution over the support and is therefore very limited in evaluating generative models.  \n\n\nminor details:\n- In eq. 3 the integration should be w.r.t dP(x) for it to be monte-carlo approximated as it is in eq. 4.\n- Not 100% I understand what the authors try to say here - \"we pick the latent variable z and error ||G(z) \u2212 x||2 that corresponds to the smallest error instead of picking the latent variable that Adam Kingma and Ba (2014) finds.\""}, "signatures": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576450692795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2586/Reviewers"], "noninvitees": [], "tcdate": 1570237720721, "tmdate": 1576450692815, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Official_Review"}}}, {"id": "HJlxTZnitB", "original": null, "number": 2, "cdate": 1571697079725, "ddate": null, "tcdate": 1571697079725, "tmdate": 1572972318817, "tddate": null, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "invitation": "ICLR.cc/2020/Conference/Paper2586/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "This work proposed a new goodness of fit measure for generative network evaluations, which is based on how well the network can generate the training data. The measure is zero if the network could perfectly recover the training data, and would represent how far it is from generating the training set in the average manner of the total least square sense, where the one-to-one mapping between the generated data and the training sample is constructed through latent space optimization. Using the proposed measure, the authors showed an interesting trend present in the DCGAN training and the impact of the residual connection. The authors might want to add some discussion in Section 4.2 regarding why the residual connection is detrimental for covering the support.  Increasing the model complexity through larger latent space dimension and learning mixtures is proposed as solutions to improve the measure as well.\n\nWith all the interesting results presented, I still have the concerns about the sensitivity of the proposed measure:\n- It is an average over the training data or the selected sample. Above Section 4, the authors argued that \"\\hat{F}(G) > 0 meaning that we do not observe any memorization\". This seems overly assertive. Since the measure is an average over the training data, it has difficulty to differentiate between one network which has almost zero value for part of the training data but large values for the rest, and another network with roughly the same \\hat{F}(G) value but small values for all training data. The variance could help, but can not resolve this issue. This would be more important when the training data contains noise or outliers.\n- It only concerns the generation of the training data, but not the sampled data from the network (at least not directly). Therefore it has no direct control of the fidelity of the generated samples.\n- As shown by the authors, the proposed measure can be considered as the approximation of the true probability support not covered by the generative models, which also defines a necessary condition to avoid mode collapse. But what about the other part? It would have difficulty comparing two models with the same support but different high-density areas. Indeed, there are existing works which consider both the precision and recall of the generative models [1, 2, 3], and directly work with the generated samples instead of the training data. These should be discussed and compared with, not just the FID scores which have already been shown to have issues [3]. \n\nSome notations:\n- In the last equation on Page 2,  should it be L_{G} instead of L_{D}?\n- In the first equation on Page 3, should the denominator be N_{B} instead of N_{N}?\n- \"Optimality\" in terms of generative models may depend on the downstream tasks. I do not think there exists a universal definition of \"optimality\" for generative models.\n\n[1] M.S.M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via precision and recall. NeurIPS 2018.\n[2] L. Simon, R. Webster, and J. Rabin. Revisiting precision and recall definition for generative model evaluation. ICML 2019.\n[3] T. Kynkaanniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric for assessing generative models. Arxiv:1904.06991."}, "signatures": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2586/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["lorenzo.luzi.28@gmail.com", "randallbalestriero@gmail.com", "richb@rice.edu"], "title": "A GOODNESS OF FIT MEASURE FOR GENERATIVE NETWORKS", "authors": ["Lorenzo Luzi", "Randall Balestriero", "Richard Baraniuk"], "pdf": "/pdf/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "abstract": "We define a goodness of fit measure for generative networks which captures how well the network can generate the training data, which is necessary to learn the true data distribution.\nWe demonstrate how our measure can be leveraged to understand mode collapse in generative adversarial networks and provide practitioners with a novel way to perform model comparison and early stopping without having to access another trained model as with Frechet Inception Distance or Inception Score. This measure shows that several successful, popular generative models, such as DCGAN and WGAN, fall very short of learning the data distribution. We identify this issue in generative models and empirically show that overparameterization via subsampling data and using a mixture of models improves performance in terms of goodness of fit.", "keywords": ["generative adversarial networks", "goodness of fit", "inception score", "empirical approximation error", "validation metric", "frechet inception score"], "paperhash": "luzi|a_goodness_of_fit_measure_for_generative_networks", "original_pdf": "/attachment/16429311ac9b9527e8d65e59ddbfea4d3c7577a5.pdf", "_bibtex": "@misc{\nluzi2020a,\ntitle={A {\\{}GOODNESS{\\}} {\\{}OF{\\}} {\\{}FIT{\\}} {\\{}MEASURE{\\}} {\\{}FOR{\\}} {\\{}GENERATIVE{\\}} {\\{}NETWORKS{\\}}},\nauthor={Lorenzo Luzi and Randall Balestriero and Richard Baraniuk},\nyear={2020},\nurl={https://openreview.net/forum?id=BklsagBYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklsagBYPS", "replyto": "BklsagBYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2586/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1576450692795, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2586/Reviewers"], "noninvitees": [], "tcdate": 1570237720721, "tmdate": 1576450692815, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2586/-/Official_Review"}}}], "count": 9}