{"notes": [{"id": "rJxQTwAAaE", "original": null, "number": 5, "cdate": 1559320506722, "ddate": null, "tcdate": 1559320506722, "tmdate": 1559330667519, "tddate": null, "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "content": {"title": "Questions regarding \\Theta", "comment": "Hi, I found your paper to be very interesting and have some questions regarding the implementation. \n\n1. How did you determine the constrained set \\Theta and what does it mean in terms of implementation? Is it just weight clipping? If yes, what range do you use to clip?\n\n2. When solving the inner maximization in Algorithm 1, how did you use \\epsilon?\n\n3. What does WRM stand for?\n\nThank you!"}, "signatures": ["(anonymous)"], "readers": ["everyone"], "nonreaders": [], "writers": ["(anonymous)"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791683417, "id": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Area_Chair"], "cdate": 1512791683417}}}, {"tddate": null, "ddate": null, "tmdate": 1546570475487, "tcdate": 1509127909457, "number": 596, "cdate": 1518730171899, "id": "Hk6kPgZA-", "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "forum": "Hk6kPgZA-", "original": "SJ6kPgWC-", "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference"], "content": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "nonreaders": [], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1506717071958, "id": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference"], "reply": {"forum": null, "replyto": null, "writers": {"values": ["ICLR.cc/2018/Conference"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference"]}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"authors": {"required": false, "order": 1, "values-regex": ".*", "description": "Comma separated list of author names, as they appear in the paper."}, "authorids": {"required": false, "order": 2, "values-regex": ".*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "cdate": 1506717071958}}, "tauthor": "ICLR.cc/2018/Conference"}, {"id": "rJgcJQ4vlN", "original": null, "number": 21, "cdate": 1545188065524, "ddate": null, "tcdate": 1545188065524, "tmdate": 1545188065524, "tddate": null, "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "content": {"title": "Updated paper", "comment": "We have fixed some typos in the main text as well as the appendices. Thanks to members of the community for their feedback and interest in our paper."}, "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1519544404414, "tcdate": 1519544404414, "number": 19, "cdate": 1519544404414, "id": "Hy2IOylOM", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "Updated paper", "comment": "Thanks to the reviewers, AC, and senior PC for the valuable feedback; the controversy around the paper has certainly been elucidating. To reflect our understanding of the results in the paper, which we hope others share, we have updated our paper\u2019s title and introduction to make more explicit the claims (and concomitant limitations) of the work, better situating it in relation to other research. It seems that there was confusion about the results in the paper, and we have tried to clear up that our approach seeks a sweet spot balancing the goals of computational efficiency and certification of robustness. In the original submission, we noted in the introduction that \"The key feature of the penalty problem (2) is that moderate levels of robustness are achievable at essentially no computational or statistical cost for smooth losses $\\ell$.\u201d We\u2019ve tried to make this facet of the work more apparent throughout the paper, noting the connection between small perturbations/imperceptible changes to moderate levels of robustness. We believe this paper is a step towards efficiently building certifiably stable deep networks, though of course substantial work remains. We note a few possible directions for future research in the paper along with explicit limitations of our work. It is plausible to us that principled guarantees, even for complex deep networks, are possible with reasonable computational tools, and we hope others will continue to focus on these efforts."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1517260101266, "tcdate": 1517249189037, "number": 9, "cdate": 1517249189012, "id": "SyTifkpHf", "invitation": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["ICLR.cc/2018/Conference/Program_Chairs"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Program_Chairs"], "content": {"title": "ICLR 2018 Conference Acceptance Decision", "comment": "This paper attracted strong praise from the reviewers, who felt that it was of high quality and originality.  The broad problem that is being tackled is clearly of great importance.\n\nThis paper also attracted the attention of outside experts, who were more skeptical of the claims made by the paper. The technical merits do not seem to be in question, but rather, their interpretation/application. The perception by a community as to whether an important problem has been essentially solved can affect the choices made by other reviewers when they decide what work to pursue themselves, evaluate grants, etc. It's important that claims be conservative and highlight the ways in which the present work does not fully address the broader problem of adversarial examples.\n\nUltimately, it has been decided that the paper will be of great interest to the community. The authors have also been entrusted with the responsibility to consider the issues raised by the outside expert (and then echoed by the AC) in their final revisions.\n\nOne final note: In their responses to the outside expert, the authors several times remark that the guarantees made in the paper are, in form, no different from standard learning-theoretic claims: \"This criticism, however, applies to many learning-theoretic results (including those applied in deep learning).\" I don't find any comfort in this statement. Learning theorists have often focused on the form of the bounds (sqrt(m) dependence and, say, independence from the # of weights) and then they resort to empirical observations of correlation to demonstrate that the value of the bound is predictive for generalization. because the bounds are often meaningless (\"vacuous\") when evaluated on real data sets. (There are some recent examples bucking this trend.) In a sense, learning theorists have gotten off easy. Adversarial examples, however, concern security, and so there is more at stake. The slack we might afford learning theorists is not appropriate in this new context. I would encourage the authors to clearly explain any remaining work that needs to be done to move from \"good enough for learning theory\" to \"good enough for security\". The authors promise to outline important future work / open problems for the community. I definitely encourage this.\n\n\n\n\n", "decision": "Accept (Oral)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2018/Conference/-/Acceptance_Decision", "rdate": null, "ddate": null, "expdate": 1541175629000, "duedate": null, "tmdate": 1541177635767, "tddate": null, "super": null, "final": null, "reply": {"forum": null, "replyto": null, "invitation": "ICLR.cc/2018/Conference/-/Blind_Submission", "writers": {"values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values": ["ICLR.cc/2018/Conference/Program_Chairs"]}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["ICLR.cc/2018/Conference/Program_Chairs", "everyone"]}, "content": {"title": {"required": true, "order": 1, "value": "ICLR 2018 Conference Acceptance Decision"}, "comment": {"required": false, "order": 3, "description": "(optional) Comment on this decision.", "value-regex": "[\\S\\s]{0,5000}"}, "decision": {"required": true, "order": 2, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": [], "noninvitees": [], "writers": ["ICLR.cc/2018/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1541177635767}}}, {"tddate": null, "ddate": null, "tmdate": 1516840033537, "tcdate": 1516796789269, "number": 13, "cdate": 1516796789269, "id": "S1pdil8Sz", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "H1g0Nx8rf", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "Email notification received. Response will be uploaded later today.", "comment": "We just received an email notification abut this comment a few minutes ago and somehow did not receive any notification of the original comment uploaded on 21 January. We will upload a response later today."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1516839971984, "tcdate": 1516839971984, "number": 18, "cdate": 1516839971984, "id": "rkn74s8BG", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "rJnkAlLBf", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "Response uploaded below.", "comment": "Apologies for the (evidently) tardy response. We have now uploaded a response to the area chair's comments (see below)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1516814908452, "tcdate": 1516814908452, "number": 16, "cdate": 1516814908452, "id": "HJNBMS8rf", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "rklzlzBVf", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "Response to concerns raised by Madry", "comment": "Thank you for the detailed follow-up. \n\nWe will make the point that we deal with imperceptible changes clearer in the paper. We had emphasized that our work is motivated by imperceptible adversarial perturbations from the second paragraph of the paper. We will make this point even clearer and quantify our statements on performance so that there is no confusion that we mainly consider imperceptible changes.\n\nAs we have noted in our previous response, we agree with you in that robustness to larger perturbations is an important research direction. The point we made in our original response is that infinity-norms may not be the most appropriate norm to consider in this perceptible attack setting. For example, a 1-norm-constrained adversary can change a few pixels in a very meaningful way--with infinity-norms approaching 1--which may be a more suitable model for a perceptible adversary. There are a number of concurrent works on this topic that we believe could lead to more robust learning systems. \n\nIt is still open whether distributionally robust algorithms (empirically) allow hedging against large adversarial perturbations. At this point, we believe it would be imprudent to call this class of methods \u201cinherently restricted\u201d to the small perturbation regime; indeed, any heuristic method (such as one based on projected gradient descent) has the same restrictions, at least in terms of rigorous guarantees. A more thorough study\u2014on more diverse datasets, model classes and hyperparameter settings\u2014should be conducted in order to draw any meaningful conclusions. We hope to contribute to this effort in the future but we invite others as well, since we believe this is an important question for the community to answer.\n\nOur certificate of robustness given in Theorem 3 is efficiently computable for small values of rho, or equivalently, for imperceptible attacks. Hence, this data-dependent certificate provides a upper bound on the worst-case loss so that you are guaranteed to do no worse than this number with high probability. For the achieved level of robustness (rho hat in our notation), our bounds do imply that we are robust to perturbation budgets of this size. Hence, we would argue that Theorem 3 is indeed a flavor of result that satisfies the desiderata you described.\n\nThere are limitations, and we hope that subsequent work will improve our learning guarantees with a better dependence on model size. This criticism, however, largely applies to most learning-theoretic results applied to deep learning.\n\nAs we mentioned in our introduction, we agree that recent advances in verification techniques for deep learning are a complementary and important research direction for achieving robust learning systems. Our understanding of these techniques is that they currently have prohibitive computational complexity, even on small datasets such as MNIST. Our results complement these approaches by providing a weaker statistical guarantee with computational effort more comparable to the vanilla training times.\n\nThe motivation of this paper comes from the fact that formal guarantees on arbitrary levels of robustness is NP-hard. We study the regime of small to moderate levels of robustness to provide guarantees for this regime."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1516797411810, "tcdate": 1516797411810, "number": 15, "cdate": 1516797411810, "id": "rJnkAlLBf", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "S1pdil8Sz", "signatures": ["ICLR.cc/2018/Conference/Paper596/Area_Chair"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Area_Chair"], "content": {"title": "OpenReview bug, it seems", "comment": "Sorry for the rush created by this likely OpenReview bug.  A response today would be most appreciated!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1516795079976, "tcdate": 1516795079976, "number": 12, "cdate": 1516795079976, "id": "H1g0Nx8rf", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["ICLR.cc/2018/Conference/Paper596/Area_Chair"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Area_Chair"], "content": {"title": "Dear authors...", "comment": "You have been contacted now by the Area Chair and the Program Chair and asked to respond to comments by the Area Chair. It is imperative that you respond."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1515695721918, "tcdate": 1515687943703, "number": 4, "cdate": 1515687943703, "id": "rklzlzBVf", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "forum": "Hk6kPgZA-", "replyto": "rJBbuPTmz", "signatures": ["~Aleksander_Madry1"], "readers": ["everyone"], "writers": ["~Aleksander_Madry1"], "content": {"title": "Re: [Part I] Our goal is to defend against imperceptible perturbations. More empirical evaluations available.", "comment": "[I put my reply here as the threads below are now a bit hard to follow.]\n\nThank you for responding to my comments and making the effort to provide more data. This indeed helps me understand this work better.\n\nI agree that studying the regime of small adversarial perturbation budget epsilon is a very valid research goal. I think, however, that it is important to explicitly mention in the paper that this is the target. Especially, as the proposed methods seem to be inherently restricted to apply to only such a small epsilon regime. \n\nI am not sure though that I agree with the argument why the regime of larger values of epsilon might be less interesting. Yes, some of the larger perturbations will be clearly visible to a human, but some (e.g., the ones that correspond to a change of the background color or its pattern) will not - and we still would like to be robust to them. After all, security guarantees are about getting \"for all\", not \"for some\" guarantees. \n\nNow, regarding being explicit about the constants in the bounds, I agree that many optimization and statistical learning guarantees do not provide not provide explicit constants. However, I think the situation in the context considered here is fundamentally different. \n\nAfter all, for example, in the context of generalization bounds, we always have a meaningful way of checking if a given bound \"triggered\" for a given model and dataset by testing its performance on a validation/test set. When we talk about robustness guarantee, the whole point is to have it hold even against attacks that we are not able to produce ourselves (but the adversary might). Then, we really need a very concrete guarantee of the form \"(With high probability) the model classifies correctly 90% of the test set against perturbation budget of epsilon <= 0.1\u201d. \n\nIn the light of this, providing a guarantee of the form \"(With high probability) the model correctly classifies 90% of the test set against perturbation budget of some positive epsilon\", which is what the proposed guarantees seem to provide, is somewhat less meaningful. (One could argue that, after all, there is always some positive epsilon for which the model is robust.)\n\nIt might be worth noting that, e.g., for MNIST, we currently are able to deliver guarantees of the former (explicit) type. For instance, there is a recent work of Kolter and Wong (https://arxiv.org/abs/1711.00851). Although they provide such guarantees via verification techniques and not by proving an explicit generalization bound.\n\nFinally, I am not sure how much bearing the formal NP-hardness of certifying the robustness has here. (I assume you are referring to the result in Appendix B.) Could you elaborate?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791683417, "id": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Area_Chair"], "cdate": 1512791683417}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642476616, "tcdate": 1511800281907, "number": 1, "cdate": 1511800281907, "id": "HJ-1AnFlM", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Review", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["ICLR.cc/2018/Conference/Paper596/AnonReviewer2"], "readers": ["everyone"], "content": {"title": "a very interesting approach to adversarial training based on robustness over Wasserstein balls", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples. The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution. In particular, the authors adopt the Wasserstein distance to define the ambiguity sets. This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost. The theoretical results in the paper are supported by experiments.\n\nOverall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642476527, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/AnonReviewer2", "ICLR.cc/2018/Conference/Paper596/AnonReviewer1", "ICLR.cc/2018/Conference/Paper596/AnonReviewer3"], "reply": {"forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642476527}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642476577, "tcdate": 1511887853232, "number": 2, "cdate": 1511887853232, "id": "HySlNfjgf", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Review", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["ICLR.cc/2018/Conference/Paper596/AnonReviewer1"], "readers": ["everyone"], "content": {"title": "Adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ", "rating": "9: Top 15% of accepted papers, strong accept", "review": "This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned). I think the idea is intuitive and reasonable, the result is nice. Although it only holds when light robustness are imposed, but in practice, this seems to be more of the case than say large deviation/adversary exists. As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642476527, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/AnonReviewer2", "ICLR.cc/2018/Conference/Paper596/AnonReviewer1", "ICLR.cc/2018/Conference/Paper596/AnonReviewer3"], "reply": {"forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642476527}}}, {"tddate": null, "ddate": null, "original": null, "tmdate": 1515642476541, "tcdate": 1512147964590, "number": 3, "cdate": 1512147964590, "id": "rkx-2-y-f", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Review", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["ICLR.cc/2018/Conference/Paper596/AnonReviewer3"], "readers": ["everyone"], "content": {"title": "Very interesting principled analysis of robust learning", "rating": "9: Top 15% of accepted papers, strong accept", "review": "In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0.\n\nSince the min-max problem is intractable in general, what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem. If the duality parameter is large enough, the functions become convex given that the initial losses are smooth. \n\nWhat follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions. Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "writers": [], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1511845199000, "tmdate": 1515642476527, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Review", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers"], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/AnonReviewer2", "ICLR.cc/2018/Conference/Paper596/AnonReviewer1", "ICLR.cc/2018/Conference/Paper596/AnonReviewer3"], "reply": {"forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "writers": {"values": []}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"required": true, "order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"required": true, "order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,200000}"}, "rating": {"required": true, "order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"required": true, "order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1519621199000, "cdate": 1515642476527}}}, {"tddate": null, "ddate": null, "tmdate": 1515186674787, "tcdate": 1515186674787, "number": 5, "cdate": 1515186674787, "id": "rkix5PTQf", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "Hk2kQP3Qz", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "Addressing concerns", "comment": "Thank you for your interest in our paper. We appreciate your detailed feedback.\n\n1. This is a fair criticism; it seems to apply generally to most learning-theoretic guarantees on deep learning (though see the recent work of Dziugaite and Roy, https://arxiv.org/abs/1703.11008 and Bartlett, Foster, Telgarsky https://arxiv.org/pdf/1706.08498.pdf). We believe that our statistical guarantees in Theorems 3 and 4 are steps towards a principled understanding of adversarial training. Replacing our current covering number arguments with more intricate notions such as margin based-bounds (Bartlett et al. 2017)) would extend the scope of our theoretical guarantees; as Bartlett et al. provide covering number bounds, it seems likely that we could massage them into applying in Theorem 3 (Eqs. (11)-(12)). This is a meaningful future research direction.\n\n\n2. In Figure 2, we plot our certificate of robustness on two datasets (omitting the statistical error term) and observe that our data-dependent upper bound on the worst-case performance is reasonable. This roughly implies that our adversarial training procedure generalizes, allowing us to learn to defend against attacks on the test set.\n\n\u201cIn the experimental sections, good performance is achieved at test time. But it would be more convincing if the performance for training data is also shown. The current experiments don't seem to evaluate generalization of the proposed WRM. Furthermore, analysis of other classification problems (cifar10, cifar 100, imagenet) is highly desired.\u201c\n\nThese are both great suggestions. We are currently working on experiments with subsets of Imagenet and will include them in a revision (soon we hope).\n\n3. Our adversarial training algorithm has intimate connections with other previously proposed heuristics. Our main theoretical contribution is that for small adversarial perturbations, we can show both computational and statistical guarantees for our procedure. More specifically, the computational guarantees for our algorithm are indeed based on the curvature of the L2-norm; provably efficient computation of attacks based on infinity-norms remains open."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1515186612695, "tcdate": 1515186612695, "number": 4, "cdate": 1515186612695, "id": "rJ63YwTQM", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "BJVnpJPXM", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "[Part I] Our goal is to defend against imperceptible perturbations. More empirical evaluations available. ", "comment": "Thank you for your interest in our paper. We appreciate the detailed feedback and probing questions.\n\nUpon your suggestions during our meeting at NIPS, we have included a more extensive empirical evaluation of our algorithm. Most notably, we trained and tested our method\u2014alongside other baselines, including [MMSTV17]\u2014on large values of adversarial budgets. We further compared our algorithm trained against L2-norm Lagrangian attacks against other heuristic methods trained against infinity-norm attacks. Lastly, we proposed a (heuristic) proximal variant of our algorithm that learns to defend against infinity-norm attacks. See Appendices A.4, A.5, and E for the relevant exposition and figures.\n\n1. Empirical Evaluation on Large Adversarial Budgets\n\nOur primary motivation of this paper is to provide a theoretically principled algorithm that can defend against small adversarial perturbations. In particular, we are concerned with provable procedures against small adversarial perturbations that can fool deep nets but are imperceptible to humans. Our main finding in the original empirical experiments in Section 4 was that for such small adversarial perturbations, our principled algorithm matches or outperforms other existing heuristics. (See also point 2 below.)\n\nThe adversarial budget epsilon = .3 in the infinity-norm you suggest allows attacks that are highly visible to the human eye. For example, one can construct hand-tuned perturbations that look significantly different from the original image (see https://www.dropbox.com/sh/c6789iwhnooz5po/AABBpU_mg-FRRq7PT1LzI0GAa?dl=0). Defending against such attacks is certainly interesting, but was not our main goal. This probably warrants a longer discussion, but it is not clear to us that infinity-norm-bounded attacks are most appropriate if one allows perceptible image modifications. An L1-budgeted adversary might be able to make small changes in some part of the image, which yields a different set of attacks.\n\nIn spite of the departure of large perturbations from our nominal goal of protection against small changes, we test our algorithm on attacks with large adversarial budgets in Appendix A.4. In this case, our algorithm is a heuristic\u2014as are other methods for large adversarial budgets\u2014but we nevertheless match the performance of other methods (FGM, IFGM, PGM) trained against L2-norm adversaries.\n\nSince our computational guarantees are based on strong concavity w.r.t. Lp-norms for p \\in (1, 2], our robustly-fitted network defends against L2-norm attacks. Per the suggestion to compare against networks trained to defend against infinity-norm attacks\u2014and we agree, this is an important comparison that we did not perform originally (though we should have)\u2014we compared our method with other heuristics in Appendix A.5.1. On imperceptible L2 and infinity-norm attacks, our algorithm outperforms other heuristics trained to defend against infinity-norm attacks (Figures 11 and 12). On larger attacks, particularly infinity-norm attacks, we observe that other heuristics trained on infinity-norm attacks outperform our method (Figure 12). In this sense, the conclusions we reached from the main figures in our paper\u2014where we considered imperceptible perturbations\u2014are still valid: we match or outperform other heuristic methods for small perturbations.\n\n(Continued in Part II)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"ddate": null, "tddate": 1515186511699, "tmdate": 1515186576938, "tcdate": 1515186497472, "number": 3, "cdate": 1515186497472, "id": "HyFBKPp7z", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "BJVnpJPXM", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "[Part II] Our goal is to defend against imperceptible perturbations. More empirical evaluations available.", "comment": "2. Theoretical Guarantees\n\nThe motivation for our work is that computing the worst-case perturbation of a deep network under norm-constraints is typically intractable. As we state in the introduction, we simply give up on computing worst-case perturbations at arbitrary budget levels, instead considering small adversarial perturbations. Our theoretical guarantees are concerned with imperceptible changes; we give computational and statistical guarantees for such small (adversarial) perturbations. This is definitely a limit of the approach; given that it is NP hard to certify robustness for larger perturbations this may be challenging to get around.\n\nOur main theoretical guarantee is the certificate of robustness\u2014a data-dependent upper bound on the worst-case performance\u2014given in Theorem 3. This upper bound applies in general, although its efficient computation is only guaranteed for large penalty parameters \\gamma and smooth losses. Similarly, as you note, Theorems 2 and 4 only apply in such regimes. To address this, we augment our theoretical guarantees for small adversarial budgets with empirical evaluations in Section 4 and Appendix A. We empirically checked if our level of \\gamma = .385 (=.04 * C_2) is above the estimated smoothness parameter at the adversarially trained model and observed that this condition is satisfied on 98% of the training data points.\n\nOur guarantees indeed depend on the problem-dependent smoothness parameter. As with most optimization and statistical learning guarantees, this value is often unknown. This limitation applies to most learning-theoretic results, and we believe that being adaptive to such problem-dependent constants is a meaningful future research direction. With that said, it seems likely (though we have not had time to verify this) that the recent work of Bartlett et al. (https://arxiv.org/pdf/1706.08498.pdf) should apply--it provides covering number bounds our Theorem 3 (Eq. (11-12)) can use.\n\nWe hope that our theoretical guarantees are a step towards understanding the performance of these adversarial training procedures. Gaps still remain; we hope future work will close this gap."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1515186243502, "tcdate": 1515186201974, "number": 2, "cdate": 1515186201974, "id": "Hkzmdv67G", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "H1wDpaNbM", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "Comparison with Roy et al. (2017)", "comment": "Thank you for bringing our attention to Roy et al. (2017). In Section 4.3, we adapted our adversarial training algorithm in the supervised learning setting to reinforcement learning; this approach shares similar motivations as Roy et al. (2017)\u2014and more broadly, the robust MDP literature\u2014where we also solve approximations of the worst-case Bellman equation. Compared to our Wasserstein ball, Roy et al. (2017) uses more simple and tractable worst-case regions. While they give convergence guarantees for their algorithm, the empirical performance of these different worst-case regions remains open.\n\nAnother key difference in our experiments is that we assumed access to the simulator for updating the underlying state. This allows us to explore bad regions better. Nevertheless, our adversarial state update in Eqn (20) can be replaced with an adversarial reward update for settings where the simulator cannot be accessed."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1515186172886, "tcdate": 1515186172886, "number": 1, "cdate": 1515186172886, "id": "rJBbuPTmz", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["ICLR.cc/2018/Conference/Paper596/Authors"], "readers": ["everyone"], "writers": ["ICLR.cc/2018/Conference/Paper596/Authors"], "content": {"title": "Response to Reviews", "comment": "We thank the reviewers for their time and positive feedback. We will use the comments and suggestions to improve the quality and presentation the paper. In addition to cleaning up our exposition, we added some content to make our main points more clear. We address these main revisions below.\n\nOur formulation (2) is general enough to include a number of different adversarial training scenarios. In Section 2 (and more thoroughly in Appendix D), we detail how our general theory can be modified in the supervised learning setting so that we learn to defend against adversarial perturbations to only the feature vectors (and not the labels). By suitably modifying the cost function that defines the Wasserstein distance, our formulation further encompasses other variants such as adversarial perturbations only to a fixed small region of an image.\n\nWe emphasize that our certificate of robustness given in Theorem 3 applies for any level of robustness \\rho. Our results imply that the output of our principled adversarial training procedure has worst-case performance no worse than this data-dependent certificate. Our certificate is efficiently computable, and we plot it in Figure 2 for our experiments. We see that in practice, the bound indeed gives a meaningful performance guarantee against attacks on the unseen test sets.\n\nWhile the primary focus of our paper is on providing provable defenses against imperceptible adversarial perturbations, we supplement our previous results with a more extensive empirical evaluation. In Appendix A.4, we augment our results by evaluating performance against L2-norm adversarial attacks with larger adversarial budgets (higher values of \\rho or \\epsilon). Our method also becomes a heuristic for such large values of adversarial budgets, but we nevertheless match the performance of other methods (FGM, IFGM, PGM) trained against L2-norm adversaries. In Appendix A.5.1, we further compare our method\u2014\u2014which is trained to defend against L2-norm attacks\u2014\u2014with other adversarial training algorithms trained against inf-norm attacks. We also propose a new (heuristic) proximal algorithm for solving our Lagrangian problem with inf-norms, and test its performance against other methods in Appendix A.5.2. In both sections, we observe that our method is competitive with other methods against imperceptible adversarial attacks, and performance starts to degrade as the attacks become visible to the human eye.\n\nAgain, we appreciate the reviewers' close reading and thoughtful comments."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1516825730934, "id": "ICLR.cc/2018/Conference/-/Paper596/Official_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs"}, "signatures": {"values-regex": "ICLR.cc/2018/Conference/Paper596/AnonReviewer[0-9]+|ICLR.cc/2018/Conference/Paper596/Authors|ICLR.cc/2018/Conference/Paper596/Area_Chair|ICLR.cc/2018/Conference/Program_Chairs", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Paper596/Authors_and_Higher", "ICLR.cc/2018/Conference/Paper596/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Paper596/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Area_Chair", "ICLR.cc/2018/Conference/Program_Chairs"], "cdate": 1516825730934}}}, {"tddate": null, "ddate": null, "tmdate": 1515119332259, "tcdate": 1515119332259, "number": 3, "cdate": 1515119332259, "id": "Hk2kQP3Qz", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Rigorous approach to Adversarial training but some concerns", "comment": "The problems are very well formulated (although only the L2 case is discussed). Identifying a concave surrogate in this mini-max problem is illuminating. The interplay between optimal transport, robust statistics, optimization and learning theory make the work a fairly thorough attempt at this difficult problem. Thanks to the authors for turning many intuitive concepts into rigorous maths. There are some potential concerns, however: \n\n1. The generalization bounds in THM 3, Cor 1, THM 4 for deep neural nets appear to be vacuous, since they scale like \\sqrt (d/n), but d > n for deep learning. This is typical, although such generalization bounds are not common in deep adversarial training. So establishing such bounds is still interesting.\n\n2. Deep neural nets generalize well in practice, despite the lack of non-vacuous generalization bounds. Does the proposed WRM adversarial training procedure also generalize despite the vacuous bounds? \n\nIn the experimental sections, good performance is achieved at test time. But it would be more convincing if the performance for training data is also shown. The current experiments don't seem to evaluate generalization of the proposed WRM. Furthermore, analysis of other classification problems (cifar10, cifar 100, imagenet) is highly desired. \n\n3. From an algorithmic viewpoint, the change isn't drastic. It appears that it controls the growth of the loss function around the L2 neighbourhood of the data manifold (thanks to the concavity identified). Since L2 geometry has good symmetry, it makes the decision surface more symmetrical between data (Fig 1). \n\nIt seems to me that this is the reason for the performance gain at test time, and the size of such \\epsilon tube is the robust certificate. So it is unclear how much success is due to the generalization bounds claimed. \n\nI think there is enough contribution in the paper, but I share the opinion of Aleksander Madry, and would like to be corrected for missing some key points."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791683417, "id": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Area_Chair"], "cdate": 1512791683417}}}, {"tddate": null, "ddate": null, "tmdate": 1514761644468, "tcdate": 1514761644468, "number": 2, "cdate": 1514761644468, "id": "BJVnpJPXM", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["~Aleksander_Madry1"], "readers": ["everyone"], "writers": ["~Aleksander_Madry1"], "content": {"title": "An interesting attempt but some of the key claims seem to be inaccurate and miss comparison to proper baselines. ", "comment": "Developing principled approaches to training adversarially robust models is an important (and difficult) challenge. This is especially the case if such an approach is to offer provable guarantees and outperform state of the art methods. \n\nHowever, after reading this submission, I am confused by some of the key claims and find them to be inaccurate and somewhat exaggerated. In particular, I believe that the following points should be addressed and clarified:\n\n1. The authors claim their methods match or outperform existing methods. However, their evaluations seem to miss some key baselines and parameter regimes. \n    \nFor example, when reporting the results for l_infty robustness - a canonical evaluation setting in most previous work - the authors plot (in Figure 2b) the robustness only for the perturbations whose size eps (as measured in the l_infty norm) is between 0 and 0.2. (Note that in Figure 2b the x-axis is scaled as roughly 2*eps.) However, in order to properly compare against prior work, one needs to be able to see the scaling for larger perturbations.\n\nIn particular, [MMSTV\u201917] https://arxiv.org/abs/1706.06083 gives a model that exhibits high robustness even for perturbations of l_infty size 0.3. What robustness does the approach proposed in this work offer in that regime? \n\nAs I describe below, my main worry is that the theorems in this work only apply for very small perturbations (and, in fact, this seems to be an inherent limitation of the whole approach). Hence, it would be good to see if this is true in practice as well.  \nIn particular, Figure 2b suggests that this method will indeed not work for larger perturbations. I thus wonder in what sense the presented results outperform/match previous work?\n\nAfter a closer look, it seems that this discrepancy occurs because the authors are reproducing the results of [MMSTV\u201917] using l_2 based adversarial training.  [MMSTV\u201917] uses l_infity based training and achieves much better results than those reported in this submission. This artificially handicaps the baseline from [MMSTV\u201917]. That is, there is a significantly better baseline that is not reflected in Figure 2b. I am not sure why the authors decided to do that.\n\n2. It is hard to properly interpret what actual provable guarantees the proposed techniques offer. More concretely, what is the amount of perturbation that models trained using these techniques are provably robust to? \n\nBased on the presented theorems, it is unclear why they should yield any non-vacuous generalization bounds. \n\nIn particular, as far as I can understand, there might be no uniform bound on the amount of perturbation that the trained model will be robust to. This seems to be so as the provided guarantees (see Theorem 4) might give different perturbation resistance for different regions of the underlying distribution. In fact, it could be that for a significant fraction of points we have a (provable) robustness guarantee only for vanishingly small perturbations. \n\nMore precisely, note that the proposed approach uses adversarial training that is based on a Lagrangian formulation of finding the worst case perturbation, as opposed to casting this primitive as optimization over an explicitly defined constraint set. These two views are equivalent as long as one has full flexibility in setting the Lagrangian penalization parameter gamma. In particular, for some instances, one needs to set gamma to be *small enough*, i.e., sufficiently small so as it does not exclude norm-eps vectors from the set of considered perturbations. (Here, eps denotes the desired robustness measured in a specific norm such as l_infty, i.e., the prediction of our model should not change under perturbations of magnitude up to eps.)\n\nHowever, the key point of the proposed approach is to ensure that gamma is always set to be *large enough* so as the optimized function (i.e., the loss + the Lagrangian penalization) becomes concave (and thus provably tractable). Specifically, the authors need gamma to be large enough to counterbalance the (local) smoothness parameter of the loss function. \n\nThere seems to be no global (and sufficiently small) bound on this smoothness and, as a result, it is unclear what is the value of the eps-based robustness guarantee offered once gamma is set to be as large as the proposed approach needs it to be.\n\nFor the same reason (i.e., the dependence on the smoothness parameter of the loss function that is not explicitly well bounded), the provided generalization bounds - and thus the resulting robustness guarantees - might be vacuous for actual deep learning models. \n\nIs there something I am missing here? If not, what is the exact nature of the provable guarantees that are offered in the proposed work?\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791683417, "id": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Area_Chair"], "cdate": 1512791683417}}}, {"tddate": null, "ddate": null, "tmdate": 1512525151114, "tcdate": 1512525151114, "number": 1, "cdate": 1512525151114, "id": "H1wDpaNbM", "invitation": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "forum": "Hk6kPgZA-", "replyto": "Hk6kPgZA-", "signatures": ["(anonymous)"], "readers": ["everyone"], "writers": ["(anonymous)"], "content": {"title": "Very cool!", "comment": "Very interesting work! I was wondering how the robust MDP/RL setup compares to http://papers.nips.cc/paper/6897-reinforcement-learning-under-model-mismatch.pdf ? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Certifying Some Distributional Robustness with Principled Adversarial Training", "abstract": "Neural networks are vulnerable to adversarial examples and researchers have proposed many heuristic attack and defense mechanisms. We address this problem through the principled lens of distributionally robust optimization, which guarantees performance under adversarial input perturbations.  By considering a Lagrangian penalty formulation of perturbing the underlying data distribution in a Wasserstein ball, we provide a training procedure that augments model parameter updates with worst-case perturbations of training data. For smooth losses, our procedure provably achieves moderate levels of robustness with little computational or statistical cost relative to empirical risk minimization. Furthermore, our statistical guarantees allow us to efficiently certify robustness for the population loss. For imperceptible perturbations, our method matches or outperforms heuristic approaches.\n", "pdf": "/pdf/f35b7c6c945a42b2b053053616d2393ae0ee6304.pdf", "TL;DR": "We provide a fast, principled adversarial training procedure with computational and statistical performance guarantees.", "paperhash": "sinha|certifying_some_distributional_robustness_with_principled_adversarial_training", "authors": ["Aman Sinha", "Hongseok Namkoong", "John Duchi"], "keywords": ["adversarial training", "distributionally robust optimization", "deep learning", "optimization", "learning theory"], "authorids": ["amans@stanford.edu", "hnamk@stanford.edu", "jduchi@stanford.edu"], "_bibtex": "@inproceedings{\nsinha2018certifiable,\ntitle={Certifiable Distributional Robustness with Principled Adversarial Training},\nauthor={Aman Sinha and Hongseok Namkoong and John Duchi},\nbooktitle={International Conference on Learning Representations},\nyear={2018},\nurl={https://openreview.net/forum?id=Hk6kPgZA-},\n}"}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1512791683417, "id": "ICLR.cc/2018/Conference/-/Paper596/Public_Comment", "writers": ["ICLR.cc/2018/Conference"], "signatures": ["ICLR.cc/2018/Conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"replyto": null, "forum": "Hk6kPgZA-", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2018/Conference/Authors_and_Higher", "ICLR.cc/2018/Conference/Reviewers_and_Higher", "ICLR.cc/2018/Conference/Area_Chairs_and_Higher", "ICLR.cc/2018/Conference/Program_Chairs"]}, "content": {"title": {"required": true, "order": 0, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"required": true, "order": 1, "description": "Your comment or reply (max 5000 characters).", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2018/Conference/Paper596/Authors", "ICLR.cc/2018/Conference/Paper596/Reviewers", "ICLR.cc/2018/Conference/Paper596/Area_Chair"], "cdate": 1512791683417}}}], "count": 22}