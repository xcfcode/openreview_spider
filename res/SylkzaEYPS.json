{"notes": [{"id": "SylkzaEYPS", "original": "rkliBBFIDH", "number": 398, "cdate": 1569438983473, "ddate": null, "tcdate": 1569438983473, "tmdate": 1577168276262, "tddate": null, "forum": "SylkzaEYPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "Ki4tlY21qy", "original": null, "number": 1, "cdate": 1576798695263, "ddate": null, "tcdate": 1576798695263, "tmdate": 1576800940314, "tddate": null, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Decision", "content": {"decision": "Reject", "comment": "This paper presents an encoder-decoder based architecture to generate summaries. The real contribution of the paper is to use  a recoder matrix which takes the output from an existing encoder-decoder network and tries to generate the reference summary again. The output here is basically the softmax layer produced by the first encoder-decoder network which then goes through a feed-forward layer before being fed as embeddings into the recoder. So, since there is no discretization, the whole model can be trained jointly. (the original loss of the first encoder-decoder model is used as well anyway).\n\nI agree with the reviewers here, that this whole model can in fact be viewed as a large encoder-decoder model, its not really clear where the improvements come from. Can you just increase the number of parameters of the original encoder-decoder model and see if it performs as good as the encoder-decoder + recoder? The paper also does not achieve SOTA on the task as there are other RL based papers which have been shown to perform better, so the choice of the recorder model is also not empirically justified. I recommend rejection of the paper in its current form.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795717182, "tmdate": 1576800267430, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper398/-/Decision"}}}, {"id": "BJgibAOkiS", "original": null, "number": 5, "cdate": 1572994562935, "ddate": null, "tcdate": 1572994562935, "tmdate": 1572994562935, "tddate": null, "forum": "SylkzaEYPS", "replyto": "ryxm26_1sr", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment", "content": {"title": "<continuation of above>", "comment": ">  how was the length penalty determined to be the graduated curve mentioned in section 5? I would expect to have comparison against other approaches\n\nThe length penalty function was formed based on intuition and a few adhoc trials. It is a mechanism for controlling length and did not significantly affect quality otherwise, so we did not experiment extensively with it or perform comparisons with other approaches such as those we cited in Section 3.3. One of the strengths of our approach is that we do not require changes to the summarizer model, which the cited alternatives require.\n\n\n> On the human evaluation experiments: the difference between the two models is quite small\n\nOur improvements compare favorably against other state-of-the-art work, as discussed in Section 5.2, so I'm not sure why you say this.\n\n\n> the workers were not allowed to say that the models were equally good/bad.\n\nWe did not allow \"equal\" ratings because summaries truly equal in quality would be randomly assigned as a win for either side and cancel out in aggregate, so allowing \"equal\" ratings would only reduce the number of useful ratings by allowing the rater to avoid making a tough decision in close cases.\n\n\n> there is no inter-annotator agreement\n\nAs mentioned in Section 5.2, we do not need a confident score for any one example, since we are assessing the quality of the algorithm. Ideally we would sample from the space of all (viewer, example) pairs, so limiting each example to one worker allows us to sample a wider diversity of examples using a given number of workers -- we used 300 (viewer, example) pairs for 11490 examples. We could also have limited each worker to 1 example instead of 5 to get a greater diversity of viewers, but we felt that may expose us to a higher fraction of ratings from workers making mistakes on their first and only example.\n\nConfident results for each example would only be needed if the ratings were to be used as training data for another algorithm, which is not the case for us.\n\n\n> Showing only the first 400 tokens of the original document would incorrectly disadvantage models selecting content from later in the articles.\n\nBoth the test model and the comparison baseline (See et al. 2017) were limited to 400 in line with their work, so there is no disadvantage for either.\n\n\nGiven the various misunderstandings, we would appreciate it if you could review the paper again after rereading. Thank you.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper398/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylkzaEYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper398/Authors|ICLR.cc/2020/Conference/Paper398/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172056, "tmdate": 1576860537041, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment"}}}, {"id": "ryxm26_1sr", "original": null, "number": 4, "cdate": 1572994474723, "ddate": null, "tcdate": 1572994474723, "tmdate": 1572994474723, "tddate": null, "forum": "SylkzaEYPS", "replyto": "rklzwRhQKH", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment", "content": {"title": "You have misunderstood the paper entirely. ", "comment": "You appear to have misunderstood the recoder and how it helps the summarizer, and this is causing confusion for the questions below.\n\n\n> The intuition offered in the paper is that a good summary should produce itself via the recoder network.... It doesn't make sense to my that a summary should produce itself via a neural network, unless we are training an auto-encoder.\n\nThe intuition behind the recoder is that a good summary should be *useful as input for producing the reference summary*, not necessarily *equal* the reference or \"produce itself\". This is exactly why we need the recoder. The decoder's output summaries will almost never equal the reference summary, causing a disparity between the sequence prefix that the summarizer sees in training versus test.\n\nThis is discussed in the Introduction, so please review that section for further clarification.\n\n\n> I find it odd that ROUGE score is dismissed as a loss to train against ... but then 1 ROUGE point difference is considered a \"significant\" improvement....\n\nWe never dismissed ROUGE, which is why we evaluated using ROUGE in Section 5.1. We said that the recoder can in principle capture more complex notions of quality than ROUGE, so ROUGE *alone* is not enough, and that's why we presented human evaluations in Section 5.2.\n\nI would also say that 1 point is not an insignificant improvement. Most of the reported ROUGE scores for the near state-of-the-art models cited vary within a range of about 2 points, such as 39.47 to 41.69 for ROUGE-1.\n\n\n> making such claims without statistical significance testing is misleading.  \n\nI do not see how what we presented is misleading, especially when confidence intervals output by the ROUGE tool was always available in our submitted code and results: https://github.com/iclr2020recoder/code_for_paper . We did not list them in the paper to reduce clutter in the limited space available. This omission is in line with other papers such as Celikyilmaz et al. (2018). \n\nOne reason to omit them is that the confidence intervals are generally small (+/- 0.2 points). As an example, here are the results for pgen_cov+recoder (lambda=0.1), which can be seen in the github repository link:\n\nROUGE-1: 0.4044 with confidence interval (0.4022, 0.4065)\nROUGE-2: 0.1815 with confidence interval (0.1793, 0.1838)\nROUGE-3: 0.3690 with confidence interval (0.3668, 0.3712)\n\n\n> Sure it has flaws, if there is something better why not use it for evaluation? Furthermore, claiming that the recoder does a better job requires evidence. Why not train this extra function against human judgements?\n\nWe did evaluate using human evaluations as a way to improve on the limitations of the ROUGE heuristic, so I'm not sure what you're asking here. Together these evaluations are evidence of the recoder's improvement.\n\nI'm also not sure what you mean by training \"this extra function\" against human judgements. It would not make sense to train the recoder using human judgements. You may be thinking of reinforcement learning style algorithms, discussed in Related Work, that train toward a metric. \n\n\n> The approach is in my view a kind of actor-critic approach; the recoder could play that role, and in fact the Bahdanau et al. 2016 paper cited does this. However no comparison is offered, be it theoretical or experimental. Furthermore, the criticism that sequence-level training requires differentiable losses is incorrect; MIXER for example does train against scores such as ROUGE that are not differentiable. Furthermore, the BSO approach by Wiseman and Rush (2016) cited does give a continuous output to optimize for seq2seq that is asked for in the beginning of page 4.\n\nThis is all addressed in the Related Work section, which I kindly request that you review, after you have corrected the misunderstandings I noted above. Actor-critic and other reinforcement learning approaches such as REINFORCE are discussed to an extent that we feel adaquate to address what you brought up here. There we discussed how Bahdanau et al. (2016),  Ranzato et al. (2016)'s MIXER, and Wiseman and Rush (2016) are relevant yet different from our work. In Section 5.2 our experimental results are also compared against approaches that use reinforcement learning, such as Chen & Bansal (2018).\n\nYou may also have the same misunderstanding as Official Blind Review #3, so please see my response to that review. \n\n<continued in next comment>"}, "signatures": ["ICLR.cc/2020/Conference/Paper398/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper398/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylkzaEYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper398/Authors|ICLR.cc/2020/Conference/Paper398/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172056, "tmdate": 1576860537041, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment"}}}, {"id": "Hke1liOkor", "original": null, "number": 2, "cdate": 1572993767198, "ddate": null, "tcdate": 1572993767198, "tmdate": 1572993767198, "tddate": null, "forum": "SylkzaEYPS", "replyto": "rkeIUEz6YB", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment", "content": {"title": "Addressing your comments", "comment": "Thanks for your comments.\n\n> \"A presents either a word ....\" --> this sentence is not clear.\n\nThanks for catching this. It should be \"A token represents either a word ....\". Will correct.\n\n> \"Embedded representations ... differ somewhat from w_i\"--> Please clarify this aspect with more details.\n\nThanks for this suggestion. We'll add a sentence to clarify that w^R_t is a weighted sum of embedding vectors instead of an individual one like w_i.\n\n> In Figure 1, the proposed model with recoder seems to be suffering from issues related to redundancy and referential clarity, as it repeats the name \"malia\" several times. Would you comment on why this is the case?\n\nIn many cases the coverage mechanism reduces repetition at the phrase level, but usually not down to the level of individual words such as malia. There is no special penalty for redundancy in the standard ml loss, and the same is true for the recoder except from verbosity due to the length loss. Moreover, this dataset's reference summaries, which are usually 3 individual sentences intended as bullet points, may not always resemble a coherent paragraph.\n\n> It would be great if you could provide more details on the selection criteria/qualifications of the mechanical turk workers. Also, it is not clear why each example was given to only one worker and not to multiple workers. Wouldn't it be ideal to evaluate each example by multiple workers to get a sense of the inter-rater agreement? Please clarify.\n \nWe required workers to be Masters (a common requirement), which the Mechanical Turk system had identified to be \"high performers\".\n\nAs mentioned in Section 5.2, we do not need a confident score for any one example, since we are assessing the quality of the algorithm. Ideally we would sample from the space of all (viewer, example) pairs, so limiting each example to one worker allows us to sample a wider diversity of examples using a given number of workers -- we used 300 (viewer, example) pairs for 11490 examples. We could also have limited each worker to 1 example instead of 5 to get a greater diversity of viewers, but we felt that may expose us to a higher fraction of ratings from workers making mistakes on their first and only example.\n\nConfident results for each example would only be needed if the ratings were to be used as training data for another algorithm, which is not the case for us.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper398/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylkzaEYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper398/Authors|ICLR.cc/2020/Conference/Paper398/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172056, "tmdate": 1576860537041, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment"}}}, {"id": "BygbNOd1iB", "original": null, "number": 1, "cdate": 1572993065472, "ddate": null, "tcdate": 1572993065472, "tmdate": 1572993065472, "tddate": null, "forum": "SylkzaEYPS", "replyto": "rklzdrx-cS", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment", "content": {"title": "There is a core misunderstanding.", "comment": "Thanks for your comments. I believe there is a core misunderstanding in how the recoder helps train the summarizer, so let me clarify. The key contribution is the recoder as a *loss function* for the summarizer's beam decoded outputs. What you referred to:\n\n> \"backpropagating through the softmax weights during training and using the argmax during inference\"\n\nis to help train the summarizer through beam search decoding, which is not an element of our work. The recoder helps train the summarizer only by exposing it to its own output (y1...yt-1), without backpropagation to previous timesteps through beam search decoding. We tried to make this subtle point clear in Section 3.1, where we said \"propagating errors to P_t improves the summarizer via exposure to what it would see at test time, even if the summarizer is not 'aware' of the search mechanism and cannot optimize for it\".\n\nPropagating through beam search decoding would be accomplished by techniques such as Straight Through Gumbel-Softmax that you cited, or such as Goyal et al. (2017) that we cited. These would be complementary additions to the recoder, as discussed in Section 4.2. We also discussed how our work differs from approaches such as reinforcement learning in Section 4.1. Other models that re-encode their output are discussed in Section 4.3.\n\nThe MeanSum work you cited is a related work in a different problem space; thanks for this reference. They also use the idea of re-encoding an output summary, but they differ in that their loss function l_sim is based on cosine distance in the hidden space, in contrast with our recoder loss J^R. Their \"reconstruction cycle loss\" variant is closer to our work, except they reconstruct the original reviews, which would be analogous in our problem to training the recoder to output the original article instead of the reference summary. We decided against doing this because in general the summary is a lossy representation of the original (mentioned in Section 4.3), an incongruence MeanSum also mentioned as a shortcoming. We will add a citation to MeanSum and Straight Through Gumbel-Softmax and bring up these differences in the paper.\n\nI understand it is difficult to parse out such complexities given a terse presentation, and would appreciate it if you could review the paper again after this very important clarification. Thank you.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper398/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "SylkzaEYPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper398/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper398/Authors|ICLR.cc/2020/Conference/Paper398/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172056, "tmdate": 1576860537041, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper398/Authors", "ICLR.cc/2020/Conference/Paper398/Reviewers", "ICLR.cc/2020/Conference/Paper398/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper398/-/Official_Comment"}}}, {"id": "rklzwRhQKH", "original": null, "number": 1, "cdate": 1571176025954, "ddate": null, "tcdate": 1571176025954, "tmdate": 1572972600325, "tddate": null, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review": "This paper proposes to use an additional component to the commonly used encoder-decoder approach for summarization, which is referred to as the recoder, which is an RNN-syle component that takes the output of the decoder. The intuition offered in the paper is that a good summary should produce itself via the recoder network, and in training it together with the original encoder-decoder it should improve its performance as it would be able to capture more than what the word-level loss does.\n\nI have the following objections to this paper:\n- I can't see why this extra component should improve the quality of the summary produced. If say our encoder-decoder architecture models the training data perfectly, then a recoder that does not do anything would be the right choice. Taking this further, a recoder could actually be fixing problems in the output of the decoder, and thus not providing a good training signal. It doesn't make sense to my that a summary should produce itself via a neural network, unless we are training an auto-encoder. The experiments validate this; the difference in ROUGE score are less than a point, which is the kind of fluctuation one expects due to random seed differences, etc.\n\n- I find it odd that ROUGE score is dismissed as a loss to train against (referred to as a heuristic in the end of section 4.1), but then 1 ROUGE point difference is considered a \"significant\" improvement (making such claims without statistical significance testing is misleading). Sure it has flaws, if there is something better why not use it for evaluation? Furthermore, claiming that the recoder does a better job requires evidence. Why not train this extra function against human judgements? Assuming that what is wanted is to train a model that estimates the quality of the summary, it would make sense to look at the approaches used for the similar task of machine translation quality estimation: https://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00056\n\n- The approach is in my view a kind of actor-critic approach; the recoder could play that role, and in fact the Bahdanau et al. 2016 paper cited does this. However no comparison is offered, be it theoretical or experimental. Furthermore, the criticism that sequence-level training requires differentiable losses is incorrect; MIXER for example does train against scores such as ROUGE that are not differentiable. Furthermore, the BSO approach by Wiseman and Rush (2016) cited does give a continuous output to optimize for seq2seq that is asked for in the beginning of page 4.\n\n- In the experiments, how was the length penalty determined to be the graduated curve mentioned in section 5? I would expect to have comparison against other approaches that try to train encoder-decoder to improve summarization, such as those mentioned in section 4.1.\n\n- On the human evaluation experiments: the difference between the two models is quite small, especially given that the workers were not allowed to say that the models were equally good/bad. Furthermore, there is no inter-annotator agreement, as each comparison was done by a single crowd worker. Showing only the first 400 tokens of the original document would incorrectly disadvantage models selecting content from later in the articles. Finally, showing the reference summary creates another bias, since equally good summaries can disagree on what content to include. It might be helpful to look at some recent work on manual evaluation of summarization that tried to address these issues: https://arxiv.org/abs/1906.01361\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper398/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper398/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575411882432, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper398/Reviewers"], "noninvitees": [], "tcdate": 1570237752728, "tmdate": 1575411882444, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper398/-/Official_Review"}}}, {"id": "rkeIUEz6YB", "original": null, "number": 2, "cdate": 1571787853766, "ddate": null, "tcdate": 1571787853766, "tmdate": 1572972600290, "tddate": null, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposed an encoder-decoder-based summarization network as a loss function within a similar encoder-decoder-based summarization framework to demonstrate that the proposed model obtains better automatic and human evaluation scores compared to the baseline model of See et al. (2017) with just traditional loss functions. Overall, the paper is well-written and the presented results, analyses, and comparisons appear to be reasonable. One notable advantage of the proposed model would be to circumvent the approaches that rely on the evaluation metric, ROUGE as a reward component e.g. in a reinforcement learning setting, although with the expense of additional memory and time complexity.   \n\nFew comments:\n\n- \"A presents either a word ....\" --> this sentence is not clear.\n\n- \"Embedded representations ... differ somewhat from w_i\"--> Please clarify this aspect with more details.\n\n- In Figure 1, the proposed model with recoder seems to be suffering from issues related to redundancy and referential clarity, as it repeats the name \"malia\" several times. Would you comment on why this is the case?\n\n- It would be great if you could provide more details on the selection criteria/qualifications of the mechanical turk workers. Also, it is not clear why each example was given to only one worker and not to multiple workers. Wouldn't it be ideal to evaluate each example by multiple workers to get a sense of the inter-rater agreement? Please clarify. "}, "signatures": ["ICLR.cc/2020/Conference/Paper398/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper398/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575411882432, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper398/Reviewers"], "noninvitees": [], "tcdate": 1570237752728, "tmdate": 1575411882444, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper398/-/Official_Review"}}}, {"id": "rklzdrx-cS", "original": null, "number": 3, "cdate": 1572042090254, "ddate": null, "tcdate": 1572042090254, "tmdate": 1572972600249, "tddate": null, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "invitation": "ICLR.cc/2020/Conference/Paper398/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I did not assess the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review": "This paper introduces an encoder-decoder as a differentiable loss function for sequential autoregressive generation tasks and more specifically for summarization. \nThis is done by adding a recorder network that that takes the decoded sequence from the summarizer as input and is trained to output the reference summary.\n\nI see a fundamental issue with this work:\n\n* During inference, authors decode from the probability distribution of the seq2seq model using beam search. \n* But for training (original seq2seq + recorder) authors backpropagate the NLL loss (which is fully differentiable) of the recorder on reference summaries through the softmax probabilities of outputs from the seq2seq model. \n\n>> This whole architecture can be seen as a traditional end-to-end seq2seq model with non-linearity and normalization (softmax) in the middle. \n\nAdditionally: \n>> \"backpropagating through the softmax weights during training and using the argmax during inference\" falls into a long line of work for propagating non-differential objective functions through continuous relaxations \n of categorical latent variables, more specifically the \"straight through\"  and \"gumbel-softmax\" (see refs.)\nThese methods have proven to be a strong alternative to reinforcement learning to train non-differential objectives and have been implemented quite a lot for sequence generation mainly for SeqGANs and even for text summarization connections to this line of work must be established in this paper. \n\nreferences \n- Estimating or propagating gradients through stochastic \u00b4neurons for conditional computation. \nBengio et al. 2013 arXiv preprint arXiv:1308.3432, 2013.\n- CATEGORICAL REPARAMETERIZATION WITH GUMBEL-SOFTMAX\nJang et al. 2018 https://arxiv.org/pdf/1611.01144.pdf\n- GANS for Sequences of Discrete Elements with the Gumbel-softmax Distribution\nhttps://arxiv.org/pdf/1810.05739.pdf\n- Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders\nhttps://arxiv.org/pdf/1805.04843.pdf\n- MeanSum : A Neural Model for Unsupervised Multi-Document Abstractive Summarization\nhttps://arxiv.org/pdf/1810.05739.pdf\n\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper398/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper398/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"abstract": "We present a new approach to defining a sequence loss function to train a summarizer by using a secondary encoder-decoder as a loss function, alleviating a shortcoming of word level training for sequence outputs. The technique is based on the intuition that if a summary is a good one, it should contain the most essential information from the original article, and therefore should itself be a good input sequence, in lieu of the original, from which a summary can be generated. We present experimental results where we apply this additional loss function to a general abstractive summarizer on a news summarization dataset. The result is an improvement in the ROUGE metric and an especially large improvement in human evaluations, suggesting enhanced performance that is competitive with specialized state-of-the-art models.", "title": "Encoder-decoder Network as Loss Function for Summarization", "code": "https://github.com/iclr2020recoder/code_for_paper", "keywords": ["encoder-decoder", "summarization", "loss functions"], "authors": ["Glen Jeh"], "TL;DR": "We present the use of a secondary encoder-decoder as a loss function to help train a summarizer.", "authorids": ["glenjeh@gmail.com"], "pdf": "/pdf/8c806d12834d2b996c2a0eef2530302cd11ac798.pdf", "paperhash": "jeh|encoderdecoder_network_as_loss_function_for_summarization", "original_pdf": "/attachment/1ffeb9aab2eb93a5779547c47748d4c2c314f89f.pdf", "_bibtex": "@misc{\njeh2020encoderdecoder,\ntitle={Encoder-decoder Network as Loss Function for Summarization},\nauthor={Glen Jeh},\nyear={2020},\nurl={https://openreview.net/forum?id=SylkzaEYPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SylkzaEYPS", "replyto": "SylkzaEYPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper398/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575411882432, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper398/Reviewers"], "noninvitees": [], "tcdate": 1570237752728, "tmdate": 1575411882444, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper398/-/Official_Review"}}}], "count": 9}