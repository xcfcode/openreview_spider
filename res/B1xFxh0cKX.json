{"notes": [{"id": "B1xFxh0cKX", "original": "SklPRGh5tm", "number": 1094, "cdate": 1538087920665, "ddate": null, "tcdate": 1538087920665, "tmdate": 1545355423076, "tddate": null, "forum": "B1xFxh0cKX", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "SJgwmbWQgN", "original": null, "number": 1, "cdate": 1544913182780, "ddate": null, "tcdate": 1544913182780, "tmdate": 1545354492461, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Meta_Review", "content": {"metareview": "This paper proposes a \u201cguided\u201d evolution strategy method where the past surrogate gradients are used to construct a covariance matrix from which future perturbations are sampled. The bias-variance tradeoff is analyzed and the method is applied to real-world examples.\n\nThe method is not entirely new, and discussion of related work as well as comparison with them is missing. The main contribution is in the analysis and application to real-world examples, and the paper should be rewritten focusing on these contributions, while discussing existing work on this topic thoroughly.\n\nDue to these issue, I recommend to reject this paper.\n", "confidence": "5: The area chair is absolutely certain", "recommendation": "Reject", "title": "Related work is overlooked and not compared with."}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1094/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352969056, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1094/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1094/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352969056}}}, {"id": "ByxowIZyJV", "original": null, "number": 5, "cdate": 1543603810674, "ddate": null, "tcdate": 1543603810674, "tmdate": 1543603810674, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "HkeFVwDRAm", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "content": {"title": "Multiple Hansen 2011 references!", "comment": "Our apologies--we were talking about different Hansen 2011 references. We thought the reviewer was referring to \"The CMA Evolution Strategy: A Tutorial\" (http://www.cmap.polytechnique.fr/~nikolaus.hansen/cmatutorial110628.pdf), but only today noticed this technical report \"Injecting External Solutions Into CMA-ES\" (https://hal.inria.fr/inria-00628254/document). We apologize for the confusion.\n\nThis Hansen 2011 reference is indeed very relevant, we thank the reviewer for bringing it to our attention. After a quick read, the main difference with our work is that we inject external information into the covariance matrix from which perturbations are sampled, whereas Hansen 2011 injections solutions by replacing the samples themselves. We are reading this reference more carefully and will update our paper accordingly."}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625431, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xFxh0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1094/Authors|ICLR.cc/2019/Conference/Paper1094/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625431}}}, {"id": "HkeFVwDRAm", "original": null, "number": 4, "cdate": 1543563056835, "ddate": null, "tcdate": 1543563056835, "tmdate": 1543563056835, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "HkesDWvop7", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "content": {"title": "Regarding Hansen 2011", "comment": "Thank you for your response. \n\nAs the title of Hansen 2011 tells, it of course consider injecting external solutions or directions. On the first page of this paper, they say,\n--\nExternal or modified proposal solutions or directions can have a variety of sources.\n\u2022 a gradient or Newton direction;\n\u2022 an optimal solution of a surrogate model built from already evaluated solutions;\n\u2022 the best-ever solution seen so far;\n--\n\nThe authors might misunderstand Hansen 2011 since the external solutions and directions (gradients) are transformed as a solution in the algorithm description. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/AnonReviewer3"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1094/AnonReviewer3", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625431, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xFxh0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1094/Authors|ICLR.cc/2019/Conference/Paper1094/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625431}}}, {"id": "HkesDWvop7", "original": null, "number": 3, "cdate": 1542316387115, "ddate": null, "tcdate": 1542316387115, "tmdate": 1542316387115, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "rJeWFl_Y37", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "content": {"title": "Thank you for your review. Comments below:", "comment": "Thank you for your review. Comments below:\n\nRegarding the first point, we agree that comparisons against other adaptive ES methods (CMA-ES and NES) would be very useful, and are currently performing these comparisons. We will update the paper with these results. However, there is a fundamental limitation with CMA-ES and NES in that they are both purely black box (no gradient information) optimizers. This can be especially seen in the current Figure 1 in the paper, where CMA-ES is not able to take advantage of the initial external gradient information available to Guided ES or SGD, thus fails to make quick progress on the problem--it does begin to accelerate past standard ES as the covariance begins to adapt. NES will have the same issue. Again, we are running experiments to confirm this intuition.\n\nRegarding the second point, we carefully read through the Hansen 2011 reference again and could find no reference on using external gradient information to adapt the covariance matrix. As far as we can tell, Hansen 2011 focuses purely on adapting the covariance using information from iterates encountered during the optimization trajectory. To the best of our knowledge, our work is the first to propose incorporating surrogate\u201d gradient information into an ES algorithm, as well as to analyze the bias and variance of the resulting gradient estimate."}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625431, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xFxh0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1094/Authors|ICLR.cc/2019/Conference/Paper1094/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625431}}}, {"id": "S1gvmZPsTQ", "original": null, "number": 2, "cdate": 1542316318986, "ddate": null, "tcdate": 1542316318986, "tmdate": 1542316318986, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "rkxvkSUTnX", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "content": {"title": "Thank you for your review. Comments below:", "comment": "Thank you for your review. Comments below:\n\nRegarding clarity, we appreciate the reviewer\u2019s comments and have added more context throughout the paper. We have added pseudocode in the main text, and moved more of the bias-variance derivation to the appendix (to make room for more exposition of the method in 3.2).\n\nIn addition, we would appreciate it if the reviewer could elaborate on which aspects of the paper they thought could use more context. Specifically, if Fig1a is insufficient to explain the method, what additional information would the reviewer have appreciated?\n\nRegarding experiments, we are running more baseline comparisons against other adaptive evolutionary strategy methods (CMA-ES and natural evolutionary strategies, or NES). If the reviewer has other specific suggestions as to what would make the experiments more exhaustive, we would appreciate them.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625431, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xFxh0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1094/Authors|ICLR.cc/2019/Conference/Paper1094/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625431}}}, {"id": "ryx-zWDiaX", "original": null, "number": 1, "cdate": 1542316297145, "ddate": null, "tcdate": 1542316297145, "tmdate": 1542316297145, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "Bke4RAO6nQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "content": {"title": "Thank you for the review. Responses below:", "comment": "Thank you for the review. Responses below:\n\n\u201cThe proposed guided search seems similar to (stochastic) quasi-Newton methods. For instance the form in (2) is indeed a rank-one update of the gradient. What is authors take on this relationship?\u201d\nQuasi-Newton methods assume access to first-order information about the objective, whereas our focus is on black-box optimization. Critically, we do not assume that the \u201csurrogate\u201d gradient information we are provided is reliable. Providing a way to robustly use this information when it is useful and to discard it when it is not is our primary contribution.\n\nOur update is indeed inspired by quasi-Newton methods, and we now note this in the maintext. In particular, as the author notes, we adapt the search covariance with a history of k past gradient estimates similar to how the approximate inverse Hessian is updated according to the past k gradient evaluations in L-BFGS. However, we are not trying to approximate the inverse Hessian. In our application, we are updating the covariance of the distribution used to perturb parameters.\n\n\u201cThe analysis assumes that the gradient exists. The proposed method is interesting when the gradients are not available. Therefore, it is not clear in what sense this analysis would apply to general functions.  The authors also assume that the second order Taylor expression is exact. Is this absolutely necessary? Would the analysis work when the function is approximated locally with its second order expansion?\u201c\nThese assumptions were made largely to simplify the presentation of the bias-variance analysis.  In the deep learning applications we focused on, the gradient of most loss functions exist, however, they may not be tractable (due to intractable integrals over nuisance variables) or may not be useful (e.g., the gradient of a hard thresholding function is 0). We dropped the higher order Taylor remainder to declutter the exposition, however, the analysis still holds (up to higher order error terms) when the function is locally approximated to 2nd order around each iterate. \n\n\u201cI guess the equation in (2) is satisfied irrespective of the distribution of the \\epsilon_i vectors. If I am right, then what is the role of the particular distribution used for sampling from the subspace of surrogate gradients?\u201d\nCorrect, the equation in (2) does not depend on the particular distribution for \\epsilon. We chose a Gaussian distribution because it is: simple, easy to sample from, and has bounded variance. This is also consistent with previous work on evolutionary strategies (CMA-ES and NES). It would be interesting to, in future work, explore different choices for this distribution either analytically or empirically.\n\n\u201cThe authors state \"ES has seen a resurgence in popularity in recent years (Salimans et al., 2017; Mania et al., 2018).\" Both cited papers are not published in any conference or journal. Is there some recent but published work to support the statement?\u201d\nWe have added additional published citations [1, 2, 3, 4] all of which use evolutionary strategies in concert with neural networks.\n\nWe would appreciate if the reviewer could expand on their justification for the given score, or consider revising it in the context of our responses here.\n\n[1] Cui et al. Evolutionary Stochastic Gradient Descent for Optimization of Deep Neural Networks (NIPS 2018)\n[2] Houthooft et. al. Evolved Policy Gradients (NIPS 2018)\n[3] Ha and Schmidhuber. World Models. (NIPS 2018)\n[4] Ha, David. Neuroevolution for deep reinforcement learning problems. (GECCO 2018)"}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621625431, "tddate": null, "super": null, "final": null, "reply": {"forum": "B1xFxh0cKX", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1094/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1094/Authors|ICLR.cc/2019/Conference/Paper1094/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers", "ICLR.cc/2019/Conference/Paper1094/Authors", "ICLR.cc/2019/Conference/Paper1094/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621625431}}}, {"id": "Bke4RAO6nQ", "original": null, "number": 3, "cdate": 1541406411847, "ddate": null, "tcdate": 1541406411847, "tmdate": 1541533427698, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Review", "content": {"title": "Good idea but its relation with a similar approach is overlooked and analysis is oversimplified", "review": "In this manuscript, the authors propose an approach that combines random search with the surrogate gradient information. To this end, the proposed method samples from the subspace of the surrogate gradients. This subspace is constructed by storing the previous surrogate gradients. After several assumptions, the authors also a give a discussion on variance-bias trade-off as well as a discussion on hyperparameter optimization. The manuscript ends with numerical experiments.\n\nThe proposed guided search seems similar to (stochastic) quasi-Newton methods. For instance the form in (2) is indeed a rank-one update of the gradient. What is authors take on this relationship?\n\nThe analysis assumes that the gradient exists. The proposed method is interesting when the gradients are not available. Therefore, it is not clear in what sense this analysis would apply to general functions.  The authors also assume that the second order Taylor expression is exact. Is this absolutely necessary? Would the analysis work when the function is approximated locally with its second order expansion? \n\nI guess the equation in (2) is satisfied irrespective of the distribution of the \\epsilon_i vectors. If I am right, then what is the role of the particular distribution used for sampling from the subspace of surrogate gradients?\n\nThe authors state \"ES has seen a resurgence in popularity in recent years (Salimans et al., 2017; Mania et al., 2018).\" Both cited papers are not published in any conference or journal. Is there some recent but published work to support the statement?\n\n", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Review", "cdate": 1542234307643, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335871388, "tmdate": 1552335871388, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rkxvkSUTnX", "original": null, "number": 2, "cdate": 1541395679417, "ddate": null, "tcdate": 1541395679417, "tmdate": 1541533427499, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Review", "content": {"title": "Improve random search by building a subspace of the previous surrogate gradients for derivative-free optimization; good results but paper lacks clarity and is quite hard to follow.", "review": "Summary: The paper proposes a method to improve random search by building a subspace of the previous k surrogate gradients, mixing it with an isotropic Gaussian distribution to improve the search. Results reported shows are good compared to other approaches for learning weights of neural networks. However, paper lacks clarity and is quite hard to follow.\n\nQuality: The paper presents a well-designed approach that is able to deal with optimization in high dimensionality space, by building a lower order surrogate model build upon the previous gradients computed with this surrogate model. The analysis appears to be correct and provide credential to the approach. Results reported are very good. However, testing is relatively limited to few cases. More experimental results on a good set of problems with several methods would have made the paper stronger and more convincing.\n\nClarity: The paper is hard to follow. Maybe because I am not completely familiar with the topic, but many elements presented lacks some context. The authors appear clearly to be knowledgeable of their topics, but lacks the capacity to provide all required background to follow their thoughts. The method could have been better illustrated, I found that Fig. 1a not enough to explain the method, while Fig. 1b and other training curves not useful to understand the approach. Some pseudo-code to illustrate the use of the proposed method might certainly help to improve clarity. Sec. 3.2 is not enough to understand well the approach.\n\nOriginality: The approach is allowing a nice trade-off between pure random search and guide search through a surrogate model over a subspace of limited dimensionality. This is in-line of some work on the use of ES for training neural networks, but I am not aware of other similar work although I am not super knowledgeable of the field. \n\nSignificance: The approach can have its impact for optimizing deep networks with no gradient, but more exhaustive experimental testing would be required.\n\nPros and cons:\n+ Sound approach\n+ Good theoretical support of the approach (bias-variance analysis)\n+ Great results reported\n+ Of importance for optimizing without gradients\n- Presentation of the method lacking many details and not very clear\n- Overall quality of the paper is subpar, tend to be very textual and hard to follow in several parts\n- Experiments are not exhaustive and detailed. Loss plots are provided for some methods compared. Looks more like a preliminary validation. \n\nI think that if the paper can be rewritten to be more tight, clearer in its presentation, with figures and pseudo-code to illustrate the method better, with more exhaustive testing, it can be really great. Current, the method appears to be great, but the writing quality of the paper is not yet there.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Review", "cdate": 1542234307643, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335871388, "tmdate": 1552335871388, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "rJeWFl_Y37", "original": null, "number": 1, "cdate": 1541140601419, "ddate": null, "tcdate": 1541140601419, "tmdate": 1541533427296, "tddate": null, "forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "invitation": "ICLR.cc/2019/Conference/-/Paper1094/Official_Review", "content": {"title": "Interesting idea, but not really new", "review": "The idea of this paper is to accelerate the OpenAI type evolution strategy by introducing\n1. non-isotropic distribution, where the covariance matrix is of form I + UU^t; and\n2. external information such as a surrogate gradient to determine U.\nThe experiments show promising results. I think it is the right direction to go, however at the same time, these ideas are not really new. \n\nThe first point is well studied in the context of evolution strategies in e.g., (Sun et al., arxiv 2011), (Loshchilov, Evolutionary Computation 2015), (Akimoto et al, GECCO 2016). They all have covariance matrix of form I + UU^t or a bit richer. There are mainly two advantages over full CMA-ES or NES: 1) computationally cheap, and 2) faster adaptation of the covariance matrix. The current paper does not adapt the covariance matrix and use an external information to guide the distribution. Therefore, it is different from the above work, but I suggest to compare Guided ES with these methods to see the effect of external information purely.  \n\nThe second point, using external information to change the distribution shape, is also investigated in reference (Hansen, INRIA TechRep 2011), where external information such as a good point or a good directions (gradient) is injected in order to adapt the covariance matrix. \n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1094/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search", "abstract": "Many applications in machine learning require optimizing a function whose true gradient is unknown, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the tradeoffs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems including truncated unrolled optimization and training neural networks with discrete variables, demonstrating improvement over both standard evolutionary strategies and first-order methods (that directly follow the surrogate gradient). We provide a demo of Guided ES at: redacted URL", "keywords": ["evolutionary strategies", "optimization", "gradient estimators", "biased gradients"], "authorids": ["nirum@google.com", "lmetz@google.com", "gjt@google.com", "damichoi@google.com", "jaschasd@google.com"], "authors": ["Niru Maheswaranathan", "Luke Metz", "George Tucker", "Dami Choi", "Jascha Sohl-Dickstein"], "TL;DR": "We propose an optimization method for when only biased gradients are available--we define a new gradient estimator for this scenario, derive the bias and variance of this estimator, and apply it to example problems.", "pdf": "/pdf/0b14c36140480f115b0847908574fc08e2bfb320.pdf", "paperhash": "maheswaranathan|guided_evolutionary_strategies_escaping_the_curse_of_dimensionality_in_random_search", "_bibtex": "@misc{\nmaheswaranathan2019guided,\ntitle={Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search},\nauthor={Niru Maheswaranathan and Luke Metz and George Tucker and Dami Choi and Jascha Sohl-Dickstein},\nyear={2019},\nurl={https://openreview.net/forum?id=B1xFxh0cKX},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1094/Official_Review", "cdate": 1542234307643, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "B1xFxh0cKX", "replyto": "B1xFxh0cKX", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1094/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335871388, "tmdate": 1552335871388, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1094/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}], "count": 10}