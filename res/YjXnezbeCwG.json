{"notes": [{"id": "YjXnezbeCwG", "original": "WB04dj6Wkir", "number": 2869, "cdate": 1601308318434, "ddate": null, "tcdate": 1601308318434, "tmdate": 1614985728455, "tddate": null, "forum": "YjXnezbeCwG", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "T3YtNxarDmm", "original": null, "number": 1, "cdate": 1610040411574, "ddate": null, "tcdate": 1610040411574, "tmdate": 1610474009015, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper improves the wait-k based simultaneous NMT by training on an adaptive wait-m policy with a controller determining the lag for sentence pair.  The controller is trained with RL to minimize the loss on a validation set. The overall model is reasonable, which is well presented. I however have the following two concerns\n1. There is a clear mismatch between training/inference strategies, which raises two problems\n    1. The motivation:  the authors tried to explain that in discussion,  but it is not convincing enough\n    2. The title is misleading since there is no future information to use during inference \n2. The experiments is not convincing enough in that a) the improvement over baseline is modest, and b) comparison to adaptive wait-k and other strong baseline is insufficient \n\nIn conclusion I would suggest to reject this paper.\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040411558, "tmdate": 1610474008999, "id": "ICLR.cc/2021/Conference/Paper2869/-/Decision"}}}, {"id": "QVDftfLek_7", "original": null, "number": 8, "cdate": 1606273673266, "ddate": null, "tcdate": 1606273673266, "tmdate": 1606274756972, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "bJvd70uEp-", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment", "content": {"title": "Reply to \"still difficult to motivate this work\"", "comment": "Thanks for your quick response!\n\n> Towards \u201cwhether the improvements are worth the effort\u201d\n1.\tWe make consistent improvement on three IWSLT tasks and one WMT task over a series of baselines (including both heuristic and adaptive baselines). The results for IWSLT tasks are available at Figure 2, Table 1 Figure 4 and Table 5. The results for the WMT task are at Figure 3, Table 6 and Figure 7.\n2.\tCompared to standard wait-k, our method only requires about 20%-30% time without too much hyperparameter tuning (see Table 2). In comparison, to use wait-k*, we need to train multiple wait-k models with different waiting thresholds. For CL, we need to carefully design the annealing strategy. Both wait-k* and CL require much additional training time.\n3.\tOur code has been released (anonymously) for reproducibility. \n\n> Towards \u201cespecially given the existence of more powerful adaptive methods\u201d\n\n1.\tWe compare with five adaptive methods, including WIW, WID, MILk, MMA-IL and MMA-H on IWSLT En$\\to$Vi translation. The results are shown in the Figure 4(a) of the manuscript.  We can see that when AL $\\ge 5.0$, our method outperforms all baseline models, and when AL < 5.0, our method performs slightly worse than MMA-IL and MMA-H. That is, with our proposed method, wait-k could surpass the performance of many adaptive baselines. We will consider applying our method directly to adaptive algorithms in the future.\n2.\tRecently, [REF 1] proposed a method that can adaptively leverage a set of wait-k models for decoding.  We make a combination with this method and obtain further improvement.  Please kindly re-check the [General reply: combination with adaptive decoding/inference](https://openreview.net/forum?id=YjXnezbeCwG&noteId=zng-7L28CR8).\n\nIn summary, \n\n1.\tOn IWSLT En$\\to$Vi, our method generally achieves better results than adaptive methods like WIW, WID, MILk, MMA-IL and MMA-H.\n2.\tWe make a first step on combing our method with an adaptive decoding strategy [REF 1] and obtain more improvement. We will combine our method with more adaptive algorithms in the future.\n\n**References**\n\n[REF 1] Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma,Hairong Liu, and Liang Huang. Simultaneous translation policies: From fixed to adaptive. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, pp. 2847\u20132853, 2020a. doi: 10.18653/v1/2020. acl-main.254. URL https://www.aclweb.org/anthology/2020.acl-main.254\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YjXnezbeCwG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2869/Authors|ICLR.cc/2021/Conference/Paper2869/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment"}}}, {"id": "bJvd70uEp-", "original": null, "number": 7, "cdate": 1606235688085, "ddate": null, "tcdate": 1606235688085, "tmdate": 1606235688085, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "l8khlnuz2_q", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment", "content": {"title": "still difficult to motivate this work", "comment": "Thanks for your response. I appreciate that you went to the trouble of testing out an annealing heuristic, but I'm not sure that this changes the basic picture. I'm convinced that you do better than the heuristics, but wonder whether the improvements are worth the effort, especially given the existence of more powerful adaptive methods."}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YjXnezbeCwG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2869/Authors|ICLR.cc/2021/Conference/Paper2869/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment"}}}, {"id": "MoSIdgJdNOz", "original": null, "number": 6, "cdate": 1606211137117, "ddate": null, "tcdate": 1606211137117, "tmdate": 1606222724534, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "BXmSWQDi7qu", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment", "content": {"title": "Comments for AnonReviewer4", "comment": "> [Q1] method is not significantly better in most cases, especially on the WMT En-De dataset. Given that the WMT dataset is much larger than IWSLT datasets, does this suggest that the proposed method may not work well on larger dataset?\n\nWe agree that that gaps between our method and wait-k on WMT dataset are not so large as those for IWSLT datasets, but our method is still consistently better than all baselines. In the new version, we include the results of all heuristic baselines on WMT dataset in Figure 3, and find that our method consistently outperform heuristic baselines. We will keep working on larger datasets.\n\n> [Q2] I'm also curious about the reason of using adaptive wait-m only during training since it is a more straight forward idea to apply adaptive policy during both training and inference. Would it be better if th eadaptive policy (by redesign features) was used on inference as well?\n\nThanks for your suggestions, and we will redesign the features in the future version to get an RL-based adaptive inference policy. Currently, we can extend our model to an adaptive inference strategy to further boost the performance. Please kindly check the \n\"[General reply](https://openreview.net/forum?id=YjXnezbeCwG&noteId=zng-7L28CR8)\" for more details."}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YjXnezbeCwG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2869/Authors|ICLR.cc/2021/Conference/Paper2869/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment"}}}, {"id": "k4yW5DLs0Z", "original": null, "number": 4, "cdate": 1606210654566, "ddate": null, "tcdate": 1606210654566, "tmdate": 1606222505471, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "3pHCV0Rw_Z2", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment", "content": {"title": "Comments for AnonReviewer2", "comment": "Thanks for your review comments.\n\n> [Q1] The proposed method is intuitively strange because of mismatching between training/inference strategies.\n\nWe are aware that there is training/inference mismatch in our method. Despite the mismatch, our method still brings much benefits, and the reason is as follows: To get a perfect translation, we should use the entire source sentence as input, which is not possible in simultaneous translation during inference. Access to future information reduces the gap of information between perfect translation and simultaneous translation, and improves training performance. Thus, although the training/test mismatch might increase the generalization error, we improve the final translation quality, as shown by the experiments.\n\n> [Q2] It is also unclear to choose this method rather than adaptive wait-k methods, e.g., one referred as Zheng et al. (2020a), which may solve a similar problem directly. The paper needs at least some comparison of these kind of methods to figure out the advantages of the proposed method.\n\nFollowing your suggestion, we conduct some experiments with Zheng et al. (2020a). Please kindly check \"[General reply](https://openreview.net/forum?id=YjXnezbeCwG&noteId=zng-7L28CR8)\" for details.\n\n> [Q3] Algorithm 1 involves a suspicious use of the development set...\n\nLeveraging development set to improve the models is widely used in meta-learning literature [REF1, REF2] and neural architecture search literature [REF3, REF4]. The objective function in these works is usually formulated as a bi-level optimization, where we want to minimize the validation loss of model $\\theta^*$ (on the dev set), and $\\theta^*$ is obtained on the training corpus. \n\n> [Q4] The title sounds misleading: the proposed method still does not learn how to use future information because it uses only k look-ahead information at inference (same as usual wait-k methods), i.e., there is nothing special to represent \"future\".\n\nThe \"future information\" is utilized in the training process rather than inference process. In this paper, we want to improve wait-$k$ without introducing additional cost during inference stage. At training time, to obtain a better wait-$k$, we introduce an RL controller to adaptively provide future information (i.e., $m$ words ahead where $m>k$) and eventually obtain better results. Sorry for the misleading. After the anonymous period, we will change the title.\n\n> [Q5] Figure 1 involves some common mistakes of using bar charts: it must use 0 as the origin, and must not shorten the bar.\n\nSorry that the figure is misleading. Please check out new Figure 1.\n\n**References**:\n\n[REF 1] Learning to Teach, https://arxiv.org/abs/1805.03643\n\n[REF 2] Learning to Teach with Dynamic Loss Functions, https://arxiv.org/abs/1810.12081\n\n[REF 3] DARTS: Differentiable Architecture Search, https://arxiv.org/abs/1806.09055\n\n[REF 4] Efficient Neural Architecture Search via Parameter Sharing, https://arxiv.org/abs/1802.03268"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YjXnezbeCwG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2869/Authors|ICLR.cc/2021/Conference/Paper2869/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment"}}}, {"id": "YN-JT1Yc0qI", "original": null, "number": 3, "cdate": 1606210391447, "ddate": null, "tcdate": 1606210391447, "tmdate": 1606222404464, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "E0QnlM00xUq", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment", "content": {"title": "Comments for AnonReviewer1", "comment": "Thanks for your review comments.\n\n> [Q1] the improvements are quite marginal comparedwith wait-k* and random\u2026\n\nWe respectively disagree with the claim that the improvements are marginal. As shown in Table 5 (in Appendix C), in most cases, our method outperforms the heuristic baselines including wait-$k^*$, CL and random by 0.5-1.5 points. In addition, the improvements brought by heuristic baselines are inconsistent under different language pairs and different $k$ values, while our method brings more consistent improvements.\n\n> [Q2] For the experiments, the authors did not compare with other agent-based or adaptive methods.\n\nWe did compare our method with two agent-based methods (wait-if-diff and wait-if-worse [REF 1]) and two adaptive methods (MMA [REF 2] and MILk [REF 3]), and the results are shown in Figure 4(a) and Figure 6(a). Our method outperforms most baselines, except that our method performs similarly with MMA under low latency.\n\n> [Q3] This paper only designs a controller for training. I think we could also have a controller for the inference. \u2026 Then it will be very similar to Gu. et al's RL-based methods.\n\nThanks for the suggestion. In this work, our focus is to leverage future information to improve the wait-k inference. We will design a controller for inference in the future. \n\n1. We try to implement Gu et al\u2019s RL method (i.e.,[REF 4]), but due to time limitation, we were not able to get reasonable results. The main difficulty is that using the adaptive strategy makes it hard to conduct batch inference, and thus makes the model extremely slow. Besides, Gu et al\u2019s is outperformed by wait-k [REF 5] (please check Figure 5~8 of [REF 5]). A possible explanation is that simply optimizing the inference stage is not enough, and optimizing the training process is crucial.\n2. Alternatively, we can combine our method with another adaptive decoding strategy. Please refer to \"[General reply](https://openreview.net/forum?id=YjXnezbeCwG&noteId=zng-7L28CR8)\".\n\n> [Q4] the m of wait-m is defined on each training pair, but I think this will dramatically increase the training time and hard todo batch training\n\nWe empirically verify that our method does not dramatically increase the training time. We record the training speed of wait-k and our methods on IWSLT datasets, and our method requires 20% - 30% additional training time. This additional calculation is mainly brought by calculating training loss as part of the input feature $\\varphi$. If we remove the training loss feature, our method only requires ~5% additional training time. The detailed results are reported in Section 5.3. \n\nFor implementation, wait-$m$ training strategy is implemented by masking the encoder-decoder attention. To use a different $m$ for each different data pair, we just need to create a different mask for each data pair, where the masks are of the same shape and can be easily batched.\n\n**References**:\n\n[REF 1] Kyunghyun Cho and Masha Esipova. Can neural machine translation do simultaneous translation? CoRR, abs/1606.02012, 2016. URL http://arxiv.org/abs/1606.02012.\n\n[REF 2] Xutai Ma, Juan Pino, James Cross, Liezl Puzon, and Jiatao Gu. Monotonic multihead attention. In 8th International Conference on Learning Representations, 2020\n\n[REF 3] Naveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang, Wei Li, and Colin Raffel. Monotonic infinite lookback attention for simultaneous machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1313\u20131323, Florence, Italy, 2019. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/P19-1126.\n\n[REF 4] Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li. Learning to translate in real time with neural machine translation. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pp. 1053\u2013 1062, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/E17-1099.\n\n[REF 5] STACL: Simultaneous Translation with Implicit Anticipation and Controllable Latency using Prefix-to-Prefix Framework, https://www.aclweb.org/anthology/P19-1289.pdf"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YjXnezbeCwG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2869/Authors|ICLR.cc/2021/Conference/Paper2869/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment"}}}, {"id": "zng-7L28CR8", "original": null, "number": 2, "cdate": 1606210155422, "ddate": null, "tcdate": 1606210155422, "tmdate": 1606222255611, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment", "content": {"title": "General reply: combination with adaptive decoding/inference", "comment": "We extend our method to an adaptive version by combining with [REF 1], which conducts adaptive decoding by combining a set of standarad wait-$m$ models. More specifically, to use [REF 1], we first prepare a set of pre-trained wait-$m$ models with different $m$ values, and then adaptively select the waiting threshold during inference. Therefore, we can combine [REF 1] with our method, where the wait-$m$ models are obtained through our strategy. \n\nWe conduct experiments on IWSLT En$\\to$Vi and have the following observations: (1) our method outperforms [REF 1]; (2) after combing our approach with [REF 1], the performance can be further improved, which shows that our method is complementary to adaptive inference strategies like [REF 1].\n\nMore details can be found at the last paragraph of Section 4.2, the last paragraph of Section 5.2, and Figure 4(b). \n\nWhat we want to emphasize is that, to use the method in [REF1], we need to prepare a set of wait-k models in advance, which increase the training cost. Besides, the inference process will be significantly slower, as we need to decode using multiple models simultaneously. In comparison, our method focuses on how to improve a single wait-k strategy without introducing additional inference cost, and the additional computational overhead in training process is acceptable (see Section 5.3).\n\n**References**:\n\n[REF 1] Baigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma,Hairong Liu, and Liang Huang. Simultaneous translation policies: From fixed to adaptive. In Proceedings of the 58th Annual Meeting of the Association forComputational Linguistics, pp. 2847\u20132853, 2020a. doi: 10.18653/v1/2020.\nacl-main.254. URL https://www.aclweb.org/anthology/2020.acl-main.254"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YjXnezbeCwG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2869/Authors|ICLR.cc/2021/Conference/Paper2869/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment"}}}, {"id": "l8khlnuz2_q", "original": null, "number": 5, "cdate": 1606210819511, "ddate": null, "tcdate": 1606210819511, "tmdate": 1606210947136, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "Xzgy6V12UU", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment", "content": {"title": "Comments for AnonReviewer3", "comment": "> [Q1] The comparisons to adaptive baselines are less convincing. They are shown only for one small-data language pair, no implementation details are given\u00a0(for instance, architectures and model capacities), and the relative resultsare very different from those in the literature (MILK << wait-k, WIW,WID). I think the paper would be stronger if these were simply omitted.\n\nSorry that the comparison is confusing. For MMA and MILk, we directly use the results reported in [REF 1], and the results for MILk are reproduced results by [REF 1]. For WIW and WID, we re-implement these algorithms using the exact same model architecture and training setting as in [REF 1]. We have listed the implementation details in Appendix B.3. We are not sure why the results for MILk are that low. We will reproduce MMA and MILk on several more dataset to get more comparisons.\n\n> [Q2] My main reaction to this work is that it should be possible to get most of the gains using some predetermined curriculum inspired by figure 5 (random sampling at the beginning, annealing to some m>k) that is effective for a broad range of inference-time k's.\n\nThanks for your suggestion. We conduct experiments on this idea, and find that this idea does not bring much improvement compared to wait-$k$. The detailed settings and results are repoted in Appendix D.3 (see the paragraph startswith \"an annealing strategy\"). A possible reason is that this baseline cannot guarantee the best \"annealing\" strategy for each separate k, while our method can adaptively find the optimal strategy. Also from Figure 5, we can find that the learned strategy for different inference-time k\u2019s are pretty different.\n\n> [Q3] Controller feature (6): will this value ever repeat during an entire training run?\n\nIt is not repeated.\u00a0 Besides, as in Table 4, we find that removing this feature causes the performance to drop (see the last row of Table 4).\n\n> [Q4] For Mk, since this runs only over the validation set and you are using RL, why not use the actual wait-k BLEU score?\n\nThis is because validation loss score is faster to obtain (since no decoding process is required) and easier to optimize (a smaller perturbation of the sequences might cause a significantly different BLEU score).\n\n> [Q5] You need to include all heuristic baselines in figure 4.\n\nThanks for your suggestion. We now include all heuristic baselines in Figure 3 of the new version. The heuristic baselines do not bring much improvement over wait-$k$ on WMT'15 dataset, and our method still outperforms these baselines.\n\n**References**:\n\n[REF 1] Xutai Ma, Juan Pino, James Cross, Liezl Puzon, and Jiatao Gu. Monotonic multihead attention. In 8th International Conference on Learning Representations, 2020\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "YjXnezbeCwG", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper2869/Authors|ICLR.cc/2021/Conference/Paper2869/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923843642, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Comment"}}}, {"id": "E0QnlM00xUq", "original": null, "number": 4, "cdate": 1603927500425, "ddate": null, "tcdate": 1603927500425, "tmdate": 1605043085829, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review", "content": {"title": "complicated method with marginal improvements", "review": "The authors observed that some lookahead information during training time is helpful to improve the translation accuracy for simultaneous translation. Base on this observation, this paper proposes to use RL-based methods to learn a certain number of lookahead words during the training of the wait-k-based simultaneous translation model.\n\nThis paper proposes a new approach for improving the translation quality and the results indeed show some improvements over the baseline methods. However, I still have the following concerns:\n1) I think the proposed RL-based methods are very completed (in terms of hyperparameter searching, extra training time compared with other baselines) but the improvements are quite marginal compared with wait-k* and random.\n\n2) For the experiments, the authors did not compare with other agent-based or adaptive methods. \n\n3) This paper only designs a controller for training. I think we could also have a controller for the inference. In this way, I believe we could use regular wait-k training, and use a controller to decide a smaller k during inference. Then it will be very similar to Gu. et al's RL-based methods. I suggest the authors include this method's experiments as well.\n\n4) If I did not misunderstand, the m of wait-m is defined on each training pair, but I think this will dramatically increase the training time and hard to do batch training. How slow is your training compared with baseline wait-k? I believe the baseline wait-k is already very slow.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087011, "tmdate": 1606915775828, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2869/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review"}}}, {"id": "BXmSWQDi7qu", "original": null, "number": 1, "cdate": 1603890621199, "ddate": null, "tcdate": 1603890621199, "tmdate": 1605024114217, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review", "content": {"title": "The effectiveness of the proposed method is not convincing", "review": "This paper improves the wait-k based simultaneous NMT by training on an adaptive wait-m policy. The proposed method and experiments are clearly described. Experiments demonstrate that the proposed method is significant better than the wait-k baseline.\n\nHowever, compared to heuristic-based baselines, seemingly the proposed method is not significantly better in most cases, especially on the WMT En-De dataset. Given that the WMT dataset is much larger than IWSLT datasets, does this suggest that the proposed method may not work well on larger dataset?\n\nI\u2019m also curious about the reason of using adaptive wait-m only during training since it is a more straightforward idea to apply adaptive policy during both training and inference. Would it be better if the adaptive policy (by re-design features) was used on inference as well? \n", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087011, "tmdate": 1606915775828, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2869/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review"}}}, {"id": "Xzgy6V12UU", "original": null, "number": 2, "cdate": 1603900410220, "ddate": null, "tcdate": 1603900410220, "tmdate": 1605024114150, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review", "content": {"title": "Using a sledgehammer to kill a fly?", "review": "This paper proposes a new training method for wait-k simultaneous translation. Rather than training on prefix pairs where the target prefix lags the source by k tokens, it uses an RL controller to determine an optimal lag for each sentence pair. The controller uses a small set of features intended to capture training progress, and is trained with REINFORCE to minimize wait-k loss on a validation set, in alternation with main training steps. This method shows consistent gains over various wait-k training heuristics, and some gains over other approaches that adapt the lag at inference time.\n\nThe paper is very well written and organized. The method makes sense, and the experiments are quite thorough, comparing to a competitive set of heuristic baselines, and showing credible - though fairly modest - gains in this setting.\n\nThe comparisons to adaptive baselines are less convincing. They are shown only for one small-data language pair, no implementation details are given (for instance, architectures and model capacities), and the relative results are very different from those in the literature (MILK << wait-k, WIW, WID). I think the paper would be stronger if these were simply omitted.\n\nGiven its ease of implementation and efficiency, I think there is room for a paper focusing on improving wait-k training, even if wait-k isn\u2019t quite state of the art. But having to set up an RL controller detracts from this picture, especially since the gains over various heuristics - in particular the random heuristic that works for any inference-time k - aren\u2019t spectacular. My main reaction to this work is that it should be possible to get most of the gains using some predetermined curriculum inspired by figure 5 (random sampling at the beginning, annealing to some m > k) that is effective for a broad range of inference-time k\u2019s. Note that this is quite different from the CL baseline included in the results. As it stands, I fear this paper risks being a dead end: too complex to be worth implementing, as the field moves on past wait-k.\n\nQuestions and suggestions:\n\n1. Controller feature (6): will this value ever repeat during an entire training run?\n2. For Mk, since this runs only over the validation set and you are using RL, why not use the actual wait-k BLEU score?\n3. You need to include all heuristic baselines in figure 4.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087011, "tmdate": 1606915775828, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2869/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review"}}}, {"id": "3pHCV0Rw_Z2", "original": null, "number": 3, "cdate": 1603922468970, "ddate": null, "tcdate": 1603922468970, "tmdate": 1605024114080, "tddate": null, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "invitation": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review", "content": {"title": "Promising methods, but suspicious settings of model training", "review": "This paper proposes a training strategy for simultaneous translation to choose appropriate amount of look-ahead information for each decoding. Based on the observation that the wait-k method can be improved by training with longer information, the method introduces a function to determine its length given the current example (source x, target y, and the translation model f). It is used as only a guidance during training, and it remains the same decoding criterion at inference.\n\nThe method is interesting and has promising improvements compared with bare wait-k methods according to the experiments, but the paper seems to have some major questions which should affect the conclusion. I recommend to revise the paper appropriately, especially to resolve following concerns:\n\n- The proposed method is intuitively strange because of mismatching between training/inference strategies. It is also unclear to choose this method rather than adaptive wait-k methods, e.g., one referred as Zheng et al. (2020a), which may solve a similar problem directly. The paper needs at least some comparison of these kind of methods to figure out the advantages of the proposed method.\n- Algorithm 1 involves a suspicious use of the development set: it is used directly to optimize a parameter. Specifically, since \\omega is optimized using the D_va, it brings information of the D_va into f, resulting that the training process does not include any strategy to avoid overfitting (in other words, your training set is actually D_tr + D_va, and there is no so-called development set).\n\nMinor comments:\n\n- The title sounds misleading: the proposed method still does not learn how to use future information because it uses only k look-ahead information at inference (same as usual wait-k methods), i.e., there is nothing special to represent \"future\".\n- Figure 1 involves some common mistakes of using bar charts: it must use 0 as the origin, and must not shorten the bar. If you want to focus on differences between each value, you should use other chart.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2869/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2869/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Learning to Use Future Information in Simultaneous Translation", "authorids": ["~Xueqing_Wu1", "~Yingce_Xia1", "~Lijun_Wu1", "~Shufang_Xie1", "weiqing.liu@microsoft.com", "~Tao_Qin1", "~Tie-Yan_Liu1"], "authors": ["Xueqing Wu", "Yingce Xia", "Lijun Wu", "Shufang Xie", "Weiqing Liu", "Tao Qin", "Tie-Yan Liu"], "keywords": ["sequence learning", "simultaneous machine translation"], "abstract": "Simultaneous neural machine translation (briefly, NMT) has attracted much attention recently. In contrast to standard NMT, where the NMT system can access the full input sentence, simultaneous NMT is a prefix-to-prefix problem, where the system can only utilize the prefix of the input sentence and thus more uncertainty and difficulty are introduced to decoding. Wait-k inference is a simple yet effective strategy for simultaneous NMT, where the decoder generates the output sequence $k$ words behind the input words. For wait-k inference, we observe that wait-m training with $m>k$ in simultaneous NMT (i.e., using more future information for training than inference) generally outperforms wait-k training. Based on this observation, we propose a method that automatically learns how much future information to use in training for simultaneous NMT. Specifically, we introduce a controller to adaptively select wait-m training strategies according to the network status of the translation model and current training sentence pairs, and the controller is jointly trained with the translation model through bi-level optimization. Experiments on four datasets show that our method brings 1 to 3 BLEU point improvement over baselines under the same latency. Our code is available at https://github.com/P2F-research/simulNMT .", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "wu|learning_to_use_future_information_in_simultaneous_translation", "one-sentence_summary": "We propose a new method for simultaneous translation, which is guided by a controller (trained via reinforcement learning) that can adaptively leverage future infomation to improve translation quality.", "pdf": "/pdf/18636e7a63fe0bb25b0cbff233365dd9a9ce542c.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=xWjZGpuQ-B", "_bibtex": "@misc{\nwu2021learning,\ntitle={Learning to Use Future Information in Simultaneous Translation},\nauthor={Xueqing Wu and Yingce Xia and Lijun Wu and Shufang Xie and Weiqing Liu and Tao Qin and Tie-Yan Liu},\nyear={2021},\nurl={https://openreview.net/forum?id=YjXnezbeCwG}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "YjXnezbeCwG", "replyto": "YjXnezbeCwG", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2869/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538087011, "tmdate": 1606915775828, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2869/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2869/-/Official_Review"}}}], "count": 13}