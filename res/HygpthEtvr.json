{"notes": [{"id": "HygpthEtvr", "original": "SklzW5vaUB", "number": 95, "cdate": 1569438852754, "ddate": null, "tcdate": 1569438852754, "tmdate": 1583912054527, "tddate": null, "forum": "HygpthEtvr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "zVfRqwwQZ", "original": null, "number": 9, "cdate": 1578473106845, "ddate": null, "tcdate": 1578473106845, "tmdate": 1578473106845, "tddate": null, "forum": "HygpthEtvr", "replyto": "QoP9SXa4tN", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment", "content": {"title": "mu is a hyperparameter, but the performance is not sensitive to its choice.", "comment": "\nThank you very much for the comment!\n\nA numerical test on the sensitivity of the choice of mu was included in the initial submission, but removed in the revised version.\n\nIn our current work, mu is a hyperparameter that requires some tuning, because the demand on accuracy and sparsity may vary for different problems. But it is enough to find a coarse range, and within that range, the performance is quite similar.\n\nWe did not intend to push for the best score in our simulations. Instead, we are more interested in demonstrating the sparsity achieved by using L1 regularization. Besides, in our simulation setup, the achieved accuracy of ProxSGD is similar to or better than SGD (with momentum) and other adaptive stochastic algorithms. So there is still room for improvement in accuracy.\n\nThank you again for your comment!\n\nYang"}, "signatures": ["ICLR.cc/2020/Conference/Paper95/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygpthEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper95/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper95/Authors|ICLR.cc/2020/Conference/Paper95/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176472, "tmdate": 1576860539470, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment"}}}, {"id": "QoP9SXa4tN", "original": null, "number": 1, "cdate": 1576865714049, "ddate": null, "tcdate": 1576865714049, "tmdate": 1576865797608, "tddate": null, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Public_Comment", "content": {"title": "Selecting mu for sparse NN", "comment": "I would like to start by congratulating the authors on the acceptance of a thought-provoking work! \n\nI have a question on the selection of mu for sparse NN. In the numerical experiment, mu = 5*10^(-5). However, there is no sensitivity analysis for the selection of this parameter, like Figure 3 for the epsilon_0. I wonder if the algorithm is sensitive to the selection of mu? In addition, what is the strategy for selecting mu in practice? I ask because the testing accuracy reported in the numerical analysis section does not seem to be the state-of-the-art for, e.g. CIFAR-100 under DenseNet-201 (Fig. 2, central panel), for which the testing accuracy can be 80% plus.\n\nAs I have great interest in this work, I would greatly appreciate if there is a chance that my questions can be clarified. Again, congratulations on the achievement!"}, "signatures": ["~Shih-Kang_Chao1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Shih-Kang_Chao1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygpthEtvr", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504214024, "tmdate": 1576860573075, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper95/-/Public_Comment"}}}, {"id": "BekNMrbbZm", "original": null, "number": 1, "cdate": 1576798687338, "ddate": null, "tcdate": 1576798687338, "tmdate": 1576800947785, "tddate": null, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This paper proposes a new gradient-based stochastic optimization algorithm by adapting theory for proximal algorithms to the non-convex setting. \n\nThe majority of reviewers voted for accept. The authors are encouraged to revise with respect to reviewer comments.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795705063, "tmdate": 1576800252767, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper95/-/Decision"}}}, {"id": "Skx2lGd6Kr", "original": null, "number": 1, "cdate": 1571811828097, "ddate": null, "tcdate": 1571811828097, "tmdate": 1574362680555, "tddate": null, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #3", "review": "\n[Summary]\nThis paper proposes Prox-SGD, a theoretical framework for stochastic optimization algorithms that (1) incorporates momentum and coordinate-wise scaling as in Adam, and (2) can handle constraint and (non-smooth) regularizers through the proximal operator. With proper choices of hyperparameters, the algorithm is shown to converge asymptotically to stationarity, for smooth non-convex loss + convex constraint/regularizer. The algorithm is empirically tested on training binary and sparse neural nets on MNIST and CIFAR-10.\n\n[Pros]\nThe theoretical framework that incorporates most of the commonly used tweaks in stochastic optimization for deep learning, and a convergence result that establishes broadly the asymptotic convergence to stationarity.\n\n[Cons]\nThe result of Theorem 1 sounds rather a straightforward application of classical results; important sub-cases such as Adam violates the assumption (Adam has \\rho_t = \\rho so \\sum \\rho_t^2 = \\infty) and thus are not contained in this case. From this it seems to me Theorem 1 says things mostly about the \u201ceasier\u201d sub-cases, and thus is perhaps not very surprising and a bit limited in bringing in new messages. \n\nThe experiments are mostly done on simple problems --- 3 of the 4 figures are on MNIST. The specific tasks (training sparse / binary neural nets with MLP / vanilla CNN architectures) considered in the experiments are all very extensively studied in prior work, and the results in this paper says at most that the proposed Prox-SGD works for these tasks.\n\nOverall, I like the idea in this paper that we can put together a unified framework for stochastic optimization algorithms and incorporate things like momentums and regularizations that were previously treated separately. However, beyond proposing such a framework, it seems that contributions on both the theoretical and empirical side are a bit limited at this point.\n\n***\n\nThank the authors for the response and the efforts in revising the paper. I am glad to see the additional experiments for training sparse networks on CIFAR-100 (a much harder task than MNIST and also CIFAR-10) in which the proposed method works well. This largely resolved my concerns on the experimental side. \n\nHowever, I'd still like to hold my evaluation on the theoretical side, in that approaches for handling constraints / non-smoothness / momentums are fairly well understood in the optimization literature. The present result (Theorem 1) conveys the message that these approaches can be combined to work (give an algorithm that converges to stationary points if it converges), but is not really a result that gives us new understandings / novel proof techniques beyond that.\n\nI have slightly improved my rating to reflect my updated evaluation.\n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper95/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575869865939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper95/Reviewers"], "noninvitees": [], "tcdate": 1570237757149, "tmdate": 1575869865953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Review"}}}, {"id": "HJxfvYuhor", "original": null, "number": 2, "cdate": 1573845337607, "ddate": null, "tcdate": 1573845337607, "tmdate": 1573851167340, "tddate": null, "forum": "HygpthEtvr", "replyto": "HJxQie4LcS", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment", "content": {"title": "We sincerely appreciate the reviewer for performing a thorough and constructive review. We also thank the reviewer\u2019s positive feedback to our contributions.", "comment": "\n1. What is the time complexity of solving the sub convex problem at every iteration? The authors did not discuss this in the experiments, but this is very important in evaluating the applicability of the proposed algorithm especially on large-scale problems. \n\nReply: We thank the reviewer for the constructive comment. \n\nOur algorithm needs to perform the soft-thresholding operation, but the other algorithms need to compute the subgradient of L1 norm function. To address this comment, we have measured the epoch time for the experiment on CNN and CIFAR-10 in Sec. 3.2. The computation time per epoch is: ADAM 17.24s, AMSGrad 17.44s, ADABound 16.38s, Prox-SGD 16.04s. \n\nFor the experiment on DenseNet-201 and CIFAR-100, the soft-thresholding operation in Prox-SGD increases the training time: the average time per epoch for Prox-SGD is 3.5min, SGD with momentum 2.8min and ADAM 2.9min. In view of the higher level of sparsity achieved by Prox-SGD (92-94%) compared with SGD with momentum (70%) and ADAM (20-30%), this increase in computation is reasonable and affordable.\n\nFor the experiment on BNN, the proposed Prox-SGD has closed-form updates. Furthermore, the gradient with respect to a has a closed-form expression as well and it does not involve back-propogation. The average time per epoch for full-precision DNN and Prox-SGD is roughly the same: 13.06s and 12.21s. So doubling the problem parameters in Prox-SGD does not seem to increase the computational complexity. \nThese observations are also included in the revised paper.\n\n***\n\n2. The authors should provide more explanation on the term $\\tau(t)$: It seems it is only used as an auxiliary parameter in the quadratic approximation of $f$, and that it doesn't affect the convergence asymptotically. But I would imagine it would affect the practical convergence at the beginning of the algorithm? \n\nReply: We thank the reviewer for pointing out this issue.\n\nOn the one hand, to guarantee the theoretical convergence, it is sufficient that it is lower bounded by a constant that is larger than 0. It is not even necessarily time varying. On the other hand, we agree with the reviewer that the choice of tau would impact the algorithm\u2019s empirical performance. In our paper, we did not propose a new method to update tau. In our simulations, we adopted the same rule to update tau as ADAM.\n\nWe have revised the discussion after the theorem to make the above point clear and emphasized again in the simulations.\n\n***\n\n3. The authors have repeatedly mentioned that using the averaged gradient $v(t)$ is very important for the convergence analysis of the algorithm. But I did not see how this is the case from the analysis discussed in the paper (I didn't check the proof in reference Ruszczynski (1980)). As this is important in justifying the algorithm, I think the authors should include a discussion to provide some intuition on this in the paper. \n\nReply: We thank the reviewer for the constructive comment. The averaged gradient $v(t)$ will converge to the true (but unknown) gradient of the loss function $f$, and the stochastic algorithm will eventually resembles the (deterministic) gradient descent algorithm. This property is very important in establishing the convergence. \n\nAlthough an intuitive explanation was given in (13)-(17), we have revised the paper to make the above point more clear. In particular, we added a comment after Theorem 1 to discuss its importance. We also included the proof to make the paper self-contained.\n\n***\n\n4. Line (15): should $vx(t)$ be $x(t)$ instead? If not, where does the term come from?\n\nReply: We thank the reviewer for pointing out this typo. We have corrected this typo and proofread the whole paper again.\n\n***\n\nWe thank the reviewer again for the constructive comments. We kindly and respectfully ask the reviewer to consider updating the rating if the comments are addressed to the reviewer\u2019s satisfaction.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper95/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper95/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygpthEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper95/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper95/Authors|ICLR.cc/2020/Conference/Paper95/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176472, "tmdate": 1576860539470, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment"}}}, {"id": "HyxySddhsB", "original": null, "number": 1, "cdate": 1573845047364, "ddate": null, "tcdate": 1573845047364, "tmdate": 1573850996174, "tddate": null, "forum": "HygpthEtvr", "replyto": "Byx3w9ePqS", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment", "content": {"title": "We sincerely appreciate the reviewer for performing a thorough and constructive review. We also thank the reviewer\u2019s positive feedback to our contributions.", "comment": "\nComment: It should be noted that objective functions for many NNs are not smooth (eg ReLU-based networks, which are used in the experiments) and the convergence argument does not apply directly to these. \n\nReply: The smoothness of the loss function is a common assumption in convergence analysis, and there is some work showing empirically the loss function tends to be smooth even when the nonsmooth ReLu activation function is used (see Figure 1 and the discussion in https://arxiv.org/pdf/1712.09913 for example). But we agree with the reviewer that the theoretical assumptions may not always be fully satisfied by practical NNs due to, e.g., ReLU, batch normalization and dropout. We have included such a remark after Theorem 1 in the revision.\n\n***\n\nComment: The paper could be improved by more extensively optimizing hyper parameters of all algorithms in the experimental evaluation. From the current experiments, it is not possible to evaluate whether performance differences are coming from differences in learning rate schedules, etc, or from the proposed algorithm specifically. For instance, the competitor algorithms are run with fixed momentum and learning rate parameters, while the proposed algorithm is run with a handcrafted decay in these parameters. Ideally, the hyperparameters should be optimized out such that each algorithm uses its best settings. At minimum, the choice of hyperparameter schedule for the algorithm should be justified. \n\nReply: We agree with the reviewer that all algorithms\u2019s convergence speed and achieved accuracy depend heavily on the choice of hyperparameters. \n\nOn the one hand, to ensure a fair comparison, the hyperparameters of the benchmark algorithms are chosen either according to the inventors\u2019 recommendations, or according to a hyperparameter grid search. We have included such a remark in the revision. Besides, please note that Prox-SGD can converge to a very sparse solution and this accelerates the convergence to a certain extent.\n\nOn the other hand, we pay special attention to the special structure promoted by the proposed Prox-SGD algorithm, namely, nonsmooth regularization and constraints. The achieved empirical improvement is predicted and supported by theory. To show this, we include a new simulation on DenseNet-201 and CIFAR-100 (Figure 3 in the revised paper) which shows that the proposed Prox-SGD with different hyperparameters all generate a very sparse NN, although they lead to different training loss and test accurary.\n\n***\n\nComment: The paper could also be strengthened by comparing the runtime of the proposed algorithm to prior methods. Prox SGD trains faster in terms of iterations (hyper parameter differences aside), but how about wall clock time? This is particularly important in the binary case where additional optimization parameters are added and updated in each iteration. \n\nReply: We thank the reviewer for the constructive comment. \n\nOur algorithm needs to perform the soft-thresholding operation, but the other algorithms need to compute the subgradient of L1 norm function. To address this comment, we have measured the epoch time for the experiment on CNN and CIFAR-10 in Sec. 3.2. The computation time per epoch is: ADAM 17.24s, AMSGrad 17.44s, ADABound 16.38s, Prox-SGD 16.04s.\n\nFor the experiment on DenseNet-201 and CIFAR-100, the soft-thresholding operation in Prox-SGD increases the training time: the average time per epoch for Prox-SGD is 3.5min, SGD with momentum 2.8min and ADAM 2.9min. In view of the higher level of sparsity achieved by Prox-SGD (92-94%) compared with SGD with momentum (70%) and ADAM (20-30%), this increase in computation is reasonable and affordable.\n\nFor the experiment on BNN, the proposed Prox-SGD has closed-form updates. Furthermore, the gradient with respect to the vector variable $a$ has a closed-form expression as well and it does not involve back-propogation. The average time per epoch for full-precision DNN and Prox-SGD is roughly the same: 13.06s and 12.21s. So doubling the problem parameters in Prox-SGD does not seem to increase the computational complexity. \n\nThese observations are also included in the revised paper.\n\n***\n\nComment: The main theoretical result is presented with a sketch of a proof, and I did not attempt to reconstruct the argument from the named sources. It could be useful to provide a full proof (perhaps in an appendix) to allow the work to be self-contained. \n\nReply: We thank the reviewer for the constructive comment. We have included the full proof to make the paper self-contained.\n\n***\n\nLast but not least, we thank the reviewer for pointing out the typos. We have corrected them and proofread the whole paper again.\n\n***\n\nWe thank the reviewer again for the constructive comments. We kindly and respectfully ask the reviewer to consider updating the rating if the comments are addressed to the reviewer\u2019s satisfaction."}, "signatures": ["ICLR.cc/2020/Conference/Paper95/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper95/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygpthEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper95/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper95/Authors|ICLR.cc/2020/Conference/Paper95/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176472, "tmdate": 1576860539470, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment"}}}, {"id": "HkeLEhthiB", "original": null, "number": 5, "cdate": 1573850157931, "ddate": null, "tcdate": 1573850157931, "tmdate": 1573850157931, "tddate": null, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment", "content": {"title": "We sincerely appreciate the reviewers' comments", "comment": "The comments have provided a good opportunity to clarify the confusions and greatly improved the quality of our work!"}, "signatures": ["ICLR.cc/2020/Conference/Paper95/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper95/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygpthEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper95/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper95/Authors|ICLR.cc/2020/Conference/Paper95/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176472, "tmdate": 1576860539470, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment"}}}, {"id": "HyelrcOnjH", "original": null, "number": 3, "cdate": 1573845560309, "ddate": null, "tcdate": 1573845560309, "tmdate": 1573846816305, "tddate": null, "forum": "HygpthEtvr", "replyto": "Skx2lGd6Kr", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment", "content": {"title": "We thank the reviewer for taking time to perform a thorough and constructive review.", "comment": "\nComment: The result of Theorem 1 sounds rather a straightforward application of classical results; important sub-cases such as Adam violates the assumption (Adam has $\\rho_t = \\rho$ so $\\sum \\rho_t^2 = \\infty$) and thus are not contained in this case. From this it seems to me Theorem 1 says things mostly about the \u201ceasier\u201d sub-cases, and thus is perhaps not very surprising and a bit limited in bringing in new messages. \n\nReply: We respectfully disagree with the reviewer.\n\nThe intention of Theorem 1 (on the convergence of the proposed Prox-SGD algorithm) is not to propose new convergence conditions that are weaker than state-of-the-art conditions such as ADAM for unconstrained and smooth optimization problems. On the contrary, it is to provide a convergence guarantee for the proposed Prox-SGD algorithm which can be applied to train neural networks under nonsmooth regularization and constraints. Despite the fact that the proof largely follows from classical results, it does not compromise the importance of the theorem which provides the convergence analysis for the proposed Prox-SGD algorithm.\n\nAs a matter of fact, nonsmooth regularization and constraints are difficult subcases that have not been addressed before. To our best knowledge, the proposed Prox-SGD is the first convergent algorithm proposed to train neural networks under nonsmooth regularization and constraints. We have revised the paper accordingly to make our motivations and contributions more clear.\n\n***\n\nComment: The experiments are mostly done on simple problems --- 3 of the 4 figures are on MNIST. The specific tasks (training sparse / binary neural nets with MLP / vanilla CNN architectures) considered in the experiments are all very extensively studied in prior work, and the results in this paper says at most that the proposed Prox-SGD works for these tasks. \n\nReply: We thank the reviewer for the constructive comment.\n\nIn this paper, we aim at proposing a stochastic algorithm that is applicable for a wide class of problems, rather than proposing application-specific solutions. We chose sparse and binary neural networks as example applications as they are important and challenging. The performance of the proposed Prox-SGD algorithm is illustrated by the excellent empirical results and the potential is thus consolidated. Consequently, our work contains significant contributions to both theory and applications.\n\nFor sparse neural networks, motivated by the reviewer\u2019s comment, we removed the MNIST experiments and included a new experiment on DenseNet-201 for CIFAR-100. Compared to the benchmark algorithms, the proposed Prox-SGD achieves a much sparse neural network without any loss in the training error and test accuracy.\n\nFor the binary neural networks, although MNIST is a simple dataset, state-of-the-art algorithm does not deliver satisfactory performance. Therefore this experiment still delivers an important message: the notable performance improvement demonstrates the advantage of incorporating constraints into the neural networks, which can only be trained efficiently by the proposed algorithm.\n\n***\n\nComment: Overall, I like the idea in this paper that we can put together a unified framework for stochastic optimization algorithms and incorporate things like momentums and regularizations that were previously treated separately. However, beyond proposing such a framework, it seems that contributions on both the theoretical and empirical side are a bit limited at this point.\n\nReply: We thank the reviewer for the comment.\n\nWe would like to stress again that we intend to propose a stochastic algorithm that can train neural networks with nonsmooth regularization and constraints. Momentum is essential to guarantee the convergence, and this is in sharp contrast to unconstrained optimization problems where momentum is only optional.\n\nFrom the theoretical perspective, we have proposed the first convergent stochastic algorithm for training neural networks with nonsmooth regularization and constraints.\n\nTo strengthen our empirical contribution, we included a new experiment on DenseNet201 for CIFAR-100. For the example applications considered in this paper, the proposed Prox-SGD can (1) generate a neural network that is much sparser than state-of-the-art algorithms, and (2) generate a binary neural net that has a much higher accuracy than state-of-the-art algorithms.\n\n***\n\nWe thank the reviewer again for the valuable comments. We have revised the paper accordingly to make the above points more clear. We kindly and respectfully ask the reviewer to consider updating the rating if the comments are addressed to the reviewer\u2019s satisfaction.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper95/Authors"], "readers": ["everyone", "ICLR.cc/2020/Conference/Paper95/Reviewers"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "HygpthEtvr", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper95/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper95/Authors|ICLR.cc/2020/Conference/Paper95/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504176472, "tmdate": 1576860539470, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper95/Authors", "ICLR.cc/2020/Conference/Paper95/Reviewers", "ICLR.cc/2020/Conference/Paper95/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Comment"}}}, {"id": "HJxQie4LcS", "original": null, "number": 2, "cdate": 1572384923188, "ddate": null, "tcdate": 1572384923188, "tmdate": 1572972639269, "tddate": null, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes a new gradient-based stochastic optimization algorithm (with gradient averaging) by adapting theory for proximal algorithms (originally developed for convex problems) to the non-convex setting. The main idea is to first use an averaged gradient plus a quadratic term to locally approximate the non-convex function with a convex smooth function before applying the proximal operator on it. As a result, the algorithm will be able to solve non-smooth (e.g., l1-regularized) and constrained non-convex problems, which will be very useful for optimization problems arising from deep learning.\n\nI think this is a potentially good paper that proposes an algorithm for wide applicability. But I still see some issues that prompts me to ask the following questions:\n\n1. What is the time complexity of solving the sub convex problem at every iteration? The authors did not discuss this in the experiments, but this is very important in evaluating the applicability of the proposed algorithm especially on large-scale problems.\n2. The authors should provide more explanation on the term \\tau (t): It seems it is only used as an auxiliary parameter in the quadratic approximation of f, and that it doesn't affect the convergence asymptotically. But I would imagine it would affect the practical convergence at the beginning of the algorithm?\n3. The authors have repeatedly mentioned that using the averaged gradient v(t) is very important for the convergence analysis of the algorithm. But I did not see how this is the case from the analysis discussed in the paper (I didn't check the proof in reference Ruszczynski (1980)). As this is important in justifying the algorithm, I think the authors should include a discussion to provide some intuition on this in the paper.\n4. Line (15): should vx(t) be x(t) instead? If not, where does the term come from?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper95/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575869865939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper95/Reviewers"], "noninvitees": [], "tcdate": 1570237757149, "tmdate": 1575869865953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Review"}}}, {"id": "Byx3w9ePqS", "original": null, "number": 3, "cdate": 1572436580041, "ddate": null, "tcdate": 1572436580041, "tmdate": 1572972639225, "tddate": null, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "invitation": "ICLR.cc/2020/Conference/Paper95/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\n\nThis paper introduces and analyzes a training algorithm for neural networks with non smooth regularization and weight constraints (such as sparsity or binarization). The analysis shows that, under assumptions, the proposed algorithm converges almost surely to a stationary point. Experimental results show that the proposed algorithm can train both sparse and binary neural networks.\n\nMajor comments:\n\nThis paper presents an interesting combination of theoretical analysis and experiments demonstrating the benefits of the proposed training algorithm. Because the setting considered is fairly general, it is likely to be widely useful in a variety of settings that have up to now been approached with case-specific algorithms (eg training binary NNs). \n\nIt should be noted that objective functions for many NNs are not smooth (eg ReLU-based networks, which are used in the experiments) and the convergence argument does not apply directly to these. \n\nThe paper could be improved by more extensively optimizing hyper parameters of all algorithms in the experimental evaluation. From the current experiments, it is not possible to evaluate whether performance differences are coming from differences in learning rate schedules, etc, or from the proposed algorithm specifically. For instance, the competitor algorithms are run with fixed momentum and learning rate parameters, while the proposed algorithm is run with a handcrafted decay in these parameters. Ideally, the hyperparameters should be optimized out such that each algorithm uses its best settings. At minimum, the choice of hyperparameter schedule for the algorithm should be justified. \n\nThe paper could also be strengthened by comparing the runtime of the proposed algorithm to prior methods. Prox SGD trains faster in terms of iterations (hyper parameter differences aside), but how about wall clock time? This is particularly important in the binary case where additional optimization parameters are added and updated in each iteration.\n\nThe main theoretical result is presented with a sketch of a proof, and I did not attempt to reconstruct the argument from the named sources. It could be useful to provide a full proof (perhaps in an appendix) to allow the work to be self-contained.\n\nThe paper is clearly written and easy to follow.\n\n\nTypos:\nIn general more care should be taken with the equations:\nEq. 7 x^t should be x(t)\nEq. 7 differs slightly from Alg. 1 step 3 because of the \\mu term in step 3. I would remove \\mu \nPg 3, the indicator function is introduced as \\sigma_X then put in the equation as \\delta_X"}, "signatures": ["ICLR.cc/2020/Conference/Paper95/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper95/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["yang.yang@itwm.fraunhofer.de", "yaxiong.yuan@uni.lu", "avraam.chatzimichailidis@itwm.fraunhofer.de", "r.j.g.v.sloun@tue.nl", "lei.lei@uni.lu", "symeon.chatzinotas@uni.lu"], "title": "ProxSGD: Training Structured Neural Networks under Regularization and Constraints", "authors": ["Yang Yang", "Yaxiong Yuan", "Avraam Chatzimichailidis", "Ruud JG van Sloun", "Lei Lei", "Symeon Chatzinotas"], "pdf": "/pdf/2e4a2c27aad4203712646c19389c185bbe43adc7.pdf", "TL;DR": "We propose a convergent proximal-type stochastic gradient descent algorithm for constrained nonsmooth nonconvex optimization problems", "abstract": "In this paper, we consider the problem of training neural networks (NN). To promote a NN with specific structures, we explicitly take into consideration the nonsmooth regularization (such as L1-norm) and constraints (such as interval constraint). This is formulated as a constrained nonsmooth nonconvex optimization problem, and we propose a convergent proximal-type stochastic gradient descent (Prox-SGD) algorithm. We show that under properly selected learning rates, momentum eventually resembles the unknown real gradient and thus is crucial in analyzing the convergence. We establish that with probability 1, every limit point of the sequence generated by the proposed Prox-SGD is a stationary point. Then the Prox-SGD is tailored to train a sparse neural network and a binary neural network, and the theoretical analysis is also supported by extensive numerical tests.", "keywords": ["stochastic gradient descent", "regularization", "constrained optimization", "nonsmooth optimization"], "paperhash": "yang|proxsgd_training_structured_neural_networks_under_regularization_and_constraints", "code": "https://github.com/optyang/proxsgd; https://github.com/cc-hpc-itwm/proxsgd", "_bibtex": "@inproceedings{\nYang2020ProxSGD:,\ntitle={ProxSGD: Training Structured Neural Networks under Regularization and Constraints},\nauthor={Yang Yang and Yaxiong Yuan and Avraam Chatzimichailidis and Ruud JG van Sloun and Lei Lei and Symeon Chatzinotas},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=HygpthEtvr}\n}", "full_presentation_video": "", "original_pdf": "/attachment/f783ab183448956c6b6042e8ca42b7ad65217a63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "HygpthEtvr", "replyto": "HygpthEtvr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper95/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575869865939, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper95/Reviewers"], "noninvitees": [], "tcdate": 1570237757149, "tmdate": 1575869865953, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper95/-/Official_Review"}}}], "count": 11}