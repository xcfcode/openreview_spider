{"notes": [{"id": "B1l5m6VFwr", "original": "BylA7veDDr", "number": 460, "cdate": 1569439010262, "ddate": null, "tcdate": 1569439010262, "tmdate": 1577168225933, "tddate": null, "forum": "B1l5m6VFwr", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "EINS: Long Short-Term Memory with Extrapolated Input Network Simplification", "authors": ["Nicholas I-Hsien Kuo", "Mehrtash T. Harandi", "Nicolas Fourrier", "Gabriela Ferraro", "Christian Walder", "Hanna Suominen"], "authorids": ["u6424547@anu.edu.au", "mehrtash.harandi@monash.edu", "nicolas.fourrier@devinci.fr", "gabriela.ferraro@csiro.au", "gabriela.ferraro@data61.csiro.au", "christian.walder@data61.csiro.au", "hanna.suominen@anu.edu.au"], "keywords": ["recurrent neural network", "RNN", "long short-term memory", "LSTM", "gated recurrent network", "GRU", "dynamical mathematics", "interpretability"], "TL;DR": "This paper modelled cell states of LSTMs and GRUs as dynamic Hopfield networks to present the novel light-weight RNN of EINS with either comparable, or better performances than the LSTM in a wide range of tasks.", "abstract": "This paper contrasts the two canonical recurrent neural networks (RNNs) of long short-term memory (LSTM) and gated recurrent unit (GRU) to propose our novel light-weight RNN of Extrapolated Input for Network Simplification (EINS). We treat LSTMs and GRUs as differential equations, and our analysis highlights several auxiliary components in the standard LSTM design that are secondary in importance. Guided by these insights, we present a design that abandons the LSTM redundancies, thereby introducing EINS. We test EINS against the LSTM over a carefully chosen range of tasks from language modelling and medical data imputation-prediction through a sentence-level variational autoencoder and image generation to learning to learn to optimise another neural network. Despite having both a simpler design and fewer parameters, this simplification either performs comparably, or better, than the LSTM in each task.", "pdf": "/pdf/4f39da671e773ba60688f198069ca074f3c2da53.pdf", "paperhash": "kuo|eins_long_shortterm_memory_with_extrapolated_input_network_simplification", "original_pdf": "/attachment/b80eefdb29e3530aca5103c47f6bdd229612f2b2.pdf", "_bibtex": "@misc{\nkuo2020eins,\ntitle={{\\{}EINS{\\}}: Long Short-Term Memory with Extrapolated Input Network Simplification},\nauthor={Nicholas I-Hsien Kuo and Mehrtash T. Harandi and Nicolas Fourrier and Gabriela Ferraro and Christian Walder and Hanna Suominen},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l5m6VFwr}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "EFj5MHtyvT", "original": null, "number": 1, "cdate": 1576798697065, "ddate": null, "tcdate": 1576798697065, "tmdate": 1576800938645, "tddate": null, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper460/-/Decision", "content": {"decision": "Reject", "comment": "The paper proposed new version of LSTM which is claimed to abandon the redundancies in LSTM.  It is weak both in theory and experiments.  All reviewers gave clear rejects and the AC agree.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EINS: Long Short-Term Memory with Extrapolated Input Network Simplification", "authors": ["Nicholas I-Hsien Kuo", "Mehrtash T. Harandi", "Nicolas Fourrier", "Gabriela Ferraro", "Christian Walder", "Hanna Suominen"], "authorids": ["u6424547@anu.edu.au", "mehrtash.harandi@monash.edu", "nicolas.fourrier@devinci.fr", "gabriela.ferraro@csiro.au", "gabriela.ferraro@data61.csiro.au", "christian.walder@data61.csiro.au", "hanna.suominen@anu.edu.au"], "keywords": ["recurrent neural network", "RNN", "long short-term memory", "LSTM", "gated recurrent network", "GRU", "dynamical mathematics", "interpretability"], "TL;DR": "This paper modelled cell states of LSTMs and GRUs as dynamic Hopfield networks to present the novel light-weight RNN of EINS with either comparable, or better performances than the LSTM in a wide range of tasks.", "abstract": "This paper contrasts the two canonical recurrent neural networks (RNNs) of long short-term memory (LSTM) and gated recurrent unit (GRU) to propose our novel light-weight RNN of Extrapolated Input for Network Simplification (EINS). We treat LSTMs and GRUs as differential equations, and our analysis highlights several auxiliary components in the standard LSTM design that are secondary in importance. Guided by these insights, we present a design that abandons the LSTM redundancies, thereby introducing EINS. We test EINS against the LSTM over a carefully chosen range of tasks from language modelling and medical data imputation-prediction through a sentence-level variational autoencoder and image generation to learning to learn to optimise another neural network. Despite having both a simpler design and fewer parameters, this simplification either performs comparably, or better, than the LSTM in each task.", "pdf": "/pdf/4f39da671e773ba60688f198069ca074f3c2da53.pdf", "paperhash": "kuo|eins_long_shortterm_memory_with_extrapolated_input_network_simplification", "original_pdf": "/attachment/b80eefdb29e3530aca5103c47f6bdd229612f2b2.pdf", "_bibtex": "@misc{\nkuo2020eins,\ntitle={{\\{}EINS{\\}}: Long Short-Term Memory with Extrapolated Input Network Simplification},\nauthor={Nicholas I-Hsien Kuo and Mehrtash T. Harandi and Nicolas Fourrier and Gabriela Ferraro and Christian Walder and Hanna Suominen},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l5m6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795722887, "tmdate": 1576800274277, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper460/-/Decision"}}}, {"id": "ryl1wDf3tB", "original": null, "number": 1, "cdate": 1571723094610, "ddate": null, "tcdate": 1571723094610, "tmdate": 1572972592573, "tddate": null, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper460/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper tries to analyze LSTM and GRU through differential equations and proposes an alternative formulation.\n\nI vote to reject this paper. I think the analysis has many flaws and only minor connection to performance of RNNs, and empirically the success is marginal at best.\n\nThe paper main point is based on re-writing the discrete dynamical system as a discrete integration of a ODE.  The way it is formulated in the paper has several flaws.\n First, the mixed timestamp is unnatural for ODEs. if s'=a then the Euler integrator would be s_t=s_{t-1}+h*a_{t-1} not s_t=s_{t-1}+h*a_t (eq.2). \nSecond, the timescales isn't part of the continuous ODE but part of the discrete integration. Integrators with dynamic step size defiantly do exist, but the step size does not depend on the data like in GRU/LSTM but based on approximation accuracy. In any case even if there was a real strong connection, I didn't see \"an illuminating perspective\" from the ODE connection.\n\nDetailed remarks:\n- I didn't get the connection of the Newton-Rapson method and ODEs, as the Newtow-Rapson is an iterative algorithm that has nothing to do with ODEs (or no connection was made in the paper).\n- In 3.4 you claim to do theoretical work and make strong remarks such as \"In other words, LSTMs have the potential to have more equilibrium states\" but I didn't find any support in the work itself. There was not even a clear well defined theorem/claim let alone a proof.\n- Comparing the EINS to LSTM regarding the number of parameters is a bit misleading, as the right comparison is with GRU which is the common \"slim\" alternative.\n- Experimentally the work only shows (table 1+3) very little improvement over LSTM and no comparison to GRUs.\n\n\nMinor remark:\n- In the first sentence, \"difficult to interpret\" is not really related to the paper.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper460/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper460/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EINS: Long Short-Term Memory with Extrapolated Input Network Simplification", "authors": ["Nicholas I-Hsien Kuo", "Mehrtash T. Harandi", "Nicolas Fourrier", "Gabriela Ferraro", "Christian Walder", "Hanna Suominen"], "authorids": ["u6424547@anu.edu.au", "mehrtash.harandi@monash.edu", "nicolas.fourrier@devinci.fr", "gabriela.ferraro@csiro.au", "gabriela.ferraro@data61.csiro.au", "christian.walder@data61.csiro.au", "hanna.suominen@anu.edu.au"], "keywords": ["recurrent neural network", "RNN", "long short-term memory", "LSTM", "gated recurrent network", "GRU", "dynamical mathematics", "interpretability"], "TL;DR": "This paper modelled cell states of LSTMs and GRUs as dynamic Hopfield networks to present the novel light-weight RNN of EINS with either comparable, or better performances than the LSTM in a wide range of tasks.", "abstract": "This paper contrasts the two canonical recurrent neural networks (RNNs) of long short-term memory (LSTM) and gated recurrent unit (GRU) to propose our novel light-weight RNN of Extrapolated Input for Network Simplification (EINS). We treat LSTMs and GRUs as differential equations, and our analysis highlights several auxiliary components in the standard LSTM design that are secondary in importance. Guided by these insights, we present a design that abandons the LSTM redundancies, thereby introducing EINS. We test EINS against the LSTM over a carefully chosen range of tasks from language modelling and medical data imputation-prediction through a sentence-level variational autoencoder and image generation to learning to learn to optimise another neural network. Despite having both a simpler design and fewer parameters, this simplification either performs comparably, or better, than the LSTM in each task.", "pdf": "/pdf/4f39da671e773ba60688f198069ca074f3c2da53.pdf", "paperhash": "kuo|eins_long_shortterm_memory_with_extrapolated_input_network_simplification", "original_pdf": "/attachment/b80eefdb29e3530aca5103c47f6bdd229612f2b2.pdf", "_bibtex": "@misc{\nkuo2020eins,\ntitle={{\\{}EINS{\\}}: Long Short-Term Memory with Extrapolated Input Network Simplification},\nauthor={Nicholas I-Hsien Kuo and Mehrtash T. Harandi and Nicolas Fourrier and Gabriela Ferraro and Christian Walder and Hanna Suominen},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l5m6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper460/Reviewers"], "noninvitees": [], "tcdate": 1570237751802, "tmdate": 1574723077812, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper460/-/Official_Review"}}}, {"id": "rkem21chFH", "original": null, "number": 2, "cdate": 1571753898949, "ddate": null, "tcdate": 1571753898949, "tmdate": 1572972592539, "tddate": null, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper460/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "\nGeneral Comments\nThe authors proposed a new type of gated recurrent unit for RNNs, and show comparable performance on a number of RNN applications, some achieved with less number of parameters.  \n\nDetailed Comments\n\nSection 3 (Interpretation)\nThis section basically draws similarity between the RNN update equations with discrete temporal difference equations.  It\u2019s unclear why showing that the equations look similar is motivating.  It would be better if the authors can borrow analyses done on differential equations and apply them here.\n\nSection 5 (Experiments) \nTo me, the main contribution of this work lies (potentially) in the newly proposed EINS unit.  \nTo demonstrate that it\u2019s practically appealing, below are my suggestions.  \n# of parameters is only practically meaningful for saving storage. More interesting analyses should include memory cost during training/test (i.e., the total number of activations, and the required gradients for learning), and computational time.   \nEase of tuning (i.e., is this method easier to tune than LSTM/GRU).  One benefit of a new method can be that it\u2019s more robust to tuning-/hyper-parameters.  If the authors can show that EINS is easier to tune, this can be a good result.\nExperiments are diverse, not I rather see a large scale experiment because the strength of the proposed method was motivated as a \u201csimpler\u201d method.  The provided experiments do not strengthen the claim that EINS is \u201csimpler\u201d for the practitioners. Understandably, this will require a lot of effort, but many of the interesting applications of RNNs as of today are large-scale speech recognition, language modelling, and video recognition.  It\u2019s unclear whether EINS has any benefits in that regime.\n\nOverall, the authors demonstrated a variant of RNN updates, but neither the theoretical nor  empirical evidence was strong enough to 1. Convince practitioners to switch from LSTM/GRU to EINS, 2. Provide insight/understanding to the workings of RNNs.  \nIt\u2019s interesting to look for new building blocks for neural nets.  However, perhaps more interesting than manually designing it, maybe you can use the reasonings in this paper to design constraints for neural architecture search model.  \n\nBest,\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper460/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper460/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EINS: Long Short-Term Memory with Extrapolated Input Network Simplification", "authors": ["Nicholas I-Hsien Kuo", "Mehrtash T. Harandi", "Nicolas Fourrier", "Gabriela Ferraro", "Christian Walder", "Hanna Suominen"], "authorids": ["u6424547@anu.edu.au", "mehrtash.harandi@monash.edu", "nicolas.fourrier@devinci.fr", "gabriela.ferraro@csiro.au", "gabriela.ferraro@data61.csiro.au", "christian.walder@data61.csiro.au", "hanna.suominen@anu.edu.au"], "keywords": ["recurrent neural network", "RNN", "long short-term memory", "LSTM", "gated recurrent network", "GRU", "dynamical mathematics", "interpretability"], "TL;DR": "This paper modelled cell states of LSTMs and GRUs as dynamic Hopfield networks to present the novel light-weight RNN of EINS with either comparable, or better performances than the LSTM in a wide range of tasks.", "abstract": "This paper contrasts the two canonical recurrent neural networks (RNNs) of long short-term memory (LSTM) and gated recurrent unit (GRU) to propose our novel light-weight RNN of Extrapolated Input for Network Simplification (EINS). We treat LSTMs and GRUs as differential equations, and our analysis highlights several auxiliary components in the standard LSTM design that are secondary in importance. Guided by these insights, we present a design that abandons the LSTM redundancies, thereby introducing EINS. We test EINS against the LSTM over a carefully chosen range of tasks from language modelling and medical data imputation-prediction through a sentence-level variational autoencoder and image generation to learning to learn to optimise another neural network. Despite having both a simpler design and fewer parameters, this simplification either performs comparably, or better, than the LSTM in each task.", "pdf": "/pdf/4f39da671e773ba60688f198069ca074f3c2da53.pdf", "paperhash": "kuo|eins_long_shortterm_memory_with_extrapolated_input_network_simplification", "original_pdf": "/attachment/b80eefdb29e3530aca5103c47f6bdd229612f2b2.pdf", "_bibtex": "@misc{\nkuo2020eins,\ntitle={{\\{}EINS{\\}}: Long Short-Term Memory with Extrapolated Input Network Simplification},\nauthor={Nicholas I-Hsien Kuo and Mehrtash T. Harandi and Nicolas Fourrier and Gabriela Ferraro and Christian Walder and Hanna Suominen},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l5m6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper460/Reviewers"], "noninvitees": [], "tcdate": 1570237751802, "tmdate": 1574723077812, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper460/-/Official_Review"}}}, {"id": "ryl9bIlc9H", "original": null, "number": 3, "cdate": 1572632066514, "ddate": null, "tcdate": 1572632066514, "tmdate": 1572972592506, "tddate": null, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "invitation": "ICLR.cc/2020/Conference/Paper460/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have read many papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #4", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This work proposes a simplified variant of LSTM, motivated by the connection of gated variants of RNN, LSTM and GRU, and Hopfield network. Technically, the author shows that the GRU is a special case of the dynamic Hopfield model. And then the author proposes three simplifications on LSTM, which are suggested by the formulation of the dynamic Hopfield model. A wide range of tasks are included in the experiment and the results prove the effectiveness of the simplification. \n\nHowever, I found it really hard to align this work with other existing works with similar or the same efforts. There has been already many works on variants of LSTM, such as SRU, fastGRNN, Clockwork RNN and etc. But none of them are employed as the baseline model, or even mentioned in the related work. Furthermore, there is not even a related work section in the paper. Therefore, this work should not be accepted.\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper460/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper460/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "EINS: Long Short-Term Memory with Extrapolated Input Network Simplification", "authors": ["Nicholas I-Hsien Kuo", "Mehrtash T. Harandi", "Nicolas Fourrier", "Gabriela Ferraro", "Christian Walder", "Hanna Suominen"], "authorids": ["u6424547@anu.edu.au", "mehrtash.harandi@monash.edu", "nicolas.fourrier@devinci.fr", "gabriela.ferraro@csiro.au", "gabriela.ferraro@data61.csiro.au", "christian.walder@data61.csiro.au", "hanna.suominen@anu.edu.au"], "keywords": ["recurrent neural network", "RNN", "long short-term memory", "LSTM", "gated recurrent network", "GRU", "dynamical mathematics", "interpretability"], "TL;DR": "This paper modelled cell states of LSTMs and GRUs as dynamic Hopfield networks to present the novel light-weight RNN of EINS with either comparable, or better performances than the LSTM in a wide range of tasks.", "abstract": "This paper contrasts the two canonical recurrent neural networks (RNNs) of long short-term memory (LSTM) and gated recurrent unit (GRU) to propose our novel light-weight RNN of Extrapolated Input for Network Simplification (EINS). We treat LSTMs and GRUs as differential equations, and our analysis highlights several auxiliary components in the standard LSTM design that are secondary in importance. Guided by these insights, we present a design that abandons the LSTM redundancies, thereby introducing EINS. We test EINS against the LSTM over a carefully chosen range of tasks from language modelling and medical data imputation-prediction through a sentence-level variational autoencoder and image generation to learning to learn to optimise another neural network. Despite having both a simpler design and fewer parameters, this simplification either performs comparably, or better, than the LSTM in each task.", "pdf": "/pdf/4f39da671e773ba60688f198069ca074f3c2da53.pdf", "paperhash": "kuo|eins_long_shortterm_memory_with_extrapolated_input_network_simplification", "original_pdf": "/attachment/b80eefdb29e3530aca5103c47f6bdd229612f2b2.pdf", "_bibtex": "@misc{\nkuo2020eins,\ntitle={{\\{}EINS{\\}}: Long Short-Term Memory with Extrapolated Input Network Simplification},\nauthor={Nicholas I-Hsien Kuo and Mehrtash T. Harandi and Nicolas Fourrier and Gabriela Ferraro and Christian Walder and Hanna Suominen},\nyear={2020},\nurl={https://openreview.net/forum?id=B1l5m6VFwr}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1l5m6VFwr", "replyto": "B1l5m6VFwr", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper460/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1574722376000, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper460/Reviewers"], "noninvitees": [], "tcdate": 1570237751802, "tmdate": 1574723077812, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper460/-/Official_Review"}}}], "count": 5}