{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028561443, "tcdate": 1490028561443, "number": 1, "id": "B1lYzOFpse", "invitation": "ICLR.cc/2017/workshop/-/paper32/acceptance", "forum": "HyhbYrGYe", "replyto": "HyhbYrGYe", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.", "pdf": "/pdf/d865b01fdcb63f01389bfaeba231464e94e490e8.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": [], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028562038, "id": "ICLR.cc/2017/workshop/-/paper32/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "HyhbYrGYe", "replyto": "HyhbYrGYe", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028562038}}}, {"tddate": null, "nonreaders": null, "tmdate": 1490021229812, "tcdate": 1489772784140, "number": 1, "id": "Hk_xWjtoe", "invitation": "ICLR.cc/2017/workshop/-/paper32/public/comment", "forum": "HyhbYrGYe", "replyto": "rkTFDWA5e", "signatures": ["~George_Tucker1"], "readers": ["everyone"], "writers": ["~George_Tucker1"], "content": {"title": "RE: Review", "comment": "Thank you for the careful review and helpful feedback.\n\n1) Yes, we agree that in some cases, more competitive baselines exist. There was a tradeoff in implementing state-of-the-art baselines with all of the bells and whistles and trying the technique across multiple domains and different model architectures. For this workshop submission, we decided it would of more interest to focus on broadly evaluating the technique, but we see the merit of the other approach too. \n\nPreliminary results of label smoothing with the CTC objective yielded a small improvement when no language model was used. We smoothed all non-blank tokens at all timesteps using an auxiliary cost function. However, note that unlike the seq2seq networks that directly output next-token predictions, CTC comes with its own loss function and it is not obvious how to best apply the smoothing - Do you force a smooth distribution of the non-blank tokens? Do you smooth the blank? Do you extract alignments and only smooth the emission locations, or you indiscriminately smooth all locations? Furthermore, CTC needs a language model for optimal decoding. This will need to be tuned together with smoothing. Exploring CTC and label smoothing is thus an interesting topic, but may be of more interest to a speech focused venue, and treating it thoroughly exceeds the 3-page limitation of the workshop.\n\n2) We agree that the theoretical justification is mostly speculative (our primary contribution is the extensive empirical evaluation), however, as you suggest, we hypothesize that it is due to gradients saturating. We notice that once an example is correctly predicted, the only way to increase the log likelihood is to make the prediction sharper. Hence the outputs become uncalibrated and unless capacity is controlled in some way, the network will put very sharp distributions on its predictions. These regularizers prevent this behavior. Moreover, the Inception paper notes that the gradient vanishes on confident correctly predicted samples. This does not happen with label smoothing and the confidence penalty, which means that we do get some training signal for the lower layers even on correct predictions. This may improve data efficiency when doing multiple epochs (normally during the later passes only the few samples that the net doesn\u2019t get confidently have any influence). This is similar to the trick from Yann LeCun\u2019s \u201cEfficient Backprop\u201d in which the hyperbolic tangent was used on the output of the net, but it was scaled to have a range of [-1.1, 1.1], while the targets were +-1. Thus the net was never driven into saturation. \n\nCan you clarify the experiment you're suggesting to test this? Do you mean a regression task or classification with sigmoid outputs and L2 loss?\n\nA second point is that for misclassified examples, the network can get large gradients which may slow training. The confidence penalty would encourage the model to place mass on all classes, which would reduce the norm of these gradients, which is confirmed in the plot of the gradient norm. \n\nLastly, we can interpret our approach as a regularizer encouraging the predictive distribution to be close to uniform. So, when the model has little evidence it is regularized to the uniform, however, when sufficient evidence is accumulated, the predictive distribution matches the data.\n\n"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.", "pdf": "/pdf/d865b01fdcb63f01389bfaeba231464e94e490e8.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": [], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487194372253, "tcdate": 1487194372253, "id": "ICLR.cc/2017/workshop/-/paper32/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper32/reviewers"], "reply": {"forum": "HyhbYrGYe", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487194372253}}}, {"tddate": null, "tmdate": 1489188599553, "tcdate": 1489188599553, "number": 2, "id": "Hykbw3gjx", "invitation": "ICLR.cc/2017/workshop/-/paper32/official/review", "forum": "HyhbYrGYe", "replyto": "HyhbYrGYe", "signatures": ["ICLR.cc/2017/workshop/paper32/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper32/AnonReviewer2"], "content": {"title": "Thorough evaluation of smoothing techniques, interesting introduction of confidence penalization", "rating": "7: Good paper, accept", "review": "The paper proposes using confidence as a term for regularization in neural networks, helping to prevent overfitting by penalizing overly confident predictions.  The experiments range across a number of fields and architectures, helping to show both the generality of the technique and where it appears to be most helpful.\n\nThe work and experiments are rather detailed and exhaustive, especially when delving in to the Appendix for specific details of the various experiments. The confidence penalty regularization is compared to and combined with dropout and label smoothing. I do agree with another reviewer that some of the baseline systems are weaker than others. Seeing an LSTM used without recurrent dropout (variational dropout, zoneout, ...) as a baseline for language modeling is unfortunate for example. Even with that acknowledged, the results and analysis over a variety of datasets is enough to convince me of the capability of confidence penalization as a regularization technique.\n\nOverall, I think the paper makes a good contribution to the knowledge and application of various smoothing techniques and introduces the benefits of confidence penalization as a competing and/or complementary regularization technique. The paper is clearly written and detailed in the number and variety of experiments performed.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.", "pdf": "/pdf/d865b01fdcb63f01389bfaeba231464e94e490e8.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": [], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489188600328, "id": "ICLR.cc/2017/workshop/-/paper32/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper32/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper32/AnonReviewer1", "ICLR.cc/2017/workshop/paper32/AnonReviewer2"], "reply": {"forum": "HyhbYrGYe", "replyto": "HyhbYrGYe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper32/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper32/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489188600328}}}, {"tddate": null, "tmdate": 1489012613489, "tcdate": 1489012613489, "number": 1, "id": "rkTFDWA5e", "invitation": "ICLR.cc/2017/workshop/-/paper32/official/review", "forum": "HyhbYrGYe", "replyto": "HyhbYrGYe", "signatures": ["ICLR.cc/2017/workshop/paper32/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper32/AnonReviewer1"], "content": {"title": "Another softmax smoothing technique", "rating": "5: Marginally below acceptance threshold", "review": "The paper proposes to add a regularizing term to the objective function which penalizes the estimation of distributions with small entropy. It is one of these small tricks that were tried out by various groups even though only few people mention it in publications because the changes in performance are small and the additional hyperparameter makes it unattractive. This is also reflected in the paper here, as the improvements are very small compared to the baselines and usually vanish if more care is taking w.r.t. traditional regularization approaches. \n\nFurther remarks:\n\n- The evaluation is done on a broad spectrum of tasks, but the selections of the baseline systems is questionable. Especially on WSJ, there is no good reason to take an attention based seq2seq model but not also a network trained in a hybrid fashion or with CTC. Especially the CTC experiment would have been of great interest since the criterion tends to favor sharp probabilities.\n\n- A theoretical perspective on the convergence is not well established and a proper justification why this is should be able to improve neural network training is missing (except for the norms of the gradients on MNIST).  If the argument is that gradients saturate too quickly if probabilities go too high then I would like to see an experiment with the squared error criterion as comparison, where this effect is not that large. \n\nIn total I appreciate the work and broad evaluation but would suggest to include this method in a larger comparison paper that describes several of these tricks. The paper is well written and certainly correct, and the required scope is clearly limited within a workshop. Yet I would like to see some of the points here addressed before recommending acceptance. ", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.", "pdf": "/pdf/d865b01fdcb63f01389bfaeba231464e94e490e8.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": [], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489188600328, "id": "ICLR.cc/2017/workshop/-/paper32/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper32/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper32/AnonReviewer1", "ICLR.cc/2017/workshop/paper32/AnonReviewer2"], "reply": {"forum": "HyhbYrGYe", "replyto": "HyhbYrGYe", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper32/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper32/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489188600328}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487712007515, "tcdate": 1487194371638, "number": 32, "id": "HyhbYrGYe", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "HyhbYrGYe", "original": "HkCjNI5ex", "signatures": ["~Gabriel_Pereyra1"], "readers": ["everyone"], "content": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.", "pdf": "/pdf/d865b01fdcb63f01389bfaeba231464e94e490e8.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": [], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "original": {"tddate": null, "replyto": null, "ddate": null, "active": true, "tmdate": 1484966408203, "tcdate": 1478284454349, "number": 295, "id": "HkCjNI5ex", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "HkCjNI5ex", "signatures": ["~Gabriel_Pereyra1"], "readers": ["everyone"], "content": {"title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We propose regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. We connect our confidence penalty to label smoothing through the direction of the KL divergence between networks output distribution and the uniform distribution. We exhaustively evaluate our proposed confidence penalty and label smoothing (uniform and unigram) on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and our confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyper-parameters.\n", "pdf": "/pdf/04b04a9f263837bae9c3f6a6a99e73348c93c626.pdf", "TL;DR": "We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning.", "paperhash": "pereyra|regularizing_neural_networks_by_penalizing_confident_output_distributions", "conflicts": ["google.com"], "keywords": ["Deep learning", "Supervised Learning", "Speech", "Structured prediction"], "authors": ["Gabriel Pereyra", "George Tucker", "Jan Chorowski", "Lukasz Kaiser", "Geoffrey Hinton"], "authorids": ["pereyra@google.com", "gjt@google.com", "chorowski@google.com", "lukaszkaiser@google.com", "geoffhinton@google.com"]}, "writers": [], "nonreaders": []}, "originalWritable": false, "originalInvitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}, "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}