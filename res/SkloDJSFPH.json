{"notes": [{"id": "SkloDJSFPH", "original": "BylqtpadvH", "number": 1779, "cdate": 1569439586906, "ddate": null, "tcdate": 1569439586906, "tmdate": 1577168231581, "tddate": null, "forum": "SkloDJSFPH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["youngjoon.yoo@navercorp.com", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "sangdoo.yun@navercorp.com", "jungwoo.ha@navercorp.com"], "title": "Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling", "authors": ["YoungJoon Yoo", "Sanghyuk Chun", "Jaejun Yoo", "Sangdoo Yun", "Jung Woo Ha"], "pdf": "/pdf/c657e6eb2a26048a960d110f2e8d453054b1057d.pdf", "TL;DR": "We propose a effective confidence-based approximation method that can be plugged in to various auto-regressive models with a proved convergence.", "abstract": "We propose a generic confidence-based approximation that can be plugged in and simplify an auto-regressive generation process with a proved convergence. We first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. Our experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lower computational cost while preserving the sequential relationship of the data.}", "keywords": ["Neural approximation method", "Auto-regressive model", "Sequential sample generation"], "paperhash": "yoo|neural_approximation_of_an_autoregressive_process_through_confidence_guided_sampling", "original_pdf": "/attachment/dc094030c3fcdda2e02c935947773d34f5100d1c.pdf", "_bibtex": "@misc{\nyoo2020neural,\ntitle={Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling},\nauthor={YoungJoon Yoo and Sanghyuk Chun and Jaejun Yoo and Sangdoo Yun and Jung Woo Ha},\nyear={2020},\nurl={https://openreview.net/forum?id=SkloDJSFPH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "HbBlKsMRoZ", "original": null, "number": 1, "cdate": 1576798732322, "ddate": null, "tcdate": 1576798732322, "tmdate": 1576800904110, "tddate": null, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "invitation": "ICLR.cc/2020/Conference/Paper1779/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents a technique for approximately sampling from autoregressive models using something like a a proposal distribution and a critic. The idea is to chunk the output into blocks and, for each block, predict each element in the block independently from a proposal network, ask a critic network whether the block looks sensible and, if not, resampling the block using the autoregressive model itself. \nThe idea in the paper is interesting, but the paper would benefit from\n- a better relation to existing methods\n- a better experimental section, which details the hyper-parameters of the algorithm (and how they were chosen) and which provides error bars on all plots (and tables)", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youngjoon.yoo@navercorp.com", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "sangdoo.yun@navercorp.com", "jungwoo.ha@navercorp.com"], "title": "Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling", "authors": ["YoungJoon Yoo", "Sanghyuk Chun", "Jaejun Yoo", "Sangdoo Yun", "Jung Woo Ha"], "pdf": "/pdf/c657e6eb2a26048a960d110f2e8d453054b1057d.pdf", "TL;DR": "We propose a effective confidence-based approximation method that can be plugged in to various auto-regressive models with a proved convergence.", "abstract": "We propose a generic confidence-based approximation that can be plugged in and simplify an auto-regressive generation process with a proved convergence. We first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. Our experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lower computational cost while preserving the sequential relationship of the data.}", "keywords": ["Neural approximation method", "Auto-regressive model", "Sequential sample generation"], "paperhash": "yoo|neural_approximation_of_an_autoregressive_process_through_confidence_guided_sampling", "original_pdf": "/attachment/dc094030c3fcdda2e02c935947773d34f5100d1c.pdf", "_bibtex": "@misc{\nyoo2020neural,\ntitle={Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling},\nauthor={YoungJoon Yoo and Sanghyuk Chun and Jaejun Yoo and Sangdoo Yun and Jung Woo Ha},\nyear={2020},\nurl={https://openreview.net/forum?id=SkloDJSFPH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718959, "tmdate": 1576800269530, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1779/-/Decision"}}}, {"id": "H1l8W_ndtS", "original": null, "number": 1, "cdate": 1571502077939, "ddate": null, "tcdate": 1571502077939, "tmdate": 1572972424541, "tddate": null, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "invitation": "ICLR.cc/2020/Conference/Paper1779/-/Official_Review", "content": {"rating": "6: Weak Accept", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The paper presents a technique for approximately sampling from autoregressive models using something like a a proposal distribution and a critic. The idea is to chunk the output into blocks and, for each block, predict each element in the block independently from a proposal network, ask a critic network whether the block looks sensible and, if not, resampling the block using the autoregressive model itself.\n\nIn broad strokes the approach makes sense. It assumes, essentially, that parts of the sequence are hard to predict and parts are easy and, if there are enough easy parts, this procedure should lead to faster inference.\n\nThe paper's writing is not ideal. There are some grammatical mistakes that harm reading (for example, the second paragraph of the introduction says \"However, these models must infer each element of the data x \u2208 RN step by step in a serial manner, requiring O(N) times more than other non-sequential estimators\", where it is unclear what is O(N) more than what, how is this measured, etc). That said I was mostly able to follow all key points.\n\nThe authors do not point out the obvious connection to GANs, which also rely on a critic network to decide whether a sample looks like it comes from the correct distribution, except in GANs the critic is jointly trained with the generator (as opposed to here where it's trained after) and in GANs the critic is only used at training time, while here the critic is used to accelerate sampling (the better the critic the faster this method can sample).\n\nI wish the experimental results were a little more explicit about the time vs quality tradeoff; I expected to see more plots with pareto curves, since as-is it's hard to judge the magnitude of the tradeoffs involved. I'd also like a more thorough analysis on why there is a non-monotonic tradeoff in some experiments (table 1, figure 2(b)) between the amount of approximation and the sample quality; this makes me think something else is going on here as this approximate inference method should just decrease quality, never increase it.\n\nOverall I lean towards accepting the paper, but I encourage the authors to revise the writing and to add a few plots explicitly showing the time vs quality tradeoff both in likelihood (wrt the full model) and in downstream metrics like FID."}, "signatures": ["ICLR.cc/2020/Conference/Paper1779/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1779/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youngjoon.yoo@navercorp.com", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "sangdoo.yun@navercorp.com", "jungwoo.ha@navercorp.com"], "title": "Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling", "authors": ["YoungJoon Yoo", "Sanghyuk Chun", "Jaejun Yoo", "Sangdoo Yun", "Jung Woo Ha"], "pdf": "/pdf/c657e6eb2a26048a960d110f2e8d453054b1057d.pdf", "TL;DR": "We propose a effective confidence-based approximation method that can be plugged in to various auto-regressive models with a proved convergence.", "abstract": "We propose a generic confidence-based approximation that can be plugged in and simplify an auto-regressive generation process with a proved convergence. We first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. Our experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lower computational cost while preserving the sequential relationship of the data.}", "keywords": ["Neural approximation method", "Auto-regressive model", "Sequential sample generation"], "paperhash": "yoo|neural_approximation_of_an_autoregressive_process_through_confidence_guided_sampling", "original_pdf": "/attachment/dc094030c3fcdda2e02c935947773d34f5100d1c.pdf", "_bibtex": "@misc{\nyoo2020neural,\ntitle={Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling},\nauthor={YoungJoon Yoo and Sanghyuk Chun and Jaejun Yoo and Sangdoo Yun and Jung Woo Ha},\nyear={2020},\nurl={https://openreview.net/forum?id=SkloDJSFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1779/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1779/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575297774859, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1779/Reviewers"], "noninvitees": [], "tcdate": 1570237732410, "tmdate": 1575297774873, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1779/-/Official_Review"}}}, {"id": "ByxpVK-jtB", "original": null, "number": 2, "cdate": 1571653940921, "ddate": null, "tcdate": 1571653940921, "tmdate": 1572972424502, "tddate": null, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "invitation": "ICLR.cc/2020/Conference/Paper1779/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review": "The authors consider the problem of sampling time series.\nTo solve the problem they propose a method that is based on the autoregression model. The novelty here lies in the proposed sampling methods: we start with a sampling of a prior and then try to generate data according to the restored distribution. We learn two functions: signal recovery and confidence prediction.\nThe main hyperparameter of the algorithm $\\varepsilon$ identifies how much samples we accept.\nThe distinctive feature of the algorithm is speed-up for the sample generation process.\n\nWeak reject\n\nThere are a significant number of works on video generation, see e.g. [1, 2], references therein and articles that cite these two articles. The problem setting seems pretty similar. It seems like a good idea to compare to these methods (and it seems that video generation is a very resource-demanding procedure, and they don't use parallel applications similar to proposed in the paper. What is the reason?) Most of the approaches use only one frame to generate video, but it seems that LSTM in these methods will benefit from using of multiple frames as input (and will be able to transfer information in autoregression manner by transferring all they need in a hidden state).\nThe article, in my opinion, will benefit from comparison to these approaches or at least by using some benchmarks from these works to demonstrate feasibility of the considered approach, also it seems that these works are good for demonstration of parallelization capabilities (as in many cases the same idea applies).\n\nNot minor comments:\n1. In Figure 2 (a) it is not clear how the data and prediction were generated. According to the procedure in Figure 1 and text we use the same input for all approaches. However solid lines for different epsilons are different.\n2. The effect of the dependence of recovery of quality for Figure 2 (b) is not explained and is controversial: we get the smallest error for intermediate acceptance ratio, however, there is also a decrease of error if we further increase the gauge threshold (btw the term gauge threshold is new to machine learning community, consider replacement of it)\n3. More simpler examples will benefit the paper, as we'll be able to know more fundamental properties of the proposed approach. \n\n\nMinor technical comments:\n1. s. 3.1. predictor predicts\ncommas in equation (8)\n2. Figure 2: no axis labels for the left plot, use for label \"acceptance ratio\" red color font & for label \"L1 error\" blue color font\n3. Table 1 bracket after l_1 is missing\n4. Maybe $\\sigma$ is not the best designation of confidence, as it can be confused with the variance\n5. Figure 1: some indexes should be not $x_{i + 2}$, but $x_{i + j}$, $x_{i + M}$. Also, some \">\" before \\epsilon should be \"<\"\n6. \"a auto-encoder architecture\" -> \n\"an auto-encoder architecture\"\n\n[1] J. He et al. Probabilistic Video Generation using Holistic Attribute Control. 2018. ECCV\n[2] E. Denton, R.Fergus. Stochastic Video Generation with a Learned Prior. 2018."}, "signatures": ["ICLR.cc/2020/Conference/Paper1779/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1779/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youngjoon.yoo@navercorp.com", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "sangdoo.yun@navercorp.com", "jungwoo.ha@navercorp.com"], "title": "Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling", "authors": ["YoungJoon Yoo", "Sanghyuk Chun", "Jaejun Yoo", "Sangdoo Yun", "Jung Woo Ha"], "pdf": "/pdf/c657e6eb2a26048a960d110f2e8d453054b1057d.pdf", "TL;DR": "We propose a effective confidence-based approximation method that can be plugged in to various auto-regressive models with a proved convergence.", "abstract": "We propose a generic confidence-based approximation that can be plugged in and simplify an auto-regressive generation process with a proved convergence. We first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. Our experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lower computational cost while preserving the sequential relationship of the data.}", "keywords": ["Neural approximation method", "Auto-regressive model", "Sequential sample generation"], "paperhash": "yoo|neural_approximation_of_an_autoregressive_process_through_confidence_guided_sampling", "original_pdf": "/attachment/dc094030c3fcdda2e02c935947773d34f5100d1c.pdf", "_bibtex": "@misc{\nyoo2020neural,\ntitle={Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling},\nauthor={YoungJoon Yoo and Sanghyuk Chun and Jaejun Yoo and Sangdoo Yun and Jung Woo Ha},\nyear={2020},\nurl={https://openreview.net/forum?id=SkloDJSFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1779/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1779/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575297774859, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1779/Reviewers"], "noninvitees": [], "tcdate": 1570237732410, "tmdate": 1575297774873, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1779/-/Official_Review"}}}, {"id": "ryxp5HFpFr", "original": null, "number": 3, "cdate": 1571816853186, "ddate": null, "tcdate": 1571816853186, "tmdate": 1572972424463, "tddate": null, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "invitation": "ICLR.cc/2020/Conference/Paper1779/-/Official_Review", "content": {"experience_assessment": "I have published one or two papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "N/A", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper addresses the sequential limitation of autoregressive model when doing sampling. Specifically, instead of sampling next observations in a sequential fashion, this paper generates future observations in parallel, with the help of i.i.d. future priors. With the help of learned confidence model, the model is able to get trade-off between speed and approximation accuracy. Experiments on synthetic data and image generation with PixelCNN++ show the comparable results while being significantly faster than baseline.\n\nOverall the paper is well motivated, with an interesting design of the variational distribution to approximate the true autoregressive distribution. The design of the confidence model looks a bit heuristic, but the trade-off ability between efficiency and quality it brings is also quite interesting. \n\nBelow are some minor comments:\n\n1. The theoretical analysis is basically comment about the objective which is less interesting. However more interesting guarantees would be: 1) with the additional correction term added, how would it help with reducing the variance; 2) As the q_{\\theta, \\phi} is always in a limited form due to the parallelism requirement, how bad the approximation could be in the worst case ---- I\u2019m not asking for these results, but any form of discussion would be helpful.\n\n2. The author only compared with the raw PixelCNN++. Would any of the existing AR-speedup method be applicable for a comparison? \n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1779/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1779/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["youngjoon.yoo@navercorp.com", "sanghyuk.c@navercorp.com", "jaejun.yoo@navercorp.com", "sangdoo.yun@navercorp.com", "jungwoo.ha@navercorp.com"], "title": "Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling", "authors": ["YoungJoon Yoo", "Sanghyuk Chun", "Jaejun Yoo", "Sangdoo Yun", "Jung Woo Ha"], "pdf": "/pdf/c657e6eb2a26048a960d110f2e8d453054b1057d.pdf", "TL;DR": "We propose a effective confidence-based approximation method that can be plugged in to various auto-regressive models with a proved convergence.", "abstract": "We propose a generic confidence-based approximation that can be plugged in and simplify an auto-regressive generation process with a proved convergence. We first assume that the priors of future samples can be generated in an independently and identically distributed (i.i.d.) manner using an efficient predictor. Given the past samples and future priors, the mother AR model can post-process the priors while the accompanied confidence predictor decides whether the current sample needs a resampling or not. Thanks to the i.i.d. assumption, the post-processing can update each sample in a parallel way, which remarkably accelerates the mother model. Our experiments on different data domains including sequences and images show that the proposed method can successfully capture the complex structures of the data and generate the meaningful future samples with lower computational cost while preserving the sequential relationship of the data.}", "keywords": ["Neural approximation method", "Auto-regressive model", "Sequential sample generation"], "paperhash": "yoo|neural_approximation_of_an_autoregressive_process_through_confidence_guided_sampling", "original_pdf": "/attachment/dc094030c3fcdda2e02c935947773d34f5100d1c.pdf", "_bibtex": "@misc{\nyoo2020neural,\ntitle={Neural Approximation of an Auto-Regressive Process through Confidence Guided Sampling},\nauthor={YoungJoon Yoo and Sanghyuk Chun and Jaejun Yoo and Sangdoo Yun and Jung Woo Ha},\nyear={2020},\nurl={https://openreview.net/forum?id=SkloDJSFPH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "SkloDJSFPH", "replyto": "SkloDJSFPH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1779/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1779/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575297774859, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1779/Reviewers"], "noninvitees": [], "tcdate": 1570237732410, "tmdate": 1575297774873, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1779/-/Official_Review"}}}], "count": 5}