{"notes": [{"id": "B1xSperKvH", "original": "S1xIM-WFPr", "number": 2573, "cdate": 1569439933471, "ddate": null, "tcdate": 1569439933471, "tmdate": 1583912020481, "tddate": null, "forum": "B1xSperKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "ICLR.cc/2020/Conference"}, {"id": "i1zKlyyBEw", "original": null, "number": 1, "cdate": 1576798752414, "ddate": null, "tcdate": 1576798752414, "tmdate": 1576800883197, "tddate": null, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "After the rebuttal, all reviewers rated this paper as a weak accept. \nThe reviewer leaning towards rejection was satisfied with the author response and ended up raising their rating to a weak accept.  The AC recommends acceptance.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795716117, "tmdate": 1576800266179, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Decision"}}}, {"id": "SklebdhWoB", "original": null, "number": 3, "cdate": 1573140472444, "ddate": null, "tcdate": 1573140472444, "tmdate": 1573932474276, "tddate": null, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "This paper examines combining two approaches of obtaining a trained spikingneural network (SNN). The first approach of previous work is converting the weights of a trained artificial neural network (ANN) with a given architecture, to the weights and thresholds of a SNN, and the second approach uses a surrogate gradient to train an SNN with backpropagation. The true novelty of the paper seems to be in showing that combining the two approaches sequentially, trains a SNN that requiresfewer timesteps to determine an output which achieves state of the art performance. This is summarized by Table 1. However, it does not mention how many epochs it takes to train an SNN from scratch, nor compare this to the total training time (ANN training + SNN fine-tuning) of their approach. They also claim a novel spike-time based surrogate gradient function (eq. 11), but it is very practicallysimilar to the ones explored in the referenced Wu. et al 2018 (eq. 27 for instance), and these should be properly contrasted showing that this novel surrogate function is actually helpful (the performance/energy efficiency might only come from the hybrid approach). The authors argue for SOTA performance in Table 2, but the comparison to other work doesn\u2019t clearly separate their performance from the otherlisted works; For example the accuracy gain against Lee et al.,2019 only comes from the architecture being VGG16 as opposed to VGG9, as can be seen from comparing with the VGG9 architecture from Table 1, furthermore they take the sameamount of timesteps, which is supposed to be the principle gain of this work.\n\nSome small suggestions that are independent from the above:\n\n1.The most similar or relevant version of equation (2) in previous work could be referenced nearby for context.\n\n2.The last sentence of the first paragraph on p.4 \u201cthe outputs from each copy...\u201d is confusing. Are you just meaning to describe BPTT? \n\n3.Typos: sec7 4th line \u201cneruons\u201d, sec 2.2 \u201cboth the credit\u201d (remove \u201cthe\u201d)\n\n---------------\nFollowing the author response I have upgraded my rating.", "review_assessment:_checking_correctness_of_derivations_and_theory": "N/A"}, "signatures": ["ICLR.cc/2020/Conference/Paper2573/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2573/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575385806623, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2573/Reviewers"], "noninvitees": [], "tcdate": 1570237720915, "tmdate": 1575385806639, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Official_Review"}}}, {"id": "BkgpDWL5sB", "original": null, "number": 5, "cdate": 1573704037146, "ddate": null, "tcdate": 1573704037146, "tmdate": 1573704037146, "tddate": null, "forum": "B1xSperKvH", "replyto": "S1gQnUOTFS", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your remarks. You have summarized our paper very nicely and we appreciate your time and effort for reviewing our work. Please let us know if you have any suggestions that can help to increase the score of the paper. We have added Appendix A and B to compare different pseudo-derivatives and the training effort for training an SNN from scratch compared to hybrid conversion-and-STDB training."}, "signatures": ["ICLR.cc/2020/Conference/Paper2573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xSperKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2573/Authors|ICLR.cc/2020/Conference/Paper2573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139282, "tmdate": 1576860544481, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment"}}}, {"id": "H1xRH-89sr", "original": null, "number": 4, "cdate": 1573704006066, "ddate": null, "tcdate": 1573704006066, "tmdate": 1573704006066, "tddate": null, "forum": "B1xSperKvH", "replyto": "Bye9F-4CFS", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "We thank the reviewer for their time and effort to review our work. Please find below the explanation for individual comments.\n\n1. Reviewer's comment: This paper proposes methods to initialize and train spiking NNs (SNNs) as an alternative to ANNs, not driven primarily by improved loss or generalization, but by energy efficiency improvements derived from timing-event based sparse operation rather than asynchronous sweeps. The backpropagation method introduced is sensible, as are the experiments on known datasets to show its effectiveness.\nThe paper is well written (apart from the miniscule Figure 3 containing the main result).\n\nAuthor's response:  Thank you for pointing this out. We have edited Figure 3.\n\n2. Reviewer's comment: I recommend acceptance, with caveats: the energy performance is actually not directly calculated, but speculatively estimated, it depends on the computational architecture chosen to implement the respective networks. I point out that ANNs need to be trained first to properly initialize an SNN, so the relative training effort claimed is less impressive, but energy performance does count in actual operational practice - training is (or should) be a small fraction of that.\n\nAuthor's response: We absolutely agree with the reviewer that the energy estimate is dependent on the network architecture as the number of spikes will vary with architecture. We compute the efficiency of the network in terms of latency (number of time-steps) and the number of spikes per image during inference. We have added Appendix B to compare the training and testing effort for both ANN and SNN.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xSperKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2573/Authors|ICLR.cc/2020/Conference/Paper2573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139282, "tmdate": 1576860544481, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment"}}}, {"id": "B1gEUxI5iB", "original": null, "number": 3, "cdate": 1573703756147, "ddate": null, "tcdate": 1573703756147, "tmdate": 1573703756147, "tddate": null, "forum": "B1xSperKvH", "replyto": "Hke4VJI5iS", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment", "content": {"title": "Response to Review #4 Part 2/2 ", "comment": "4. Reviewer's comment: \"The authors argue for SOTA performance in Table 2, but the comparison to other work doesn\u2019t clearly separate their performance from the other listed works; For example the accuracy gain against Lee et al.,2019 only comes from the architecture being VGG16 as opposed to VGG9, as can be seen from comparing with the VGG9 architecture from Table 1, furthermore they take the same amount of timesteps, which is supposed to be the principle gain of this work\".\n\nAuthor's response: Thank you for mentioning this. In Table 2, we compare the best results (over all architectures) reported in various works and compare it with our best results for same datasets. As correctly pointed out by the reviewer, the hybrid approach performs at par (for similar architecture) with the results reported by Lee et al. (2019). Lee et al. (2019) employed a backpropagation mechanism to train the SNN from scratch. As we show in Appendix B, the effort for training SNN from scratch is 10X more compared to hybrid training. Therefore, for most practical purposes it is not possible to train deeper SNNs from scratch. In our approach, we start with a network that performs well with large number of time-steps and train the network with spike-based BPTT to reduce the number of inference time-steps. The entire goal of training in spiking regime is to reduce the number of time-steps and therefore it is very natural to achieve similar performance if the same network is trained from scratch with same number of time-steps. The benefit we achieve with hybrid training is convergence within few epochs because we start with a good initialization and therefore this technique can scale to deeper networks and larger datasets.\n\n5. Reviewer's comment: \"Some small suggestions that are independent from the above: 1. The most similar or relevant version of equation (2) in previous work could be referenced nearby for context\". \n\nAuthor's response: We have added a reference for the iterative modeling of the LIF neuron in Section 2.1\n\n6. Reviewer's comment: \"The last sentence of the first paragraph on p.4 \u201cthe outputs from each copy...\u201d is confusing. Are you just meaning to describe BPTT?\" \n\nAuthor's response: We have edited the sentence to be more concise and clearer. Yes, we are referring to the BPTT mechanism.\n\n7. Reviewer's comment: \"Typos: sec7 4th line \u201cneruons\u201d, sec 2.2 \u201cboth the credit\u201d (remove \u201cthe\u201d)\"\n\nAuthor's response:  Thank you for pointing these out. We have made the corrections in the paper."}, "signatures": ["ICLR.cc/2020/Conference/Paper2573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xSperKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2573/Authors|ICLR.cc/2020/Conference/Paper2573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139282, "tmdate": 1576860544481, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment"}}}, {"id": "Hke4VJI5iS", "original": null, "number": 2, "cdate": 1573703467705, "ddate": null, "tcdate": 1573703467705, "tmdate": 1573703467705, "tddate": null, "forum": "B1xSperKvH", "replyto": "SklebdhWoB", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment", "content": {"title": "Response to Review #4 Part 1/2", "comment": "We thank the reviewer for the detailed comments. Please find below the explanation for individual comments.\n\n1. Reviewer's comment: \"This paper examines combining two approaches of obtaining a trained spiking neural network (SNN). The first approach of previous work is converting the weights of a trained artificial neural network (ANN) with a given architecture, to the weights and thresholds of a SNN, and the second approach uses a surrogate gradient to train an SNN with backpropagation. The true novelty of the paper seems to be in showing that combining the two approaches sequentially, trains a SNN that requires fewer timesteps to determine an output which achieves state of the art performance. This is summarized by Table 1.\"\n\nAuthor's response: The proposed hybrid technique trains an SNN that can process the input information in fewer time-steps compared to purely ANN-SNN conversion methods as well as with this technique the training effort in the spiking domain is reduced. Backpropagation training with spikes is expensive because multiple iterations are performed in the forward pass and it requires larger memory to store all the activations for backpropagation. The training effort for ANN and SNN is compared in Appendix B of the revised manuscript. The high training effort for SNN hindered its application for large and complex datasets like ImageNet. The hybrid approach makes it possible to train SNNs for large datasets with a reasonable amount of time and memory by having a good initialization and only fine-tuning in spiking domain for few epochs to reduce the number of time-steps. \n\n2. Reviewer's comment: \"However, it does not mention how many epochs it takes to train an SNN from scratch, nor compare this to the total training time (ANN training + SNN fine-tuning) of their approach\".\n\nAuthor's response: Thank you for pointing this out. We performed simulations to quantify the effort of training SNN from scratch compared to the hybrid training technique with similar hardware constraints. The results are shown in Appendix B of the revised manuscript.\n\n3. Reviewer's comment: \"They also claim a novel spike-time based surrogate gradient function (eq. 11), but it is very practically similar to the ones explored in the referenced Wu. et al 2018 (eq. 27 for instance), and these should be properly contrasted showing that this novel surrogate function is actually helpful (the performance/energy efficiency might only come from the hybrid approach)\".\n\nAuthor's response: Thank you for pointing this out. Wu et al. (2018) mentioned several approximations for the derivative of the spike function. Similar approximations have been proposed in other works as well (Bellec et al., 2018; Zenke & Ganguli 2018; Shrestha & Orchard, 2018). These approximations are either a linear or exponential function of (u-Vt) where u is the membrane potential and Vt the threshold voltage. In our work, we propose to replace the quantity (u- Vt) with \u0394t, where \u0394t is the time from the last spike. \u0394t and (u- Vt) have similar behavior, i.e., their value is small close to the spike event and the value increases as we move away from the spike event. Therefore, \u0394t works as a good replacement for (u- Vt) and it may lead to energy/computation benefits. All possible values of \u0394t is bounded and known once the number of time-steps is fixed (Equation 12), therefore we can pre-compute the gradient values and store it in a look-up table for faster access during backpropagation. On the other hand, (u- Vt) is a dynamically changing real number and the gradient must be computed during the simulation. The gradient is computed at every time-step for BPTT and the look-up table may provide faster access compared to computing the gradient based on (u- Vt). The exact benefit in energy is dependent on the overall system architecture and evaluating it is beyond the scope of this paper. We tested our hybrid technique with other approximations based on (u- Vt) and achieved similar performance. The proposed hybrid technique is a general methodology for training deep SNNs and any existing spike-based backpropagation mechanism can be used in this technique. We have added Appendix A to compare various approximations of the gradient of spike function.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2573/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "B1xSperKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2573/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2573/Authors|ICLR.cc/2020/Conference/Paper2573/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504139282, "tmdate": 1576860544481, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2573/Authors", "ICLR.cc/2020/Conference/Paper2573/Reviewers", "ICLR.cc/2020/Conference/Paper2573/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Official_Comment"}}}, {"id": "S1gQnUOTFS", "original": null, "number": 1, "cdate": 1571813034690, "ddate": null, "tcdate": 1571813034690, "tmdate": 1572972320559, "tddate": null, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper presents a fine-tuning method of models converted from standard encoding and SGD training to Spike/NN's. \nThe key point of the paper is that directly training S/NN's with spike-back-prop is slow and inefficient, while directly inferencing with converted models is also inefficient due to the large integration window required to get a good estimate of the spiking neuron potential. The authors claim, and to a good extent show that, their proposed method is best of both worlds: train the models efficiently with standard encoding / SGD, this is something we know works and scale well, then convert and fine-tune with spike-backprop to get models that perform well under a shorter integration window, and thus are more efficient at inference time. The intuition is that models can achieve shorter integration windows while keeping good results because, under the assumptions made by the proposed algorithm, the fine-tuning is effectively unrolling neuron dynamics that can be trained with BPPT, in a way similar to LSTM/Recurrent models. In that case, since model dynamics are taken into account during fine-tuning, it results in better performance even under shorter time-windows. This is an interesting concept, since the training doesn't only consider a mean-field estimate of the spike-activation, but it looks at  spiking neuron dynamics with an higher granularity. The paper is well written, clear and easy to understand. Results are comparatively competitive and code is made available."}, "signatures": ["ICLR.cc/2020/Conference/Paper2573/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2573/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575385806623, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2573/Reviewers"], "noninvitees": [], "tcdate": 1570237720915, "tmdate": 1575385806639, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Official_Review"}}}, {"id": "Bye9F-4CFS", "original": null, "number": 2, "cdate": 1571860865906, "ddate": null, "tcdate": 1571860865906, "tmdate": 1572972320512, "tddate": null, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "invitation": "ICLR.cc/2020/Conference/Paper2573/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proposes methods to initialize and train spiking NNs (SNNs) as an alternative to ANNs, not driven primarily by improved loss or generalization, but by energy efficiency improvements derived from timing-event based sparse operation rather than asynchronous sweeps. The backpropagation method introduced is sensible, as are the experiments on known datasets to show its effectiveness. The paper is well written (apart from the miniscule Figure 3 containing the main result). \nI recommend acceptance, with caveats: the energy performance is actually not directly calculated, but speculatively estimated, it depends on the computational architecture chosen to implement the respective networks. I point out that ANNs need to be trained first to properly initialize an SNN, so the relative training effort claimed is less impressive, but energy performance does count in actual operational practice - training is (or should) be a small fraction of that."}, "signatures": ["ICLR.cc/2020/Conference/Paper2573/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2573/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["rathi2@purdue.edu", "srinivg@purdue.edu", "priya.panda@yale.edu", "kaushik@purdue.edu"], "title": "Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation", "authors": ["Nitin Rathi", "Gopalakrishnan Srinivasan", "Priyadarshini Panda", "Kaushik Roy"], "pdf": "/pdf/6b27e9db132d2fe4c67bfdf18940211c2fc5ee5e.pdf", "abstract": "Spiking Neural Networks (SNNs) operate with asynchronous discrete events (or spikes) which can potentially lead to higher energy-efficiency in neuromorphic hardware implementations. Many works have shown that an SNN for inference can be formed by copying the weights from a trained Artificial Neural Network (ANN) and setting the firing threshold for each layer as the maximum input received in that layer. These type of converted SNNs require a large number of time steps to achieve competitive accuracy which diminishes the energy savings. The number of time steps can be reduced by training SNNs with spike-based backpropagation from scratch, but that is computationally expensive and slow. To address these challenges, we present a computationally-efficient training technique for deep SNNs. We propose a hybrid training methodology: 1) take a converted SNN and use its weights and thresholds as an initialization step for spike-based backpropagation, and 2) perform incremental spike-timing dependent backpropagation (STDB) on this carefully initialized network to obtain an SNN that converges within few epochs and requires fewer time steps for input processing. STDB is performed with a novel surrogate gradient function defined using neuron's spike time. The weight update is proportional to the difference in spike timing between the current time step and the most recent time step the neuron generated an output spike. The SNNs trained with our hybrid conversion-and-STDB training perform at $10{\\times}{-}25{\\times}$ fewer number of time steps and achieve similar accuracy compared to purely converted SNNs. The proposed training methodology converges in less than $20$ epochs of spike-based backpropagation for most standard image classification datasets, thereby greatly reducing the training complexity compared to training SNNs from scratch. We perform experiments on CIFAR-10, CIFAR-100 and ImageNet datasets for both VGG and ResNet architectures. We achieve top-1 accuracy of $65.19\\%$ for ImageNet dataset on SNN with $250$ time steps, which is $10{\\times}$ faster compared to converted SNNs with similar accuracy.", "code": "https://github.com/nitin-rathi/hybrid-snn-conversion", "keywords": ["spiking neural networks", "ann-snn conversion", "spike-based backpropagation", "imagenet"], "paperhash": "rathi|enabling_deep_spiking_neural_networks_with_hybrid_conversion_and_spike_timing_dependent_backpropagation", "TL;DR": "A hybrid training technique that combines ANN-SNN conversion and spike-based backpropagation to optimize training effort and inference latency.", "_bibtex": "@inproceedings{\nRathi2020Enabling,\ntitle={Enabling Deep Spiking Neural Networks with Hybrid Conversion and Spike Timing Dependent Backpropagation},\nauthor={Nitin Rathi and Gopalakrishnan Srinivasan and Priyadarshini Panda and Kaushik Roy},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=B1xSperKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/06d3301e4055a7f462decc9d4c94cb34c2432b63.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "B1xSperKvH", "replyto": "B1xSperKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2573/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575385806623, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2573/Reviewers"], "noninvitees": [], "tcdate": 1570237720915, "tmdate": 1575385806639, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2573/-/Official_Review"}}}], "count": 9}