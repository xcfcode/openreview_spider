{"notes": [{"id": "NNd0J677PN", "original": "Cuu7iYYHr3V", "number": 1332, "cdate": 1601308148744, "ddate": null, "tcdate": 1601308148744, "tmdate": 1614985727028, "tddate": null, "forum": "NNd0J677PN", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "SUsACjaTYQI", "original": null, "number": 1, "cdate": 1610040412933, "ddate": null, "tcdate": 1610040412933, "tmdate": 1610474010579, "tddate": null, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper adapts the semi-supervised DP learning methods based on voting to FL. Specifically, PATE and private-kNN. The adaptation is fairly straightforward as those methods rely on averaging of votes a primitive that is a standard part of FL. The framework assumes that unlabeled data from the same distribution is available to the server, a very strong assumption.\nAs pointed out in the reviews, the empirical evaluation has a number of major issues. For example, the comparison is with fully-supervised SGD based techniques instead of a gradient-based semi-supervised approach.\n"}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"forum": "NNd0J677PN", "replyto": "NNd0J677PN", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040412920, "tmdate": 1610474010563, "id": "ICLR.cc/2021/Conference/Paper1332/-/Decision"}}}, {"id": "-xEbLI3yp1U", "original": null, "number": 2, "cdate": 1603870742698, "ddate": null, "tcdate": 1603870742698, "tmdate": 1606745101019, "tddate": null, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review", "content": {"title": "Simple and effective ideas, some details are missing", "review": "Summary:\nThe paper proposes two approaches (i.e., PATE-FL and Private-kNN-FL) to train a differentially private global model in a federated setting based on [1] and [2]. In PATE-FL, each client first trains a teacher model using their local dataset. The teacher models are used to make noisy predictions on a public dataset. Then, the public dataset with predicted labels is used to train a final model. In Private-kNN-FL, instead of training a teacher model, the prediction depends on the labels of the nearest neighbors. The experiments show that PATE-FL and Private-kNN-FL can outperform DP-FedAvg in terms of agent-level DP and instance-level DP, respectively.\n\n[1] Scalable Private Learning with Pate   ICLR 2018\n\n[2] Private-kNN: Practical Differential Privacy for Computer Vision   CVPR 2020\n\nPros:\n\n(1) The studied problem is interesting and important.\n\n(2) The proposed approaches have quite good performance compared with DP-FedAvg.\n\nCons:\n\n(1) The work does not have sufficient algorithmic contributions. PATE-FL simply applies PATE [1] in the federated setting without the data splitting since the data are naturally partitioned. Also, Private-kNN-FL uses Private-kNN [2] in local clients to label the public dataset. The algorithms are an extension of existing algorithms in the federated setting without proposing new techniques. The theoretical analysis seems can be easily derived from the existing theorems with a modified sensitivity.\n\n(2) The proposed approach lacks some important details. In Private-kNN-FL, a data-independent feature extractor is used. However, the authors did not mention how to get the extractor. In federated learning scenarios, the training data are usually sensitive and thus it is challenging to get a good and public feature extractor for the data. If the quality of the feature extractor is bad, then the labeling may be quite bad especially when the data are non-i.i.d. distributed (which is a common case in federated learning). Also, it is not clear how to set $k$ for Private-kNN-FL. \n\n(3) The experiments are not clear and solid enough. Some experimental details are missing. Also, more experiments are needed. Please see the detailed comments. \n\nDetailed comments:\n\n(1) The experimental details are missing: the number of communication rounds for FedAvg, the value of $k$, the feature extractor, the test dataset of Digit task.\n\n(2) I\u2019d like to see the experiments given different numbers of queries/$\\varepsilon$. The authors may add a figure to show the accuracy versus $\\varepsilon$ of all approaches.\n\n(3) The experiments for agent-level DP evaluation can be further improved. It seems that each dataset is i.i.d. partitioned into different clients. The authors can try non-i.i.d. partition [3,4], which is a key feature in federated learning.\n\n[3] Communication-Efficient Learning of Deep Networks from Decentralized Data \t\tAISTATS 2017\n\n[4] Federated Learning with Matched Averaging  \tICLR 2020\n\n\n=======Post-rebuttal comments=========\n\nThanks for the authors' response. However, my main concerns on the feature extractor and the contributions are still not well addressed. Moreover,\n\n(1) The authors agree that a good feature extractor is usually hard to obtain in practice. The feature extractor will significantly limit the applicability of the proposed approach.\n\n(2) I found that the authors use ImageNet pretrained models in the experiments. This setting is weird. In my knowledge, the pretrained models are seldom used in the existing federated learning studies. The model may already be good enough before training and thus cannot well show the effectiveness of the algorithms. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121109, "tmdate": 1606915776210, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1332/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review"}}}, {"id": "vAf2IILpKUA", "original": null, "number": 3, "cdate": 1605904045538, "ddate": null, "tcdate": 1605904045538, "tmdate": 1606297060228, "tddate": null, "forum": "NNd0J677PN", "replyto": "gyqU8BSrGXM", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment", "content": {"title": "Response to Reviewer 5", "comment": "Thank you for the helpful feedback. We have updated the paper with experiments on DP-FedAvg with global public data.\n\n> **Q1.** Algorithmic novelty.\n\nPlease refer to the clarification at the beginning of the rebuttal and the remark on page 1 of the updated paper.\n \n> **Q2.** \u201cPATE-FL for agent-level and Private-kNN-FL for instance-level experiments. They two are not compared with one another.\u201d\n\nOur focus is on comparing voting-based and gradient-based methods, so we wish to avoid confusion by suggesting comparisons among PATE-FL and Private-kNN-FL. Having said that, we make the above choice as:\n  - Private-kNN-FL enjoys a stronger instance-level DP (by a factor of k) compared to PATE-FL (Theorem 3). It is straightforward to apply Private-kNN-FL for instance-level DP experiments.\n  - PATE-FL for agent-level experiments made for a clearer exposition. \n\n> **Q3.** \u201cIn Table 3, from which domain were the different agents extracted?\u201d\n \nDomainNet consists of data from six domains. For each column in Table 3 (e.g., the Clipart column), we consider Clipart domain as the server while data from the remaining five domains are assigned to five local agents.\n \n> **Q4.** \u201cAll results in Table 1 and Table 2 rely on domain adaptation approach, \u2026 the baselines FedAvg, DP-FedAvg and DP-FedSGD are not designed for this setting.\u201d\n\nTo distinguish our method with the baselines, we tried the following two ways.\n\n  - Table 1, CelebA and MNIST do not use domain adaptation, the same setting as the baselines. \u201cSVHN,MNIST->USPS\u201d uses domain adaptation. Our method yields consistent advantages, even comparable to non-DP FedAvg.\n  - We tried to apply domain adaptation to improve DP-FedAvg, so that the setting is the same as our method. (To our best, there is no existing work on DP-FedAvg to handle domain adaptation.)\n\nBefore submission, we implemented DP-FedAvg with Domain Adversarial Neural Network (DANN) [1]. Evaluation is conducted following Table 1, but leads to worse performance than DP-FedAvg (e.g., with same $\\epsilon$, DP-FedAvg  is 76.3%, DP-FedAvg+DA is 71.2% on SVHN,MNIST->USPS). Thus we did not report that in our original submission (updated in Table 1 with discussions at the bottom of page 7).  \n\n[1]  Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky.\n\n> **Q5.** Domain adaptation method used for SVHN and MNIST?  and the performance of FedAvg and DP-FedAvg.\n\nDANN is used for domain adaptation. Performance is updated in Table 1, i.e.\nFedAvg achieves 87.6% acc., FedAvg + DA is with 86.9% acc..  Under the same privacy budget, DP-FedAvg achieves acc. 76.3% while DP-FedAvg + Da achieves 71.2%.\n\n> **Q6.** \u201cExample 2 appears a bit artificial\u201d\n\nWe have updated Example 2 in paper to show that: Under the most simplified condition, we consider two gradients, $\\Delta_{1}$ larger than threshold and $\\Delta_{2}$ less than threshold. Clipping will lead to $\\Delta_{1}$ being biased and further averaging the two gradients leads to estimation bias. This simple example can be extended to more general cases, leading to the same conclusion."}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NNd0J677PN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1332/Authors|ICLR.cc/2021/Conference/Paper1332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860944, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment"}}}, {"id": "LThgqPFdIA", "original": null, "number": 2, "cdate": 1605902748579, "ddate": null, "tcdate": 1605902748579, "tmdate": 1606296847422, "tddate": null, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment", "content": {"title": "Clarification on novelty and contributions", "comment": "We thank all the reviewers for their feedback. We have updated the paper to incorporate the suggestions, with updates in red.\n \n- Major Additions:\n  - Experiment of FedAvg / DP-FedAvg with domain adaptation (Table 1). (R4 and R5)\n  - Experiment of agent-level DP with non-i.i.d partitioned data and a privacy-accuracy tradeoff figure (Table 1 and Figure 2). (R1)\n  - A remark on novelty in the introduction.\n\n- Important Minor Changes:\n  - We have updated \u201cExample 2\u201d in the paper to clarify that __clipping__ is an inherent issue in gradient based methods.\n\n### Remark of Algorithmic Novelty\n\nWhile our proposed PATE-FL and Private-kNN-FL share algorithmic traits with PATE and Private-kNN, we extend them in non-trivial ways:\n\n1) Federated Learning with Differential Privacy (DPFL) introduces new constraints and challenges:\n  - Key algorithmic components (privacy amplification by sampling,  noisy screening) in PATE or Private-kNN do not hold in DPFL. \n  - An attacker in DPFL can eavesdrop on all network traffic, while only seeing the final models in standard private learning.\n\n2) **Agent-level DP** is first proposed in PATE-FL and Private-kNN-FL as a novel notion, where PATE and Private-kNN only provide instance-level DP.\n \n3) PATE-FL and Private-kNN-FL can address **non-I.I.D** data distributions from local agents, whereas PATE assumes I.I.D distribution for each teacher model.\n\nThe table below compares our DPFL methods to prior DP works, showing that we are the first to study the key technical challenges (privacy and communication) that arise in DPFL.\n\n|  | PATE | PATE-FL | Private-kNN | Private-kNN-FL |\n|:-|:-:|:-:|:-:|:-:|\n|Screening | Yes | N/A | Yes | N/A |\n| Subsampling | N/A | N/A| yes | N/A|\n| Data I.I.D | Yes | No | Yes | No |\n| DP Notion (Add/Remove what?) | Instance | Instance / Agent | Instance | Instance / Agent |\n| Attacker observes | Pseudo-labels | All network traffic | Pseudo-labels | All network traffic |"}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NNd0J677PN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1332/Authors|ICLR.cc/2021/Conference/Paper1332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860944, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment"}}}, {"id": "hzSntzoTEw", "original": null, "number": 5, "cdate": 1605904500563, "ddate": null, "tcdate": 1605904500563, "tmdate": 1606295296598, "tddate": null, "forum": "NNd0J677PN", "replyto": "WZxDK3dur9O", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "Thank you for the comments. We have updated the paper with new experiments on DP-FedAvg with the global public data. \n\n> **Q1.** \u201cNovelty of the new algorithms.\u201d\n\nPlease refer to the clarification at the beginning of the rebuttal and the remark on page 1 of the updated paper.\n\n> **Q2.** \u201cAdd comparisons with SCAFFOLD.\u201d\n\nSCAFFOLD does not handle differential privacy. It\u2019s interesting future work to consider how SCAFFOLD may be extended to a DP setting, which we outline in an updated discussion in Sec 3.2. \n\nWe have looked closely into their technical results and we are happy to report that **there are no contradictions**.  At the high level,  the SCAFFOLD paper shows that with a smoothness assumption, using multiple local steps reduces the stochastic gradient\u2019s variance without introducing substantial bias, while we show that in a more general family of problems there are hard cases where multiple local steps (even if the variance is 0 in all of them) are essentially increasing learning rates in the outer loop, hence satisfying the same convergence lower bound for all Krylov space methods.\n\nAcknowledging SCAFFOLD with better per-round convergence performance for FL methods, we would like to emphasize:\n  - We focus on convergence of DPFL methods, not FL methods\n  - Counting iteration rounds, DP-FedAvg requires many rounds while Private-kNN-FL and PATE-FL require only one round. \n\n> **Q3.** \u201cIn Example 2, is it an inherent issue or hyperparameter tuning?\u201d\n\nIt is an issue of clipping. Clipping will result in gradient estimate bias, i.e., when sample with gradient larger than threshold is clipped, further averaging with some other gradients will result in a loss of gradient signal. We have updated Example 2 to make it clearer. Moreover, a larger clipping threshold requires a larger scale of noise injection, which also suggests clipping is an inherent issue in gradient-based methods. The choice of threshold will not be data-dependent unless the hyper-parameter tuning is done in a differentially private fashion.\n\n\n> **Q4.** \u201cglobal unlabeled dataset may not always be feasible. The proposed approach requires that the global data is similar to the private dataset.\u201d\n\nIndeed, public datasets may not always be available. However, we study the new problem of federated learning with a DP constraint and follow the standard DP settings of PATE and Private-kNN where public data is leveraged. Agnostic FL studies such as [1, 2] also assume unlabeled server data.\nWe emphasize again, as stated in the paper and shown experimentally in Tables 2 and 3, that distribution of global data at server can be **different** from that of private data at local agents.\n\n> **Q5.** \u201cOne should consider variations of federated averaging with the global public data for a fair comparison.\u201d\n\n**We aim to study how DP can enhance general FL frameworks**. To study different FL variants for our DP framework is outside the scope of this paper, but possible future work. We self-implement the baseline DP-FedAvg on top of FedAvg. Our purpose is to compare the voting based methods (ours) to the gradient based methods (DP-FedAvg). \n\n[1] Agnostic federated learning.  Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.\n\n[2] Federated adversarial domain adaptation. Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko.\n\n[3] Domain-adversarial training of neural networks. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois Laviolette, Mario Marchand, and Victor Lempitsky."}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NNd0J677PN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1332/Authors|ICLR.cc/2021/Conference/Paper1332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860944, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment"}}}, {"id": "jVTzKFY0uGv", "original": null, "number": 9, "cdate": 1605905082871, "ddate": null, "tcdate": 1605905082871, "tmdate": 1606294059316, "tddate": null, "forum": "NNd0J677PN", "replyto": "hzVZNcMXBUT", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We thank the reviewer for the feedback. We acknowledge that details of some related work are omitted or deferred to the appendix, which we will try to include within space constraints.\n \n> **Q1.** \u201cThe clipping threshold is not defined in the main text.  Example 2 needs explanation. What is $\\epsilon$ followed by a down-arrow?\u201d\n \n- Clipping: in DP-FedAvg algorithm, we enforce clipping for per-agent update $\\triangle_i$ by performing $\\triangle_i / max(1, \\|\\|\\triangle_i\\|\\|_2/ \\tau)$. We clarified it in our paper now.\n\n- Example 2: we use a simplified example with two gradients, $\\Delta_{1}$ larger than threshold and $\\Delta_{2}$ less than threshold. Clipping will lead to $\\Delta_{1}$ being biased and further averaging the two gradients leads to estimation bias. We have updated Example 2 in the paper.\n\n- Down-arrow:  it is a symbol to indicate that the lower value corresponds to the better performance for this $\\epsilon$ evaluation.\n \n \n> **Q2.** \u201cStatements like \"lower bounded by O(BG/\u221aT).\" should be $\\Omega$.\n\nThanks for pointing this out. We have fixed it in the updated paper.\n\n> **Q3.** \u201c Why the label with maximum vote is a reasonable approach. (a rating for a product and 10 people chose 1 but 9 people chose 8, 9, and 10, would I think the right rating is 1?)\u201d\n\nMajority voting of teachers is a key premise of PATE and private-kNN, which allows model-agnostic private learning and improves private-utility tradeoff. Intuitively, the voting represents the belief of each agent towards the true label of a candidate example, based on their own private data, which is a way to transfer knowledge from the distributed private data. Our results show that if the teachers are mostly unanimous, we incur a much smaller privacy loss.\n\nIn the product rating example, our algorithm will output 1 with a larger probability but will have some probability outputting other labels as well. The randomness ensures differential privacy. The pseudo-label may not be the true label, even if the teachers agree."}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NNd0J677PN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1332/Authors|ICLR.cc/2021/Conference/Paper1332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860944, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment"}}}, {"id": "Jps-2YVC7je", "original": null, "number": 7, "cdate": 1605904730788, "ddate": null, "tcdate": 1605904730788, "tmdate": 1606293327369, "tddate": null, "forum": "NNd0J677PN", "replyto": "-xEbLI3yp1U", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We thank the reviewer for the feedback. We have updated the paper with more experimental details and added two new experiments as suggested.\n \n \n> **Q1.** \u201cwork algorithmic contributions.\u201d\n \nPlease refer to the clarification at the beginning of the rebuttal and the remark on page 1 of the updated paper.\n \n \n> **Q2.** \u201cThe theoretical analysis seems can be easily derived from the existing theorems with a modified sensitivity.\u201d\n  \nWhile Theorem 3 comes from the Gaussian Mechanism with different sensitivity, our analysis of sensitivity itself is new. In fact, we gain by reducing privacy cost from original instance-level DP (PATE) by 50% in the stronger agent-level DP. Moreover, Theorem 5 is novel. To show the privacy loss is exponentially smaller when there is a high margin, our proof involves two steps:\n  1) Show the output labels match noiseless majority votes with probability exponentially close to 1 (Lemma 12 in appendix);\n  2) Bound the data-dependent RDP condition on the high probability event (Lemma 13 in appendix). More theoretical analysis can be found in appendix E.2.\n \n \n> **Q3.** \u201cThe labeling may be biased as the feature extractor can be bad when data are non-i.i.d. How to set k for private-kNN-FL?\u201d\n \nWe agree that obtaining a good feature extractor could be difficult in practice. We simply use a pretrained ImageNet model, but the iterative feature updating and labeling mechanism of Private-kNN then achieves substantially improved performance within just 3-5 iterations. \n\nOur Private-kNN-FL allows more flexible choices for k, where DP-FedAvg requires the same network architecture for each local agent. In experiments, each agent returns the top 5% of all its neighbors\u2019 predictions.\n \n\n> **Q4.** \u201cSome experimental details are missing, e.g., how to get the extractor, #communication round for FedAvg and the test dataset in digit task.\u201d\n \nWe defer some details (e.g, the feature extractor, test dataset) to the appendix due to space limitations. We have updated the paper to incorporate more experimental details. \n\nWe instantiate the data independent feature extractor using the network backbone without the classifier layer. For example, both AlexNet and Resnet50 in Table 2 are Imagenet pre-trained. When evaluating the instance-level DP using ``AlexNet\u201d backbone, we construct the feature extractor by dropping the classifier layer in Alexnet. \n \nFor the digits task test set, we split 3000 records from USPS as the unlabeled public data and the remaining records (~4000 records) are used for testing.\n\nFor communication rounds in FedAvg, we run FedAvg until the algorithm converges.  We report the pre-defined maximum number of communication rounds.\n \n| | Digit | CelebA | Office-Caltech | Domain-Net |\n|:-:|:-:|:-:|:-:|:-:|\n| # Communication round | 150   | 100    | 50 | 200  |\n \nNots that the proposed methods only require one round communication. More discussions on parameter setting for DP-FedAvg / DP-FedSGD can be found in Section F of the appendix.\n\n> **Q5.** \u201cThe authors can try non-i.i.d. partition for agent-level DP, which is a key feature in federated learning.\u201d\n \nThanks for the suggestion. We have updated the paper with a non-i.i.d partition experiment (the third row in Table 1).\n \nWe choose a similar experimental setting as in the AISTATS-17 paper (Communication-Efficient Learning of Deep Networks from Decentralized Data ) as the reviewer suggests. We divide the training set of sorted MNIST into 100 agents, such that each agent has samples from 6 digits only under the non-i.i.d. assumption. Each agent gets 600 data points from 6 classes. We split 30% of the testing set in MNIST as the available unlabeled public data and the remaining testing set used for testing. As in Table 1, our method still achieves consistently better performance than DP-FedAvg, i.e., PATE-FL (ours) achieves acc. 95.1% +- 0.3% while DP-FedAvg achieves acc. 84.2%+-0.2% with the same privacy cost 3.7.\n \n \n> **Q6.** \u201cAuthors may add a figure to show the accuracy versus $\\epsilon$ of all approaches.\u201d\n \nThanks for the suggestion. We have updated the paper with a new privacy-accuracy tradeoff figure. The experimental setting is the same as the non-i.i.d partition experiment with MNIST dataset. For each fixed privacy budget at the x-axis, we do a grid search on all hyperparameters (e.g., #queries and noise scale for PATE-FL  and  #communication round, noise scale for DP-FedAvg). In Figure 2, the accuracy of PATE-FL is consistently higher than DP-FedAvg, which implies that our method is advantageous in a wide range of $\\epsilon$."}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "NNd0J677PN", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1332/Authors|ICLR.cc/2021/Conference/Paper1332/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923860944, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Comment"}}}, {"id": "hzVZNcMXBUT", "original": null, "number": 1, "cdate": 1603315899933, "ddate": null, "tcdate": 1603315899933, "tmdate": 1605024471021, "tddate": null, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review", "content": {"title": "Review of Voting-based Approaches For Differentially Private Federated Learning", "review": "Objective of the paper:\nThe objective of the paper is to provide new federated learning algorithms based on aggregating labels from a voting scheme, instead of aggregating the gradients directly, to achieve more efficient differentially private algorithms.\n\n\nStrong Points:\n1)  It looks like the methods get strong results.\n2)  Mix of theoretical results (in supplemental materials) and practical results.  \n\nWeak Points:\n1)   The paper is not written as well as I would like.  Terms like clipping threshold are never actually defined in the main text.  Expressions like \"each data\".  Example 2 isn't well explained (why will it be clipped to 0)?  Statements like \"lower bounded by O(BG/\u221aT)\"  (if it's a lower bound, shouldn't that be Omega not O?).  Tables are confusing (what is epsilon followed by a down-arrow?).  \n2)  The paper assumes a lot of background knowledge on the recent related work.  (I suppose that can't be helped, but it does make it a hard read.)  I don't understand how the \"votes\" work, in that I don't know what are the \"labels\" being voted on, or why the label with maximum vote is a reasonable approach.   (If I asked for a rating for a product and 10 people chose 1 but 9 people chose 8, 9, and 10, would I think the right rating is 1?)  \n\nOverall Rating:   I am left with the paper being borderline;  I will tentatively place it above the threshold based on the results looking good, and see what other reviewers suggest.  \n\nQuestions for Authors:\n\nOther Feedback:  I think in the end I was not the right reviewer for this paper, and I would defer to other reviewers (and raise my score as needed).  The paper suffers perhaps from being \"too big\" for the too-low page limit offered by ICLR.  The authors have tried to cram in a lot of experiments up front (and some theory in the supplemental materials), but the paper seems written essentially for people in the area.  Not enough up front explanation;  lots of assumed knowledge on the part of the reader in my mind.  ", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121109, "tmdate": 1606915776210, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1332/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review"}}}, {"id": "WZxDK3dur9O", "original": null, "number": 3, "cdate": 1604515742845, "ddate": null, "tcdate": 1604515742845, "tmdate": 1605024470900, "tddate": null, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review", "content": {"title": "A new private federated learning\u00a0algorithm", "review": "Federated learning enables distributed clients to train a model without sharing the data with each other. This is typically achieved by a gradient descent type algorithm such as federated averaging. The paper argues that federated learning via gradient updates has issues and proposes to use a voting based method for training machine learning models using unlabeled global data.\n\nI have several concerns about the paper:\n\n1. The novelty in the new algorithms is not clear.\u00a0 PATE-FL and Private-kNN-FL are largely similar to the original PATE and Private-kNN algorithms.\n2. The paper argues that the existing gradient descent type algorithms suffer from slow convergence. However, recent works such as SCAFFOLD (https://arxiv.org/pdf/1910.06378.pdf) provide fast convergence rates for gradient based federated learning. It would be good to add comparisons with these new algorithms.\n3. In Example 2, the problem can be mitigated by setting the threshold to \\tau + \\alpha. Hence its not clear if this is an inherent issue in gradient based methods or if it is just an issue with hyperparameter tuning.\n4. Unlike standard works, the proposed algorithms assume the existence of a global unlabeled dataset, which is not always feasible. Furthermore, the approach inherently requires that the global data is similar to the private datasets, which has not been quantified. For a fair comparison in experiments, one should consider variations of federated averaging with the global public data.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121109, "tmdate": 1606915776210, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1332/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review"}}}, {"id": "gyqU8BSrGXM", "original": null, "number": 4, "cdate": 1604668120507, "ddate": null, "tcdate": 1604668120507, "tmdate": 1605024470839, "tddate": null, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "invitation": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review", "content": {"title": "Marginally above acceptance", "review": "Voting-based approaches for DP\n\n1/ Summary of the paper\n\nThis paper addresses the problem of learning a private model in an FL setting. It assumes that the server has an unlabelled dataset on which learning has to be performed. The paper proposes two extensions of known algorithms, PATE and Private-KNN in this setting. In both cases, up to variations, the approach is \"one-shot\" and amounts to getting consensus labels for the public dataset from the different FL agent, the consensus being obtained with DP guarantees. Then, a model is trained on the server using these consensus labels.\nThe paper provides R\u00e9nyi DP guarantees for both algorithms. Experiments on real datasets in 2 settings (many agents and few ones) show an improved performance over baseline DP FL algorithms.\n\n2/ Strong and weak points\n\n- The paper makes a very good distinction of the number of agents and the DP guarantees it implies in both settings, with the notion of instance-level (i.e. at the individual sample level) vs agent-level (i.e. client) DP.\n- The paper is well-motivated, and section 3 is very helpful in this regard (even if I have some remarks, see additional comments)\n- The paper is well written and easy to follow\n\n- A weakness of the paper is its limited algorithmic novelty: as far as I understand, PATE already considered N subsets of a large dataset, while here the subsets are already provided by the agents; Private-KNN-FL is more novel insofar as private-KNN only considered a single dataset, but here as well the jump from 1 to N is the only novelty.\n- Another weakness of the paper is its experimental validation, see supporting arguments.\n\n3/ Recommendation\n\nGiven the limited algorithmic novelty and the limitations of the experimental validation, I think this paper is marginally above acceptance threshold despite its interesting viewpoint and theoretical contributions.\n\n4/ Supporting arguments\n\n- All results in Table 1 and Table 2 rely on domain adaptation approach, with the final model being tested on a domain held by the server. While the proposed approaches really train a model on this domain (at least for the samples, not the labels), the baselines FedAvg, DP-FedAvg and DP-FedSGD are not designed for this setting by default, as they minimise the average losses of the clients and never see the server distribution. Out-of-domain generalisation is an issue of its own in FL, and the mix of both issues at the same time makes it impossible to really compare results with one another, rendering all these results less significant.\n- Theorem 3 provides privacy guarantees of both approaches in the agent-level and instance-level DP cases, allowing to compare the proposed approaches with one another. However, in the experimental section, PATE-FL is compared to a baseline agent-level method, and Private-KNN-FL approach is compared with instance-level methods. Private-KNN-FL and PATE-FL are never compared with one another experimentally, which would have been helpful to understand when one is better than the other (especially for the ablation experiments of Fig 2).\n\n5/ Questions and comments\n\n- Could you detail the experimental methodology to get results from table 3? In particular, from which domain were the different agents extracted from? Does each column correspond to a different training?\n- What Domain adaptation method is used for SVHn and MNIST in Table 1? What is the corresponding performance of FedAvg and DP-FedAvg when applying it to them as well?\n- In Section 3.1, example 2 appears a bit artificial to me insofar as it is more the heterogeneity of the different clients (with positive and negative x_i) which yields a null gradient than the clipping. In particular, this is a low-dimensional example, whereas in high-dimensions it is well known that the magnitude of the gradients play less of a role, see e.g. the effectiveness of SignSGD, Adam or TernaryGrad compression methods in the FL setting.\n", "rating": "6: Marginally above acceptance threshold", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "signatures": ["ICLR.cc/2021/Conference/Paper1332/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1332/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Voting-based Approaches For Differentially Private Federated Learning", "authorids": ["~Yuqing_Zhu1", "~Xiang_Yu1", "~Yi-Hsuan_Tsai1", "~Francesco_Pittaluga2", "~Masoud_Faraki2", "~Manmohan_Chandraker3", "~Yu-Xiang_Wang1"], "authors": ["Yuqing Zhu", "Xiang Yu", "Yi-Hsuan Tsai", "Francesco Pittaluga", "Masoud Faraki", "Manmohan Chandraker", "Yu-Xiang Wang"], "keywords": [], "abstract": "While federated learning (FL) enables distributed agents to collaboratively train a centralized model without sharing data with each other, it fails to protect users against inference attacks that mine private information from the centralized model. Thus, facilitating federated learning methods with differential privacy (DPFL) becomes attractive. Existing algorithms based on privately aggregating clipped gradients require many rounds of communication, which may not converge, and cannot scale up to large-capacity models due to explicit dimension-dependence in its added noise. In this paper, we adopt the knowledge transfer model of private learning pioneered by Papernot et al. (2017; 2018) and extend their algorithm PATE, as well as the recent alternative PrivateKNN (Zhu et al., 2020) to the federated learning setting. The key difference is that our method privately aggregates the labels from the agents in a voting scheme, instead of aggregating the gradients, hence avoiding the dimension dependence and achieving signi\ufb01cant savings in communication cost. Theoretically, we show that when the margins of the voting scores are large, the agents enjoy exponentially higher accuracy and stronger (data-dependent) differential privacy guarantees on both agent-level and instance-level. Extensive experiments show that our approach signi\ufb01cantly improves the privacy-utility trade-off over the current state-of-the-art in DPFL.", "one-sentence_summary": "voting-based differentially private federated learning algorithms for vision applications", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zhu|votingbased_approaches_for_differentially_private_federated_learning", "supplementary_material": "/attachment/a742ddec82d7b6a22f2e3cf8a8f71b9301f7822e.zip", "pdf": "/pdf/9a922fc74f7a996f132d34d9235f12b5eef7b655.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=Afxs1ZWKwR", "_bibtex": "@misc{\nzhu2021votingbased,\ntitle={Voting-based Approaches For Differentially Private Federated Learning},\nauthor={Yuqing Zhu and Xiang Yu and Yi-Hsuan Tsai and Francesco Pittaluga and Masoud Faraki and Manmohan Chandraker and Yu-Xiang Wang},\nyear={2021},\nurl={https://openreview.net/forum?id=NNd0J677PN}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "NNd0J677PN", "replyto": "NNd0J677PN", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1332/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538121109, "tmdate": 1606915776210, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1332/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1332/-/Official_Review"}}}], "count": 11}