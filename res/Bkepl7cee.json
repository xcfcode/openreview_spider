{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396421037, "tcdate": 1486396421037, "number": 1, "id": "SJTb2G8Oe", "invitation": "ICLR.cc/2017/conference/-/paper188/acceptance", "forum": "Bkepl7cee", "replyto": "Bkepl7cee", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The paper describes a parametric version of the exponential linear unit (ELU) activation function. The novelty of the contribution is limited, and the experimental evaluation in its current form is not convincing."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396421506, "id": "ICLR.cc/2017/conference/-/paper188/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "Bkepl7cee", "replyto": "Bkepl7cee", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396421506}}}, {"tddate": null, "tmdate": 1483365552473, "tcdate": 1483305346165, "number": 4, "id": "HJ5FbxPrx", "invitation": "ICLR.cc/2017/conference/-/paper188/official/review", "forum": "Bkepl7cee", "replyto": "Bkepl7cee", "signatures": ["ICLR.cc/2017/conference/paper188/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper188/AnonReviewer4"], "content": {"title": "Interesting and potentially very valuable - if that really works", "rating": "5: Marginally below acceptance threshold", "review": "The paper deals with a very important issue of vanishing gradients and the quest for a perfect activation function. Proposed is an approach of learning the activation functions during the training process. I find this research very interesting, but I am concerned that the paper is a bit premature.\n\nThere is a long experimental section, but I am not sure what the conclusion is. The authors appear to be somewhat confused themselves. The amount of \"maybe\" \"could mean\", \"perhaps\" etc. statements in the paper is exceptionally high. For this paper to be accepted it needs a bold statement about the performance, with a solid evidence. In my opinion, that is lacking as of now. This approach is either a breakthrough or a dud, and after reading the paper I am not convinced which case it is.\n\nThe theoretical section could be made a little clearer.\n\nFinally, how is the performance affected. The huge advantage if ReLU is in the fact that the formula is so simple and thus not costly to evaluate. How do PELU-s compare.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483305346816, "id": "ICLR.cc/2017/conference/-/paper188/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper188/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper188/AnonReviewer2", "ICLR.cc/2017/conference/paper188/AnonReviewer3", "ICLR.cc/2017/conference/paper188/AnonReviewer1", "ICLR.cc/2017/conference/paper188/AnonReviewer4"], "reply": {"forum": "Bkepl7cee", "replyto": "Bkepl7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483305346816}}}, {"tddate": null, "tmdate": 1482937517582, "tcdate": 1482937517582, "number": 3, "id": "rkUhNUWBx", "invitation": "ICLR.cc/2017/conference/-/paper188/official/review", "forum": "Bkepl7cee", "replyto": "Bkepl7cee", "signatures": ["ICLR.cc/2017/conference/paper188/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper188/AnonReviewer1"], "content": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "rating": "7: Good paper, accept", "review": "This paper presents a new non-linear function for CNN and deep neural networks. \nThe new non-linearity reports some gains on most datasets of interest, and can be used in production networks with minimal increase in computation.", "confidence": "5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483305346816, "id": "ICLR.cc/2017/conference/-/paper188/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper188/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper188/AnonReviewer2", "ICLR.cc/2017/conference/paper188/AnonReviewer3", "ICLR.cc/2017/conference/paper188/AnonReviewer1", "ICLR.cc/2017/conference/paper188/AnonReviewer4"], "reply": {"forum": "Bkepl7cee", "replyto": "Bkepl7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483305346816}}}, {"tddate": null, "tmdate": 1482340003385, "tcdate": 1482340003385, "number": 3, "id": "ryjjLE_Ng", "invitation": "ICLR.cc/2017/conference/-/paper188/public/comment", "forum": "Bkepl7cee", "replyto": "HJsj95b4l", "signatures": ["~Ludovic_Trottier1"], "readers": ["everyone"], "writers": ["~Ludovic_Trottier1"], "content": {"title": "Answer to AnonReviewer2", "comment": "First, we would like to thank you for your review and the time you have spent for doing it.\n\nAnswer to your comment:\n\nSection 5.2 Parameter Progression contains a discussion of the results shown in Figure 3. As we suggest, we believe adopting these behaviors helped Vgg disentangling redundant neurons early during training. Since peak activations scatter the inputs more than flat ones, spreading neurons at the lower layers may have allowed the network to unclutter neurons activating similarly. Although we are aware that we do not have a detailed explanation, the results shown in Figure 3 point in that direction. Note that we observed this behavior only for Vgg trained on CIFAR-10. As we wrote in our answers to your pre-review questions, this was not the case for other networks. We considered showing the parameter progression of Vgg to be more of interest for the reasons given in our discussion in section 5.2.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287693794, "id": "ICLR.cc/2017/conference/-/paper188/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkepl7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper188/reviewers", "ICLR.cc/2017/conference/paper188/areachairs"], "cdate": 1485287693794}}}, {"tddate": null, "tmdate": 1481937776107, "tcdate": 1481937776107, "number": 2, "id": "B1O_XMMEe", "invitation": "ICLR.cc/2017/conference/-/paper188/official/review", "forum": "Bkepl7cee", "replyto": "Bkepl7cee", "signatures": ["ICLR.cc/2017/conference/paper188/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper188/AnonReviewer3"], "content": {"title": "Experiment design is unconvincing", "rating": "4: Ok but not good enough - rejection", "review": "This paper proposes a modification of the ELU activation function for neural networks, by parameterizing it with 2 trainable parameters per layer. This parameter is proposed to more effectively counter vanishing gradients. \n\nMy main concern regarding this paper is related to the authors' claims about the effectiveness of PELU. The analysis in Sections 2 and 3 discusses how PELU might improve training by combating gradient propagation issues. This by itself does not imply that improved generalization will result, only that models may be easier to train. However, the experiments all seek to demonstrate improved generalization performance.\nBut this could in principle be due to a better inductive bias, and have nothing to do with the optimization analysis. None of the experiments are designed to directly support the stated theoretical advantage of PELU compared to ELU in optimizing models.\n\nIn the response to the pre-review question, the authors state that the claims in Section 2 and 3.3 are meant to apply to generalization performance. I fail to see how this is true for most claims, except the flexibility claim. As the authors agree, better training may or may not lead to better out-of-sample performance. I can only agree that having flexibility can sometimes help the network adapt its inductive bias to the problem (instead of overfitting), but this is a much weaker claim compared to the mathematical justifications for improved optimization.\n\nOn selection of learning hyperparameters:\nThe authors state in the discussion on OpenReview that the learning rates selected were favorable to ReLU, and not PELU. However, this does not guarantee that they were not unfavorable to ELU. It raises the question: can a regime be constructed where ELU has better performance than PELU? If so, how can we draw the conclusion that PELU is better?\n\nOverall, I am not yet convinced by the experimental setup and the match between theory and experiments in this paper.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483305346816, "id": "ICLR.cc/2017/conference/-/paper188/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper188/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper188/AnonReviewer2", "ICLR.cc/2017/conference/paper188/AnonReviewer3", "ICLR.cc/2017/conference/paper188/AnonReviewer1", "ICLR.cc/2017/conference/paper188/AnonReviewer4"], "reply": {"forum": "Bkepl7cee", "replyto": "Bkepl7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483305346816}}}, {"tddate": null, "tmdate": 1481906851328, "tcdate": 1481906851328, "number": 1, "id": "HJsj95b4l", "invitation": "ICLR.cc/2017/conference/-/paper188/official/review", "forum": "Bkepl7cee", "replyto": "Bkepl7cee", "signatures": ["ICLR.cc/2017/conference/paper188/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper188/AnonReviewer2"], "content": {"title": "A parameterized variant of ELU non-linearity ", "rating": "6: Marginally above acceptance threshold", "review": "Authors present a parameterized variant of ELU and show that the proposed function helps to deal with vanishing gradients in deep networks in a way better than existing non-linearities. They present both a theoretical analysis and practical validation for presented approach. \n\nInteresting observations on statistics of the PELU parameters are reported. Perhaps explanation for the observed evolution of parameters can help better understand the non-linearity. It is hard to evaluate the experimental validation presented given the difference in number of parameters compared to other approaches.\n ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1483305346816, "id": "ICLR.cc/2017/conference/-/paper188/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper188/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper188/AnonReviewer2", "ICLR.cc/2017/conference/paper188/AnonReviewer3", "ICLR.cc/2017/conference/paper188/AnonReviewer1", "ICLR.cc/2017/conference/paper188/AnonReviewer4"], "reply": {"forum": "Bkepl7cee", "replyto": "Bkepl7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1483305346816}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481211258369, "tcdate": 1478271160337, "number": 188, "id": "Bkepl7cee", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "Bkepl7cee", "signatures": ["~Ludovic_Trottier1"], "readers": ["everyone"], "content": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481211172889, "tcdate": 1481211172882, "number": 2, "id": "Skp7pxPXl", "invitation": "ICLR.cc/2017/conference/-/paper188/public/comment", "forum": "Bkepl7cee", "replyto": "SkEcNykmg", "signatures": ["~Ludovic_Trottier1"], "readers": ["everyone"], "writers": ["~Ludovic_Trottier1"], "content": {"title": "Answers related to experimental observations", "comment": "\nQuestion 1.\n\n\"From Figure 2 validation error in case of NIN and ResNet18 is significantly better when using PELU than ELU, while PELU and ELU perform similar in All-CNN and Overfeat experiments. Any possible explanation for such behavior?\"\n\nAnswer:\n\nThe main difference between both regimes is that regime #2 starts at a slower learning rate (0.01 instead of 0.1) that lasts for a longer number of epochs (19 instead of 10). Slow learning rate sometimes imped training. The parameters are not updated with large gradient steps and are caught in a small neighborhood near their initial values. The parameter optimization is more difficult in that case, since the parameter space is not exhaustively explored. This sometimes leads to reduced performance. It may be the case here that the PELU networks trained with regime #2 are not able to well adjust the forms of their activations due to too small gradient steps. We also observe (as we discuss in section 4.2) that the test error increases starting at epoch 44 for both All-CNN and Overfeat, while the training error (not shown) continues decreasing. This shows possible overfitting and suggests that the training regime may be detrimental at the later epochs for the PELU networks. \n\nAlthough we do not have a clear explanation to why this is the case, we agree with you that we could improve our discussion in the paper. Therefore, we will update the last paragraph of section 4.2 by adding a comment on the smaller learning rate of regime #2.\n\n\nQuestion 2.\n\n\"Can the authors comment on the statistics of learned bias in Vgg on cifar-10 experiment? and also PELU parameter progression from other experiments?\"\n\nAnswer:\n\nRegarding the learned biases in Vgg, we did not see any striking particularities. Note that we only looked at the learned values at the end of the training phase.\n\nWith respect to the parameter progression of section 5.2, we used Batch Normalization (BN) at different places in the network. We tried without BN, with BN before PELU and with BN after the max pooling. The saturation points of the activations converged in a similar fashion as shown in Figure 3. All layers used a saturation closed to zero, except the last one which used a saturation lower than zero. Note that the slopes converged more differently then the saturations. The goal of that experiment was to show that Vgg did not need negative saturations in all its layers for improving its performance, which may be a particularity of the network. This was not the case for the other networks. The convergence of both the saturations and the slopes were more distinct. Although we did not see any apparent particularities, we observed that the convergence were often non monotonic."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287693794, "id": "ICLR.cc/2017/conference/-/paper188/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkepl7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper188/reviewers", "ICLR.cc/2017/conference/paper188/areachairs"], "cdate": 1485287693794}}}, {"tddate": null, "tmdate": 1481211056861, "tcdate": 1481211056850, "number": 1, "id": "SkKn3xwQe", "invitation": "ICLR.cc/2017/conference/-/paper188/public/comment", "forum": "Bkepl7cee", "replyto": "HkSB0Gy7g", "signatures": ["~Ludovic_Trottier1"], "readers": ["everyone"], "writers": ["~Ludovic_Trottier1"], "content": {"title": "Answers related to experiment design", "comment": "Question 1. \n\n\"How do the experimental results support the arguments in the rest of the paper for PELU? The results are all showing (possibly) improved generalization, while the arguments are all made in favor of improved training.\"\n\nAnswer:\n\nWe would like to clarify that we always intended our arguments to refer to the generalization performance. We are aware that lower train error rates do not directly lead to lower generalization errors. Overly complex models may have low train error rates because they can capture random noises in the training data, but have large test error rates because they fail at capturing the underlying relationship (overfitting). \n\nIn that sense, our analysis of the results in sections 4 (experimentations) and 5 (discussion) always refer to the test error rates instead of train error rates. We constructed our arguments based on the observation that our approach achieved lower test error rates. We also perform several experiments on various datasets and networks for improving our confidence that our approach does indeed improve generalization performances. \n\nWith regard to the other arguments that we formulate in the rest of the paper, we also intended them to refer to the generalization performance. For instance, when we write: \u201cThis in turn improves the back-propagation weight and bias updates.\u201d (last paragraph of section 2), we imply that it would lead to better generalization performance. Similarly, when we write: \u201ca behavior that would be hurtful for training.\u201d (last paragraph of section 3.3), we imply that it would lead to worst generalization performance. Since we know about overfitting, we always look for a model that has smaller test error rates instead of smaller train error rates.\n\nAlthough we consider that our arguments are not actively misleading the reader to refer to the train error rates, we could add clarifications where it is needed. We would like to have your suggestions on which parts you consider we should update.\n\n\nQuestion 2.\n\n\"Since you compared PELU vs other activation function for networks using same learning rate schedule, how can you be sure that the improved speed or performance is not simply because the learning rate happens to favor PELU?\"\n\nAnswer:\n\nWe took this into consideration by using the learning rates (along with the step decay schedules) that favored the most ReLU. For training ResNet on the CIFAR datasets, we used the same learning rate and the step decay schedule as found in the Facebook's repository fb.resnet.torch (https://github.com/facebook/fb.resnet.torch). The learning rate starts at 0.1 and is divided by 10 at epoch 81, and by 10 again at epoch 122. Shah et al., 2016 also used this schedule for training their ResNet using ELU. Since the authors of fb.resnet.torch performed several experiments with various learning rates and step decays, we were confident that the chosen schedule was the best one for BN+ReLU, and did not favor PELU.\n\nFurthermore, we used two different training regimes for ImageNet. The first one was taken from https://gist.github.com/szagoruyko/0f5b4c5e2d2b18472854, while the second one from https://github.com/soumith/imagenet-multiGPU.torch. Both these regimes were optimized to perform best with ReLU. We also combined the two regimes with 4 different networks. ResNet18 and NiN uses the first one, while Overfeat and All-CNN uses the second. By using all these combinations, we were more confident that the reported error rate reductions were not simply because the learning rate and step decay schedules happened to favor PELU.\n\nWe however agree with you that this could be made more clear in the paper. In that sense, we will update the first paragraphs of section 4.1 and 4.2, and specify that the experiments follows fb.resnet.torch for CIFAR and imagenet-multiGPU.torch for ImageNet respectively."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287693794, "id": "ICLR.cc/2017/conference/-/paper188/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "Bkepl7cee", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper188/reviewers", "ICLR.cc/2017/conference/paper188/areachairs"], "cdate": 1485287693794}}}, {"tddate": null, "tmdate": 1480695356977, "tcdate": 1480695356972, "number": 2, "id": "HkSB0Gy7g", "invitation": "ICLR.cc/2017/conference/-/paper188/pre-review/question", "forum": "Bkepl7cee", "replyto": "Bkepl7cee", "signatures": ["ICLR.cc/2017/conference/paper188/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper188/AnonReviewer3"], "content": {"title": "Experiment design", "question": "How do the experimental results support the arguments in the rest of the paper for PELU? The results are all showing (possibly) improved generalization, while the arguments are all made in favor of improved training.\n \nSince you compared PELU vs other activation function for networks using same learning rate schedule, how can you be sure that the improved speed or performance is not simply because the learning rate happens to favor PELU?"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959417040, "id": "ICLR.cc/2017/conference/-/paper188/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper188/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper188/AnonReviewer2", "ICLR.cc/2017/conference/paper188/AnonReviewer3"], "reply": {"forum": "Bkepl7cee", "replyto": "Bkepl7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959417040}}}, {"tddate": null, "tmdate": 1480680587928, "tcdate": 1480680587923, "number": 1, "id": "SkEcNykmg", "invitation": "ICLR.cc/2017/conference/-/paper188/pre-review/question", "forum": "Bkepl7cee", "replyto": "Bkepl7cee", "signatures": ["ICLR.cc/2017/conference/paper188/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper188/AnonReviewer2"], "content": {"title": "questions on experimental observations", "question": "From Figure 2 validation error in case of NIN and ResNet18 is significantly better when using PELU than ELU, while PELU and ELU perform similar in All-CNN and Overfeat experiments. Any possible explanation for such behavior?\n\nCan the authors comment on the statistics of learned bias in Vgg on cifar-10 experiment? and also PELU parameter progression from other experiments? "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parametric Exponential Linear Unit for Deep Convolutional Neural Networks", "abstract": "The activation function is an important component in Convolutional Neural Networks (CNNs). For instance, recent breakthroughs in Deep Learning can be attributed to the Rectified Linear Unit (ReLU). Another recently proposed activation function, the Exponential Linear Unit (ELU), has the supplementary property of reducing bias shift without explicitly centering the values at zero. In this paper, we show that learning a parameterization of ELU improves its performance. We analyzed our proposed Parametric ELU (PELU) in the context of vanishing gradients and provide a gradient-based optimization framework. We conducted several experiments on CIFAR-10/100 and ImageNet with different network architectures, such as NiN, Overfeat, All-CNN and ResNet. Our results show that our PELU has relative error improvements over ELU of 4.45% and 5.68% on CIFAR-10 and 100, and as much as 7.28% with only 0.0003% parameter increase on ImageNet. We also observed that Vgg using PELU tended to prefer activations saturating closer to zero, as in ReLU, except at the last layer, which saturated near -2. Finally, other presented results suggest that varying the shape of the activations during training along with the other parameters helps controlling vanishing gradients and bias shift, thus facilitating learning.", "pdf": "/pdf/835bd08a99c77e67beae76c6419f84fdca860dad.pdf", "TL;DR": "Learning a parameterization of the ELU activation function improves its performance.", "paperhash": "trottier|parametric_exponential_linear_unit_for_deep_convolutional_neural_networks", "keywords": [], "conflicts": ["ulaval.ca", "ift.ulaval.ca"], "authors": ["Ludovic Trottier", "Philippe Gigu\u00e8re", "Brahim Chaib-draa"], "authorids": ["ludovic.trottier.1@ulaval.ca", "philippe.giguere@ift.ulaval.ca", "brahim.chaib-draa@ift.ulaval.ca"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959417040, "id": "ICLR.cc/2017/conference/-/paper188/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper188/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper188/AnonReviewer2", "ICLR.cc/2017/conference/paper188/AnonReviewer3"], "reply": {"forum": "Bkepl7cee", "replyto": "Bkepl7cee", "writers": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper188/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959417040}}}], "count": 11}