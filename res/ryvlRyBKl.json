{"notes": [{"tddate": null, "nonreaders": null, "tmdate": 1490191983678, "tcdate": 1489549964175, "number": 2, "id": "Sk4c5ELsx", "invitation": "ICLR.cc/2017/workshop/-/paper147/official/review", "forum": "ryvlRyBKl", "replyto": "ryvlRyBKl", "signatures": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "content": {"title": "Interesting research direction but no significant result yet", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes to apply adversarial attack techniques in order to \"fool\" agents already trained by deep reinforcement learning techniques. It shows that small modifications of the input image (here the screen of Atari games) may cause significant drops in the agent's performance, even in situations where the adversary does not have access to the exact model (or even learning algorithm) used by the agent.\n\nThe paper is clear and the results definitely show there are cases where input alterations invisible to the human eye can completely \"break\" an agent. Although this is important to know and motivates further research toward more robust reinforcement learning approaches, the contribution of this paper remains very limited. It is a straightforward application of existing adversarial techniques for image classification, taking advantage of the fact that a trained policy is essentially a classifier.\n\nThe key result -- that it is possible to alter an agent's policy through adversarial state modification -- is thus not surprising. The potentially more novel and interesting aspect of this research, that is not done here, would be to analyze and understand the differences observed between various learning algorithms and adversarial techniques, and use this understanding to derive more robust algorithms (or new adversarial strategies).\n\nI realize that workshop submissions are supposed to be somewhat premature work, but in my opinion in its current state it is still too premature, since there is no significant learning to be gained at this point.\n\nUpdate after discussion: I increased my score as this work actually seems more novel than I originally thought, even if it is re-using existing techniques.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489549964953, "id": "ICLR.cc/2017/workshop/-/paper147/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper147/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper147/AnonReviewer1", "ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "reply": {"forum": "ryvlRyBKl", "replyto": "ryvlRyBKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489549964953}}}, {"tddate": null, "tmdate": 1490191872725, "tcdate": 1490191872725, "number": 4, "id": "rkt-8Wl2g", "invitation": "ICLR.cc/2017/workshop/-/paper147/official/comment", "forum": "ryvlRyBKl", "replyto": "ByJw_Y13e", "signatures": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "content": {"title": "Re: Clarification of transferability across algorithms", "comment": "Thank you, this is much clearer now! I will update my review accordingly (even if it's probably too late)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663906, "tcdate": 1487367663906, "id": "ICLR.cc/2017/workshop/-/paper147/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper147/reviewers", "ICLR.cc/2017/workshop/paper147/areachairs"], "cdate": 1487367663906}}}, {"tddate": null, "tmdate": 1490159702611, "tcdate": 1490159702611, "number": 6, "id": "ByJw_Y13e", "invitation": "ICLR.cc/2017/workshop/-/paper147/public/comment", "forum": "ryvlRyBKl", "replyto": "BJ-2fO6jg", "signatures": ["~Sandy_Huang1"], "readers": ["everyone"], "writers": ["~Sandy_Huang1"], "content": {"title": "Clarification of transferability across algorithms", "comment": "All the policies we consider are trained with one of three deep RL algorithms: DQN, TRPO, and A3C. For the \"transferability across algorithms\" scenario, the target policy is trained with one of these three (e.g., DQN), and the adversary\u2019s policy is trained with one of the other two algorithms (e.g., TRPO or A3C). As a result, the two policies could end up being quite different, for instance in terms of their strategy or the high-level features they extract.\n\nYou are correct that the only difference for the \"transferability across policies\u201d scenario is the random initialization. This causes the two policies to encounter different states and actions during training, which may also lead to differences in the fully-trained policies."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663917, "tcdate": 1487367663917, "id": "ICLR.cc/2017/workshop/-/paper147/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper147/reviewers"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367663917}}}, {"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028630745, "tcdate": 1490028630745, "number": 1, "id": "r11vdtTog", "invitation": "ICLR.cc/2017/workshop/-/paper147/acceptance", "forum": "ryvlRyBKl", "replyto": "ryvlRyBKl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028631278, "id": "ICLR.cc/2017/workshop/-/paper147/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "ryvlRyBKl", "replyto": "ryvlRyBKl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028631278}}}, {"tddate": null, "tmdate": 1490023080654, "tcdate": 1490023080654, "number": 3, "id": "BJ-2fO6jg", "invitation": "ICLR.cc/2017/workshop/-/paper147/official/comment", "forum": "ryvlRyBKl", "replyto": "rJjn8vcie", "signatures": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "content": {"title": "Re: Author Clarifications", "comment": "Ok thanks, indeed it would be important to clarify. In particular, can you tell me in a few words how the deep RL algorithm used by the adversary is \"different\" from the one used by the target policy? If I read 4.2 correctly there is a situation where the only difference is the random initialization (which seems pretty minor to me), but you have a 2nd scenario (\"transferability across algorithms\") where it is not clear (to me). Thanks!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663906, "tcdate": 1487367663906, "id": "ICLR.cc/2017/workshop/-/paper147/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper147/reviewers", "ICLR.cc/2017/workshop/paper147/areachairs"], "cdate": 1487367663906}}}, {"tddate": null, "tmdate": 1489823410658, "tcdate": 1489823410658, "number": 5, "id": "rJjn8vcie", "invitation": "ICLR.cc/2017/workshop/-/paper147/public/comment", "forum": "ryvlRyBKl", "replyto": "HkzQQV5sl", "signatures": ["~Sandy_Huang1"], "readers": ["everyone"], "writers": ["~Sandy_Huang1"], "content": {"title": "Author Clarifications", "comment": "Thank you for your response - we appreciate this opportunity to clarify. We use the second approach in our work: the adversary\u2019s policy is trained separately with deep RL, without any interaction with the target policy. So, our results do show that adversarial perturbations designed to fool one policy will often also fool other policies trained for the same task, even if they were trained with a different deep RL algorithm.\n\nWe will explain this more clearly in the final version. We agree that it is valuable to describe how adversarial examples in the context of reinforcement learning differ from those in supervised learning, and will also add that to the final version."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663917, "tcdate": 1487367663917, "id": "ICLR.cc/2017/workshop/-/paper147/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper147/reviewers"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367663917}}}, {"tddate": null, "tmdate": 1489810202125, "tcdate": 1489810202125, "number": 2, "id": "HkzQQV5sl", "invitation": "ICLR.cc/2017/workshop/-/paper147/official/comment", "forum": "ryvlRyBKl", "replyto": "SkNPJKYie", "signatures": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "content": {"title": "Re: Differences between our work and previous work on black-box attacks", "comment": "Ok thanks, but in 4.2 you cite only [1], which I guess means you are using the first approach, i.e. the adversary is \"querying the targeted model on a set of algorithmically-chosen inputs\". The way I read this, the adversary doesn't care that the targeted model represents a policy in a RL setting, from its point of view it's just a classifier on images...\n\nI'm afraid it's a bit late to delve into the details (sorry about that, there was a problem with my email and I didn't get the notification that I had a paper to review until after the deadline had passed). I'm going to give an extra point just in case, because it's likely I didn't understand correctly, but if this goes through I hope you can expand a bit the paper to explain more clearly what these adversarial algorithms are doing, and how their application differs from a classification setting."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663906, "tcdate": 1487367663906, "id": "ICLR.cc/2017/workshop/-/paper147/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper147/reviewers", "ICLR.cc/2017/workshop/paper147/areachairs"], "cdate": 1487367663906}}}, {"tddate": null, "tmdate": 1489764188315, "tcdate": 1489764188315, "number": 4, "id": "SkNPJKYie", "invitation": "ICLR.cc/2017/workshop/-/paper147/public/comment", "forum": "ryvlRyBKl", "replyto": "Sk-uonusl", "signatures": ["~Sandy_Huang1"], "readers": ["everyone"], "writers": ["~Sandy_Huang1"], "content": {"title": "Differences between our work and previous work on black-box attacks", "comment": "Thank you for your response. In the supervised learning setting considered by previous work involving black-box attacks, there are two main approaches to the black-box scenario: (1) the adversary\u2019s classifier is trained by querying the targeted model on a set of algorithmically-chosen inputs [1], and (2) the adversary\u2019s classifier is trained on a dataset collected independently for the same task [2,3]. The second approach has the benefit of not requiring any interaction with the targeted model before adversarial examples are submitted, but it requires that the adversary be capable of collecting labeled data for the task of interest (which is often expensive). All of the above work has been conducted on image datasets such as MNIST and ImageNet. \n\nIn our work, the adversary must find adversarial examples that transfer across policies trained with different deep RL algorithms. It is not obvious a priori that transferability would hold given the different nature of RL applications. Indeed, when the adversary\u2019s policy is trained with a different deep RL algorithm than the target policy, the two policies encounter different sequences of states and actions during training (analogous to different datasets in supervised learning) in addition to the fact that the two policies\u2019 parameters are updated differently depending on the training algorithm. Our work shows that despite these additional challenges, transferability attacks still hold, which paves the way for black-box attacks against RL agents.\n\n[1] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, and A. Swami. Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples. ASIACCS 2017.\n[2] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. ICLR 2014.\n[3] N. Papernot, P. McDaniel, and I. Goodfellow. Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples. arXiv 2016."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663917, "tcdate": 1487367663917, "id": "ICLR.cc/2017/workshop/-/paper147/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper147/reviewers"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367663917}}}, {"tddate": null, "tmdate": 1489714024855, "tcdate": 1489714024855, "number": 1, "id": "Sk-uonusl", "invitation": "ICLR.cc/2017/workshop/-/paper147/official/comment", "forum": "ryvlRyBKl", "replyto": "ByDPbPdsg", "signatures": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "content": {"title": "Re: Author Response", "comment": "Thanks for the clarification. However I'm afraid I fail to see how this is different from an adversary being able to affect a classifier without having access to the exact classification function, which from what I understand by reading this submission, is already known to be possible. I have to admit though that I'm not an expert in the field of adversarial techniques, so I may be missing something here -- I decreased the confidence in my review score by one notch accordingly."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663906, "tcdate": 1487367663906, "id": "ICLR.cc/2017/workshop/-/paper147/official/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "noninvitees": [], "invitees": ["ICLR.cc/2017/workshop/paper147/reviewers", "ICLR.cc/2017/workshop/paper147/areachairs"], "cdate": 1487367663906}}}, {"tddate": null, "tmdate": 1489690975303, "tcdate": 1489690975303, "number": 2, "id": "ByDPbPdsg", "invitation": "ICLR.cc/2017/workshop/-/paper147/public/comment", "forum": "ryvlRyBKl", "replyto": "Sk4c5ELsx", "signatures": ["~Sandy_Huang1"], "readers": ["everyone"], "writers": ["~Sandy_Huang1"], "content": {"title": "Author Response", "comment": "Thank you for your feedback. Although policies are comparable to image classifiers (and can in fact be trained with supervised learning, for instance in behavioral cloning), the key difference is that we consider policies trained with reinforcement learning. For policies trained with RL, the initialization of a policy\u2019s parameters and the RL algorithm used to train the policy determine which states and actions are explored during training. Thus, two policies trained with different RL algorithms could end up learning different higher-level representations or even different strategies for accomplishing the same task. Based on this, we believe it is indeed surprising that adversarial examples remain effective when the adversary does not have access to the policy used by the agent -- which is likely the case in real-world situations.\n\nWe do observe that certain adversarial strategies are more transferable than others in these black-box scenarios. For instance, allowing the adversary to change just a single pixel in the input image is particularly transferable. This has implications for perturbations in the physical world, where an adversary may be able to make a few small changes to physical objects (e.g., strategically add a few dabs of paint to a stop sign) to cause a wide range of trained policies to behave incorrectly. We agree that impactful directions of future work are to develop more robust learning algorithms and discover new adversarial strategies."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663917, "tcdate": 1487367663917, "id": "ICLR.cc/2017/workshop/-/paper147/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper147/reviewers"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367663917}}}, {"tddate": null, "tmdate": 1489470633546, "tcdate": 1489470633546, "number": 1, "id": "Sy-24-Bol", "invitation": "ICLR.cc/2017/workshop/-/paper147/public/comment", "forum": "ryvlRyBKl", "replyto": "S1AXiYgie", "signatures": ["~Sandy_Huang1"], "readers": ["everyone"], "writers": ["~Sandy_Huang1"], "content": {"title": "Author Response", "comment": "Thank you for your comments and suggestions! We agree it would be interesting to evaluate adversarial examples in the context of continuous control problems; we plan to investigate this in future work and would be happy to present any preliminary results at the workshop.\n\nRegarding novelty, machine learning researchers usually must carry out two roles: the role of a scientist discovering and documenting new phenomena, and the role of an engineer leveraging knowledge of scientific phenomena to build new systems. While our submission does not contribute a new algorithm and thus has low novelty from an engineering point of view, we argue that it documents a new phenomenon (adversarial examples for RL policies) and hence has novelty from a scientific point of view.\n\nIn particular, we believe that our observations showing that adversarial examples transfer across deep RL training algorithms will be beneficial to the community. This makes adversarial-example attacks against RL agents effective even in black-box scenarios where the adversary does not know which specific algorithm was used to train the target policy (which corresponds to a realistic threat model for real-world applications)."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367663917, "tcdate": 1487367663917, "id": "ICLR.cc/2017/workshop/-/paper147/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper147/reviewers"], "reply": {"forum": "ryvlRyBKl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367663917}}}, {"tddate": null, "tmdate": 1489177381784, "tcdate": 1489177381784, "number": 1, "id": "S1AXiYgie", "invitation": "ICLR.cc/2017/workshop/-/paper147/official/review", "forum": "ryvlRyBKl", "replyto": "ryvlRyBKl", "signatures": ["ICLR.cc/2017/workshop/paper147/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper147/AnonReviewer1"], "content": {"title": "Review", "rating": "6: Marginally above acceptance threshold", "review": "# Summary\nThis paper discusses adversarial examples in the deep RL context. The paper used FGSM method to generate adversarial examples and showed that it is easy to fool an RL policy network by injecting a small noise to the input image. The paper also shows that adversarial attack is possible in a black-box scenario where the adversary does not have complete access to the neural network. Although the paper applied an existing technique (FGSM) rather than proposing a new idea/method, this is the first work that discusses adversarial attacks in RL setting to the best of my knowledge.\n\n# Novelty: this paper does not present a new method.\n# Clarity: the paper is well-written and easy to follow.\n# Significance: discussion on adversarial examples in RL will be interesting to the research community.\n# Quality: the method is demonstrated only in Atari games (visual observation with discrete action space). Demonstration on various RL domains (e.g., continuous control problems, robotics domain) could be interesting. More analysis of the behavior of the policy given adversarial examples would be interesting.\n\n# Pros\n- First work that discusses adversarial examples in RL\n- Empirical results on white-box and black-box attack scenarios are interesting.\n\n# Cons\n- No novel algorithm or method\n- Empirical study is limited to a single domain with discrete action space (not critical)", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489549964953, "id": "ICLR.cc/2017/workshop/-/paper147/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper147/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper147/AnonReviewer1", "ICLR.cc/2017/workshop/paper147/AnonReviewer2"], "reply": {"forum": "ryvlRyBKl", "replyto": "ryvlRyBKl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper147/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489549964953}}}, {"tddate": null, "replyto": null, "nonreaders": null, "ddate": null, "tmdate": 1487478802022, "tcdate": 1487367662984, "number": 147, "id": "ryvlRyBKl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "ryvlRyBKl", "signatures": ["~Sandy_Huang1"], "readers": ["everyone"], "content": {"title": "Adversarial Attacks on Neural Network Policies", "abstract": "Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of  adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a  significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial .", "pdf": "/pdf/73b9c0206710c1b49a73307edcd0024fd92ece3a.pdf", "TL;DR": "Adversarial examples can significantly degrade test-time performance of trained neural network policies, even in black-box scenarios.", "paperhash": "huang|adversarial_attacks_on_neural_network_policies", "keywords": ["Deep learning", "Reinforcement Learning"], "conflicts": ["berkeley.edu", "psu.edu", "openai.com"], "authors": ["Sandy Huang", "Nicolas Papernot", "Ian Goodfellow", "Yan Duan", "Pieter Abbeel"], "authorids": ["shhuang@cs.berkeley.edu", "ngp5056@cse.psu.edu", "ian@openai.com", "rocky@openai.com", "pieter@openai.com"]}, "writers": [], "details": {"replyCount": 12, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 13}