{"notes": [{"id": "Bylx-TNKvH", "original": "HJxFmfI8wS", "number": 362, "cdate": 1569438967854, "ddate": null, "tcdate": 1569438967854, "tmdate": 1583912028553, "tddate": null, "forum": "Bylx-TNKvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "1_TCjkGw6k", "original": null, "number": 1, "cdate": 1576798694249, "ddate": null, "tcdate": 1576798694249, "tmdate": 1576800941263, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Decision", "content": {"decision": "Accept (Poster)", "comment": "This work proves that the weights of feed-forward ReLU networks are determined, up to a specified set of symmetries, by the functions they define. Reviewers found the paper easy to read and the proof technically sound. There was some debate over the motivation for the paper, Reviewer 1 argues that there is no practical significance for the result, a point that the authors do not deny. I appreciate the concerns raised by Reviewer 1, theorists in machine learning should think carefully about the motivation for their work. However, while there is no clear practical significance of this work, I believe there is value to accepting it. Because the considered question concerns a sufficiently fundamental property of neural networks, and the proof is both easy to read and provides insights into a well studied class of models, I believe many researchers will find value in reading this paper.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795718338, "tmdate": 1576800268805, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper362/-/Decision"}}}, {"id": "SJxkVFSoKB", "original": null, "number": 1, "cdate": 1571670311511, "ddate": null, "tcdate": 1571670311511, "tmdate": 1574258693424, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_checking_correctness_of_experiments": "N/A", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #1", "review": "\nIn this paper, the authors studied the equivalence class of ReLU networks with non-increasing weights, and proved that permutation and scaling are the only function preserving weight transformations. The proof technique is novel, and provides some insights in the geometry space of the loss surface. I think the proof technique could have its general implications to some other research, however, the direct value of this paper is not very clear.\n\n1)\tThe paper starts with the discussions on the redundancy introduced by over-parametrization, as one of its motivations. However, what is discussed in this paper is actually far distant from over-parametrization. The redundancy in over-parametrized networks and the redundancy in ReLU networks are different concepts and the connection between them is not well established. The over-parametrization is talking about the smooth information flow brought by wide intermediate layers, however, the equivalence class in this paper is more about the mathematical properties of the activation functions and the topological connections of the neural networks. I feel that the authors have some confusing understanding on these two concepts.\n\n2)\tThe theory was established regarding restrictive types of ReLU networks (feedforward, with non-increasing width). However, many widely used networks are not of this kind. Without extensions to other shapes of the networks, convolutional and recurrent structures, and many kind of normalization transformations (e.g., batch norm and layer norm), the practical value of this paper is limited. \n\n3)\tSome important references are missing. The following paper has in-depth theoretical analysis on ReLU networks, and characterizes the redundancy brought by positive-scale invariant properties of ReLU networks. It is strange that the authors did not cite it.  It would be necessary for the authors to discuss their additional technical contributions given this ICLR 2019 paper.\n-  Q. Meng, et al. G-SGD: Optimizing ReLU Neural Networks in its Positively Scale-Invariant Space, ICLR 2019\n\n4)\tThe practical implication of the theories in this paper is not clearly discussed. What if we know there are only these two kinds of redundancies? What kind of new algorithms and practices can be inspired by such theoretical understandings.\n\n\n** I read the author rebuttal. Different people may have different criteria on evaluating a paper and my criterion is not only about \"what\", but more importantly about \"so what\". I still think the authors should think harder about the implications of their work, either theoretically or practically. Furthermore, I understand that some published papers also use restricted settings to ease their proofs, however, I personally do not think this is well justified. I think for top conference like ICLR, more solid and less restrictive works are preferred.  I could at most adjust my score to weak reject.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper362/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575261668263, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper362/Reviewers"], "noninvitees": [], "tcdate": 1570237753256, "tmdate": 1575261668277, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Review"}}}, {"id": "HkgHROFFsS", "original": null, "number": 5, "cdate": 1573652685055, "ddate": null, "tcdate": 1573652685055, "tmdate": 1573652685055, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "SJxkVFSoKB", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment", "content": {"title": "Clarification of \"over-parametrization\"", "comment": "Following up on your feedback, we have uploaded a revision of the paper in which we have hopefully clarified the intended meaning of \"over-parameterization\"."}, "signatures": ["ICLR.cc/2020/Conference/Paper362/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bylx-TNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper362/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper362/Authors|ICLR.cc/2020/Conference/Paper362/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172568, "tmdate": 1576860534365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment"}}}, {"id": "SylTylPHir", "original": null, "number": 4, "cdate": 1573380068533, "ddate": null, "tcdate": 1573380068533, "tmdate": 1573380520291, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "SJxkVFSoKB", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment", "content": {"title": "Response to Review #1", "comment": "Thank you for your detailed feedback.\nHowever, we believe that your assessment is based on a serious misunderstanding. We would like to rectify this, and we are glad that the OpenReview system gives us the opportunity to start a dialogue on this.\n\nFrom your comments, we have the impression that you're an expert in deep learning algorithms and optimization. However, our manuscript is not trying to make a contribution to optimization or learning algorithms, but to the study of mathematical properties and the representational power of deep networks. As we discuss in our detailed reply below, the points you raise -as valid as they are from an optimization perspective- are in no way contradictory to our submission making a valuable contribution to the field of deep network theory.\n\nIn light of this clarification, we ask you to reconsider and raise your overly critical score, or at least adjust the self-assessment of your experience. We do not doubt at all your experience in your field, but this is not the field of our submission. Otherwise, we expect your criticism to destroy the manuscript's chances of acceptance, despite the otherwise positive reviews.\n\n**********************************************\nDetailed comments:\n\n\n1) \"over-parametrization\"\n\nClearly, there are many ways in which a network can be \"over-parametrized\". When studying the optimization properties of deep networks, over-parametrization indeed typically relates to \"wider-than-necessary\" intermediate layers, as this seems to help convergence to a good solution. That's why we also mention it in the manuscript. But fundamentally, over-parametrization simply means that many parameter choices yield identical functions, and in the manuscript, we use it in this sense. We will upload a revision in which this point is hopefully clearer.\n\n\n2) \"restricted architecture\"\n\nIndeed, our results hold for feed-forward ReLU networks with non-increasing width but of arbitrary depth. This is, in fact, a quite broad class of networks for which to establish a mathematically rigorous theoretical result as we do. Proving results about deep network architectures is notoriously hard, and many other theoretical studies restrict themselves to much smaller classes of network architectures, e.g. networks with only a single hidden layer, \"linear deep networks\" that have no non-linearity between layers, or they establish results not for any fixed architecture but characterize the limit behavior, e.g. when the layer width tends to infinity.\n\nOf course, in practice, many other architectures are used, and it would indeed be great to prove similar results as ours for these. That would go far beyond the scope of an ICLR submission, though. We hope, however, that the proof techniques we introduce in our submission will carry over to some of these architectures (at least convolutional and recurrent ones), thereby laying the foundations for follow-up work.\n\n\n3) missing reference to [Meng et al, ICLR 2019]\n\nThank you for pointing us to this reference. We'll be happy to include and discuss it. Its content is completely orthogonal to our work, though: Meng and co-authors build on the indeed well-known fact that ReLU networks are positively scale-invariant and they construct a vector space that is rich enough to represent ReLU networks, yet is positively scale-invariant itself. This allows for easier/better optimization. This is indeed a very nice and elegant result. However, it does not answer or even discuss if there exist *other* symmetries than positive scale-invariance and permutation-invariance, which is the question we study and solve.\n\nAs a remark: In Section 11.2, Meng et al use the term \"over-parametrized\" in essentially the same way we do: the same function can be obtained by different choices of parameters (here: signs of skeleton weights).\n\n\n4) practical implications\n\nWe do not see immediate practical applications of our result, as it essentially is a negative one: no other symmetries exist. But we do not agree that this would in any way diminish our contribution.\n\nMathematical results should be judged on their own merit, by the insights they provide and the future work they inspire, not by their immediate practical usefulness. Our results prove a mathematical fact about the representation power of deep ReLU networks that had been a long-standing open question before. Our analysis also provides new insights into the space of functions that deep networks can or cannot represent, and our proof techniques provide new tools to the theory community for studying other properties of ReLU networks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper362/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bylx-TNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper362/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper362/Authors|ICLR.cc/2020/Conference/Paper362/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172568, "tmdate": 1576860534365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment"}}}, {"id": "BygPU3ISor", "original": null, "number": 3, "cdate": 1573379150707, "ddate": null, "tcdate": 1573379150707, "tmdate": 1573379150707, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "Bygn2z2nYr", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment", "content": {"title": "Response to Review #3", "comment": "Thank you for your feedback!\n\nFollowing your suggestion, we have updated the manuscript to include a discussion of the applicability of the proof to leaky ReLU (page 8; end of Section 6).\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper362/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bylx-TNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper362/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper362/Authors|ICLR.cc/2020/Conference/Paper362/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172568, "tmdate": 1576860534365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment"}}}, {"id": "HJxI1gIriH", "original": null, "number": 2, "cdate": 1573375966453, "ddate": null, "tcdate": 1573375966453, "tmdate": 1573375966453, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "HygiRiyRtB", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment", "content": {"title": "Response to Review #2", "comment": "Thank you for your feedback!\nWe agree that the setting where the functions are not exactly the same is very interesting and important.\nFor the reasons you mention, we keep this problem for future work for now."}, "signatures": ["ICLR.cc/2020/Conference/Paper362/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bylx-TNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper362/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper362/Authors|ICLR.cc/2020/Conference/Paper362/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172568, "tmdate": 1576860534365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment"}}}, {"id": "Bygn2z2nYr", "original": null, "number": 2, "cdate": 1571762867777, "ddate": null, "tcdate": 1571762867777, "tmdate": 1572972604960, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "8: Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper proves that, modulo permutation and scaling, ReLU networks with non-increasing widths are uniquely characterized by the function they induce (excepting some degenerate cases).  This result is not apriori obvious and is of interest.\n\nAuthors are commended for balancing brevity, intelligibility, and precision.  However, it is not clear which elements of the proof technique are inapplicable to leaky ReLUs.  It would be helpful to include a brief discussion on this.\n\nI recommend acceptance, since I didn't find any proof errors and the contribution is clear.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper362/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575261668263, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper362/Reviewers"], "noninvitees": [], "tcdate": 1570237753256, "tmdate": 1575261668277, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Review"}}}, {"id": "HygiRiyRtB", "original": null, "number": 3, "cdate": 1571843027291, "ddate": null, "tcdate": 1571843027291, "tmdate": 1572972604914, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "N/A", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper shows that for ReLU networks satisfying certain conditions, the weights and biases leading to the exact functional form are the ones obtained by neuron permutations and rescalings, and that no other reparametrizations preserving the function exist. The authors provide an explicit algorithm applying these symmetries.\n\nDisclaimer: My knowledge in this particular subfield is limit and I therefore cannot assess the novelty of the approach.\n\nI find the topic very interesting and the authors\u2019 approach reasonable. I appreciate that they clearly qualify the assumptions used. The apparent tensions between the intuition that many different reparametrizations can exist and their result suggesting that in fact only very few weight space points lead to the same function is also discussed in the conclusion, which I really appreciate.\n\nIt would be interesting to study the regime where the functions are not exactly the same. In particular, this is very relevant because we only care about answers on a discrete set of points (train set / test set), and on top of that we only care about the argmax of the logits, rather than the actual detailed answer. I believe such a result would be significantly stronger and more relevant to the practical applications of DNNs, however, I understand that it might be more difficult to obtain.\n\nOverall, I enjoyed this paper and I think it deals with an important problem.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper362/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575261668263, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper362/Reviewers"], "noninvitees": [], "tcdate": 1570237753256, "tmdate": 1575261668277, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Review"}}}, {"id": "BJlnq_gZKS", "original": null, "number": 1, "cdate": 1570994324226, "ddate": null, "tcdate": 1570994324226, "tmdate": 1570994349416, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "Bkef8RXAuS", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment", "content": {"comment": "Thank you for your comment. Yes, fold-sets appear in previous work and we do not claim this definition to be novel or original. Fold-sets do play a central role in our approach, however, and it is useful to have a short name for them. But you are completely right that the fold-set is nothing else as \"the boundaries between linear regions\" or \"the complement of the union of polyhedra\" (and that it is a hyperplane arrangement for one-layer nets).\n\nWe also thank you for linking the work (Serra et al, 2018); it is clearly relevant and we will reference it.", "title": "Reply to comment"}, "signatures": ["ICLR.cc/2020/Conference/Paper362/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bylx-TNKvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper362/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper362/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper362/Authors|ICLR.cc/2020/Conference/Paper362/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504172568, "tmdate": 1576860534365, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper362/-/Official_Comment"}}}, {"id": "Bkef8RXAuS", "original": null, "number": 1, "cdate": 1570811466021, "ddate": null, "tcdate": 1570811466021, "tmdate": 1570811466021, "tddate": null, "forum": "Bylx-TNKvH", "replyto": "Bylx-TNKvH", "invitation": "ICLR.cc/2020/Conference/Paper362/-/Public_Comment", "content": {"comment": "I like the direction followed by the authors, but I would like to point out that what you are defining as fold-sets is already known by other names in the literature. In the simpler case of a shallow network, this is a hyperplane arrangement. More generally, this is the union of polyhedra covering the input space. See, for example, Theorem 2 of Raghu et al. (2017), which is further expanded in Theorem 20 of https://arxiv.org/abs/1711.02114\n\nIn Figure 2 of the arxiv paper mentioned above, we also identify how the activation hyperplanes of subsequent layers look very different in the input space. In Figure 3 (d), we show that these boundaries for a particular neuron can in fact be disconnected.", "title": "Fold-sets, hyperplane arrangements, and union of polyhedra"}, "signatures": ["~Thiago_Serra1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Thiago_Serra1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Functional vs. parametric equivalence of ReLU networks", "authors": ["Mary Phuong", "Christoph H. Lampert"], "authorids": ["bphuong@ist.ac.at", "chl@ist.ac.at"], "keywords": ["ReLU networks", "symmetry", "functional equivalence", "over-parameterization"], "TL;DR": "We prove that there exist ReLU networks whose parameters are almost uniquely determined by the function they implement.", "abstract": "We address the following question: How redundant is the parameterisation of ReLU networks? Specifically, we consider transformations of the weight space which leave the function implemented by the network intact. Two such transformations are known for feed-forward architectures: permutation of neurons within a layer, and positive scaling of all incoming weights of a neuron coupled with inverse scaling of its outgoing weights. In this work, we show for architectures with non-increasing widths that permutation and scaling are in fact the only function-preserving weight transformations. For any eligible architecture we give an explicit construction of a neural network such that any other network that implements the same function can be obtained from the original one by the application of permutations and rescaling. The proof relies on a geometric understanding of boundaries between linear regions of ReLU networks, and we hope the developed mathematical tools are of independent interest.\n", "pdf": "/pdf/a9a5109b890165a94b4e2837b7af5e4570101a85.pdf", "paperhash": "phuong|functional_vs_parametric_equivalence_of_relu_networks", "_bibtex": "@inproceedings{\nphuong2020functional,\ntitle={Functional vs. parametric equivalence of Re{\\{}LU{\\}} networks},\nauthor={Mary Phuong and Christoph H. Lampert},\nbooktitle={International Conference on Learning Representations},\nyear={2020},\nurl={https://openreview.net/forum?id=Bylx-TNKvH}\n}", "full_presentation_video": "", "original_pdf": "/attachment/d88317d338002a3bbf0b6f09cc950188dd11e71b.pdf", "appendix": "", "poster": "", "spotlight_video": "", "slides": ""}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "Bylx-TNKvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504210167, "tmdate": 1576860568016, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper362/Authors", "ICLR.cc/2020/Conference/Paper362/Reviewers", "ICLR.cc/2020/Conference/Paper362/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper362/-/Public_Comment"}}}], "count": 11}