{"notes": [{"id": "H1leCRNYvS", "original": "B1gHArqdDr", "number": 1417, "cdate": 1569439431857, "ddate": null, "tcdate": 1569439431857, "tmdate": 1577168260332, "tddate": null, "forum": "H1leCRNYvS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "YM7d5VIXRo", "original": null, "number": 1, "cdate": 1576798722776, "ddate": null, "tcdate": 1576798722776, "tmdate": 1576800913802, "tddate": null, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Decision", "content": {"decision": "Reject", "comment": "This paper introduces a probabilistic generative model which mixes a variational autoencoder (VAE) with an energy based model (EBM). As mentioned by all reviewers (i) the motivation of the model is not well justified (ii) experimental results are not convincing enough. In addition (iii) handling sets is not specific to the proposed approach, and thus claims regarding sets should be revised.\n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795720558, "tmdate": 1576800271410, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Decision"}}}, {"id": "H8Xg7Ttb9aS", "original": null, "number": 3, "cdate": 1575782842870, "ddate": null, "tcdate": 1575782842870, "tmdate": 1575782842870, "tddate": null, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "1: Reject", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "title": "Official Blind Review #4", "review": "I don't think this paper should be accepted.  In my opinion, the mix of EBM and VAE is not really compelling;  and it is not clear at all to me that one gets much from the \"V\" in this setting.  Furthermore,  the experimental results are not great either qualitatively (by the standards of generative-models-of-images in 2019) or quantitatively (even by the standards of GAN papers).  Finally, the author's claims about sets seems tacked on, and unrelated to the rest of the paper.  Modern neural networks (attention/transformers/graph-nn, etc...) handle sets naturally, and could be used with any other conditional generative model.  To my eye, the results in e.g. figure 6 do not really seem like the model is matching the set, and the authors make no attempt to formalize or quantify how well their model generates things \"in the set\".  \n", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory."}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575785408225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Reviewers"], "noninvitees": [], "tcdate": 1570237737685, "tmdate": 1575785408240, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Review"}}}, {"id": "HyxnX7kusr", "original": null, "number": 9, "cdate": 1573544740512, "ddate": null, "tcdate": 1573544740512, "tmdate": 1573544740512, "tddate": null, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment", "content": {"title": "Draft updated ", "comment": "The draft has been updated with Figure 8 & 9 added to the Appendix. Other parts remain unchanged."}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1leCRNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1417/Authors|ICLR.cc/2020/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156323, "tmdate": 1576860543206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment"}}}, {"id": "S1euMM1_oB", "original": null, "number": 8, "cdate": 1573544463927, "ddate": null, "tcdate": 1573544463927, "tmdate": 1573544463927, "tddate": null, "forum": "H1leCRNYvS", "replyto": "SylieMJdjS", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment", "content": {"title": "Author's response to Reviewer #1 part 2/2", "comment": "4. Quality of the Set HBAE\nAs the reviewer noted, the image set reconstruction and generation task is a novel problem with no prior reference. It is also challenging from the methodology perspective as most of the reconstruction based models would fail, because there is no one-to-one mapping between the set representation and elements within the set. We thus believe that the results we achieve with Set HBAE are significant and non-trivial, as the model clearly learns meaningful information on both the two datasets evaluated in the paper. Evaluation is challenging w.r.t. generative models in general, and this is especially true for our set reconstruction/generation case. However, we have shown quantitatively that the learned representations from Set HBAE shows promising results in the semi-supervised learning case, which serves as additional evidence for the effectiveness of the model. \n\nMinors:\nFigure 1: We are adopting the plate notation [3] to explain the difference between the three models from a graphical model perspective. Here we want to highlight that a VAE assumes that all dimensions x_k of x are conditionally independent given the latent z, while an HBAE models the full interaction between all dimensions of x, with the EBM. The Set-HBAE assumes that the elements (there are M of them) within a set are conditionally independent given z, but the interaction between feature dimensions are still fully modeled. We will make this more explicit in the updated manuscript.\n\nFace alignment: Celeba comes with aligned faces, while VGGFace2 does not align them. We use the native forms of the respective datasets.\n\n\nRefs:\t\n[1] Stochastic Variational Video Prediction, Babaeizadeh et al\n[2] World Models, Ha and Schmidhuber\n[3] https://en.wikipedia.org/wiki/Plate_notation\n[4] Deep Sets, Zaheer et al\n[5] Exploiting generative models In discriminative classifiers, Jaakkola and Haussler\n[6] Improved Techniques for Training GANs, Salimans et al"}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1leCRNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1417/Authors|ICLR.cc/2020/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156323, "tmdate": 1576860543206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment"}}}, {"id": "SylieMJdjS", "original": null, "number": 7, "cdate": 1573544434875, "ddate": null, "tcdate": 1573544434875, "tmdate": 1573544434875, "tddate": null, "forum": "H1leCRNYvS", "replyto": "rJlN9wthFH", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment", "content": {"title": "Author's response to Reviewer #1 part 1/2", "comment": "We thank reviewer 1 for the comments. We are glad to see that the reviewer has found our theoretical contribution interesting. We would also like to emphasize that it is not our goal in this paper to achieve the state-of-the-art results on generative modeling of images, but rather we focus on showing a new framework that enables solving novel and challenging problems (e.g., generating self consistent sets of images), which we will further elaborate below.\n\n1. Motivation and application of HBAE\nWe apologize for not properly motivating HBAE and agree that part of the materials in the related work could be adopted to the introduction section, which we will do. Here we provide two additional points w.r.t. the potential application of HBAE. 1) Having the ability to stochastically reconstruct inputs in a generative model has important applications in video prediction [1] and model based reinforcement learning [2]. In particular, both [1] and [2] utilize VAE as the building block for their respective applications, and both can be potentially improved with HBAE which produces much better reconstructions and generations compared with a standard VAE. Although we do not directly evaluate HBAE\u2019s usefulness in the aforementioned applications, as it is outside the scope of the intended contribution, we believe such an extension is straightforward and we leave it as future work. 2) Generative modeling on sets of structured data with Set-HBAE, which fills in the gap between generative modeling of vectorized inputs and discriminative modeling of set inputs (e.g., see [4]). Given the success of applying generative modeling to discriminative learning on vectorized inputs (see, e.g., [5, 6]), it is reasonable to assume set generative models like Set-HBAE will play a similar role in set discriminative tasks like those in [4].\n\n2. Absolute visually quality of generated examples in single image HBAE\nWe agree that absolute visual quality is not as good as state-of-the-art GAN generation methods. However, we\u2019d like to emphasize two points here. First, keep in mind that the ground truth images are at a low resolution of 64x64 for all experiments and bilinear interpolation is used for display; high-resolution modeling is out of the scope of the current work due to compute issues. Second, compared with VAE counterparts, our HBAE outputs drastically better (and stochastic) reconstructions and unconditional samples, which is the main critique of VAE. The visual quality of the generated samples of HBAE is slightly worse than the GAN counterpart in the control experiment setting, but GANs do not directly offer mechanisms to produce reconstructions. There are also known ways of improving the absolute quality of HBAE\u2019s samples, including training bigger models, using larger batch sizes etc, which we leave as future work. In addition, we have tried to train an additional encoder with the trained GAN generator as decoder by minimizing l_1 loss, and showed that this gives much worse reconstruction results than both VAE and HBAE. We have updated the draft to include an Appendix to reflect the aforementioned comparisons (Figure 8&9 in the updated draft; compare to HBAE results in Figure 4 showing much crisper reconstructions and samples).\n\n3. Results on the face datasets: identity preserving and controlled variation of generation \nHBAE by nature is an unsupervised learning method; when applied to datasets consisting of human faces, it happens to be able to learn reconstructions that vary the input face in certain interesting ways. However, there is very little reason to believe that the reconstructions learned by an HBAE should preserve the identity of a human face, as the notion of identity is not fed into the model in any way, and there are numerous other sources of variation in the datasets. Similarly, we agree that it\u2019d be interesting if the HABE automatically learns a disentangled representation that provides meaningful attributes of human faces, but this is difficult to achieve without explicit supervision. We thus leave both directions as future work, where one can extend HBAE to a semi-supervised setting, achieving desired control over certain factors. These directions will be emphasized in the Discussion section of the updated manuscript."}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1leCRNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1417/Authors|ICLR.cc/2020/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156323, "tmdate": 1576860543206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment"}}}, {"id": "HklwgvFXjB", "original": null, "number": 6, "cdate": 1573258991135, "ddate": null, "tcdate": 1573258991135, "tmdate": 1573258991135, "tddate": null, "forum": "H1leCRNYvS", "replyto": "H1exIIKQsr", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment", "content": {"title": "Author's response to Reviewer #3 part 2/2", "comment": "P7: \u201cLooking at the experiments \u2026\u2026 maximum-likelihood on the data.\u201d\nThe core of the question is whether it is possible to train a reasonable EBM without an estimate or control over KL(r(x)||p(x)), with r(x) being any distribution that approximates p(x). We believe the answer is yes, given all the successful empirical applications of EBMs (Restricted Boltzmann Machines as a special case) trained with MCMC variants (Contrastive Divergence for example), also a recent contribution [6] that is extremely similar to our treatment where a generator is used in place of MCMC. In our experiments we also see that our HBAEs are able to learn to produce reasonable reconstructions. But we also argue that fully answering this question is beyond the scope of this work, because exactly the same question can be asked to the larger approximate inference community, e.g., how to tell and quantify that a MCMC chain has mixed and to what extent it affects the learning algorithm that relies on the samples produced.\n\nP8: \u201cAs shown in Figure 3 \u2026\u2026 contribution of the divergence vanishes\u201d\nWe apologize for omitting some of the implementation details, which we will update later in the draft. The generator has a tanh activation in the end, so it\u2019s outputs are in the space of [-1, 1]^d where d is the dimensionality of an image (h*w*3). Thus the entropy of the generator is upper bounded by that of the uniform distribution on the same space. Similarly the entropy of p(x|z) is also upper bounded by the same quantity. But again, both are difficult to quantify. Regarding the scaling issue of D, the scale of D is upper bounded because of the use of Spectral Normalization on every layer (including the last linear layer, which does not have non-linearity). Also another thing we have mentioned but did not state explicitly is that we use the hinge loss version of Equation (5), instead of it\u2019s raw form, similar to modern GAN implementations such as found in BigGAN [5], which further prevents the arbitrary growth of D\u2019s scale. In practice, we have found the two tricks sufficiently stabilizes the contribution of the divergence term.\n\nPou9: \u201cLastly, the Energy-function does not even \u2026\u2026 many counter-examples.\u201d\nAgain, x is bounded as stated above, in the range [-1, 1]^d. Secondly, as a minor note, Z(z)=\\int exp(-E(x|z)) dx <\\infty is not a necessary condition to ensure that the EBM is valid. Rather instead, you only need to have Z(z)=\\int exp(-E(x|z) + min_x{E(x|z)}) dx <\\infty, which is clearly bounded when x is in range [-1, 1]^d.\n\nA few other small things:\n\u201cFor z from a normal distribution \u2026\u2026 to memorize the dataset.\u201d\nThis is an interesting argument. The VAE is a regularized auto encoder, which essentially prevents extreme memorization of examples by imposing an information bottleneck regularization on the representation, in the form of KL(q(z|x)||p(z)) with p(z) being a simple prior (a prior with relatively high entropy). The example given by R3 thus will incur a high KL term loss and will likely not learn any useful information like what a VAE does. Plus, a VAE will work perfectly with N -> infinity given that all examples are drawn from the same distribution, while the counterexample will not hold.\n\n\u201cbelow (5) it is claimed that this is similar to a WGAN \u2026\u2026 inequality constraint on the jacobian\u201d\nWe agree that Equation (5) will suffer from the same difficulty of training as a WGAN, and we rely on Spectral Normalization instead, which works well in practice (see [5]).\n\n\u201cIn Fig(5) it is unclear to me how \u2026\u2026 images look reasonable.\u201d\nYes R3\u2019s understanding of Fig(5) is correct, will clarify. However, we do show samples from p(z) already in Fig(4) (right panel), with varied z\u2019, which seems to be what R3 is requesting.  \n\n\u201cMoreover, to show that the Generator \u2026\u2026 using hamilton monte-carlo\u201d\nThis a good suggestion, we will add this experiment if time permits. \n\n\u201cFinally, i have to notice that the model \u2026\u2026 is that than really an autoencoder?\u201d\nHBAE is a stochastic autoencoder, where there are multiple reconstructions instead of one. Note that this is the same as VAE, in which the \u201cdecoder\u201d is a Gaussian distribution, which is also stochastic.\n\nRefs:\n[1] Amortised MAP Inference for Image Super-resolution, S\u00f8nderby et al\n[2] Implicit Generation and Generalization in Energy-Based Models, Du and Mordatch\n[3] On Contrastive Divergence Learning, Carreira-Perpin \u0303 \u0301an and Hinton\n[4] Glow: Generative Flow with Invertible 1x1 Convolutions, Kingma and Dhariwal\n[5] Large Scale GAN Training for High Fidelity Natural Image Synthesis, Brock et al\n[6] Maximum Entropy Generators for Energy-Based Models, Kumar et al"}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1leCRNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1417/Authors|ICLR.cc/2020/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156323, "tmdate": 1576860543206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment"}}}, {"id": "H1exIIKQsr", "original": null, "number": 3, "cdate": 1573258824357, "ddate": null, "tcdate": 1573258824357, "tmdate": 1573258824357, "tddate": null, "forum": "H1leCRNYvS", "replyto": "BygOn4QjuH", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment", "content": {"title": "Author's response to Reviewer #3 part 1/2", "comment": "We thank R3 for the technical critique. We address issues raised in each paragraph (referred to as P#num) below the dashed line. For all cases we will update the draft accordingly. R3\u2019s main concern is around our approximation to the EBM; however, we\u2019d like to highlight that our main contribution is a novel generalization of VAE\u2019s decoder distribution (by using EBM as a tool), with experimental results supporting its effectiveness, including showing an application to generative modeling of sets of images for the first time.\n\nP1: \u201cThe text above \u2026\u2026 in its current form.\u201d\nWe assume that by \u201csamples from the true distribution\u201d R3 means sampling from the prior distribution p(z). We have shown qualitative results in Figure(4) right panel and in Figure 7 for single image and image set settings, respectively. Quantitative results are also included in Table 1.\n\nP2-3: \u201cLet us start from a birds-eye perspective \u2026\u2026 even though this paper does not.\u201d\nWe agree that, in theory, the support of r(x|z) needs to be large enough to make sure that the KL term makes sense and then r(x|z) can possibly approximate p(x|z). This problem is well known in the GAN literature, where tricks like instance noise [1] were proposed to address this. Another way of alleviating this problem is by constraining p(x|z) (D in GAN\u2019s context), as suggested by WGAN, which is what we adopt (by imposing Spectral normalization on every layer of D/E). Empirically, we have found this to work better than using instance noise.\n\nP4-5: \u201cThe real problem starts with eq(4) \u2026\u2026 we need a bound on log|det(d/dz' G(z,z'))|.\u201d\nDirectly approximating the entropy of r(x|z) is difficult, thus as a result we do not directly attempt to do so. And yes in theory, optimizing equation (5) in its non-parametric form will lead to extreme mode collapse of G. We first apologize for not discussing this in full detail, which we will do in the updated draft, and then provide a justification for our implementation. For equation (5), our implementation approximates the max and min procedure with a few steps of mini-batch update, w.r.t. to D and G, respectively. When updating G for one step, with D being fixed, the parameter update essentially updates the generated examples from x -> x\u2019, where x = G(z, \\psi), x\u2019 = G(z, \\psi\u2019), with \\psi\u2019 being the new parameters of G. With proper conditioning (e.g., with BN on G) and learning rate setup, one can control the update such that \\|x\u2019 - x\\| remains a small quantity, meaning that x\u2019 is a local neighborhood of x that decreases the energy. This is very much akin to what one will do for one step stochastic MCMC (see [2] for its recent application in deep EBMs), minus the noise to the gradient. As a result, a properly controlled G update approximates stochastic MCMC, which is an unbiased estimator of the unnormalized density. Approximations to MCMC have seen wide applications in the EBM literature, such as contrastive divergence [3]. Of course, MCMC itself suffers from problems of not covering enough modes of the true density (entropy lower than the true model), but it has served as a valuable tool in the bayesian inference community and has been proven useful in many applications. As a result, G will suffer from the same problem to some degree, but can still provide a reasonable approximation to the true density. A side note is that for our implementations log|det(d/dz' G(z,z\u2019))| usually doesn\u2019t exist, as z\u2019 is usually not of the same dimensionality as x.\n\nP6: \u201cThis is difficult from a theoretical view-point \u2026\u2026This is difficult to achieve with bounded determinant\u201d\nWe do not agree that our formulation of G suffers from capacity issues of learning a multimodal mapping. This is best shown in flow based models, see [4] for such an example, where z\u2019 by construction has the same dimensionality as x, the log determinant thus exists and can be bounded with proper implementations. The success of flow based models on real world dataset suggests that modern neural networks do have the capacity of constructing such a multimodal mapping from a normal distribution. The same argument can be applied to a generator of a regular GAN, where the determinant also does not exist, but it has been shown to be able to achieve the multimodal mapping from either normal or uniform distributions to real images. Our experiment results also show that our stochastic reconstructions are indeed multi-modal (figure 4, left panel)."}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "H1leCRNYvS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper1417/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper1417/Authors|ICLR.cc/2020/Conference/Paper1417/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504156323, "tmdate": 1576860543206, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Authors", "ICLR.cc/2020/Conference/Paper1417/Reviewers", "ICLR.cc/2020/Conference/Paper1417/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Comment"}}}, {"id": "BygOn4QjuH", "original": null, "number": 1, "cdate": 1570612399561, "ddate": null, "tcdate": 1570612399561, "tmdate": 1572972471530, "tddate": null, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "The article proposes to replace the conditional distribution p(x|z) in an VAE by a general graphical model parameterized by a deep neural network. Using a series of approximations it arrives at a triplet of models, where the graphical model is split into a model representing the likelihood and a model that generates samples from the distribution. the third model is the usual autoencoder. The core idea is that the latent space of the VAE serves as bottleneck so that the graphical model develops a multi-modal distribution. At the same time, the use of a neural network allows for a much better modeling than using the typical local Gaussian distribution as is used in most VAE-approaches. Experimental results show that reconstructions and markov-chains starting from reconstructions produce decent results. An extension to sets is proposed as well as a task showing performance for semi-supervised learning.\n\n-------------------------------------------\nThe text above serves mainly as a  summary of what the paper sets out to do. However, we have to check whether the paper is actually achieving this goal. In my view, it does not, and it suffers from severe problems on the theoretical side. Furthermore, important evaluations that are standard within the VAE framework are missing: samples from the true underlying distribution. In the remainder of this review, I will try to substantiate the claims regarding theory which lead me to an overall decision to reject the article in its current form.\n\nLet us start from a birds-eye perspective: the model consists of three parts:\nthe Encoder E, which i will refer to as q(z|x)\nthe Energy-function, also called E(x|z) and with -E referred to as D later on.\na generator function G(z,z') that is supposed to sample from the distribution p(x|z) = 1/Z(z) exp(-E(x|z)) using entropy generated by z'~N(0,I). I will refer to this distribution as r(x|z)\n\nThis split is smart: using a general graphical model to model p(x|z) makes it very hard to sample from the distribution and it is difficult to estimate the normalization constant Z. The paper refers to Zhai et.al (2016) to find eq(3). I really like (3) because the inequality is tight when G is producing samples from p(x|z). To be more exact, we have that the estimator for the normalization constant is log(Z) - KL(r(x|z) || p(x|z) ). This reveals the first issue: the generator must have an r(x|z) > 0 for all x except a countable infinite subset. This is difficult to achieve with dimensionality of z and z' smaller than dimensionality of x and G being deterministic. Therefore, KL(r(x|z) || p(x|z) ) is already infinite. This is a technicality that i will ignore, since we can always add a minimal amount of noise on the output of G, even though this paper does not.\n\nThe real problem starts with eq(4). Since r(x|z) is implicit, there is no way to compute its entropy, which is then just discarded. The paper argues that this is no problem, because they intend to somehow bound the entropy. We will look at this point, later. \n\nThe issue with bounding is that for this to be meaningful, we also need a bound on the entropy of p(x|z), because otherwise there is no way for r(x|z) to achieve a small KL-divergence. If we now look at (4), we can compute the optimal r without bounded entropy. The optimal r is the distribution that by (5) maximizes -E(x|z) for a given z. this result is not a multi-modal distribution but a delta-peak on the optimum and certainly not p(x|z). so the bound is super important and technically we need a bound on log|det(d/dz' G(z,z'))|. \n\nThis is difficult from a theoretical view-point as well as from a modeling view-point because we know that we have to have nearly discontinuous transformations to create sharp edges. Further, to transform z'~N(0,I), into a multi-modal distribution, there must be a function g(z')->w so that w is multi-modal. This is difficult to achieve with bounded determinant.\n\nLooking at the experiments, let's see whether the determinant is actually bounded. The paper claims that bounding can be achieved using batch-normalization. First this requires a reference on why this should be the case and show that this holds as implemented for the resnet block and second, this requires an estimate for how large the entropy can still be. if the entropy can be very large, we can not claim to train a VAE, because the error going from (3) to (4) is large. Secondly, we have to ensure that both E and G have bounded entropy, because otherwise KL(r(x|z) || p(x|z) ) will be large and there is no way to that the approximation in (3) is good, at which point E can not be claimed as doing maximum-likelihood on the data.\n\nAs shown in Figure 3, the Generator has a Conv-Block and the initial layer is a linear layer. Similarly the discriminator (or E) has a linear output layer. Therefore, the entropy is not bounded as the paper does not claim to have any entropy normalization here. The paper does not say what the activation functions are, but if the final layer of E is linear or Relu, we can add yet another problem to the list (next to the unbounded Jacobian): the VAE framework stops making sense. If we look at eq (6) or (7), the final divergence term (multiplied by beta) hinges on the fact that it provides a suitable error contribution. However, if the final layer of D is linear, we can just scale that layer arbitrarily large until the error contribution of the divergence vanishes. \n\nLastly, the Energy-function does not even necessarily model a probability function. Remember, we need that Z(z)=\\int exp(-E(x|z)) dx <\\infty. However, the definition by Figure (3) is E(x|z)=-h(x)^T(z) and the output layer of h is linear. so we can find weights such that h(x)=c \\forall x. In which case \nZ(z)=\\int exp(-c^Tz) dx = exp(-c^Tz) \\int 1 dx \nwhich is only bounded if x is bounded, which I can not see since the generator is not bounded as well. This is only one of many counter-examples.\n\n\nA few other small things:\n- For z from a normal distribution the bottleneck discussion in 2.1 last paragraph does not apply, because for a dataset with N samples it is always possible to partition the real-space of Z into N partitions with probability 1/N each. This even works with 1-dimensional normal distributions, independently of the dimensionality of x. So a multi-modal distribution for p(x|z) is really not necessary if encoder and decoder are strong enough to memorize the dataset.\n- below (5) it is claimed that this is similar to a WGAN, but WGAN have a difficult constraint to be 1-Lipschitz to make it work (which proves to be extremely difficult in practice as it translates to a similar inequality constraint on the jacobian)\n- In Fig(5) it is unclear to me how a markov-chain is defined using the model. I assume this is done by taking samples from z', generating a new x and encoding this to obtain a new z. If z' models different features from z (assuming z' is multi-modal) we would assume that the z would remain the same in each point, so no exploration of p(z) takes place. However, we can already see in the lat rows that the pictures look more strange, so some mixing is taking place but it is not clear how that relates to p(z). Therefore i would really like to see images generated by sampling z~N(0,I) and z'~N(0,I) to see whether the generated images look reasonable. \n- Moreover, to show that the Generator actually produces images that are likely under p(x|z), we would also need samples generated from the true underlying distribution p(x,z), e.g. using hamilton monte-carlo.\n- Finally, i have to notice that the model, while called hierarchival VAE, does not provide a way to compute z' from the input. is that than really an autoencoder?"}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575785408225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Reviewers"], "noninvitees": [], "tcdate": 1570237737685, "tmdate": 1575785408240, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Review"}}}, {"id": "rJlN9wthFH", "original": null, "number": 2, "cdate": 1571751820008, "ddate": null, "tcdate": 1571751820008, "tmdate": 1572972471493, "tddate": null, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "invitation": "ICLR.cc/2020/Conference/Paper1417/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "In this work a novel probabilistic generative model is introduced mixing several existing frameworks: The Hierarchical Bayes Autoencoder (HBAE) can be interpreted as a cGAN with a VAE encoder. They also incorporate a flexible learned multimodal decoder in the form of an EBM. The authors claim to produce stochastic reconstructions varying around the local data manifold of examples, and diverse unconditional samples. In addition, they present an extension of their HBAE formulation in order to model sets of inputs. This is one of the main contributions of this work since generative modeling of sets is a challenging and unsolved task. \n\nAlthough there is a clear explanation of the contributions of this paper, the motivation of this study is not precisely described. The introduction provides a good understanding of the topic; however, the authors may wish to provide several examples on the interest of the study. They provide a good description of the existing generative modeling architectures and the applications involved but not mention the importance of their contributions to the real world. \n\nThe proposed formulation seems encouraging thanks to the incorporation of multimodal decoders. The derivation of HBAE is theoretically well justified, as a result, one could replicate or further work in this paper. The authors explain in detail the procedure they took in order to arrive to the final HBAE formulation giving a clear understanding of the topic they present.\n\nBy describing the related works, they give an understandable perspective about the different drawbacks and differences of the existing methods. One could deduce the motivations of this work thanks to this section, however they should have been state clearer at the beginning. \n\nThe authors explain in detail all the different parts of the architecture, presenting precise information about the different layers. At first sight, it seems like one could reproduce the methodology used. It is clearly explained and discussed.\n\nI think the experiments conducted to evaluate this work are not enough. The authors performed several experiments in order to prove the efficiency of their methodology by showing the generated images and investigate the qualitative results. If we look at the results in figure 4, we can observe some artifacts and blur regions in the image. Moreover, the variations on the faces sometimes seem more deformations than a different human feature. It would have been interesting to evaluate the identity preservation to further investigate the quality of the images. Although the methodology, conceptually, suggested an interesting approach, the results are not so encouraging given the quality of the images achieved by other methodologies.\n\nApart from comparing quantitatively the capacity of the generated images with other related works, it would have been interesting to show that qualitatively by showing images from the mention methods. In general, there is some information missing in order to replicate the results and more details would have been appreciated. \n\nRegarding the experiments about the set of images as input, the quality of the images is similar to the simple HBAE. Since this is a more challenging approach and one could not compare with so many existing methodologies, it seems like they are going to the right direction to achieve the desired images. However, looking at the exposed results, the quality of the images is not realistic and has a lot of artifacts. Some other experiments could have been performed in order to show more interesting results.\n\nQuestions:\n\n-\tCould you better explain the meaning of figure 1? I think it was a good idea to exemplify the differences but is not clear enough.\n-\tDid you perform any face alignment when dealing with the different faces databases?\n-\tDid you think about the possibility of giving a concrete condition for varying the image?\n-\tWhich could be some concrete applications for your work?\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper1417/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper1417/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Hierarchical Bayes Autoencoders", "authors": ["Shuangfei Zhai", "Carlos Guestrin", "Joshua M. Susskind"], "authorids": ["szhai@apple.com", "guestrin@apple.com", "jsusskind@apple.com"], "keywords": [], "abstract": "Autoencoders are powerful generative models for complex data, such as images. However, standard models like the variational autoencoder (VAE) typically have unimodal Gaussian decoders, which cannot effectively represent the possible semantic variations in the space of images. To address this problem, we present a new probabilistic generative model called the \\emph{Hierarchical Bayes Autoencoder (HBAE)}. The HBAE contains a multimodal decoder in the form of an energy-based model (EBM), instead of the commonly adopted unimodal Gaussian distribution. The HBAE can be trained using variational inference, similar to a VAE, to recover latent codes conditioned on inputs. For the decoder, we use an adversarial approximation where a conditional generator is trained to match the EBM distribution. During inference time, the HBAE consists of two sampling steps: first a latent code for the input is sampled, and then this code is passed to the conditional generator to output a stochastic reconstruction. The HBAE is also capable of modeling sets, by inferring a latent code for a set of examples, and sampling set members through the multimodal decoder. In both single image and set cases, the decoder generates plausible variations consistent with the input data, and generates realistic unconditional samples. To the best our knowledge, Set-HBAE is the first model that is able to generate complex image sets.", "pdf": "/pdf/490b0c1657a43b35a2b3d8f4bdb44b0a409bb133.pdf", "paperhash": "zhai|hierarchical_bayes_autoencoders", "original_pdf": "/attachment/3c94dec01bfcd4ab1ac4b2f6444d19eb6e8fabbe.pdf", "_bibtex": "@misc{\nzhai2020hierarchical,\ntitle={Hierarchical Bayes Autoencoders},\nauthor={Shuangfei Zhai and Carlos Guestrin and Joshua M. Susskind},\nyear={2020},\nurl={https://openreview.net/forum?id=H1leCRNYvS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "H1leCRNYvS", "replyto": "H1leCRNYvS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper1417/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575785408225, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper1417/Reviewers"], "noninvitees": [], "tcdate": 1570237737685, "tmdate": 1575785408240, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper1417/-/Official_Review"}}}], "count": 10}