{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396468342, "tcdate": 1486396468342, "number": 1, "id": "H1nVhG8dx", "invitation": "ICLR.cc/2017/conference/-/paper262/acceptance", "forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "The Area Chair recommends to reject this paper given the reviewers concern about the limited significance of this work and the lack of comparisons. We encourage the authors to take into account the reviewers feedback and resubmit."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396468855, "id": "ICLR.cc/2017/conference/-/paper262/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396468855}}}, {"tddate": null, "tmdate": 1484053857275, "tcdate": 1484053857275, "number": 4, "id": "H1YwTLGUe", "invitation": "ICLR.cc/2017/conference/-/paper262/public/comment", "forum": "BJmCKBqgl", "replyto": "rkYg2xjEg", "signatures": ["~Swagath_Venkataramani1"], "readers": ["everyone"], "writers": ["~Swagath_Venkataramani1"], "content": {"title": "Response to review: Use of Xeon", "comment": "Use of Xeon: We thank the reviewer for the insightful comment. The primary goal of DyVEDeep was to reduce the average number of compute operations per input, which is platform independent. However, as the reviewer correctly points out, how the reduction in operations translates to benefits in performance depends on the implementation platform. We chose the Xeon to prototype DyVEDeep, but believe that the benefits on low-power CPUs with small numbers of cores such as Intel Atom or ARM would be quite similar. With regard to GPU implementations, the improvements are less obvious and would need experimental evaluation (we are currently pursuing this). However, as described in the response to reviewer 2 (AnonReviewer2), our approximations are coarse-grained and largely preserve regularity. Hence, we believe they would be useful on GPUs."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659574, "id": "ICLR.cc/2017/conference/-/paper262/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJmCKBqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper262/reviewers", "ICLR.cc/2017/conference/paper262/areachairs"], "cdate": 1485287659574}}}, {"tddate": null, "tmdate": 1484053572852, "tcdate": 1484053572852, "number": 3, "id": "rkTBhUzLg", "invitation": "ICLR.cc/2017/conference/-/paper262/public/comment", "forum": "BJmCKBqgl", "replyto": "BkLHl2ZEe", "signatures": ["~Swagath_Venkataramani1"], "readers": ["everyone"], "writers": ["~Swagath_Venkataramani1"], "content": {"title": "Response to review: 1.8X speedup over model-compression approach. Parallelization strategies for DyVEDeep", "comment": "We thank the reviewer for the comments. In the context of sequential implementations, we have revised the paper to include a quantitative comparison between DyVEDeep and weight compression techniques. Specifically, we applied DyVEDeep to a compressed AlexNet model available at https://github.com/songhan/Deep-Compression-AlexNet (Han et al., 2015). We achieved 1.8X improvement in performance over the model-compressed AlexNet implementation. Qualitatively, DyVEDeep is complementary to model compression techniques, as they target different opportunities. Model compression techniques prune/quantize weights of small magnitude, whereas DyVEDeep leverages other properties such as the saturating nature of neurons (SPET) and the correlation between neuron activations (SSDS and SFMA). Moreover, model compression is static whereas DyveDeep is based on dynamic techniques. Finally, the benefits of model compression are primarily seen in fully connected layers whereas the DyVEDeep primarily benefits convolutional layers. Thus DyVEDeep achieves substantial performance improvement on top of model compression approaches. \n\nIn the case of parallel implementations, the reviewer is right to point out that DyVEDeep introduces dynamism in the workload, which can have implications on efficiency. We are yet to prototype DyVEDeep on parallel systems, and are thus unable to show quantitative results on this front. However, we believe that benefits from DyVEDeep can be leveraged in the context of parallel implementations, as the approximations are introduced in a coarse-grained manner and largely preserve the regularity of computation. Consider a typical parallelization strategy, wherein each core in the system is assigned a group of neurons in a layer. In the case of SPET, since all neurons need to process the first 50% of their inputs, the workload is balanced during this phase. When the remaining inputs are evaluated for selected neurons, the workload is still roughly balanced if, on an average, equal number of neurons need to be processed on each core. However, if an imbalance exists, work (neurons) from one core needs to be dynamically migrated to another. Dynamic scheduling strategies such as work stealing, supported by many popular parallelization frameworks (e.g. TBB, Cilk etc.), could be leveraged to achieve this. Since neurons in deep networks have large numbers of inputs, the quantum of work is substantial enough to outweigh any overheads.\n\nIn the context of the SDSS technique, work is balanced in the initial phase as the features are uniformly sampled. The process of identifying which of unsampled neurons need to be evaluated also involves equal amount of work to be executed on all cores. Similar to the SPET technique, any imbalance in the process of evaluating the neurons selected can be addressed through dynamic load balancing. The final technique, SFMA, is the most coarse-grained approximation, wherein the entire 2D convolution is replaced by a scalar multiplication. The process of identifying which input features to approximate for each output feature can be uniformly partitioned across all cores. Also, since all neurons in the feature are uniformly approximated, the workload is largely uniform across all the cores.\n\nIn summary, since the approximations introduced by the proposed techniques are coarse-grained and preserve regularity, we believe the benefits from DyVEDeep can be realized in the context of parallel implementations."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659574, "id": "ICLR.cc/2017/conference/-/paper262/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJmCKBqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper262/reviewers", "ICLR.cc/2017/conference/paper262/areachairs"], "cdate": 1485287659574}}}, {"tddate": null, "tmdate": 1484053197675, "tcdate": 1484053197675, "number": 2, "id": "rkUR5IMUx", "invitation": "ICLR.cc/2017/conference/-/paper262/public/comment", "forum": "BJmCKBqgl", "replyto": "H1nMEJZ4g", "signatures": ["~Swagath_Venkataramani1"], "readers": ["everyone"], "writers": ["~Swagath_Venkataramani1"], "content": {"title": "Response to review: 1.8X speedup over Deep Compression on AlexNet", "comment": "We thank the reviewer for the comments. Please find our responses to individual questions below.\n\n1. Comparison to Deep compression: We agree with the reviewers comment that DyVEDeep targets reducing the number of computations performed, and does not reduce the model size. Based on the reviewer\u2019s suggestion, we applied the techniques proposed in DyVEDeep to the compressed AlexNet model avaliable at https://github.com/songhan/Deep-Compression-AlexNet (Han et al., 2015). We achieved 1.8X improvement in runtime for <0.5% loss in accuracy on top of the compressed AlexNet model. For quick turn-around, we used the same value of DyVEDeep hyper-parameters for this experiment that we obtained for the original AlexNet DNN. We believe that the speedup can be further improved with hyper-parameter tuning; nevertheless, this result establishes that our proposal can achieve considerable improvements over and beyond Deep Compression.\n\nWe also want to emphasize that DyVEDeep is qualitatively very different from weight compression techniques such as Deep compression. Weight compression techniques typically approximate DNNs by pruning/quantizing connections whose weight magnitudes are close to zero. On the contrary, the techniques proposed in DyVEDeep target other opportunities viz. the saturating nature of neurons, spatial correlation between neurons in a feature etc., which are not exploited by weight compression. It is also worth noting that leveraging the above opportunities requires DyVEDeep to dynamically evaluate the significance of computations. In contrast, weight compression techniques are static i.e. the same compressed model is applied to all inputs. Further, specifically in the case of Deep compression, improvement in performance is reported only for the fully connected layers, which exhibit the highest compression ratio. The authors explicitly attribute the performance improvement to the fact that the fully-connected layer weights fit in the the L3 cache of their hardware platform. Although DyVEDeep is applicable to fully-connected layers, almost all of our benefits stem from speeding up the convolutional layers of our benchmarks.\n\nIn summary, we believe DyVEDeep is complementary to Deep compression and other weight compression techniques, and can achieve benefits over and beyond them.\n\n2. Difference between SDSS and Perforated CNN: We thank the reviewer for pointing out this related work. We have added a reference to Perforated CNNs in the modified version of the paper. At a high level, the SDSS technique is related to Perforated CNNs in that it approximates the activation of a neuron as a function of its neighbours. However, the key difference is that, in SDSS, the neurons that are approximated are selected at runtime based on the magnitude and variance between activations of its neighbors. On the other hand, in Perforated CNNs, the approximated neurons are statically determined during training time. Thus, SDSS can dynamically focus approximations on regions wherein neuron activations exhibit the highest spatial correlation, potentially yielding a superior speedup vs. accuracy trade-off. It is also worth noting that the other techniques proposed in DyVEDeep viz. SPET and SFMA are completely different from Perforated CNNs.\n\n3. Baseline implementation: We used our custom implementation to realize both the baseline and DyVEDeep versions of all the benchmarks.\n\n4. Additional References: We have modified the paper to include the above references.\n\n5. Fig. 7 Y-axis: The Y-axis in Figure 7 is a normalized scale to represent the improvement in both the scalar operations and runtime. We have included this explanation in the paper. \n\n6. Typographical error:  We have corrected the typographical error in the revised version of the paper."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659574, "id": "ICLR.cc/2017/conference/-/paper262/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJmCKBqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper262/reviewers", "ICLR.cc/2017/conference/paper262/areachairs"], "cdate": 1485287659574}}}, {"tddate": null, "tmdate": 1482521976991, "tcdate": 1482521976991, "number": 2, "content": {"title": "-", "question": "-"}, "id": "SJZY6xoNg", "invitation": "ICLR.cc/2017/conference/-/paper262/pre-review/question", "forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "signatures": ["ICLR.cc/2017/conference/paper262/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper262/AnonReviewer1"], "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482521977464, "id": "ICLR.cc/2017/conference/-/paper262/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper262/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper262/AnonReviewer3", "ICLR.cc/2017/conference/paper262/AnonReviewer1"], "reply": {"forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482521977464}}}, {"tddate": null, "tmdate": 1482521584607, "tcdate": 1482521584607, "number": 3, "id": "rkYg2xjEg", "invitation": "ICLR.cc/2017/conference/-/paper262/official/review", "forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "signatures": ["ICLR.cc/2017/conference/paper262/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper262/AnonReviewer1"], "content": {"title": "Why benchmark techniques for IoT on a Xeon?", "rating": "6: Marginally above acceptance threshold", "review": "Dyvedeep presents three approximation techniques for deep vision models aimed at improving inference speed.\nThe techniques are novel as far as I know.\nThe paper is clear, the results are plausible.\n\nThe evaluation of the proposed techniques is does not make a compelling case that someone interested in faster inference would ultimately be well-served by a solution involving the proposed methods.\n\nThe authors delineate \"static\" acceleration techniques (e.g. reduced bit-width, weight pruning) from \"dynamic\" acceleration techniques which are changes to the inference algorithm itself. The delineation would be fine if the use of each family of techniques were independent of the other, but this is not the case. For example, the use of SPET would, I think, conflict with the use of factored weight matrices (I recall this from http://papers.nips.cc/paper/5025-predicting-parameters-in-deep-learning.pdf, but I suspect there may be more recent work). For this reason, a comparison between SPET and factored weight matrices would strengthen the case that SPET is a relevant innovation. In favor of the factored-matrix approach, there would I think be fewer hyperparameters and the computations would make more-efficient use of blocked linear algebra routines--the case for the superiority of SPET might be difficult to make.\n\nThe authors also do not address their choice of the Xeon for benchmarking, when the use cases they identify in the introduction include \"low power\" and \"deeply embedded\" applications. In these sorts of applications, a mobile GPU would be used, not a Xeon. A GPU implementation of a convnet works differently than a CPU implementation in ways that might reduce or eliminate the advantage of the acceleration techniques put forward in this paper.\n", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482521585135, "id": "ICLR.cc/2017/conference/-/paper262/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper262/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper262/AnonReviewer3", "ICLR.cc/2017/conference/paper262/AnonReviewer2", "ICLR.cc/2017/conference/paper262/AnonReviewer1"], "reply": {"forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482521585135}}}, {"tddate": null, "tmdate": 1481912382473, "tcdate": 1481912382473, "number": 2, "id": "BkLHl2ZEe", "invitation": "ICLR.cc/2017/conference/-/paper262/official/review", "forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "signatures": ["ICLR.cc/2017/conference/paper262/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper262/AnonReviewer2"], "content": {"title": "Interesting ideas, but I'm not sure about the significance.", "rating": "7: Good paper, accept", "review": "This work proposes a number of approximations for speeding up feed-forward network computations at inference time. Unlike much of the previous work in this area which tries to compress a large network, the authors propose algorithms that decide whether to approximate computations for each particular input example. \n\nSpeeding up inference is an important problem and this work takes a novel approach. The presentation is exceptionally clear, the diagrams are very beautiful, the ideas are interesting, and the experiments are good. This is a high-quality paper. I especially enjoyed the description of the different methods proposed  (SPET, SDSS, SFMA) to exploit patterns in the classifer. \n\nMy main concern is that the significance of this work is limited because of the additional complexity and computational costs of using these approximations. In the experiments, the DyVEDeep approach was compared to serial implementations of four large classification models --- inference in these models is order of magnitudes faster on systems that support parallelization. I assume that DyVEDeep has little-to-no performance advantage on a system that allows parallelization, and so anyone looking to speed up their inference on a serial system would want to see a comparison between this approach and the model-compression approaches. Thus, I am not sure how much of an impact this approach can have in it's current state.\n\nSuggestions:\n-I wondered what (if any) bounds could be made on the approximation errors of the proposed methods?", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482521585135, "id": "ICLR.cc/2017/conference/-/paper262/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper262/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper262/AnonReviewer3", "ICLR.cc/2017/conference/paper262/AnonReviewer2", "ICLR.cc/2017/conference/paper262/AnonReviewer1"], "reply": {"forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482521585135}}}, {"tddate": null, "tmdate": 1481860115974, "tcdate": 1481860115974, "number": 1, "id": "H1nMEJZ4g", "invitation": "ICLR.cc/2017/conference/-/paper262/official/review", "forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "signatures": ["ICLR.cc/2017/conference/paper262/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper262/AnonReviewer3"], "content": {"title": "Interesting and clearly written paper. My main concerns about this paper, are about the novelty, and the advantages of the proposed techniques over related papers in the area.", "rating": "6: Marginally above acceptance threshold", "review": "The authors describe a series of techniques which can be used to reduce the total amount of computation that needs to be performed in Deep Neural Networks. The authors propose to selectively identify how important a certain set of computations is to the final DNN output, and to use this information to selectively skip certain computations in the network. As deep learning technologies become increasingly widespread on mobile devices, techniques which enable efficient inference on such devices are becoming increasingly important for practical applications. \n\nThe paper is generally well-written and clear to follow. I had two main comments that concern the experimental design, and the relationship to previous work:\n\n1. In the context of deployment on mobile devices, computational costs in terms of both system memory as well as processing are important consideration. While the proposed techniques do improve computational costs, they don\u2019t reduce model size in terms of total number of parameters. Also, the gains obtained using the proposed method appear to be similar to other works that do allow for improvements in terms of both memory and computation (see, e.g., (Han et al., 2015)). It would have been interesting if the authors had reported results when the proposed techniques were applied to models that have been compressed in size as well.\n\nS. Han, H. Mao, and W. J. Dally. \"Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding.\" arXiv prepring arXiv:1510.00149 (2015).\n\n2. The SDSS technique in the paper appears to be very similar to the \u201cPerforated CNN\u201d technique proposed by Figurnov et al. (2015). In that work, as in the authors work, CNN activations are approximated by interpolating responses from neighbors. The authors should comment on the similarity and differences between the proposed method and the referenced work.\n\nFigurnov, Michael, Dmitry Vetrov, and Pushmeet Kohli. \"Perforatedcnns: Acceleration through elimination of redundant convolutions.\" arXiv preprint  arXiv:1504.08362 (2015).\n\nOther minor comments appear below:\n\n3. A clarification question: In comparing the proposed methods to the baseline, in Section 4, the authors mention that they used their own custom implementation. However, do the baselines use the same custom implementation, or do they used the optimized BLAS libraries?\n\n4. The authors should also consider citing the following additional references:\n * S. Tan and K. C. Sim, \"Towards implicit complexity control using variable-depth deep neural networks for automatic speech recognition,\" 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, 2016, pp. 5965-5969.\n * Graves, Alex. \"Adaptive Computation Time for Recurrent Neural Networks.\" arXiv preprint arXiv:1603.08983 (2016).\n\n5. Please explain what the Y-axis in Figure 7 represents in the text.\n\n6. Typographical Error: Last paragraph of Section 2: \u201c... are qualitatively different the aforementioned ...\u201d \u2192 \u201c... are qualitatively different from the aforementioned ...\u201d", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482521585135, "id": "ICLR.cc/2017/conference/-/paper262/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper262/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper262/AnonReviewer3", "ICLR.cc/2017/conference/paper262/AnonReviewer2", "ICLR.cc/2017/conference/paper262/AnonReviewer1"], "reply": {"forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482521585135}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1481760069200, "tcdate": 1478281675544, "number": 262, "id": "BJmCKBqgl", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "BJmCKBqgl", "signatures": ["~Swagath_Venkataramani1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "tmdate": 1481759969159, "tcdate": 1481759969124, "number": 1, "id": "B1Y16I1Eg", "invitation": "ICLR.cc/2017/conference/-/paper262/public/comment", "forum": "BJmCKBqgl", "replyto": "HJi-_S1mx", "signatures": ["~Swagath_Venkataramani1"], "readers": ["everyone"], "writers": ["~Swagath_Venkataramani1"], "content": {"title": "Response to question - Clarification about hyperparameter tuning", "comment": "Inputs used for hyperparameter tuning: \nFor the experiments presented in Sections 4/5, we randomly selected 5% of the test inputs and used it as a validation set to tune the hyper parameters. We report speedup and classification accuracy results on the remaining 95% of the test inputs. We have updated Section 4 of the paper clarifying this point.\n\nSensitivity to choice of hyperparameters:\nPlease refer to Figure in the following link: https://www.dropbox.com/s/b9tgo0z318lvtzk/paramSensitivity.png?dl=0\n\nTo quantitatively illustrate the impact of hyperparameter value on performance and accuracy, the figure in the link above shows the accuracy loss and speedup achieved as the MaxAct-thresh parameter used in the Significance-driven Selective Subsampling (SDSS) technique is varied for the AlexNet benchmark . As the MaxAct-thresh parameter is increased, we see a proportional reduction in runtime, as fewer neurons are dynamically selected for re-evaluation. The benefits saturate beyond a point. On the other hand, the accuracy loss is quite negligible initially (<1%), but begins to increase for values greater than 10. Therefore, the key is to identify the knee of this graph, wherein we achieve the most benefits with tolerable loss in classification accuracy. Our hyper parameter tuning algorithm performs a binary search on the range of hyperparameter values to identify this point. \n\nThe behavior observed in this example is quite typical; similar results were observed for all other hyperparameters.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287659574, "id": "ICLR.cc/2017/conference/-/paper262/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "BJmCKBqgl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper262/reviewers", "ICLR.cc/2017/conference/paper262/areachairs"], "cdate": 1485287659574}}}, {"tddate": null, "tmdate": 1480706050712, "tcdate": 1480706050708, "number": 1, "id": "HJi-_S1mx", "invitation": "ICLR.cc/2017/conference/-/paper262/pre-review/question", "forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "signatures": ["ICLR.cc/2017/conference/paper262/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper262/AnonReviewer3"], "content": {"title": "Clarification about hyperparameter tuning", "question": "I would like to clarify something about the experiments reported in Section 4/5. On each of the test sets, do you tune the hyperparameters on a validation set, and then report performance improvements on a separate held-out set? Or do you tune and report results on the same set? Doing the latter might suggest stronger performance improvements/accuracies then might be obtained on held-out data.\n\nAlso, how sensitive is the performance improvement to the choice of these hyperparameters?\n\nThanks in advance."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "DyVEDeep: Dynamic Variable Effort Deep Neural Networks", "abstract": "Deep Neural Networks (DNNs) have advanced the state-of-the-art on a variety of machine learning tasks and are deployed widely in many real-world products. However, the compute and data requirements demanded by large-scale DNNs remains a significant challenge. In this work, we address this challenge in the context of DNN inference. We propose Dynamic Variable Effort Deep Neural Networks (DyVEDeep), which exploit the heterogeneity in the characteristics of inputs to DNNs to improve their compute efficiency while maintaining the same classification accuracy. DyVEDeep equips DNNs with dynamic effort knobs, which in course of processing an input, identify how critical a group of computations are to classify the input. DyVEDeep dynamically focuses its compute effort only on the critical computations, while the skipping/approximating the rest. We propose 3 effort knobs that operate at different levels of granularity viz. neuron, feature and layer levels. We build DyVEDeep versions for 5 popular image recognition benchmarks on 3 image datasets---MNIST, CIFAR and ImageNet. Across all benchmarks, DyVEDeep achieves 2.1X-2.6X reduction in number of scalar operations, which translates to 1.9X-2.3X performance improvement over a Caffe-based sequential software implementation, for negligible loss in accuracy.", "pdf": "https://www.dropbox.com/s/5ynhm1wy5vu2swq/DyVEDeep-ICLR2017.pdf", "paperhash": "ganapathy|dyvedeep_dynamic_variable_effort_deep_neural_networks", "conflicts": ["purdue.edu", "iitm.ac.in"], "keywords": [], "authors": ["Sanjay Ganapathy", "Swagath Venkataramani", "Balaraman Ravindran", "Anand Raghunathan"], "authorids": ["sanjaygana@gmail.com", "venkata0@purdue.edu", "ravi@cse.iitm.ac.in", "raghunathan@purdue.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1482521977464, "id": "ICLR.cc/2017/conference/-/paper262/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper262/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper262/AnonReviewer3", "ICLR.cc/2017/conference/paper262/AnonReviewer1"], "reply": {"forum": "BJmCKBqgl", "replyto": "BJmCKBqgl", "writers": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper262/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1482521977464}}}], "count": 11}