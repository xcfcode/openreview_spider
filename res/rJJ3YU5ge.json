{"notes": [{"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396495790, "tcdate": 1486396495790, "number": 1, "id": "ryOIhGUul", "invitation": "ICLR.cc/2017/conference/-/paper308/acceptance", "forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Reject", "title": "ICLR committee final decision", "comment": "Three knowledgable reviewers recommend rejection. While the application is interesting and of commercial value, the technical contribution falls below the ICLR's bar. I encourage the authors to improve the paper and submit it to a future conference."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396496375, "id": "ICLR.cc/2017/conference/-/paper308/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396496375}}}, {"tddate": null, "tmdate": 1482342272027, "tcdate": 1482342272027, "number": 4, "id": "BJOFJS_4x", "invitation": "ICLR.cc/2017/conference/-/paper308/public/comment", "forum": "rJJ3YU5ge", "replyto": "r1slCpBVx", "signatures": ["~Tom_Zahavy1"], "readers": ["everyone"], "writers": ["~Tom_Zahavy1"], "content": {"title": "Reply to AnonReviewer3 & AnonReviewer2", "comment": "We are happy that the reviewers find our data and application interesting (AnonReviewer2), that the writing is clear and that there are a lot of useful practical experiences(AnonReviewer3). One of the challenges of ML research is to constantly improve classification results in real world domains. In this work we explored the benefits of multimodality on large scale classification problem, using deep neural networks. The reviewer raised good points about the cons of our work and had a few questions. Please find our answers below. \n\n\"The results are rather lackluster\"(AnonReviewer2) \"Performance gain is also limited\"(AnonReviewer3) - Improving the classification accuracy of state-of-the-art deep neural networks in large-scale data sets is hard. For example consider the Imagenet data set, where small improvements (of a few percents) are regarded very important, e.g., in top-1 classification, the improvement from GooggleLeNet to VGG was 1.1%. Of course, there are many differences between Imagenet and our data, but 1.6% of improvement are not to be disregarded. Second, our policy network achieves 25% of the oracle performance (1.6% out of 7.8%), thus it is not mild. Finally, this is the first work that shows that such an improvement is possible.\n\n\"Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before.\"(AnonReviewer3) \"Could have explored some intermediate architectures\"(AnonReviewer2)  - While the focus of this work is to learn a decision rule over different modalities, we also experimented with intermediate architectures such as feature level fusion in our work as the reviewer suggested. Unfortunately, and as reported in other work(e.g., Frome et al. (2013)), we couldn't receive any improvement in classification using this approach. All of these methods result in a network that classifies worse than the text network. We can add the results we received using these architectures in the next version. To the best of our knowledge, learning the decision rule using a deep network is novel to this work and has not been proposed before. \n\n\"No evaluation on standard datasets or comparison to previous works.\"(AnonReviewer2) ,\"No other dataset reported. The authors haven't mentioned releasing the Walmart dataset and it is going to be really hard to reproduce the results without the dataset.\"(AnonReviewer3) -  To the best of our knowledge, there are no standard datasets for comparison of multimodal architectures, nor successful algorithms to compare with. This is the first work that we are aware of that improves classification using deep networks via multimodality. Our data is publicly available for everyone since it can be mined easily from Walmart.com (image text and shelf for all the catalog are visible online). We will make open source our data, that is used in the paper for future benchmarking."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287628359, "id": "ICLR.cc/2017/conference/-/paper308/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJJ3YU5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper308/reviewers", "ICLR.cc/2017/conference/paper308/areachairs"], "cdate": 1485287628359}}}, {"tddate": null, "tmdate": 1482182130956, "tcdate": 1482182130956, "number": 3, "id": "r1slCpBVx", "invitation": "ICLR.cc/2017/conference/-/paper308/official/review", "forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "signatures": ["ICLR.cc/2017/conference/paper308/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper308/AnonReviewer3"], "content": {"title": "Practical large-scale multi-model architecture but lack technical novelty", "rating": "5: Marginally below acceptance threshold", "review": "This paper introduces a large-scale multi-model product classification system. The model consists of three modules, Image CNN (VGG 16 architecture), text CNN (Kim 2014) and decision-level fusion policies. The authors have tried several fusion methods: including policies taking inputs from text and image CNN probabilities; choose either CNN; average the predictions; end-to-end training. Experimental results show that text CNN alone works better than image CNN and multi-model fusion can improve the accuracy by a small margin. It is a little bit surprising that end-to-end feature level fusion works worse than text CNN alone. The writing is clear and there are a lot of useful practical experiences of learning large-scale model. However, I lean toward rejecting the paper because the following:\n\n1) No other dataset reported. The authors haven't mentioned releasing the walmart dataset and it is going to be really hard to reproduce the results without the dataset. \n2) Technical novelty is limited. All the decision-level fusion policies have been investigated by some previous methods before. \n3) Performance gain is also limited. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512627377, "id": "ICLR.cc/2017/conference/-/paper308/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper308/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper308/AnonReviewer1", "ICLR.cc/2017/conference/paper308/AnonReviewer2", "ICLR.cc/2017/conference/paper308/AnonReviewer3"], "reply": {"forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper308/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper308/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512627377}}}, {"tddate": null, "tmdate": 1482169849027, "tcdate": 1482169849027, "number": 2, "id": "S1--09r4l", "invitation": "ICLR.cc/2017/conference/-/paper308/official/review", "forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "signatures": ["ICLR.cc/2017/conference/paper308/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper308/AnonReviewer2"], "content": {"title": "Review", "rating": "5: Marginally below acceptance threshold", "review": "This paper tackles the problem of multi-modal classification of text and images.\n\nPros:\n- Interesting dataset and application.\n\nCons:\n- The results are rather lacklustre, showing a very mild improvement compared to the oracle improvement. But perhaps some insights as to whether the incorrect decisions are humanly possible would help with significance of the results.\n- Could have explored some intermediate architectures such as feature fusion + class probabilities with/without finetuning. There are no feature fusion results reported.\n- No evaluation on standard datasets or comparison to previous works.\n\nWhat is the policy learnt for CP-1? Given 2 input class probabilities, how does the network perform better than max or mean?\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512627377, "id": "ICLR.cc/2017/conference/-/paper308/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper308/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper308/AnonReviewer1", "ICLR.cc/2017/conference/paper308/AnonReviewer2", "ICLR.cc/2017/conference/paper308/AnonReviewer3"], "reply": {"forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper308/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper308/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512627377}}}, {"tddate": null, "tmdate": 1482133405254, "tcdate": 1482133405254, "number": 3, "id": "BJSsyGrVe", "invitation": "ICLR.cc/2017/conference/-/paper308/public/comment", "forum": "rJJ3YU5ge", "replyto": "B1e7GJfVg", "signatures": ["~Tom_Zahavy1"], "readers": ["everyone"], "writers": ["~Tom_Zahavy1"], "content": {"title": "RE: AnonReviewer1", "comment": "We thank the reviewer for finding our work sound and solid. The reviewer had some doubts regarding the significance and the novelty of our work to the ICLR community, let us address these issues. \n\nThe superior performance of deep learning algorithms has been observed in many Supervised, Unsupervised and recently Reinforcement Learning benchmarks. Deep networks leverage input specific priors (e.g. symmetries in an image and sequentially in a text) in order to develop state-of-the-art machine learning classifiers and work particularly well with large datasets. Our work follows the same principles. In particular, we, (1) suggest a method that can leverage the superior performance of state-of-the-art input specific deep neural network and (2), improve performance by combining different modalities together. \n\nThis paper presents a multimodal architecture that is particularly suited for fusing different input-specific deep neural networks from the following reasons. First, we present the first algorithm that achieves improvement in a large-scale classification problem by using multi-modality. Prior works that used deep networks for multimodality failed to achieve improvement in classification accuracy, while other works were limited to small datasets with a small number of labels. Second, our algorithm learns the decision rule (unlike previous papers that suggested to use pre-defined rules) using a deep network. We demonstrate that depth is important (a 2-layer policy network is outperforming a shallow network) and is also scaleable in the number of classes (unlike prior works that used confusion matrixes which do not scale with the number of labels). Moreover, our algorithm is adaptive, in the sense that it can be used with different types of modalities and different levels of classification accuracy for each modality. Last, our simulations demonstrate that using methods that were suggested in the literature before such as using pre-defined decision rules does not yield any performance improvement when used with deep networks. \n\nOur paper makes, to the best of our knowledge, the first successful attempt to improve classification accuracy when using deep networks via multimodality and will make a novel contribution to the ICLR community in our opinion. Learning successfully from multi-modal representations is crucial in many applications and can increase the applicability of deep architecture. We will be happy to emphasize the novelty and potential applications in the next revision following the reviewer's guidance.  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287628359, "id": "ICLR.cc/2017/conference/-/paper308/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJJ3YU5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper308/reviewers", "ICLR.cc/2017/conference/paper308/areachairs"], "cdate": 1485287628359}}}, {"tddate": null, "tmdate": 1481925143600, "tcdate": 1481925143600, "number": 1, "id": "B1e7GJfVg", "invitation": "ICLR.cc/2017/conference/-/paper308/official/review", "forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "signatures": ["ICLR.cc/2017/conference/paper308/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper308/AnonReviewer1"], "content": {"title": "", "rating": "4: Ok but not good enough - rejection", "review": "This paper presents a system approach to combine multiple modalities to perform classification in a practical scenario (e-commerce).\n\nIn general, I find the proposed approach in the paper sound and solid, but do not see novelty in the paper: feature fusion and decision time fusion are both standard practices in multi-modal analysis, and the rest of the paper offers no surprise in implementing such approaches. This seems to be a better fit for venues that focus more on production systems, and seems to be a bad fit for ICLR where the focus is more on research of novel algorithms and theories. ", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482512627377, "id": "ICLR.cc/2017/conference/-/paper308/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper308/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper308/AnonReviewer1", "ICLR.cc/2017/conference/paper308/AnonReviewer2", "ICLR.cc/2017/conference/paper308/AnonReviewer3"], "reply": {"forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "writers": {"values-regex": "ICLR.cc/2017/conference/paper308/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper308/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482512627377}}}, {"tddate": null, "tmdate": 1481714111934, "tcdate": 1481714111926, "number": 2, "id": "rJupFiCXg", "invitation": "ICLR.cc/2017/conference/-/paper308/public/comment", "forum": "rJJ3YU5ge", "replyto": "Sk-7TqXml", "signatures": ["~Tom_Zahavy1"], "readers": ["everyone"], "writers": ["~Tom_Zahavy1"], "content": {"title": "please fix a cited paper", "comment": "Thank you for noticing the mistake. We will fix the citation in the next revision. \n\nKind regards, \n\nThe Authors"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287628359, "id": "ICLR.cc/2017/conference/-/paper308/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJJ3YU5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper308/reviewers", "ICLR.cc/2017/conference/paper308/areachairs"], "cdate": 1485287628359}}}, {"tddate": null, "tmdate": 1480989977083, "tcdate": 1480989977076, "number": 1, "id": "Sk-7TqXml", "invitation": "ICLR.cc/2017/conference/-/paper308/public/comment", "forum": "rJJ3YU5ge", "replyto": "rJJ3YU5ge", "signatures": ["~Jung-Woo_Ha1"], "readers": ["everyone"], "writers": ["~Jung-Woo_Ha1"], "content": {"title": "please fix a cited paper ", "comment": "Hi, I am Jung-Woo Ha and the authour of a paper you cited in your work. \n\nIn the references of your manuscript, I think that \"Hyuna Pyo, Jung-Woo Ha, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. 2010.\" should be changed into \"Jung-Woo Ha, Hyuna Pyo, and Jeonghee Kim. Large-scale item categorization in e-commerce using\nmultiple recurrent neural networks. In Proceedings of KDD 2016.\"\nThe url is http://dl.acm.org/citation.cfm?id=2939678&CFID=872415977&CFTOKEN=21191614\n\nThank you.\n\nKind regards,\n\nJung-Woo."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287628359, "id": "ICLR.cc/2017/conference/-/paper308/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "rJJ3YU5ge", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper308/reviewers", "ICLR.cc/2017/conference/paper308/areachairs"], "cdate": 1485287628359}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1478710810519, "tcdate": 1478285735088, "number": 308, "id": "rJJ3YU5ge", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "rJJ3YU5ge", "signatures": ["~Tom_Zahavy1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion Architecture for Product Classification in e-commerce", "abstract": "Classifying products into categories precisely and efficiently is a major challenge in modern e-commerce. The high traffic of new products uploaded daily and the dynamic nature of the categories raise the need for machine learning models that can reduce the cost and time of human editors. In this paper, we propose a decision level fusion approach for multi-modal product classification using text and image inputs. We train input specific state-of-the-art deep neural networks for each input source, show the potential of forging them together into a multi-modal architecture and train a novel policy network that learns to choose between them. Finally, we demonstrate that our multi-modal network improves the top-1 accuracy $\\%$ over both networks on a real-world large-scale product classification dataset that we collected from Walmart.com. While we focus on image-text fusion that characterizes e-commerce domains, our algorithms can be easily applied to other modalities such as audio, video, physical sensors, etc.", "pdf": "/pdf/ce0bdaceb437f743196c88179ca1f82235ea6013.pdf", "paperhash": "zahavy|is_a_picture_worth_a_thousand_words_a_deep_multimodal_fusion_architecture_for_product_classification_in_ecommerce", "keywords": ["Multi-modal learning", "Deep learning"], "conflicts": ["technion.ac.il", "walmartlabs.com"], "authors": ["Tom Zahavy", "Alessandro Magnani", "Abhinandan Krishnan", "Shie Mannor"], "authorids": ["tomzahavy@tx.technion.ac.il", "AMagnani@walmartlabs.com", "AKrishnan@walmartlabs.com", "shie@ee.technion.ac.il"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}], "count": 9}