{"notes": [{"id": "BklVA2NYvH", "original": "B1eJO2KEwH", "number": 260, "cdate": 1569438923818, "ddate": null, "tcdate": 1569438923818, "tmdate": 1577168264697, "tddate": null, "forum": "BklVA2NYvH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "qPcnzDdBW5", "original": null, "number": 1, "cdate": 1576798691741, "ddate": null, "tcdate": 1576798691741, "tmdate": 1576800943568, "tddate": null, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Decision", "content": {"decision": "Reject", "comment": "The authors propose a framework for improving the robustness of neural networks to adversarial perturbations via optimal control techniques (Lyapunov Stability and the Pontryagin Maximum Principle, in particular). By considering a continuous-time limit of the training process, the authors use the PMP to derive udpate rules for the neural network weights that would result in a robust network. While the approach is interesting, the paper has some serious deficiencies that make it unacceptable for publication in its current form:\n\n1. Quality of empirical evaluation: The authors only report final numbers on CIFAR-10 for a fixed set of adversarial attacks. It has been observed repeatedly in the adversarial robustness literature that adversarial evaluation of neural networks has to be done carefully to guard against possible underestimation of the worst-case attack. In particular, the specific details of the adversarial attacks used (number of steps, number of initializations, performance under larger perturbation radii) that are necessary to trust the results are not given (see https://arxiv.org/pdf/1902.06705.pdf for example).\n\n2. Unclear novelty: The authors do not sufficiently explain the novelty in their approach relative to prior work (particular prior work that has used optimal control ideas in this context).\n\n3. Computational cost: The authors do not give sufficient details to judge the computational overhead of their method to judge how much more expensive it is to train with their approach relative to standard or adversarial training.\n\nWhile one reviewer voted for a weak accept, the other reviewers were in consensus on rejection. The authors did not respond during the rebuttal phase and hence the reviews were unchanged.\n\nIn summary, I vote for rejection. However, I think this paper has potentially interesting ideas that should be carefully developed and evaluated in a future revision.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795729983, "tmdate": 1576800282686, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper260/-/Decision"}}}, {"id": "S1luKEChtB", "original": null, "number": 1, "cdate": 1571771519527, "ddate": null, "tcdate": 1571771519527, "tmdate": 1572972618253, "tddate": null, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Summary:\nThe goal of this paper is to train neural networks (NNs) in a way to be robust to adversarial attacks. The authors formulate training a NN as finding an optimal controller for a discrete dynamical system. This formulation allows them to use an optimal control algorithm, called method of successive approximations (MSA), to train a NN. The authors then show how constraints can be added to this optimization problem in order to make the trained NN more robust. They show that the resulted constraint optimization problem can be formulated as a semi-definite programming and provide some experimental results. \n\nComments:\n- Although the problem studied in the paper is important and the approach is interesting, it seems the paper has been written in rush and in my opinion is not ready for publication. The writing is not good. The introduction and related work sections are incomplete and not very informative. It is not clear what has been done before and what is the contribution of this paper. The main technique/algorithm of the paper has not been explained clearly that someone can easily understand and implement it. The experimental results are not convincing. \n- There are strong claims in the paper such as \"experiments show that our method effectively improves deep model's adversarial robustness\", this is too strong, given the quality of the experiments of the paper. Or \"the constraint optimization problem can be formulated as a semi-definite programming (SDP) problem and hence can be solved efficiently\", to the best of my knowledge, SDP solvers are limited to small problems and cannot solve the large problems efficiently. \n- The area of making NNs robust to attacks is a very active area and there are many attacks and solutions out there, which require more comprehensive empirical studies of any new method. I do not see this in the paper. \n- Overall, I think this paper requires a major revision in order to be evaluated better and to be more useful for the community. "}, "signatures": ["ICLR.cc/2020/Conference/Paper260/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper260/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575227313889, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper260/Reviewers"], "noninvitees": [], "tcdate": 1570237754702, "tmdate": 1575227313900, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper260/-/Official_Review"}}}, {"id": "rkg4GwQTYH", "original": null, "number": 2, "cdate": 1571792651775, "ddate": null, "tcdate": 1571792651775, "tmdate": 1572972618210, "tddate": null, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper contributes to the robust training of neural networks as follows:\n  1) The paper uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness;\n  2) Such an objective is achieved by introducing Lyaponov stability and practically implemented through the method of successive approximations;\n  3) Empirical evaluation demonstrate that the newly introduced method performs as well as the SOTA in terms of defensive training.\n\nThe paper is well written and proposes a well motivated and theoretically original strategy to robustly train neural networks against adversarial examples.\nThe strength of the paper is definitively in its theoretical section, it would be really great to see an empirical improvement improvement on the SOTA.\nHowever, I do not believe the paper should be penalized for only matching other algorithm as it relies on a tractable and principled theoretical analysis."}, "signatures": ["ICLR.cc/2020/Conference/Paper260/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper260/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575227313889, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper260/Reviewers"], "noninvitees": [], "tcdate": 1570237754702, "tmdate": 1575227313900, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper260/-/Official_Review"}}}, {"id": "HygB_6lkqB", "original": null, "number": 3, "cdate": 1571913069316, "ddate": null, "tcdate": 1571913069316, "tmdate": 1572972618166, "tddate": null, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "1: Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Neural Networks are vulnerable to adversarial perturbations. This paper proposes a method that based on optimal control theory that uses semidefinite-programming. This is a quite popular topic in Adversarial training recently, there has been a few works in that line. There are almost no experiments in this paper. There are several typos in the paper and writing of this paper requires more work. There are several typos in this paper, for example STOA, should be SOTA (in the Section 6.) In its current state, this paper looks very rushed.\n\n\nAs Yiping Lu pointed out, the PMP statement in this paper is also wrong. At this current stage, unfortunately this paper doesn\u2019t meet the standards of ICLR. I would recommend the authors to go over the paper carefully and resubmit to a different venue.\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper260/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper260/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575227313889, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper260/Reviewers"], "noninvitees": [], "tcdate": 1570237754702, "tmdate": 1575227313900, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper260/-/Official_Review"}}}, {"id": "HkeDzuMLdr", "original": null, "number": 2, "cdate": 1570281487347, "ddate": null, "tcdate": 1570281487347, "tmdate": 1570281487347, "tddate": null, "forum": "BklVA2NYvH", "replyto": "H1ldrNsrOr", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Official_Comment", "content": {"comment": "I'm not sure I understand your question. Our method is not a modification of adversarial training. The reason why we use MSA is to reduce the size of the parameter space to add extra constraints.", "title": "Answers to the question"}, "signatures": ["ICLR.cc/2020/Conference/Paper260/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklVA2NYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference/Paper260/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper260/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper260/Reviewers", "ICLR.cc/2020/Conference/Paper260/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper260/Authors|ICLR.cc/2020/Conference/Paper260/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174029, "tmdate": 1576860541239, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference/Paper260/Reviewers", "ICLR.cc/2020/Conference/Paper260/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper260/-/Official_Comment"}}}, {"id": "H1ldrNsrOr", "original": null, "number": 2, "cdate": 1570251840242, "ddate": null, "tcdate": 1570251840242, "tmdate": 1570251840242, "tddate": null, "forum": "BklVA2NYvH", "replyto": "Skeh1cXV_B", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Public_Comment", "content": {"comment": "Thus what is the difference between PMP here  and PMP in YOPO.\nand\nwhat is the benefit of using  Successive Approximations in adversarial training?\n(at least you should have a speed comparison to demonstrate the benefit  \n\nYou said \"The advantages of training by MSA compared with gradient descent algorithms has been discussed\nin (Li et al., 2017)\" they said their benefit is avoiding saddle point\nWhy  Successive Approximations can avoid saddle point in the adversarial training? (first what is \"saddle point\" in min-max optimization?)", "title": "Additional Comment"}, "signatures": ["~Yiping_Lu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yiping_Lu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklVA2NYvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504211621, "tmdate": 1576860574730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference/Paper260/Reviewers", "ICLR.cc/2020/Conference/Paper260/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper260/-/Public_Comment"}}}, {"id": "Skeh1cXV_B", "original": null, "number": 1, "cdate": 1570154979877, "ddate": null, "tcdate": 1570154979877, "tmdate": 1570154979877, "tddate": null, "forum": "BklVA2NYvH", "replyto": "BJeq6phz_r", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Official_Comment", "content": {"comment": "Hi,\n\nThanks for your comment!\n\n- YOPO is a great work on PMP and it inspires a lot.\n- Thanks for pointing out the mistake. I didn't scrutinize the theorem so that I missed the convex constraint and I will fix it.\n- Actually, the code is not well-organized. I'm still working on it and will release it upon finishing.\n", "title": "Thanks for your comment!"}, "signatures": ["ICLR.cc/2020/Conference/Paper260/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklVA2NYvH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference/Paper260/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper260/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper260/Reviewers", "ICLR.cc/2020/Conference/Paper260/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper260/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper260/Authors|ICLR.cc/2020/Conference/Paper260/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504174029, "tmdate": 1576860541239, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference/Paper260/Reviewers", "ICLR.cc/2020/Conference/Paper260/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper260/-/Official_Comment"}}}, {"id": "BJeq6phz_r", "original": null, "number": 1, "cdate": 1570061761525, "ddate": null, "tcdate": 1570061761525, "tmdate": 1570061761525, "tddate": null, "forum": "BklVA2NYvH", "replyto": "BklVA2NYvH", "invitation": "ICLR.cc/2020/Conference/Paper260/-/Public_Comment", "content": {"comment": "Dear authors ,\nI'm one of the authors of the paper \"You only propagate once\".  First, thanks for the citation, but we still have some points need to be point out.\n- YOPO is also a method using PMP, you can see our paper section 3 in our paper for details. \n- The PMP you state in the paper is wrong, for using KKT condition you can only have *\\nabla H=0* to get \\theta = argmax H, you need more assumption {ft(x, \u03b8) : \u03b8 \u2208 \u0398t} is convex.(notation in our paper)\n- I also curious about the training speed about your method compare with YOPO. Whether there is some code for us to try?\n", "title": "PMP has already appears in YOPO and this version theorem is wrong"}, "signatures": ["~Yiping_Lu1"], "readers": ["everyone"], "nonreaders": [], "writers": ["~Yiping_Lu1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability", "authors": ["Zhiyang Chen", "Hang Su"], "authorids": ["zy-chen17@mails.tsinghua.edu.cn", "suhangss@mail.tsinghua.edu.cn"], "keywords": ["adversarial defense", "optimal control", "Lyapunov stability"], "TL;DR": "An adversarial defense method bridging robustness of deep neural nets with Lyapunov stability", "abstract": "Deep neural networks are known to be vulnerable to adversarial perturbations. In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of dynamical systems. From this viewpoint, training neural nets is equivalent to finding an optimal control of the discrete dynamical system, which allows one to utilize methods of successive approximations, an optimal control algorithm based on Pontryagin's maximum principle, to train neural nets. This decoupled training method allows us to add constraints to the optimization, which makes the deep model more robust. The constrained optimization problem can be formulated as a semi-definite programming problem and hence can be solved efficiently. Experiments show that our method effectively improves deep model's adversarial robustness.", "pdf": "/pdf/9b40938c2b940454dd36242431ecd22baa208076.pdf", "paperhash": "chen|adversarially_robust_neural_networks_via_optimal_control_bridging_robustness_with_lyapunov_stability", "original_pdf": "/attachment/9b40938c2b940454dd36242431ecd22baa208076.pdf", "_bibtex": "@misc{\nchen2020adversarially,\ntitle={Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability},\nauthor={Zhiyang Chen and Hang Su},\nyear={2020},\nurl={https://openreview.net/forum?id=BklVA2NYvH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BklVA2NYvH", "readers": {"values": ["everyone"], "description": "User groups that will be able to read this comment."}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "~.*"}}, "readers": ["everyone"], "tcdate": 1569504211621, "tmdate": 1576860574730, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["everyone"], "noninvitees": ["ICLR.cc/2020/Conference/Paper260/Authors", "ICLR.cc/2020/Conference/Paper260/Reviewers", "ICLR.cc/2020/Conference/Paper260/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper260/-/Public_Comment"}}}], "count": 9}