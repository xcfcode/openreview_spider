{"notes": [{"tddate": null, "ddate": null, "cdate": null, "original": null, "tmdate": 1490028631489, "tcdate": 1490028631489, "number": 1, "id": "ryyDdY6oe", "invitation": "ICLR.cc/2017/workshop/-/paper148/acceptance", "forum": "r1mQ01SYl", "replyto": "r1mQ01SYl", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"decision": "Accept", "title": "ICLR committee final decision"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "On Hyperparameter Optimization in Learning Systems", "abstract": "We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters  of any iterative learning algorithm. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs  in terms of running time and space requirements. The reverse-mode procedure extends previous work by (Maclaurin et al. 2015) and offers the opportunity to insert constraints on the hyperparameters in a natural way. The forward-mode procedure is suitable for stochastic hyperparameter updates, which may significantly speedup the overall hyperparameter optimization procedure. ", "pdf": "/pdf/9c28f0024cef236744b0d8a8ca02af13efffa9cb.pdf", "paperhash": "franceschi|on_hyperparameter_optimization_in_learning_systems", "keywords": [], "conflicts": ["@ucl.ac.uk", "@cs.ucl.ac.uk", "@iit.it"], "authors": ["Luca Franceschi", "Michele Donini", "Paolo Frasconi", "Massimiliano Pontil"], "authorids": ["igor_mio@hotmail.it", "mikko108382892@gmail.com", "paolo.frasconi@unifi.it", "massimiliano.pontil@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "tmdate": 1490028632039, "id": "ICLR.cc/2017/workshop/-/paper148/acceptance", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "r1mQ01SYl", "replyto": "r1mQ01SYl", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept", "Reject"]}}}, "nonreaders": [], "cdate": 1490028632039}}}, {"tddate": null, "tmdate": 1489575602632, "tcdate": 1489575602632, "number": 2, "id": "ByjnC9Lsx", "invitation": "ICLR.cc/2017/workshop/-/paper148/official/review", "forum": "r1mQ01SYl", "replyto": "r1mQ01SYl", "signatures": ["ICLR.cc/2017/workshop/paper148/AnonReviewer2"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper148/AnonReviewer2"], "content": {"title": "appropriate subject, good analysis", "rating": "6: Marginally above acceptance threshold", "review": "The topic of hyperparameter optimisation is quite obviously pertinent to ICLR.\nThe approach the authors take seems straightforward and the results presented in the recently added full paper seem good. In particular the \"real-time\" updates of the forward mode optimisation are a useful innovation. I believe many ICLR attendants would find this interesting and useful.", "confidence": "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "On Hyperparameter Optimization in Learning Systems", "abstract": "We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters  of any iterative learning algorithm. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs  in terms of running time and space requirements. The reverse-mode procedure extends previous work by (Maclaurin et al. 2015) and offers the opportunity to insert constraints on the hyperparameters in a natural way. The forward-mode procedure is suitable for stochastic hyperparameter updates, which may significantly speedup the overall hyperparameter optimization procedure. ", "pdf": "/pdf/9c28f0024cef236744b0d8a8ca02af13efffa9cb.pdf", "paperhash": "franceschi|on_hyperparameter_optimization_in_learning_systems", "keywords": [], "conflicts": ["@ucl.ac.uk", "@cs.ucl.ac.uk", "@iit.it"], "authors": ["Luca Franceschi", "Michele Donini", "Paolo Frasconi", "Massimiliano Pontil"], "authorids": ["igor_mio@hotmail.it", "mikko108382892@gmail.com", "paolo.frasconi@unifi.it", "massimiliano.pontil@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489575603456, "id": "ICLR.cc/2017/workshop/-/paper148/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper148/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper148/AnonReviewer1", "ICLR.cc/2017/workshop/paper148/AnonReviewer2"], "reply": {"forum": "r1mQ01SYl", "replyto": "r1mQ01SYl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper148/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper148/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489575603456}}}, {"tddate": null, "nonreaders": null, "tmdate": 1489525867407, "tcdate": 1489525801406, "number": 1, "id": "rJW42RHoe", "invitation": "ICLR.cc/2017/workshop/-/paper148/public/comment", "forum": "r1mQ01SYl", "replyto": "r1sV1-1og", "signatures": ["~massimiliano_pontil1"], "readers": ["everyone"], "writers": ["~massimiliano_pontil1"], "content": {"title": "thanks and link to longer paper", "comment": "Thanks for the positive comment. We now also have a longer in ArXiv (see: https://arxiv.org/pdf/1703.01785.pdf) with more detailed derivations and experimental results, covering some of your questions. "}, "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "On Hyperparameter Optimization in Learning Systems", "abstract": "We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters  of any iterative learning algorithm. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs  in terms of running time and space requirements. The reverse-mode procedure extends previous work by (Maclaurin et al. 2015) and offers the opportunity to insert constraints on the hyperparameters in a natural way. The forward-mode procedure is suitable for stochastic hyperparameter updates, which may significantly speedup the overall hyperparameter optimization procedure. ", "pdf": "/pdf/9c28f0024cef236744b0d8a8ca02af13efffa9cb.pdf", "paperhash": "franceschi|on_hyperparameter_optimization_in_learning_systems", "keywords": [], "conflicts": ["@ucl.ac.uk", "@cs.ucl.ac.uk", "@iit.it"], "authors": ["Luca Franceschi", "Michele Donini", "Paolo Frasconi", "Massimiliano Pontil"], "authorids": ["igor_mio@hotmail.it", "mikko108382892@gmail.com", "paolo.frasconi@unifi.it", "massimiliano.pontil@gmail.com"]}, "tags": [], "invitation": {"tddate": null, "tmdate": 1487367707850, "tcdate": 1487367707850, "id": "ICLR.cc/2017/workshop/-/paper148/public/comment", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["~"], "noninvitees": ["ICLR.cc/2017/workshop/paper148/reviewers"], "reply": {"forum": "r1mQ01SYl", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/workshop/reviewers", "ICLR.cc/2017/pcs"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "cdate": 1487367707850}}}, {"tddate": null, "tmdate": 1489076018937, "tcdate": 1489076018937, "number": 1, "id": "r1sV1-1og", "invitation": "ICLR.cc/2017/workshop/-/paper148/official/review", "forum": "r1mQ01SYl", "replyto": "r1mQ01SYl", "signatures": ["ICLR.cc/2017/workshop/paper148/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/workshop/paper148/AnonReviewer1"], "content": {"title": "Good paper", "rating": "6: Marginally above acceptance threshold", "review": "The paper considers an important topic in hyperparameter optimization of learning algorithms.\nUnfortunately, due to the page limit of the workshop track the material is too compressed starting from \npage 1 where most variables can be guessed but are not defined (v_t, w_t, mu, eta, m and \u03b3 in \"In this example, \u03bb = (\u00b5, \u03b3)\"). \nThe paper might benefit from figures showing optimization trajectories of hyperparameters. \nThe paper is missing the results regarding the scalability with the number of parameters and hyperparameters. \nThe results with grid search baseline are preliminary as suggested by the authors. \nA supplementary/appendix figure (instead of Figure 1- Right) with the accuracy vs time for random search and the proposed approach might improve the presentation of the experimental results. ", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"TL;DR": "", "title": "On Hyperparameter Optimization in Learning Systems", "abstract": "We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters  of any iterative learning algorithm. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs  in terms of running time and space requirements. The reverse-mode procedure extends previous work by (Maclaurin et al. 2015) and offers the opportunity to insert constraints on the hyperparameters in a natural way. The forward-mode procedure is suitable for stochastic hyperparameter updates, which may significantly speedup the overall hyperparameter optimization procedure. ", "pdf": "/pdf/9c28f0024cef236744b0d8a8ca02af13efffa9cb.pdf", "paperhash": "franceschi|on_hyperparameter_optimization_in_learning_systems", "keywords": [], "conflicts": ["@ucl.ac.uk", "@cs.ucl.ac.uk", "@iit.it"], "authors": ["Luca Franceschi", "Michele Donini", "Paolo Frasconi", "Massimiliano Pontil"], "authorids": ["igor_mio@hotmail.it", "mikko108382892@gmail.com", "paolo.frasconi@unifi.it", "massimiliano.pontil@gmail.com"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1489183200000, "tmdate": 1489575603456, "id": "ICLR.cc/2017/workshop/-/paper148/official/review", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/workshop"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/workshop/paper148/reviewers"], "noninvitees": ["ICLR.cc/2017/workshop/paper148/AnonReviewer1", "ICLR.cc/2017/workshop/paper148/AnonReviewer2"], "reply": {"forum": "r1mQ01SYl", "replyto": "r1mQ01SYl", "writers": {"values-regex": "ICLR.cc/2017/workshop/paper148/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/workshop/paper148/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,5000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1496959200000, "cdate": 1489575603456}}}, {"tddate": null, "replyto": null, "ddate": null, "tmdate": 1487700803465, "tcdate": 1487367707236, "number": 148, "id": "r1mQ01SYl", "invitation": "ICLR.cc/2017/workshop/-/submission", "forum": "r1mQ01SYl", "signatures": ["~massimiliano_pontil1"], "readers": ["everyone"], "content": {"TL;DR": "", "title": "On Hyperparameter Optimization in Learning Systems", "abstract": "We study two procedures (reverse-mode and forward-mode) for computing the gradient of the validation error with respect to the hyperparameters  of any iterative learning algorithm. These procedures mirror two ways of computing gradients for recurrent neural networks and have different trade-offs  in terms of running time and space requirements. The reverse-mode procedure extends previous work by (Maclaurin et al. 2015) and offers the opportunity to insert constraints on the hyperparameters in a natural way. The forward-mode procedure is suitable for stochastic hyperparameter updates, which may significantly speedup the overall hyperparameter optimization procedure. ", "pdf": "/pdf/9c28f0024cef236744b0d8a8ca02af13efffa9cb.pdf", "paperhash": "franceschi|on_hyperparameter_optimization_in_learning_systems", "keywords": [], "conflicts": ["@ucl.ac.uk", "@cs.ucl.ac.uk", "@iit.it"], "authors": ["Luca Franceschi", "Michele Donini", "Paolo Frasconi", "Massimiliano Pontil"], "authorids": ["igor_mio@hotmail.it", "mikko108382892@gmail.com", "paolo.frasconi@unifi.it", "massimiliano.pontil@gmail.com"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 4, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1487690420000, "tmdate": 1484242559574, "id": "ICLR.cc/2017/workshop/-/submission", "writers": ["ICLR.cc/2017/workshop"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1495466420000, "cdate": 1484242559574}}}], "count": 5}