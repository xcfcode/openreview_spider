{"notes": [{"id": "BkgGJlBFPS", "original": "SkxROFJYwB", "number": 2054, "cdate": 1569439706294, "ddate": null, "tcdate": 1569439706294, "tmdate": 1577168233929, "tddate": null, "forum": "BkgGJlBFPS", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "2Gn_k1idq4", "original": null, "number": 1, "cdate": 1576798739298, "ddate": null, "tcdate": 1576798739298, "tmdate": 1576800897013, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Decision", "content": {"decision": "Reject", "comment": "The paper presents an unsupervised method for graph representation, building upon Loukas' method for generating a sequence of gradually coarsened graphs. The contribution is an \"encoder-decoder\" architecture trained by variational inference, where the encoder produces the embedding of the nodes in the next graph of the sequence, and the decoder produces the structure of the next graph. \n\nOne important merit of the approach is  that this unsupervised representation can be used effectively for supervised learning, with results quite competitive to the state of the art. \n\nHowever the reviewers were unconvinced by the novelty and positioning of the approach. The point of whether the approach should be viewed as variational Bayesian, or simply variational approximation was much debated between the reviewers and the authors. \n\nThe area chair encourages the authors to pursue this very promising research, and to clarify the paper; perhaps the use of \"encoder-decoder\" generated too much misunderstanding. \nAnother graph NN paper you might be interested in is \"Edge Contraction Pooling for Graph NNs\", by Frederik Diehl. \n", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795725694, "tmdate": 1576800277643, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Decision"}}}, {"id": "B1xrAKUnoH", "original": null, "number": 4, "cdate": 1573837260853, "ddate": null, "tcdate": 1573837260853, "tmdate": 1573837296952, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "Byx7A-82jS", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment", "content": {"title": "Variational Bayes", "comment": "Again, I do not see the point in calling variational approximation variational Bayes is there is no prior on the parameters. You seem to be confusing variational approximation in e.g. EM where a complex distribution is replaced by a factored approximation with variational Bayes where the posterior distribution of the *parameters* is approximated by a factored distribution. Notice that using the Bayes rules does not turn a frequentist approach into a Bayesian one (eg the so-called naive Bayes classifier is not a Bayesian approach unless you add a prior on its parameter). And thus writing \"A core subject of Bayesian inference is concerned with estimating the posterior distribution p(z|x)\" where x is the observed data and z the latent variable is clearly misunderstanding of Bayesian inference. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer1", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgGJlBFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2054/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2054/Authors|ICLR.cc/2020/Conference/Paper2054/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146995, "tmdate": 1576860553667, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment"}}}, {"id": "ByxhQGLhoH", "original": null, "number": 3, "cdate": 1573835300472, "ddate": null, "tcdate": 1573835300472, "tmdate": 1573835300472, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "BJezGUojtr", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment", "content": {"title": "Response to Official Blind Review #3", "comment": "Thank you very much for raising the concerns. We address them below. Hope the response helps you reassess the contribution of the work.\n\nRE: The point of unsupervised learning.\n\nThe power of unsupervised representation comes from the fact that it is trained without knowing any downstream task. The learning uses much less information. We believe it is fair to consider that the method is successful, if the performance of a downstream task is comparable with that resulting from learning with additional supervised information.\n\nRE: When is the node representation of the coarsening sequence needed?\n\nThe use of node features (both those of the original graph and the learned ones of the coarse graphs) is explained in section 3.5. Specifically, for each graph in the sequence, the node features are pooled to form a graph embedding, such that all graph embeddings can be concatenated to form the final graph representation. A simple predictive model (e.g., MLP) is trained separately for the downstream task (e.g., graph classification). This predictive model will vary depending on the nature of the task.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2054/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgGJlBFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2054/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2054/Authors|ICLR.cc/2020/Conference/Paper2054/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146995, "tmdate": 1576860553667, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment"}}}, {"id": "Byx7A-82jS", "original": null, "number": 2, "cdate": 1573835210556, "ddate": null, "tcdate": 1573835210556, "tmdate": 1573835210556, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "SJg6jDF6tr", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment", "content": {"title": "Response to Official Blind Review #1", "comment": "Thank you very much for the comments. We respond to them in the following.\n\nRE: Significance of performance difference among methods.\n\nWe well concur that significance tests are important for assessing the performance advantage of one model over another. On the other hand, we also note that the key message of this work is that unsupervised approaches may be as competitive as supervised approaches, even if no labels are used. We do not intend to claim that our approach is superior. In this regard, a very close accuracy exactly conveys this message.\n\nRE: Terminology regarding \"variational Bayes\".\n\nThe terminology is subject to debate but we would like to share our opinion. It is not relevant to the contribution of this paper.\n\nTraditionally, the motivation for variational Bayes is approximate inference. The machinery was borrowed recently for developing generative models (VAE). More interesting in these models is the generative part (still, the mathematics is the same). The inference part serves only as a tool for training. The prior therein is an assumed distribution for the latent space; it is not used for parameters. In VAE, the parameters are with respect to the encoder network and the decoder network. Unless ones performs Bayesian deep learning, no priors on the network parameters exist for simplicity. On the other hand, the latent space is the prior for VAE. This prior may be made extremely simple (such as standard Gaussian), or slightly more expressive (such as a factored Gaussian parameterized by mean and variance), or even more complex. In this work, we find that the simple choice suffices.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2054/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgGJlBFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2054/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2054/Authors|ICLR.cc/2020/Conference/Paper2054/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146995, "tmdate": 1576860553667, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment"}}}, {"id": "r1ekj-InsS", "original": null, "number": 1, "cdate": 1573835158902, "ddate": null, "tcdate": 1573835158902, "tmdate": 1573835158902, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "SJljZmHL9S", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment", "content": {"title": "Response to Official Blind Review #2", "comment": "Thank you very much for raising the concerns. Our response is in the following. Hope the response help you reassess the contribution of the work.\n\nRE: Concern (1); difference from Kingma & Welling (2014).\n\nA few details distinguish our approach from the VAE work by Kingma & Welling:\n\n(a) Kingma & Welling laid down a variational principle for unsupervised learning but the neural architectures of encoder and decoder are not specified. A plethora of work emerges to address various data types, through designing specific encoders and decoders. In this work we focus on graph data.\n\n(b) In most (if not all) settings, an autoencoder tries to reconstruct the data itself. We, on the other hand, are concerned with constructing a different graph. This needs the adjustment of the theory and the loss function. We mathematically justify so in Section 3.1.\n\n(c) One may recall the variational graph autoencoder work by Kipf & Welling (2016). Despite under the same VAE umbrella, we need a different architecture to cope with the fact that a different graph is constructed, besides the adjustment of the variational Bayes formulation. Moreover, our work is concerned with graph-level representation rather than node-level. Hence, an additional component that forms the graph embedding from node features is needed.\n\nRE: Concern (2); unsupervised representation.\n\nThe power of unsupervised representation comes from the fact that it is trained without knowing any downstream task. The learning uses much less information. We believe it is fair to consider that the method is successful, if the performance of a downstream task is comparable with that resulting from learning with additional supervised information.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2054/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "BkgGJlBFPS", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper2054/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper2054/Authors|ICLR.cc/2020/Conference/Paper2054/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504146995, "tmdate": 1576860553667, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper2054/Authors", "ICLR.cc/2020/Conference/Paper2054/Reviewers", "ICLR.cc/2020/Conference/Paper2054/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Official_Comment"}}}, {"id": "BJezGUojtr", "original": null, "number": 1, "cdate": 1571694090214, "ddate": null, "tcdate": 1571694090214, "tmdate": 1572972389148, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Official_Review", "content": {"rating": "3: Weak Reject", "experience_assessment": "I have published one or two papers in this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I carefully checked the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #3", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "Summary: This work proposes an unsupervised hierarchical graph representation learning method, named BayesPool. The method learns a coarsening sequence of graphs together with the corresponding node representations. The coarsening sequence is learned using the method in Loukas (2019). The node representations are learned using an encoder-decoder structure, where the encoder encodes a graph to coarsened node representations, and the decoder decodes the node representations to a coarsened graph. The adopted objective function is analogous to VAE, except that the decoder does not aims to reconstruct an identical graph. Experiments on graph classification is performed on 5 different datasets, and competitive accuracy is achieved.\n\nConcerns: The authors claim that the leant representation in an unsupervised manner is more desirable in terms of generalization. However, they only provide very limited experimental results, which is not very convincing. Moreover, the authors also do not explain clearly on when the node representation of the coarsening sequence is needed."}, "signatures": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575420274308, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2054/Reviewers"], "noninvitees": [], "tcdate": 1570237728428, "tmdate": 1575420274321, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Official_Review"}}}, {"id": "SJg6jDF6tr", "original": null, "number": 2, "cdate": 1571817381211, "ddate": null, "tcdate": 1571817381211, "tmdate": 1572972389112, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "6: Weak Accept", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I carefully checked the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I did not assess the derivations or theory.", "review": "The authors propose in this paper a new unsupervised graph representation learning method. The method leverages recent advances in graph coarsening, mainly Loukas' method. The key idea of the method consists in using a reconstruction target that is not the classical one in an auto-encoder setting. More precisely, the encoder takes as an input the original adjacency matrix and node features but the decode only aims at reconstructing the coarse adjacency matrix (obtained via Loukas' method). \n\nThe experimental evaluation is quite thorough and shows that the method performs quite well, especially considering it is unsupervised but is compared to supervised representation methods. It would be nice to include statistical tests to assess the significance of the differences in cases were accuracies are very close one to another. A missing part would be to explore the relevance of the learned representation for other tasks (i.e. to use a multi task data set). Of course as the representation is learned in an unsupervised way, one can argue that the current evaluation is already providing an answer.\n\nOverall, I find the paper clear, but the variational bayes part could be much clearer. In fact I'm not sure why this is presented as variational bayes and not only variational. I do not see any prior distribution over parameters, for instance. I understand that the recent \"tradition\" in variational auto-encoder is to use this terminology, but as a (part time) bayesian, this is a bit annoying. "}, "signatures": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575420274308, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2054/Reviewers"], "noninvitees": [], "tcdate": 1570237728428, "tmdate": 1575420274321, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Official_Review"}}}, {"id": "SJljZmHL9S", "original": null, "number": 3, "cdate": 1572389635319, "ddate": null, "tcdate": 1572389635319, "tmdate": 1572972389069, "tddate": null, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "invitation": "ICLR.cc/2020/Conference/Paper2054/-/Official_Review", "content": {"experience_assessment": "I do not know much about this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I made a quick assessment of this paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "The paper proposes an unsupervised approach to learn a representation of graphs. The idea comes from an encoder-decoder architecture, which is common in related literature. The paper uses a variational Bayes approach in the learning process. Thorough experiments are provided to justify the feasibility of the method. \n\nThis paper provides an unsupervised style of learning graph representations, which may not be coupled with a specific downstream task so that it may be more useful in general; also, the experiments themselves seem to be at least comparable to the recent methods.  \n\nHowever, I vote for rejecting this submission for the following concerns. \n\n(1) I did not find too many significant differences between this paper and [Kingma & Welling, 2014] in the design of encoder-decoder architecture as well as the learning procedure (I am not an expert in this area so please correct me if I am wrong).\n\n(2) The intuition of learning the representation in an unsupervised manner is interesting and important to me, though the experiments are mostly on the classification tasks. I think it would be helpful to demonstrate the representation power of the learned representation of the graph in tackling other tasks.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper2054/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Unsupervised Hierarchical Graph Representation Learning with Variational Bayes", "authors": ["Shashanka Ubaru", "Jie Chen"], "authorids": ["shashanka.ubaru@ibm.com", "chenjie@us.ibm.com"], "keywords": ["Hierarchical Graph Representation", "Unsupervised Graph Learning", "Variational Bayes", "Graph classification"], "TL;DR": "Bayespool: An unsupervised hierarchical graph representation learning method based on Variational Bayes.", "abstract": "Hierarchical graph representation learning is an emerging subject owing to the increasingly popular adoption of graph neural networks in machine learning and applications. Loosely speaking, work under this umbrella falls into two categories: (a) use a predefined graph hierarchy to perform pooling; and (b) learn the hierarchy for a given graph through differentiable parameterization of the coarsening process. These approaches are supervised; a predictive task with ground-truth labels is used to drive the learning. In this work, we propose an unsupervised approach, \\textsc{BayesPool}, with the use of variational Bayes. It produces graph representations given a predefined hierarchy. Rather than relying on labels, the training signal comes from the evidence lower bound of encoding a graph and decoding the subsequent one in the hierarchy. Node features are treated latent in this variational machinery, so that they are produced as a byproduct and are used in downstream tasks. We demonstrate a comprehensive set of experiments to show the usefulness of the learned representation in the context of graph classification.", "pdf": "/pdf/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "code": "https://anonymous.4open.science/r/a50d6411-55f7-4e24-8f6c-6eecee118ea0/", "paperhash": "ubaru|unsupervised_hierarchical_graph_representation_learning_with_variational_bayes", "original_pdf": "/attachment/10565c2d8c02e553e387fc44d4c2904fca443840.pdf", "_bibtex": "@misc{\nubaru2020unsupervised,\ntitle={Unsupervised Hierarchical Graph Representation Learning with Variational Bayes},\nauthor={Shashanka Ubaru and Jie Chen},\nyear={2020},\nurl={https://openreview.net/forum?id=BkgGJlBFPS}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "BkgGJlBFPS", "replyto": "BkgGJlBFPS", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper2054/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575420274308, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper2054/Reviewers"], "noninvitees": [], "tcdate": 1570237728428, "tmdate": 1575420274321, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper2054/-/Official_Review"}}}], "count": 9}