{"notes": [{"id": "rked_6NFwH", "original": "B1g0Gf3DPr", "number": 638, "cdate": 1569439088150, "ddate": null, "tcdate": 1569439088150, "tmdate": 1577168232840, "tddate": null, "forum": "rked_6NFwH", "replyto": null, "invitation": "ICLR.cc/2020/Conference/-/Blind_Submission", "content": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2020/Conference"]}, "signatures": {"values": ["ICLR.cc/2020/Conference"]}, "content": {"spotlight_video": {"value-regex": ".*"}, "full_presentation_video": {"value-regex": ".*"}, "original_pdf": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}, "appendix": {"value-regex": ".*"}, "authorids": {"values-regex": ".*"}, "poster": {"value-regex": ".*"}, "authors": {"values": ["Anonymous"]}, "slides": {"value-regex": ".*"}}}, "final": [], "signatures": ["ICLR.cc/2020/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference"], "noninvitees": [], "tcdate": 1569271260237, "tmdate": 1593459412141, "id": "ICLR.cc/2020/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "6YgWl2deDI", "original": null, "number": 1, "cdate": 1576798702047, "ddate": null, "tcdate": 1576798702047, "tmdate": 1576800933960, "tddate": null, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Decision", "content": {"decision": "Reject", "comment": "The scores of the reviewers are just far to low to warrant an acceptance recommendation from the AC.", "title": "Paper Decision"}, "signatures": ["ICLR.cc/2020/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"writers": {"description": "How your identity will be displayed.", "values-regex": ["ICLR.cc/2020/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2020/Conference/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"decision": {"value-radio": ["Accept (Spotlight)", "Accept (Talk)", "Accept (Poster)", "Reject"], "description": "Decision", "required": true, "order": 2}, "title": {"value": "Paper Decision", "required": true, "order": 1}, "comment": {"value-regex": "[\\S\\s]{0,5000}", "description": "", "required": false, "order": 3}}, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}}, "expdate": 1576854540000, "duedate": 1576853940000, "multiReply": false, "readers": ["everyone"], "invitees": ["ICLR.cc/2020/Conference/Program_Chairs"], "tcdate": 1576795712486, "tmdate": 1576800261878, "super": "ICLR.cc/2020/Conference/-/Decision", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper638/-/Decision"}}}, {"id": "SJea-J2ior", "original": null, "number": 4, "cdate": 1573793541011, "ddate": null, "tcdate": 1573793541011, "tmdate": 1573794496526, "tddate": null, "forum": "rked_6NFwH", "replyto": "Skg8bjXTFH", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment", "content": {"title": "Response To Reviewer #3", "comment": "1.      Thanks for your valuable suggestions on this paper. We have made the following revisions. In Section 2, we have revised the description of the graph for RNN by adding the description of edges. We also refine Definition 1 and change to use different notations to denote node and its value. \n\n2.      Please note that the definition of the value of path was shown in the previous version and we emphasize it using boldface in the new version.\n\n3.      In Section 3, we add a formal definition of the reduction graph in Definition 3.\n\n4.      In Section 4, we change the \u201cwithout-recurrent part\u201d into the \u201cforward part\u201d and describe it before the box and revise the Skeleton method in box. \n\n5.      For the questions about the division, the division shown in Algorithm 2 means the element-wise division. The function BP(.) returns the gradient of weights, which is calculated by using the Back-Propagation method.\n\n6.      For your additional feedback, we revised our paper in the following aspects:\na.      We remove the term \u201cunfold directed graph\u201d and use the term \u201cdirected graph\u201d instead. We define the directed graph by defining its nodes and edges in Definition 2.1 and illustrate it in Figure1.\nb.      x in Eq.(3) means the input value. We add the its explanation before we use it.\nc.      We use Eq.(4) to illustrate that \u201cwe cannot directly optimize the values of all the paths by regarding each of them to be an independent parameter, because they are correlated with each other\u201d as shown in the paragraph above the Eq. (4).  So, p2, \u2026, p5 are just an example to show the dependent between paths and they are a subset of basis path. In the following part, we show that we can identify all the basis paths efficiently (skeleton method)  \nd.      Table 4: Since comparing the numbers between the different implementations is unfair, we reproduce the SGD baseline and show the number in the third row.  The second row in Table 4 is borrowed from the previous paper and just for reference. In fact, some of our reproduced numbers are better than the previous work and others are worse. The meaningful comparison is between the first row and the third row and it shows our results are always better than the baseline method\ne.      Thank you for pointing out the typos  and we change p into v_p in proofs in Appendix. .\nf.       We add one row to show the relative time cost rather than the absolute value.\n\n\nBesides the above items, we have also revised other parts of the paper including proofs, numbers in tables in the appendix according to your suggestions. Please check the new version and hope you can reconsider your rate."}, "signatures": ["ICLR.cc/2020/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rked_6NFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper638/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper638/Authors|ICLR.cc/2020/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168479, "tmdate": 1576860554103, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment"}}}, {"id": "BylsACcssS", "original": null, "number": 3, "cdate": 1573789395392, "ddate": null, "tcdate": 1573789395392, "tmdate": 1573794189586, "tddate": null, "forum": "rked_6NFwH", "replyto": "ryxFe1MT9B", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment", "content": {"title": "Response to Reviewer #2 ", "comment": "First, it is unfair to give this paper 1 point by saying that this paper is highly similar to the previous paper. The differences between our paper and the previous work G-SGD are stated in paragraph 4 in the introduction and the contribution of our work is shown in paragraph 5-7 in the introduction.  Specifically, due to the recurrent structure and the parameter sharing, this work is significantly different from the previous work that considering the MLP/CNN (Meng et al., 2019). In RNN, all paths are related not only to the RNN structure but also to the sequence length of the data. This property makes the basis path definition and identification problematic since we want to define and identify the maximal independent group from the undetermined number of paths. To solve this problem, we propose using reduction graph as a tool and we prove that the basis path of reduction graph is equivalent to the basis path of RNN. Since all the paths in the reduction graph are not related to the sequence length of the input data, we can achieve our goal and construct the path space for RNN.\n\nSecond, for your concerns about experiments, we present the experiment results with different random seeds to verify our improvement is significant.[QM1]  For the experiments for RCNN, our improvements are significant compared with the number shown in the original Recurrent CNN paper (Liang, Ming, and Xiaolin Hu. CVPR. 2015.). We can conclude that as the task becomes more difficult (from MNIST to CIFAR, from shallower NN to deeper NN), the improvement of our proposed optimization methods becomes larger.\n\nThanks for your suggestions, we have added curves for vision task in Section D in the appendix.\n \nWe hope we have addressed your concerns and you can reconsider the rate. "}, "signatures": ["ICLR.cc/2020/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rked_6NFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper638/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper638/Authors|ICLR.cc/2020/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168479, "tmdate": 1576860554103, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment"}}}, {"id": "BJg3raqoiH", "original": null, "number": 1, "cdate": 1573788996154, "ddate": null, "tcdate": 1573788996154, "tmdate": 1573794154846, "tddate": null, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment", "content": {"title": "Response to reviewers", "comment": "We sincerely thank all the reviewers for their efforts on our paper and their helpful suggestions!  \n\nWe have uploaded a new version of our paper, and the main revisions are listed below: \n\n1.\tWe add a \u201ctop-down\u201d introduction of our work to the end of the introduction.\n\n2.\tWe add some experiments. Specifically, we repeat each sequential MNIST experiment 3 times and show the mean and variance of each experiment in Table 1 and 7. Additionally, we add the Adam  as our baseline and compare our G-Adam (Adam in path space) optimizer  with Adam. The results are shown in Table 7 in the Appendix.\n\n3.\tAccording to suggestions from Reviewer #3, \n(1) we revise the definition of path in RNN and the notation for clarity;\n(2)  we add a formal definition of reduction graph in Definition 3;\n(3)we add more details about the Skeleton Method. \n\n4.\tWe fix some typos and grammar errors and proofread the proofs in Appendix.\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rked_6NFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper638/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper638/Authors|ICLR.cc/2020/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168479, "tmdate": 1576860554103, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment"}}}, {"id": "B1eWBC5ooS", "original": null, "number": 2, "cdate": 1573789240640, "ddate": null, "tcdate": 1573789240640, "tmdate": 1573789564326, "tddate": null, "forum": "rked_6NFwH", "replyto": "BJgnP_s15B", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment", "content": {"title": "Response To Reviewer #1", "comment": "Thanks for your valuable comments about the clarity. \n\nPlease note that the paragraph 5-7 in the introduction in the previous version of our paper summarize our main works and contributions. In the new version of our paper, we add a \u201ctop-down\u201d introduction to the whole work to the end of the introduction. For your concern about the experiments, we can conclude that as the task becomes more difficult (from MNIST to CIFAR, from shallower NN to deeper NN), the improvement of our proposed optimization methods becomes larger.\nFor the recurrent CNN experiments, our improvements are significant compared with the number shown in Recurrent CNN paper (Liang, Ming, and Xiaolin Hu. CVPR. 2015.). \n \nBesides, we add Adam optimizer as a new baseline. The results are shown in Table 7 in the Appendix. Additionally, we repeat each sequential MNIST experiment 3 times and list the mean and variance of each result to show our improvements are stable. "}, "signatures": ["ICLR.cc/2020/Conference/Paper638/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"required": true, "description": "Brief summary of your comment.", "order": 0, "value-regex": ".{1,500}"}, "comment": {"required": true, "description": "Your comment or reply (max 5000 characters). Add TeX formulas using the following formats: $In-line Formula$ or $$Block Formula$$", "order": 1, "value-regex": "[\\S\\s]{1,5000}"}}, "forum": "rked_6NFwH", "readers": {"values-dropdown": ["everyone", "ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/AnonReviewer.*", "ICLR.cc/2020/Conference/Paper638/Reviewers/Submitted", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response"}, "writers": {"values-copied": ["ICLR.cc/2020/Conference", "{signatures}"]}, "signatures": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+|ICLR.cc/2020/Conference/Paper638/Authors|ICLR.cc/2020/Conference/Paper638/Area_Chair[0-9]+|ICLR.cc/2020/Conference/Program_Chairs"}}, "readers": ["everyone"], "tcdate": 1569504168479, "tmdate": 1576860554103, "super": "ICLR.cc/2020/Conference/-/Comment", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "invitees": ["ICLR.cc/2020/Conference/Paper638/Authors", "ICLR.cc/2020/Conference/Paper638/Reviewers", "ICLR.cc/2020/Conference/Paper638/Area_Chairs", "ICLR.cc/2020/Conference/Program_Chairs"], "id": "ICLR.cc/2020/Conference/Paper638/-/Official_Comment"}}}, {"id": "Skg8bjXTFH", "original": null, "number": 1, "cdate": 1571793661698, "ddate": null, "tcdate": 1571793661698, "tmdate": 1572972570612, "tddate": null, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Official_Review", "content": {"experience_assessment": "I have read many papers in this area.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #3", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "This paper aims to improve the training of recurrent neural networks (RNN) with ReLU activations by optimizing in the path space instead of the weight space. Studies on multi-layered perceptrons (MLP) and convolutional neural networks (CNN) have shown that these architectures are positively scale invariant (PSI), however traditional SGD optimizes in the weight space that does not have this property. It has been shown in prior work that this mismatch can slow down the optimization process for SGD and it has been demonstrated that optimizing in the so called path space, which has the PSI property, can be faster and more efficient.  The authors of this paper aim to extend an existing path-space framework (G-SGD) that is used for ReLU networks to facilitate RNNs.  To tackle the challenge posed by the time-dependency of RNNs, they use a static representation, called reduction graph, to define the path space. First, they prove that any path in the original RNN graph can be easily obtained from paths in its reduction graph by simple operations. Then, they define the basis for the path space in the reduction graph, and show that basis paths are sufficient to represent the output of the RNN. They propose a method (Skeleton method) to generate a basis path in the reduction graph and specify an equivalent of the G-SGD algorithm for RNNs.  Through numerical simulations they demonstrate that (1) G-SGD for RNNs has a fairly low additional computational cost compared to SGD and (2) G-SGD for RNNs achieves better test accuracy compared with SGD and an additional path space based approach.\n\nEven though the paper has some original contribution, due to issues with the delivery, clarity and execution of the paper I would lean to reject it at this point. I would be willing to change my decision if the authors made significant changes to the paper in aspects detailed below. The paper also has many grammatical and typographical mistakes that hinders the reader in understanding key ideas.\n\nFirst,  Section 2 could be much better explained. It seems like the notation introduced for a node and its value are used interchangeably, the edges are not clearly defined and Definition 1 is extremely confusing (what is s\u2019 an arbitrary time step or all time steps between t and t+s, where is the list of weights mentioned in the first sentence and so on).  The definition for a key concept in the paper, the value of the path, is also not clear due to the lack of sufficient definition of the graph itself. \n\nSecond, in Section 3 the definition of the reduction graph and how it is obtained from the directed graph is not clear, which is problematic since it is one of the key ideas in the paper. Clear explanation of a recurrent edge is also missing.\n\nThird, in Section 4 terms such as \u2018without recurrent edges parts\u2019  that are used in defining the Skeleton method are very lax and needs to be defined more rigorously. It is hard to understand what the boxed steps are saying. Moreover,  in Algorithm 2 division by a matrix occurs at several points. Does this mean entry-wise division? What does the function BP(.) do? I couldn\u2019t find it defined anywhere.\n\nLastly, in Section 5 it would be fair to make a comparison with adaptive optimization methods such as Adam or AdaGrad as it is done in [Neyshabur et al.: Path SGD]. \n\nI would like to provide some additional feedback that do not impact my decision, but could potentially improve the paper. In Section 2: I cannot find an explanation what an \u2018unfold directed graph\u2019 means. In eq. (3) x is used without any prior explanation/definition. In Section 3, the message around eq. (4) is not clear, how we pick p2\u2026p5? Moreover, in the definition of basis paths P is used without explaining what it denotes.  Comments on Appendix: in Table 4 it is confusing to highlight the results corresponding to the papers method, because it suggests that these are the best results across the table, however there is a lower value in the second row. In B.1. taking derivative w.r.t p doesn\u2019t seem to make sense, it should probably be v_p (this holds for the whole derivation). In Table 6, it would be a clearer to present relative (%) results than absolute time cost.  Comment on the proofs: they are extremely verbose and could potentially be explained with more math and less words. It is difficult to follow what is going on. There is also some notation not introduced before in eq. (28). In general, I would recommend only numbering equations that are referred to in the text. "}, "signatures": ["ICLR.cc/2020/Conference/Paper638/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper638/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575215801805, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper638/Reviewers"], "noninvitees": [], "tcdate": 1570237749243, "tmdate": 1575215801819, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper638/-/Official_Review"}}}, {"id": "BJgnP_s15B", "original": null, "number": 2, "cdate": 1571956835985, "ddate": null, "tcdate": 1571956835985, "tmdate": 1572972570571, "tddate": null, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Official_Review", "content": {"experience_assessment": "I have published in this field for several years.", "rating": "3: Weak Reject", "review_assessment:_thoroughness_in_paper_reading": "I read the paper thoroughly.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #1", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review": "Motivation: The authors motivate their work with the observation that neural networks with ReLU activations are positively scale invariant. Recent works have proposed parameter space called path space to leverage this insight for feedforward as well as CNNs, but this is not really the case for Recurrent Neural Networks (due to the recurrent structure and the parameter sharing). \n\nContribution:  The authors  construct path space for RNN to employ optimization algorithms in path space. The intuition is to leverage the reduction graph of RNN to removes the influence of time-steps. Reduction graph only contains information about the weight connectivity patterns and not about the time steps. Hence,  the number of paths in reduction graph is fixed (i.e independent of number of time steps.\n\nClarity: The clarity of the paper can be improved a bit. It might be useful to give a \"top-down\" introduction somewhat at the end of introduction. \n\nExperimental Results: In order to validate the proposed method, the authors conduct experiment on sequential MNIST tasks, as well as language modelling task. The results are a bit weak in general, and improvements are very minor over the vanilla RNN baseline. It might be interesting to compare to other baselines as in PathSGD paper.\n\n\n\n"}, "signatures": ["ICLR.cc/2020/Conference/Paper638/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper638/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575215801805, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper638/Reviewers"], "noninvitees": [], "tcdate": 1570237749243, "tmdate": 1575215801819, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper638/-/Official_Review"}}}, {"id": "ryxFe1MT9B", "original": null, "number": 3, "cdate": 1572835057422, "ddate": null, "tcdate": 1572835057422, "tmdate": 1572972570526, "tddate": null, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "invitation": "ICLR.cc/2020/Conference/Paper638/-/Official_Review", "content": {"rating": "1: Reject", "experience_assessment": "I do not know much about this area.", "review_assessment:_checking_correctness_of_derivations_and_theory": "I assessed the sensibility of the derivations and theory.", "review_assessment:_checking_correctness_of_experiments": "I assessed the sensibility of the experiments.", "title": "Official Blind Review #2", "review_assessment:_thoroughness_in_paper_reading": "I read the paper at least twice and used my best judgement in assessing the paper.", "review": "This paper proposes a parameter space, called path space for RNNs with ReLU activation. For the construction of the path space, this paper utilises a reduction graph approach to minimise the difficulty brought by the parameter-sharing scheme in RNNs. Furthermore, the authors propose a Skeleton method for the efficient identification of basis path for RNNs in reduction graph.\nThe G-SGD approach used in this work is not new and has been initially applied by Meng et al., 2018.\nMeng et al., 2018 have primarily introduced G-space for neural networks and designed SGD in G-space (G-SGD) to optimise the value vector of the basis paths of neural networks.  The main difference between this manuscript and previous works (Meng et al., 2018 and Neyshabur et al., 2016.) is that the authors apply G-SGD to RNNs instead of MLPs/CNNs.\nThe reviewer is not sure if this marginal difference is sufficient to get the paper accepted in ICLR. The structure, the methodology, and the content of this paper is highly similar to the work published by Meng et al., 2018.  \nThis paper claims to obtain significantly more effective RNN models than using optimization methods in the weight space without providing any statistically significant measures.\nThe authors could have added a visual performance comparison (e.g., training loss/epoch curve and test error/epoch curve) between the RNN G-SGD and other state-of-the-art approaches.\nThe theorems and their proofs are fine (similar to Meng et al. (2018)).\n "}, "signatures": ["ICLR.cc/2020/Conference/Paper638/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2020/Conference/Paper638/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"authorids": ["11271012@bjtu.edu.cn", "meq@microsoft.com", "wche@microsoft.com", "ytliu@bjtu.edu.cn", "mazm@amt.ac.cn", "tie-yan.liu@microsoft.com"], "title": "Path Space for Recurrent Neural Networks with ReLU Activations", "authors": ["Yue Wang", "Qi Meng", "Wei Chen", "Yuting Liu", "Zhi-Ming Ma", "Tie-Yan Liu"], "pdf": "/pdf/033aae6dcdcd2317c0a560470ba486f602f6e7ce.pdf", "TL;DR": "We construct a new parameter space, called path space, for the ReLU RNN and employ optimization algorithms in it. We can obtain more effective RNN models in path space than using conventional optimization methods in the weight space.", "abstract": "It is well known that neural networks with rectified linear units (ReLU) activation functions are positively scale-invariant (i.e., the neural network is invariant to positive rescaling of weights). Optimization algorithms like stochastic gradient descent that optimize the neural networks in the vector space of weights, which are not positively scale-invariant. To solve this mismatch, a new parameter space called path space has been proposed for feedforward and convolutional neural networks. The path space is positively scale-invariant and optimization algorithms operating in path space have been shown to be superior than that in the original weight space. However, the theory of path space and the corresponding optimization algorithm cannot be naturally extended to more complex neural networks, like Recurrent Neural Networks(RNN)  due to the recurrent structure and the parameter sharing scheme over time. In this work, we aim to construct path space for RNN with ReLU activations so that we can employ optimization algorithms in path space.  To achieve the goal, we propose leveraging the reduction graph of RNN which removes the influence of time-steps, and prove that all the values of whose paths can serve as a sufficient representation of the RNN with ReLU activations. We then prove that the path space for RNN is composed by the basis paths in reduction graph, and design a \\emph{Skeleton Method} to identify the basis paths efficiently. With the identified basis paths, we develop the optimization algorithm in path space for RNN models. Our experiments on several benchmark datasets show that we can obtain significantly more effective RNN models in this way than using optimization methods in the weight space. ", "keywords": ["optimization", "neural network", "positively scale-invariant", "path space", "deep learning", "RNN"], "paperhash": "wang|path_space_for_recurrent_neural_networks_with_relu_activations", "original_pdf": "/attachment/a4606d680a81deabb4c9e9725e86bfd1da8d8df2.pdf", "_bibtex": "@misc{\nwang2020path,\ntitle={Path Space for Recurrent Neural Networks with Re{\\{}LU{\\}} Activations},\nauthor={Yue Wang and Qi Meng and Wei Chen and Yuting Liu and Zhi-Ming Ma and Tie-Yan Liu},\nyear={2020},\nurl={https://openreview.net/forum?id=rked_6NFwH}\n}"}, "tags": [], "invitation": {"reply": {"content": {"experience_assessment": {"required": true, "order": 4, "description": "Please make a selection that represents your experience correctly", "value-radio": ["I have published in this field for several years.", "I have published one or two papers in this area.", "I have read many papers in this area.", "I do not know much about this area."]}, "rating": {"value-dropdown": ["1: Reject", "3: Weak Reject", "6: Weak Accept", "8: Accept"], "order": 3, "required": true}, "review_assessment:_checking_correctness_of_experiments": {"required": true, "order": 7, "description": "If no experiments, please select N/A", "value-radio": ["I carefully checked the experiments.", "I assessed the sensibility of the experiments.", "I did not assess the experiments.", "N/A"]}, "review_assessment:_thoroughness_in_paper_reading": {"required": true, "order": 5, "description": "If this is not applicable, please select N/A", "value-radio": ["I read the paper thoroughly.", "I read the paper at least twice and used my best judgement in assessing the paper.", "I made a quick assessment of this paper.", "N/A"]}, "title": {"value-regex": "Official Blind Review #[0-9]+", "order": 1, "required": true, "description": "Please replace NUM with your AnonReviewer number (it is the number following \"AnonReviewer\" in your signatures below)", "default": "Official Blind Review #NUM"}, "review": {"value-regex": "[\\S\\s]{500,200000}", "order": 2, "description": "Provide your complete review here (500 - 200000 characters). For guidance in writing a good review, see this brief reviewer guide (https://iclr.cc/Conferences/2020/ReviewerGuide) with three key bullet points.", "required": true}, "review_assessment:_checking_correctness_of_derivations_and_theory": {"required": true, "order": 6, "description": "If no derivations or theory, please select N/A", "value-radio": ["I carefully checked the derivations and theory.", "I assessed the sensibility of the derivations and theory.", "I did not assess the derivations or theory.", "N/A"]}}, "forum": "rked_6NFwH", "replyto": "rked_6NFwH", "readers": {"values": ["everyone"], "description": "Select all user groups that should be able to read this comment."}, "nonreaders": {"values": []}, "writers": {"values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2020/Conference/Paper638/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1575215801805, "duedate": 1572706740000, "multiReply": false, "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2020/Conference/Paper638/Reviewers"], "noninvitees": [], "tcdate": 1570237749243, "tmdate": 1575215801819, "super": "ICLR.cc/2020/Conference/-/Official_Review", "signatures": ["ICLR.cc/2020/Conference"], "writers": ["ICLR.cc/2020/Conference"], "id": "ICLR.cc/2020/Conference/Paper638/-/Official_Review"}}}], "count": 9}