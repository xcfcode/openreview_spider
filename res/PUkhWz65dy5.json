{"notes": [{"id": "PUkhWz65dy5", "original": "URoDNCWHfD", "number": 3432, "cdate": 1601308380925, "ddate": null, "tcdate": 1601308380925, "tmdate": 1613736694696, "tddate": null, "forum": "PUkhWz65dy5", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 21, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "8TaoxGqYVaP", "original": null, "number": 1, "cdate": 1610040509409, "ddate": null, "tcdate": 1610040509409, "tmdate": 1610474117023, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Decision", "content": {"title": "Final Decision", "decision": "Accept (Spotlight)", "comment": "All reviewers are positive or very positive about this work. The authors successfully addressed all questions. I believe this paper should be accepted."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040509396, "tmdate": 1610474117007, "id": "ICLR.cc/2021/Conference/Paper3432/-/Decision"}}}, {"id": "M0--Zyjkn1f", "original": null, "number": 2, "cdate": 1603735562321, "ddate": null, "tcdate": 1603735562321, "tmdate": 1605886986678, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review", "content": {"title": "Official Blind Review #3", "review": "Given a rewardless environment MDP, the authors want to find a set of policies for the worst case reward function. Their process involves two steps: first to select the right set of policies and second to combine them to generate a new policy. The policy selection is made with the only goal to maximize the expected return of highest achieving policy of the set in the worst-case reward function (Equation (7)).\n\nUnfortunately the submission suffers from several serious weaknesses:\n- although the reward-set setting is quite general: within a ball in the feature space, it includes very unnatural cases that make the problem artificially too complex, in my opinion. Indeed, usually, one should consider that the worst case reward function r_min in a reward function family R is the one that is minimal in every state-action-state : r_min(s,a,s') \\eqdef \\inf_{r\\in R} r(s,a,s'). In this case, the solution to equation (7) is straightforwardly the single policy that optimizes the return on r_min. Please discuss more the interest of considering r=\\psi.w with w in a ball (it implies that if some feature takes sometimes positive and sometimes negative values, there is not clear w_min, and therefore no clear r_min).\n- besides ensuring that SMP performance is at least achieved, could the authors elaborate a bit more on why optimize the policy set according to SMP?\n- the authors never theoretically consider combining the policy, apart for stating that a good combination of policy should achieve higher performance than the best of the policy set. For clarity, I would recommend to either stick to the simplest setting of choosing the best policy given a reward function, or to consider policy election that take into account the way the policies are going to be used/combined.\n- the formalization is messy and sometimes unnecessarily confusing. Please see the series of comments below:\n- Definition 3 and Eq. 5: the argmax returns an index, not a policy. (minor)\n- v is not a value, it's a policy performance. It has been very confusing to me, as it led me to think for too long that Lemma 1 was false: choosing the policy that maximizes the value in each state is a form policy improvement that may lead to policies that are stricty better than the best of the policies. Also, I would not use the notation v, that is usually the value-function: a function of the station and not the policy performance like here, i.e. the expectation of the value-function over the initial states distribution. (easy to fix)\n- Definition 4: the argmax returns an action not a policy. (minor)\n- Equation 7: \\Pi lives in? Also instead of max_{\\psi\\in...} \\psi.w, I would use max_{i\\in [n]} \\psi^i.w. (minor)\n- Lemma 3: what is d? (minor)\n- I am still not understanding Definition 6 and Theorem 2. How do we know that the worst-case reward is unique? If we keep only the policies that achieve max performance on \\overline{w}, then we probably only keep one? How do we ensure that there is not another w that makes this policy (or set of policies) to fail?\n\nFor all these reasons, I recommend to reject the submission.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075885, "tmdate": 1606915802780, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3432/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review"}}}, {"id": "8PwZmjTaJGl", "original": null, "number": 18, "cdate": 1605886955915, "ddate": null, "tcdate": 1605886955915, "tmdate": 1605886955915, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "MaIYY1byCQa", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "I've raised my score to accept", "comment": "I'd like to thank the authors for the detailed explanations. They made good points on the interest of studying the $\\ell_2$ ball rather than the $\\ell_infty$ one. As a consequence, I recommend accepting the paper."}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "MaIYY1byCQa", "original": null, "number": 16, "cdate": 1605812886034, "ddate": null, "tcdate": 1605812886034, "tmdate": 1605812886034, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "aW2yBwLC3Hb", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to Reviewer 2, cont.", "comment": "We would like to thank the reviewer for their response. The reviewer raised a good point. There are many reasons to choose the $\\ell_2$ ball: it includes all the directions; the magnitude of the reward doesn\u2019t change the optimal policy in tabular MDPs; in robust optimization it is used when the uncertainty about some parameter is Gaussian and consequently the scaled $\\ell_2$ ball contains the true parameter with high probability; it is the standard assumption in related work and specifically in Apprenticeship Learning. We now highlight one more property that distinguishes the $\\ell_2$ ball from the $\\ell_\\infty$ ball and is in particular relevant to the reviewer\u2019s question. \n\nImplied from the reviewer\u2019s response, is that a possible solution to Eq 7 is to solve the following problem:\n$$\\max_{\\pi\\in\\Pi} \\min_{w\\in\\mathcal{W}} \\psi(\\pi)\\cdot w. $$\n\nThis formulation is similar to Apprenticeship Learning (AL) without an expert, which is different from our approach that is hierarchical. We refer the reviewer to our latest response to Reviewer 3 for a discussion on the similarities between our approach and AL and to Sec B in the supplementary material for more details. \nAs the reviewer suggested, in the case where $\\mathcal{W}$ is the $\\ell_\\infty$ ball, the internal minimization problem in the above has a single solution - a vector with -1 in all of its coordinates. However, with other norm balls the solution to the internal minimization problem is a function of the policy. In the case of the $\\ell_2$ ball, it is the negative SFs normalized. This is important, since it clarifies that solving the min-max AL problem is not as easy and typically requires solving an MDP in each iteration (see the reference for AL above for more details).\n\nNow, the fact that the worst case reward is a function of the policy (or the policy set) forces it to make a tradeoff -- it has to \u201cchoose\u201d the coordinates it \u201cwants\u201d to be more adversarial for. This tradeoff is what encourages the worst case reward to be diverse across iterations (w.r.t different sets) and as a result it induces a diverse set of policies. Diversity was an important goal in this work -- in addition to minimizing Equation 7, we were interested in the diversity of the  policies that result from that process.\nThank you for pointing this out. We hope that we answered your question. We will add this discussion when we introduce the $\\ell_2$ ball in the paper. \n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "aW2yBwLC3Hb", "original": null, "number": 15, "cdate": 1605799841554, "ddate": null, "tcdate": 1605799841554, "tmdate": 1605799841554, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "FWZtEK2_tb", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Focus on the ball reward definition", "comment": "First of, sorry for the bad formatting, I fixed it. I've also noticed only now that we could use the latex math mode and I'll use it from now on. Finally, sorry for not having been able to answer before.\n\nThank you for your clarifications. They addressed most of my point but I am still confused about my first one, which is also unfortunately my main concern. Let me first recall my initial questioning:\n- although the reward-set setting is quite general: within a ball in the feature space, it includes very unnatural cases that make the problem artificially too complex, in my opinion. Indeed, usually, one should consider that the worst case reward function $r_{min}$ in a reward function family R is the one that is minimal in every state-action-state : $r_{min}(s,a,s') = \\inf_{r\\in R} r(s,a,s')$. In this case, the solution to equation (7) is straightforwardly the single policy that optimizes the return on $r_{min}$. Please discuss more the interest of considering $r=\\psi \\dot w$ with w in a ball (it implies that if some feature takes sometimes positive and sometimes negative values, there is not clear $w_{min}$, and therefore no clear $r_{min}$).\n\nNow, indeed, I've been mistaken, because I was thinking about the $\\ell_\\infty$ ball (infinite norm), not the $\\ell_2$ ball (Euclidean norm). Can we agree that, if we take the $\\ell_\\infty$ ball instead, then the solution is trivial? So, why use the $\\ell_2$ ball? Is it closer to the model uncertainty we need to represent? If this is the case, what is the loss of using the $\\ell_\\infty$ ball instead?"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "xysqyfLEoyN", "original": null, "number": 3, "cdate": 1603868577727, "ddate": null, "tcdate": 1603868577727, "tmdate": 1605572210852, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review", "content": {"title": "Strong and non-trivial theoretical contributions, interesting empirical insight that connects directly to the theory", "review": "Summary: the authors propose to solve a family of related tasks with shared features and rewards that are linear in the features and equivalent up to scaling factor. The main contributions are as follows:\n- a novel framework for analyzing a broad family of generalized policies (policies that are generalized to arbitrary rewards in the task space), including the concept of a set improving policy (SIP), and providing two practical examples that fit this definition, namely the worst case set max policy (SMP) and the well known and studied generalized policy iteration (GPI). It is shown that it is always better to use GPI over SMP, making it an instance of SIP. \n- a novel iterative method for building a policy library for solving the worst-case reward, formulated as a convex optimization problem, along with policy improvement guarantees, an informed method for stopping the algorithm, and the ability to remove redundant policies (termed inactive policies)\n- an empirical evaluation that connects the proposed method to learning a policy library with a diverse set of skills. The theoretical results are also validated experimentally, on a grid world example and control problems from Deepmind.\n\nPros:\n- the work is of very high quality, all motivations seem sound and the theoretical results seem correct\n- the idea of active task selection for building the policy library is very interesting, and it is surprising that this has not been considered within the framework of Barreto et al., 2017 so far\n- the work could be of significance in the apprenticeship/curriculum/meta-RL community, and it is nice to see a more theoretical treatment of this topic\n\nQuestions:\n- If my understanding is correct, the authors use the orthogonal and random basis to propose w at each iteration, but evaluate the resulting SMP policies with respect to the optimized rewards from (8). I am wondering if this is a fair evaluation for the baselines, given that the policies are always evaluated on $w_t^{SMP}$, or whether a new set of tasks (a proper \"test\" set) sampled from B (the standard ball) should be used to fairly compare (8) with the baselines? This would really test the generalization of the method on new instances as well, and is also often standard in the literature for evaluating the performance of a learning policy set. In other words, how robust is the resulting policy library to solving new task instances not previously seen before?\n- Also, one thing that could explain the poor performance of the orthogonal baseline is that the reward seems to be quite sparse when most of the basis elements are set to zero (in the one-hot phi case, wouldn't they be almost always uninformative?) In this case, a more suitable baseline that directly targets diversity could be defined as finding the $w_1, w_2 \\dots w_T$ such that their coverage of the task space is maximized under some prior belief over w (e.g. the standard ball). If I am not mistaken, this problem is similar to the maximum coverage or voronoi tessellation problem, which could be solved in advance and then deployed. (e.g. Arslan, 2016)\n- Performing well relative to the worst-case performance seems reasonable so that the agent does not do poorly on any one task, but it could also be overly conservative. That is, could there be situations where optimizing the worst case leads to the agent not successfully completing the desired objective (e.g getting stuck on locally optimal solution)? \n- at each iteration when the new optimal policy is learned with respect to $w_\\Pi^{SMP}$, is the idea of SMP or GPI and previously learned policies used to help learn this new policy, or is it learned entirely from scratch (e.g. by simple epsilon-greedy)?\n\nMinor comments:\n- the legends in Figure 1a/b and the axis font in Figure 1c could be increased, same with Figure 2\n- is the $\\max_i$ necessary in equation (8)?\n\nOverall, this works proposes a coherent theory for policy improvement, that also leads to useful implementation and interesting empirical insight (and cool visualizations). It can often be hard to obtain all of these at once.\n\nArslan, Omur, and Daniel E. Koditschek. \"Voronoi-based coverage control of heterogeneous disk-shaped robots.\" 2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016.", "rating": "7: Good paper, accept", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075885, "tmdate": 1606915802780, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3432/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review"}}}, {"id": "gS93_EoucxB", "original": null, "number": 14, "cdate": 1605462575808, "ddate": null, "tcdate": 1605462575808, "tmdate": 1605462575808, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "M4zmYwbKKy1", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thank you very much for the clarifications, and for the updated paper and experiments. The authors' interpretation of risk/robustness as it relates to the framework is quite interesting. It could be interesting to see the emergence of \"safer\" behaviors of the agent on complex tasks such as driving, in the absence of the true reward, in future work. I am happy with the response and do not have further questions."}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "fyjAdsTpXor", "original": null, "number": 13, "cdate": 1605453942851, "ddate": null, "tcdate": 1605453942851, "tmdate": 1605453942851, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "nNtS773KdHC", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to authors", "comment": "I would like to thank the authors for the clarifications. I am happy with the response, as I believe it addresses the issues that I raised in my review."}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "H6BFLnJGGEL", "original": null, "number": 12, "cdate": 1605294872594, "ddate": null, "tcdate": 1605294872594, "tmdate": 1605294872594, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "WGleixvAFsu", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": " Response to reviewer #2", "comment": "We would like to thank the reviewer for replying to our response and for increasing their score. We appreciate your feedback and believe that it helped us to improve the paper. \n\nRegarding the clarification question: indeed, these are the tasks in the grid world from Section 5. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "o7lbMx2EyM", "original": null, "number": 1, "cdate": 1603732767741, "ddate": null, "tcdate": 1603732767741, "tmdate": 1605294075431, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review", "content": {"title": "Overall interesting idea. Need some clarification.", "review": "This was a well-written, and interesting paper to read!\n\nI went over the paper many times, and I am still failing to see the use case for such an approach. I have some questions and comments that need some clarification for me to properly evaluate the submission. Please, take the time to answer the following, so my review can better reflect the paper.\n\n1 - The theory developed in the paper relies on reward functions that can be represented as linear combinations of the features of the MDP. This seems to be restrictive, and intuitively, this would be the exception rather than the rule.\nWhat class of problem could be modeled under this restriction?  In many problems, there is no linear reward function that would allow an agent to achieve the desired behavior, so these techniques would not be helpful. What are some practical setting where this approach would be beneficial?\n\n2 - Lemma 3... this statement is putting an upper-bound on the worst case performance, but since the paper focuses on improvement of worst case performance, it would be beneficial to have a lower bound, but an upper bound doesn't seem too useful.   Essentially, this lemma is saying \"I can guarantee that the worst-case won't be better than this upper bound, and that for some MDP with linear reward function this upper bound is attainable.\" The problem is that we don't know what that MDP is, how likely it is that we would find it, and this lemma allow for the worst case performance to be arbitrarily bad.\nI don't think this lemma, as is, is particularly useful.\n\n3 - On the experimental section, I think there's a baseline that should be included that's missing. What if we have 1 policy and add a task descriptor or extra features to the features vector that corresponds to the type of task? How would the performance empirically compare?\n\n4 - In the learning curves for fig 1.a or 2.a, what does \"value\" (y-axis) represent? If it the return of the agent after training? If so, is it using the extrinsic reward or the transformed linear reward described in line 5 of \"DeepMind Control Suite\"?\n\n5 - Based on equation 4, for definition 2 of SIP. There is always a trivial set improving policy, right? That would correspond to picking the policy for max(v^i_w).\n", "rating": "6: Marginally above acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075885, "tmdate": 1606915802780, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3432/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review"}}}, {"id": "WGleixvAFsu", "original": null, "number": 11, "cdate": 1605294015419, "ddate": null, "tcdate": 1605294015419, "tmdate": 1605294015419, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "z0mhpzqQFy", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to authors #2", "comment": "Thank you for addressing the concerns in the above comments.\nIt's definitely interesting the fact that the performance seems to be a bit better than the baseline, to be honest, I would have expected it to not perform as well.\n\nJust to clarify, these are the same tasks on the grid-world from Section 5, correct?\n\n\nI'm adjusting my score based on your responses."}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "siTr_0UF0vY", "original": null, "number": 8, "cdate": 1605273549930, "ddate": null, "tcdate": 1605273549930, "tmdate": 1605291860027, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Revised version", "comment": "Dear reviewers, we have uploaded a revised version of our paper to reflect your comments. For your convenience, most of the changes are marked in blue color. Smaller notation changes were also fixed. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "hPekISQ_MRH", "original": null, "number": 10, "cdate": 1605290899924, "ddate": null, "tcdate": 1605290899924, "tmdate": 1605291601259, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Experiments on a test set of rewards", "comment": "Dear reviewers, we would like to thank Reviewers 1 and 4, for expressing interest in the performance of our algorithm on a test set of rewards. This is an interesting setup which we didn't consider when we submitted our paper. We have now uploaded a new version of the paper where we performed this experiment in the supplementary material Section D. We hope that this is what you referred to in your review, but if it isn't please let us know. \n\nFor your convenience, we also provide a short description of the experiment here. We trained our algorithm and the two baselines in the same manner as we did before. During evaluation, we tested the performance of each method on a holdout set of rewards that were sampled uniformly over the unit ball. The results suggest that our algorithm achieves better performance than the two baselines when measured on this set of unseen rewards. More importantly, to achieve the same level of performance, our algorithm requires significantly fewer policies than the baselines. For more details, please refer to the Supplementary D. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "z0mhpzqQFy", "original": null, "number": 9, "cdate": 1605289361659, "ddate": null, "tcdate": 1605289361659, "tmdate": 1605289361659, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "UeyCOamkc2x", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to authors", "comment": "Thank you for taking the time to address the comments, I really appreciate the effort.\nBelow are my comments to the response.\n\n- What is a use-case for this approach? So, you give an example of a robot learning locomotion in an unsupervised manner, and argue that this would help the agent prepare for worst-case in subsequent tasks. I'm trying to picture what this would look like in practice. Is what you are suggesting that by simply learning locomotion, the agent might not have covered scenarios that would encounter in follow up tasks, so worst-case performance in the locomotion skill could be poor enough that the new tasks becomes unlearnable?\n\n - Thanks for the clarification in the linear features. This makes sense.\n\n-  Thanks for the clarification in figure 4. It would also be useful to see how the actual performance of the agent compares in traditional methods. The fact that the worst-case performance is better  than for other methods would not be very useful, if the best-case performance is not good enough to complete the task. Even if for a few domains/task, I think this figure must be there.\nIn other words, by ensuring that our worst-case performance is not too bad...what are we losing from the best case performance. Depending on the scenario, it might be acceptable or it might not.\nIf time permits, please include such results.\n\n- On Lemma 3...thanks for the clarification. I see now how that would be useful.\n\n- I understand the time constraint for providing the one baseline I suggested. If you can include that, it will be very appreciated, but if you can't I won't hold it against you :)\nIf you can only add one of the suggestions I have, please make it the comparison I suggested to add for figure 4.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "UeyCOamkc2x", "original": null, "number": 7, "cdate": 1605273178158, "ddate": null, "tcdate": 1605273178158, "tmdate": 1605273221631, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "NMZ9hywLQxe", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to Reviewer 1, cont. ", "comment": "Question 2: lemma 3. \nGiven a set of policies, we have a lower bound on the worst case performance of the SMP. This is equivalent to computing the worst case reward w.r.t the current set and measuring the performance of the SMP. We do not, however, have a lower bound on what that value would be given that we run our algorithm for n iterations. That would indeed be a great contribution; we will mention it in the paper as a promising direction for future research. \n\nThat said, we want to emphasize that Lemma 3 is useful, as it provides a clear criterion for the convergence of our algorithm: whenever the upper bound provided in the lemma is achieved, we can stop adding more policies. We would like to point out that this in fact happens in practice, as we illustrate in our experiments (Figure 1a). This is surprising since often upper bounds are not attainable in practice, as the reviewer implied, which makes Lemma 3 even more relevant. \n\nQuestion 3 (experiments). \nThis is an interesting suggestion, and we are working on performing this experiment, although we are not sure if we can make it on time. Our intuition is that that sort of a baseline will be less practical to use as it typically requires many iterations until it will be able to generalize to new tasks (e.g. UVFA [4]) while our algorithm performs well after a few iterations.\n\nQuestion 4, value in figures. \nThe values shown in Figures 1a and 2a correspond to the SMP\u2019s value, given in Definition 5. The way we compute it as follows. For each policy computed by our algorithm, we estimate the associated successor features (SFs) using Monte Carlo estimation: that is, we fix the policy and run at multiple times to estimate the SFs. We run it enough times to guarantee that the estimate is accurate (see, for example, theorem 2 in [2] or lemma 5 in [3],  for a concentration bound on the approximation error for a given number of samples). Now, given a set o n policies pi (and their associated SFs), we compute the worst possible w, which we call w*, (Equation 8). We then compute the inner product between the n SFs and w* and pick the maximum of these n values (Definition 5). This is the value shown in the figures, which represent the worst possible performance of the set of n policies across all possible tasks.   \n\nQuestion 5. \nYes, you are correct: this SIP is exactly the SMP (Definition 3).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "NMZ9hywLQxe", "original": null, "number": 6, "cdate": 1605273088212, "ddate": null, "tcdate": 1605273088212, "tmdate": 1605273088212, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "o7lbMx2EyM", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "We would like to thank the reviewer for their feedback. Our response is split to two parts.\n\nQuestion: \u201cwhat can be a use case of such an approach?\u201d We believe that we presented an interesting learning framework in this work: in a world without an explicit reward, can an agent define goals to itself and discover interesting behaviors by doing so? Our experiments in DM control suggest that diverse and interesting locomotion skills (such as salta jumping) can emerge from following this process which is an interesting scientific observation. For a more concrete use case, imagine that a robot can teach itself how to move and locomote in an unsupervised discovery phase, later to use these skills when instructed to do more complicated tasks. Since the skills that the robot discovers prepare it for the worst case, some of them are likely to be useful in the future from a robustness perspective, that is, no matter what the robot discovered, we are guaranteed that the robot won\u2019t do too bad when faced with a new task. Thus, the learned skills can also be used to initialize the robot\u2019s behaviour, to be followed by a learning phase. Another interesting contribution of this work is the connection between robustness and diversity in RL: we have demonstrated that by optimizing for the worst case, a diverse set of skills emerges. \n\nQuestion 1 (the linearity assumption):\n\nIt is indeed true that for a *fixed* set of features the linearity assumption is restricting. However, it is this assumption that allows us to develop theory and thus provide theoretical guarantees for the proposed algorithm. Note that the guarantees we provide are in fact applicable in practice, as we illustrate in our experiments. For example, the experiments in the tabular MDP follow the theoretical framework exactly and achieve the upper bound that we developed (see more on that in our answer to point 2 below). The experiments in DM control are also very close to the theory, the only deviation is that we use DRL to optimize the policy for a given reward. We note that this agreement between theory and practice is in fact one of the strengths of our work, since more often than not there is a gap between the two.\n\nAll that said, we point out that in the most general scenario we are free to define the features themselves. Note that, although the rewards are linear in the features, the features themselves can be arbitrary nonlinear functions of state-action pairs. This means that when we are able to define the features the linear assumption is in fact not so strong: for example, in [1], the authors discuss how in the tabular case this is not a restriction at all, and how in continuous state spaces we can find features that approximate any reward function with a certain accuracy.   \n\nAs a somewhat counterintuitive observation, we note that in many problems it is in fact easy to handcraft simple features that generate useful behaviour. This is illustrated in our experiments with DM control, in which using the standard features provided in the suite we were able to generate rich behaviour. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) by simply using linear combinations of the standard features. \n\nThirdly, our experiments suggest that the assumption is not too restricting in interesting problems. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) under this assumption. \n\nLastly, we believe that it should be easy to generalize our approach to a more general setup where the reward is represented as a nonlinear (perhaps a DNN) of the features. In this case, the minimization over w will not be  convex but will still be possible via the same techniques (SGD). This kind of algorithm will resemble GAIL (with a GAN) and we believe that it is an exciting direction of research for future work.\n\n\n[1] Barreto, A., Hou, S., Borsa, D., Silver, D., & Precup, D. \"Fast reinforcement learning with generalized policy updates.\" Proceedings of the National Academy of Sciences (2020).\n[2] Abbeel, Pieter, and Andrew Y. Ng. \"Apprenticeship learning via inverse reinforcement learning.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n[3] Zahavy, Tom, Alon Cohen, Haim Kaplan, and Yishay Mansour. \"Apprenticeship Learning via Frank-Wolfe.\" AAAI (2020)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "FWZtEK2_tb", "original": null, "number": 5, "cdate": 1605272534552, "ddate": null, "tcdate": 1605272534552, "tmdate": 1605272534552, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "E1RGmrUN2P", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Linearity of the reward in the features", "comment": "It is indeed true that for a *fixed* set of features the linearity assumption is restricting. However, it is this assumption that allows us to develop theory and thus provide theoretical guarantees for the proposed algorithm. Note that the guarantees we provide are in fact applicable in practice, as we illustrate in our experiments. For example, the experiments in the tabular MDP follow the theoretical framework exactly and achieve the upper bound that we developed (see more on that in our answer to point 2 below). The experiments in DM control are also very close to the theory, the only deviation is that we use DRL to optimize the policy for a given reward. We note that this agreement between theory and practice is in fact one of the strengths of our work, since more often than not there is a gap between the two.\n\nAll that said, we point out that in the most general scenario we are free to define the features themselves. Note that, although the rewards are linear in the features, the features themselves can be arbitrary nonlinear functions of state-action pairs. This means that when we are able to define the features the linear assumption is in fact not so strong: for example, in [1], the authors discuss how in the tabular case this is not a restriction at all, and how in continuous state spaces we can find features that approximate any reward function with a certain accuracy.   \n\nAs a somewhat counterintuitive observation, we note that in many problems it is in fact easy to handcraft simple features that generate useful behaviour. This is illustrated in our experiments with DM control, in which using the standard features provided in the suite we were able to generate rich behaviour. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) by simply using linear combinations of the standard features. \n\nThirdly, our experiments suggest that the assumption is not too restricting in interesting problems. For example, we were able to teach the walker to do salta jumps (see the video in the supplementary material) under this assumption. \n\nLastly, we believe that it should be easy to generalize our approach to a more general setup where the reward is represented as a nonlinear (perhaps a DNN) of the features. In this case, the minimization over w will not be  convex but will still be possible via the same techniques (SGD). This kind of algorithm will resemble GAIL (with a GAN) and we believe that it is an exciting direction of research for future work.\n\n\n[1] Barreto, A., Hou, S., Borsa, D., Silver, D., & Precup, D. \"Fast reinforcement learning with generalized policy updates.\" Proceedings of the National Academy of Sciences (2020).\n[2] Abbeel, Pieter, and Andrew Y. Ng. \"Apprenticeship learning via inverse reinforcement learning.\" Proceedings of the twenty-first international conference on Machine learning. 2004.\n[3] Zahavy, Tom, Alon Cohen, Haim Kaplan, and Yishay Mansour. \"Apprenticeship Learning via Frank-Wolfe.\" AAAI (2020)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "E1RGmrUN2P", "original": null, "number": 4, "cdate": 1605272512016, "ddate": null, "tcdate": 1605272512016, "tmdate": 1605272512016, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "M0--Zyjkn1f", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "We would like to thank the reviewer for their constructive comments, we are sure that our changes to the manuscript following their suggestion have improved the quality of this work. \n\nPlease note that we made substantial changes to the paper following your suggestions. This includes addressing the $\\arg\\max$ in Definitions 3&4, and in Eq 5, and switching the notation in Eq 7. Regarding the question \u201cwhere does $\\Pi$ live in?\u201d: this is a good point, since the original submission confusingly used the same notation $\\Pi$ for the subset of policies and the set of all the policies in the MDP. We corrected that in the revised draft, such that it's clear that we optimize over a subset $\\Pi^n$ of $\\Pi$ where $\\Pi$ is the set of all the policies. \n\nScalar value functions. Thank you for the comment, we have clarified the notation regarding the value in Equation 3 to clarify that it is the expected value under the initial state distribution of the value function. \n\nQuestion: \u201ckeep only the policies that achieve max performance....?\u201d, we would like to refer the reviewer to the reformulation of reward minimization problem in equation 15. There you can see that for any feasible solution w, there is a soft inequality restricting all the policies to have a lower or equal value to that of the optimal policy. These constraints can be binding or not, but both cases are feasible. In fact, if the set of policies is diverse enough, the worst case reward will be chosen such that there is equality. For example think about the simple case where the SFs are: $(0,1)$ and $(1,0)$. The worst case reward will be $(\\frac{-1}{\\sqrt(2)}, \\frac{-1}{\\sqrt(2)})$, and the two policies will maximize it. For more general cases, we refer the reviewer to Figure 2, a, where we validate that empirically in DM control. The blue bars correspond to the number of active policies (that attain the max) after each iteration. The number of such policies is increasing and is clearly larger than 1.\n\nQuestion: \u201cwhy there is not another w that makes this policy fail\u201d, this is exactly what our proof shows. We would like to refer the reviewer to the definition of the features in the second paragraph of the preliminaries section. From there, it is clear that the features are always positive, and are for dimension d. The later answers the reviewer question regarding d in Lemma 3. The former is important for the proof of the uniqueness of the worst case reward. We hope that Lemma 5 in the revised version will answer the reviewer\u2019s concerns. Note that the places that were changed are in blue color. We explain some parts clearer and more rigorously than in the previous version and we hope that you will find that satisfactory. \n\nRegarding the reward-set setting. Please note that we defined the features to be positive. Although the features are positive w can be negative and in general will be negative. So, for a given set of policies, the solution for the worst case reward is not just the min reward at each state. For example, if your set includes only the policy $(1,0)$ then the worst case reward will be $(-1,0)$ and not $(\\frac{-1}{\\sqrt(2)}, \\frac{-1}{\\sqrt(2)})$. We hope this addresses your concern regarding w_min. Regarding your question about the linearity in the features, see our answer in a separate post(note that this answer is also an answer to R1). \n\n\u201cwhy optimize the policy set according to SMP?\u201d This is a good question. The focus of this work is on the case that the reward is unknown. Therefore, if our set only includes a single policy, then the worst case reward w.r.t to it will always be \u201cdevastating\u201d. Mathematically, this means that if the set of the policies is not diverse, then the worst case reward can choose to be minus the SFs of one of the policies (normalised), that is, to be as adversarial as possible w.r.t a single policy. When the set includes more than one policy, then the policies, under an SMP may complement each other. That is, if the reward is too adversarial w.r.t to a single policy, then it is likely that another policy in the set will be better w.r.t it. This is what happens in practice, when there is more than one policy that maximizes the worst case reward (active policies). In that case, the analytical solution of the worst case reward is not minus the SFs of one of the policies in the set. As a result, the value of the SMP w.r.t the worst case reward is better than that of that of the best policy in the set in isolation. \n\n\u201cstick to the simplest setting of choosing the best policy\u201d -- We revisited the problem formulation paragraph to make it clearer that we focus on the SMP as the mechanism that select policies. Please note that once algorithm 1 finishes and returns a set of policies, this set can be used by other SIPs (such as GPI) to yield better performance. We verify that empirically in Figure 1a. \n\nWe hope that our response addressed all the reviewer\u2019s concerns, but if it didn\u2019t, please point us to the parts that we missed. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "M4zmYwbKKy1", "original": null, "number": 3, "cdate": 1605271847854, "ddate": null, "tcdate": 1605271847854, "tmdate": 1605272010781, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "xysqyfLEoyN", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to Reviewer 4", "comment": "We would like to thank the reviewer for they/there feedback. \n\nQuestion 1.\n\nThe reviewer is correct: for the baselines, we add policies to the set by following the baseline rule (either by sampling a random reward and optimizing it or by adding an orthogonal reward and optimizing it). For each method (including ours and the baseline) we have a different set of policies $\\Pi^t$ in each iteration. At iteration t we compute the worst-case reward w.r.t to SMP\u2019s current set, $w^*(\\Pi^t)$, and report the performance of all the methods under this reward ($w^*$ is computed through (8)). This means that the reported performance of SMP is the worst possible across all the tasks, while this is not necessarily true for the baselines (that is, there might be tasks different from $w^*$ in which their performance is worse).\n\nThe reviewer\u2019s suggestion of adding a \u201ctest set\u201d of tasks is interesting. We will do our best to add this experiment to the paper before the end of the rebuttal phase, and will certainly have it in the final version of the paper. As noted above, if we replace the task $w^*$ used in our evaluation with any other task, the performance of our algorithm (SMP) will improve. So, for example, if we report the average performance of our algorithm on a \u201ctest set\u201d, as suggested, this value will be greater than the one reported. This is not necessarily the case for the baselines. On the other hand, the performance of the baselines on the test set can in fact be better than SMP\u2019s, since this is not what our algorithm is optimizing. The choice between maximizing the worst-case performance and the expected performance involves several interesting trade-offs; we elaborate on this point below. \n\nQuestion 2.\nThis is also a very interesting suggestion, it would indeed be an interesting reference point. Such an algorithm should work well if we have some prior knowledge of the distribution of w or are able to sample from it (for example, by learning online as tasks are presented to the agent). However, we conjecture that the number of policies needed by such an approach to reach a good performance level would in general be considerably larger than the number of policies used by our algorithm (since in this case one has to cover all the support of the distribution over w with non-negligible probability mass).\n\nOptimizing for the worst case scenario gives us some benefits. First, we do not need to know anything about the distribution of w. This allows us to do things like building the library of policies in a completely unsupervised way, before ever seeing an actual task. Second, we can be very efficient: in our experiments we always found a set of diverse policies after only a few iterations. Focusing on the worst-case reward can also be very useful in scenarios where bad performance has a high cost associated with it: for example, if the agent is an autonomous vehicle, the priority might be to avoid accidents.  \nAll that said, we believe that the two approaches (optimizing for the worst-case or expected performance) are in fact complimentary. Finding a feasible way to cover the space of rewards and combine it with our approach is an exciting direction for future work. \n\nWe will add the discussion above to the paper.\n\nQuestion 3.\nThis is related to the previous question, and also an interesting point. We did observe in our experiments that sometimes our algorithm converged to the optimal value \u201ctoo fast\u201d. Concretely, this meant that after adding 2-4 policies to the set, newly-added  policies were not diverse or meaningful because w*, the worst-case reward w.r.t the SMP computed through (8), was very close to being a vector with -1 in all of its coordinates. That is, after a few iterations, the benefit of adding a new policy diminished quickly. One direction that we explored to alleviate this issue was to regularize the worst-case reward w* to have zero mean. Note that removing the mean does not change the task but potentially increases the difference in the relative magnitude of the entries in w*. This did indeed help in making the policies more diverse. These experiments have been added to the supplementary material (Section C).\n\nQuestion 4.\nWe used a simple actor-critic agent with experience replay to learn each policy. We experienced both with the case where the parameters are learned from scratch and with the case where they are transferred from one task to another, but it did not seem to make a big difference (same for the experience replay). We did not use SMP or GPI to learn the new task, although this is a great idea to be explored in the future (we will mention it in the paper).\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "nNtS773KdHC", "original": null, "number": 2, "cdate": 1605271667712, "ddate": null, "tcdate": 1605271667712, "tmdate": 1605271967445, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "-f-vjNDSLiV", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment", "content": {"title": "Response to Reviewer 3", "comment": "We would like to thank the reviewer for they/there feedback. The reviewer raised a question about the policies learned by following our algorithm: how do they relate to Apprenticeship Learning (AL), and can we better show if they learned similar/different things from one another.\n\nRelation to AL. This is a great question and we debated about it when working on this paper. Both AL and our algorithm can be used to solve the same goal: achieve good performance w.r.t the worst case reward. However, AL is concerned with finding a single policy, while our algorithm is explicitly designed to find a set of policies. To be more specific, we need to refer to a specific AL algorithm, so we will refer to the projection algorithm by Abbeel & Ng (04). The basic idea in this algorithm is that given an estimate of the SFs of the expert (we use our notation though in their paper they refer to a similar quantity as feature expectations) we want to find its projection onto the SFs polytope as well as a policy whose SFs equal this projection. We refer the reviewer to Sec B in the supplementary material for more details, but provide a short summary here for the following discussion. The algorithm achieves that by maintaining a set of policies, and  convex combination of coefficients over the set of policies, known as a mixed policy, such that its SFs are the convex combination of the SFs in the set. The goal is to find the convex combination whose corresponding SFs are closest to those of the expert in the L2 norm. In each iteration, a policy is added to the set by maximizing a reward signal that is defined to be the negative gradient of this objective. \n\nThere is no direct connection between the policies that are discovered from following these two processes. This is because the intrinsic rewards that are maximised by each algorithm are essentially different. Another way to think about this is that since the policy that is returned by AL is a mixed policy, its goal is to return a set of policies that are similar to the expert, but not diverse from one another. From a geometric perspective, the policies returned by AL are the nodes of the face in the polytope that is closest to the demonstrated SFs. Even more concretely, if the SFs of the expert are given exactly (instead of being approximated from trajectories), then the AL algorithm would return a single vertex (policy). Finally, while a mixed policy can be viewed as a composition of policies, it is not a SIP. Therefore, it does not encourage diversity in the set. Our algorithm, on the other hand, is explicitly designed to return a diverse set of policies. \n\n\u201cDescription of the types of policies that the different rewards lead to and how the policy computed by the proposed approach relates (or differs) from them\u201d. We would like to refer the reviewer to Figure 3, where we visualize the policies learned by our algorithm in DM control. There we show a snippet of the trajectories taken by different policies. In the supplementary material, we further provided videos that we recorded from these policies. So, for example, the pendulum balances itself up and down, and the cheetah tries to stand on either leg, or to walk in either direction. The walker and the hopper discovered other locomotion skills that are not expected, but the key finding is that they are indeed very diverse from one another. We hope that this is what the reviewer asked for, but in case the reviewer believes that there are some missing details, or in case that we didn\u2019t answer all of they/there questions, please let us know what you think is missing and we would provide more details.\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "PUkhWz65dy5", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3432/Authors|ICLR.cc/2021/Conference/Paper3432/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923837570, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Comment"}}}, {"id": "-f-vjNDSLiV", "original": null, "number": 4, "cdate": 1603931445955, "ddate": null, "tcdate": 1603931445955, "tmdate": 1605024001110, "tddate": null, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "invitation": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review", "content": {"title": "The paper addresses an interesting problem, and the proposed approach is clear and well formalized. The paper analyzes the proposed approach theoretically as well as empirically, attaining good results.", "review": "= Overview = \n\nThe paper introduces an approach that, given a set of \"basis\" policies, constructs a high-level policy from the basis policies that is able to perform well in a variety of distinct (but related) tasks. Such tasks are described by MDPs with similar state-action spaces and similar dynamics, and differing only on the reward functions, all of which are built as a linear combination of common features.\n\nGiven a set of policies, the paper introduces the notion of \"set improving policy\" as a policy that outperforms any policy in the given set on the family of considered tasks. It provides two examples of such policies (SMP and GPI) and formalizes the problem of computing a SIP with maximal worst-case performance on the set of considered tasks as a max-min problem. It then contributes an incremental algorithm for this problem. The proposed approach is tested in a grid-world environment and the DM control suite.\n\n= Positive points =\n\nThe paper is very well written, with the proposed approach clearly motivated, presented and analyzed. The proposed approach is novel, to the extent of my knowledge, and analyzed both theoretically and empirically. \n\n= Negative points =\n\nMy main criticism is, perhaps, some lack of detail on the experimental evaluation -- particularly in the DM control suite.\n\n= Comments = \n\nOverall, I really enjoyed reading the paper. The problem addressed -- that of building a policy that performs well in a number of related tasks from a set of \"simpler\" policies -- is, in my view, quite relevant for the RL community, and has potentially interesting applications in domains such as robotics.\n\nThe proposed approach is, as far as I know, original and contributes to the state of the art. The paper briefly links its contributions to the existing literature on apprenticeship learning and hierarchical RL, but I would have appreciated some more discussion on these topics -- particularly, I'd like to better understand how the learned policy relates with policies taught through apprenticeship learning.\n\nOverall, the ideas in the paper are presented in a very clear and elegant manner and the results strike me as technically sound. The proposed approach focuses on building a set of \"basis\" policies in such a way that the policy built from them performs as well as possible in all the considered family of tasks. The method is derived from first principles, and the performance bounds provided (framed in terms of the performance of the SMP policy) are then validated empirically. \n\nFinally, the paper is evaluated in a smaller grid-world domain and in the DM control suite. One aspect that could, perhaps, be improved is concerned with the description of the empirical evaluation in the DM control suite: the paper does describe how the family of rewards for these tasks were built, but it would be good to provide some description of the types of policies that the different rewards lead to and how the policy computed by the proposed approach relates (or differs) from them.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3432/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3432/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Discovering a set of policies for the worst case reward", "authorids": ["~Tom_Zahavy2", "~Andre_Barreto1", "~Daniel_J_Mankowitz2", "~Shaobo_Hou1", "~Brendan_O'Donoghue1", "iukemaev@google.com", "~Satinder_Singh2"], "authors": ["Tom Zahavy", "Andre Barreto", "Daniel J Mankowitz", "Shaobo Hou", "Brendan O'Donoghue", "Iurii Kemaev", "Satinder Singh"], "keywords": [], "abstract": "We study the problem of how to construct a set of policies that can be composed together to solve a collection of reinforcement learning tasks. Each task is a different reward function defined as a linear combination of  known features. We consider a specific class of policy compositions which we call set improving policies (SIPs): given a set of policies and a set of tasks, a SIP is any composition of the former whose performance is at least as good as that of its constituents across all the tasks. We focus on the most conservative instantiation of SIPs, set-max policies (SMPs), so our analysis extends to any SIP. This includes known policy-composition operators like generalized policy improvement. Our main contribution is an algorithm that builds a set of policies in order to maximize the worst-case performance of the resulting SMP on the set of tasks. The algorithm works by successively adding new policies to the set. We show that the worst-case performance of the resulting SMP strictly improves at each iteration, and the algorithm only stops when there does not exist a policy that leads to improved performance. We empirically evaluate our algorithm on a grid world and also on a set of domains from the DeepMind control suite. We confirm our theoretical results regarding the monotonically improving performance of our algorithm. Interestingly, we also show empirically that the sets of policies computed by the algorithm are diverse, leading to different trajectories in the grid world and very distinct locomotion skills in the control suite.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "zahavy|discovering_a_set_of_policies_for_the_worst_case_reward", "supplementary_material": "/attachment/10c59ebfc70b17c874b0ad6e25e337116fb6c1ea.zip", "pdf": "/pdf/7a579d5544cfac14ef616dd30d162838a87cf7e4.pdf", "one-sentence_summary": "Discovering a set of diverse RL policies by optimising the robustness of the set", "venue": "ICLR 2021 Spotlight", "venueid": "ICLR.cc/2021/Conference", "_bibtex": "@inproceedings{\nzahavy2021discovering,\ntitle={Discovering a set of policies for the worst case reward},\nauthor={Tom Zahavy and Andre Barreto and Daniel J Mankowitz and Shaobo Hou and Brendan O'Donoghue and Iurii Kemaev and Satinder Singh},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=PUkhWz65dy5}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "PUkhWz65dy5", "replyto": "PUkhWz65dy5", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3432/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538075885, "tmdate": 1606915802780, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3432/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3432/-/Official_Review"}}}], "count": 22}