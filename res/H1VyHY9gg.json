{"notes": [{"tddate": null, "replyto": null, "ddate": null, "tmdate": 1489021555477, "tcdate": 1478296801354, "number": 480, "id": "H1VyHY9gg", "invitation": "ICLR.cc/2017/conference/-/submission", "forum": "H1VyHY9gg", "signatures": ["~Ziang_Xie1"], "readers": ["everyone"], "content": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "writers": [], "nonreaders": [], "details": {"replyCount": 11, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1475686800000, "tmdate": 1478287705855, "id": "ICLR.cc/2017/conference/-/submission", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": ["Theory", "Computer vision", "Speech", "Natural language processing", "Deep learning", "Unsupervised Learning", "Supervised Learning", "Semi-Supervised Learning", "Reinforcement Learning", "Transfer Learning", "Multi-modal learning", "Applications", "Optimization", "Structured prediction", "Games"]}, "TL;DR": {"required": false, "order": 3, "description": "\"Too Long; Didn't Read\": a short sentence describing your paper", "value-regex": "[^\\n]{0,250}"}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1483462800000, "cdate": 1478287705855}}}, {"tddate": null, "ddate": null, "cdate": null, "tmdate": 1486396608738, "tcdate": 1486396608738, "number": 1, "id": "HJY6hzUul", "invitation": "ICLR.cc/2017/conference/-/paper480/acceptance", "forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "signatures": ["ICLR.cc/2017/pcs"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/pcs"], "content": {"title": "ICLR committee final decision", "comment": "The reviewers are reasonably supportive of this paper. The ideas presented in the paper are nice and the results are encouraging. The authors should consider, for the final version of this work, providing comparisons to other approaches on the text8 corpus (or on the 1 Billion Words corpus, Chelba et al.).", "decision": "Accept (Poster)"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1486396609251, "id": "ICLR.cc/2017/conference/-/paper480/acceptance", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/pcs"], "noninvitees": ["ICLR.cc/2017/pcs"], "reply": {"forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "writers": {"values-regex": "ICLR.cc/2017/pcs"}, "signatures": {"values-regex": "ICLR.cc/2017/pcs", "description": "Your displayed identity associated with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your decision.", "value": "ICLR committee final decision"}, "comment": {"required": true, "order": 2, "description": "Decision comments.", "value-regex": "[\\S\\s]{1,5000}"}, "decision": {"required": true, "order": 3, "value-radio": ["Accept (Oral)", "Accept (Poster)", "Reject", "Invite to Workshop Track"]}}}, "nonreaders": [], "cdate": 1486396609251}}}, {"tddate": null, "tmdate": 1484189453232, "tcdate": 1484189453232, "number": 5, "id": "HJrzkOELg", "invitation": "ICLR.cc/2017/conference/-/paper480/public/comment", "forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "signatures": ["~Ziang_Xie1"], "readers": ["everyone"], "writers": ["~Ziang_Xie1"], "content": {"title": "Response to Reviews", "comment": "We thank the anonymous reviewers again for their valuable feedback! MT results for source-only and target-only noising have been added to Table 4.\n\nWe wish to emphasize that the work\u2019s strength is the principled theoretical connection we derive between noising and smoothing. This connection allows us to incorporate generative assumptions into the discriminative sequence prediction setting, and provides an explanation for the empirical gains by several other papers.\n\nTo paraphrase Reviewer 4, we then adapt smoothing methods from n-gram models to continuous language models. We describe analogues to not only basic methods such as interpolation but also Kneser-Ney.\n\nFinally, as all reviewers seem to agree, the method is effective and simple to apply.\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559375, "id": "ICLR.cc/2017/conference/-/paper480/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1VyHY9gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper480/reviewers", "ICLR.cc/2017/conference/paper480/areachairs"], "cdate": 1485287559375}}}, {"tddate": null, "tmdate": 1482570745314, "tcdate": 1482563889313, "number": 4, "id": "r1YV-osEx", "invitation": "ICLR.cc/2017/conference/-/paper480/public/comment", "forum": "H1VyHY9gg", "replyto": "BJz3eHiVg", "signatures": ["~Ziang_Xie1"], "readers": ["everyone"], "writers": ["~Ziang_Xie1"], "content": {"title": "Response", "comment": "Thanks for the feedback. We agree the simplicity of the method is a strength.\n\nRegarding theoretical justification: Please see our response to Reviewer 3, where we summarize the high-level theoretical justification in the paper. We also contribute the connection between smoothing and noising, which provides a theoretical explanation for the empirical results of several papers (last paragraph of Related Work).\n\nWe would also like to emphasize that we do examine incorporating generative assumptions such as discounting and diverse histories in the paper. Thanks again for the suggestions."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559375, "id": "ICLR.cc/2017/conference/-/paper480/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1VyHY9gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper480/reviewers", "ICLR.cc/2017/conference/paper480/areachairs"], "cdate": 1485287559375}}}, {"tddate": null, "tmdate": 1482563390965, "tcdate": 1482563390965, "number": 3, "id": "B1wr1oi4g", "invitation": "ICLR.cc/2017/conference/-/paper480/public/comment", "forum": "H1VyHY9gg", "replyto": "rJK-1M5Eg", "signatures": ["~Ziang_Xie1"], "readers": ["everyone"], "writers": ["~Ziang_Xie1"], "content": {"title": "Response: theoretical justification, comments", "comment": "Thanks for the feedback. We fully agree that the method is simple and improves performance.\n\nWe maintain that there is theoretical justification for why noising should work for RNN models, though perhaps this was vaguely put in the paper:\n1. RNN LMs trained using maximum likelihood often overfit. Consider a large RNN that achieves near the minimum training cross-entropy loss -- it then behaves like an n-gram model.\n2. We show the noising schemes result in pseudocounts corresponding to interpolation smoothing (Section 3.3), which helps avoid overfitting since the RNN LM is trained (again, using maximum likelihood) on the noised data.\n\nResponse to other comments:\n- p. 3&4 We\u2019ve tried to make more explicit the connection between blank noising and interpolation in the text (this seems like an easier way to explain not overfitting to specific contexts). We\u2019ve also annotated the general form equations on p. 4, which hopefully makes them more clear. More feedback always appreciated!\n- p. 5 Thanks for this helpful suggestion. We\u2019ll update the MT results with numbers where the source/target are not noised."}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559375, "id": "ICLR.cc/2017/conference/-/paper480/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1VyHY9gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper480/reviewers", "ICLR.cc/2017/conference/paper480/areachairs"], "cdate": 1485287559375}}}, {"tddate": null, "tmdate": 1482539178520, "tcdate": 1482539178520, "number": 3, "id": "BJz3eHiVg", "invitation": "ICLR.cc/2017/conference/-/paper480/official/review", "forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "signatures": ["ICLR.cc/2017/conference/paper480/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper480/AnonReviewer1"], "content": {"title": "A simplistic, empirical but practically useful approach to improve LM and MT", "rating": "6: Marginally above acceptance threshold", "review": "Strengths\n-\tA simple \u201cnoising\u201d method for improving LM\n-\t\u201cnoise\u201d added to word history by using a probabilistic distribution using N-gram smoothing\n-\tExperimental evidence that such simple techniques improve LM and MT\n\nWeaknesses\n-\tPurely empirical, with no theoretical justification\n-\tRather primitive step so far; would be nice to see future work in modeling different types of LM \u201cnoise\u201d as hidden variables and then \u201cdenoise\u201d them.\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482539179174, "id": "ICLR.cc/2017/conference/-/paper480/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper480/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper480/AnonReviewer4", "ICLR.cc/2017/conference/paper480/AnonReviewer3", "ICLR.cc/2017/conference/paper480/AnonReviewer1"], "reply": {"forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482539179174}}}, {"tddate": null, "tmdate": 1482460929213, "tcdate": 1482460929213, "number": 2, "id": "rJK-1M5Eg", "invitation": "ICLR.cc/2017/conference/-/paper480/official/review", "forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "signatures": ["ICLR.cc/2017/conference/paper480/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper480/AnonReviewer3"], "content": {"title": "", "rating": "6: Marginally above acceptance threshold", "review": "This paper proposes a data noising technique for language modeling. The main idea is to noise a word history by using a probabilistic distribution based on N-gram smoothing techniques. The paper is clearly written and shows that such simple techniques improve the performance in various tasks including language modeling and machine translation. May main concern is that the method is too simple and sounds ad hoc, e.g., there is no theoretical justification of why n-gram smoothing based data noising would be effective for recurrent neural network based language modeling.\n\nComments: \n- p. 3 \u201ccan be seen has a way\u201d -> \u201ccan be seen as a way\u201d (?)\n- p. 3. In general, the explanation about blank noising should be improved. Why does it avoid overfitting on specific contexts?\n- p. 4. It would be better to provide more detailed derivations for a general form of unigram and blank noising equations.\n- p. 5, Section 3.6: Is there any discussions about noising either/both input and output sequences with some numbers? This would be helpful information.\n", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482539179174, "id": "ICLR.cc/2017/conference/-/paper480/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper480/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper480/AnonReviewer4", "ICLR.cc/2017/conference/paper480/AnonReviewer3", "ICLR.cc/2017/conference/paper480/AnonReviewer1"], "reply": {"forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482539179174}}}, {"tddate": null, "tmdate": 1482191223594, "tcdate": 1482191223594, "number": 1, "id": "HklKZgLNe", "invitation": "ICLR.cc/2017/conference/-/paper480/official/review", "forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "signatures": ["ICLR.cc/2017/conference/paper480/AnonReviewer4"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper480/AnonReviewer4"], "content": {"title": "", "rating": "8: Top 50% of accepted papers, clear accept", "review": "This paper discusses data noising as a regularization technique for language modelling as an alternative to dropout regularization. The key idea is to adapt smoothing methods from ngram language modelling in such a fashion that they can be applied to continuous language models through noise. Through this motivation, the authors present noising analogies to standard discounting, as well as Kneser-Ney smoothing.\n\nThe experiments are convincing in that the smoothed (noised) models outperform their unregularized baselines.\n\nMy main issue with the evaluation in this paper is that there is no comparison between the noising/smoothing idea and more conventional regularizers (such as L2 and dropout) which were discussed in the paper. Likewise, it would have been interesting if the techniques proposed here had been applied to stronger base models (such as the models compared with in Table 2). Seeing that the noising technique is effectively just data augmentation it should have been reasonably trivial to blackbox the model and plug in Zaremba's or Gal's!\n\nThose weaknesses aside (and I recommend the authors investigate improving their paper by adding these experiments), this paper presents a novel method for improving neural network learning for a number of sequence based problems, and does so convincingly. I strongly recommend the paper for acceptance.", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1481932799000, "tmdate": 1482539179174, "id": "ICLR.cc/2017/conference/-/paper480/official/review", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper480/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper480/AnonReviewer4", "ICLR.cc/2017/conference/paper480/AnonReviewer3", "ICLR.cc/2017/conference/paper480/AnonReviewer1"], "reply": {"forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your review.", "value-regex": ".{0,500}"}, "review": {"order": 2, "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "value-regex": "[\\S\\s]{1,20000}"}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"]}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"]}}}, "nonreaders": [], "expdate": 1489708799000, "cdate": 1482539179174}}}, {"tddate": null, "tmdate": 1481680159154, "tcdate": 1481680159148, "number": 2, "id": "B1DQHmA7e", "invitation": "ICLR.cc/2017/conference/-/paper480/public/comment", "forum": "H1VyHY9gg", "replyto": "rkFlM317x", "signatures": ["~Ziang_Xie1"], "readers": ["everyone"], "writers": ["~Ziang_Xie1"], "content": {"title": "amount of data", "comment": "Thanks for the questions! We agree that scale of data is an important consideration. PTB contains 929k training tokens (42K sentences), 73k validation tokens, and 82k test tokens. For the IWSLT dataset, training contains 190K sentence pairs with ~5.4M tokens in total. We\u2019ve updated the paper accordingly.\n\nIt is possible to run the smoothing methods on more data. We've updated the paper with some results on Text8, which has 15.3M training tokens, 848K validation tokens, and 855K test tokens, with a 42K vocabulary in our experiments. As you suggest, as the amount of data grows, it may be necessary to use higher order n-gram statistics with more bookkeeping to obtain gains.\n\nRegarding computational costs / speed: Even for larger values of gamma and larger vocabularies (e.g. in the Text8 experiments), we do not observe significant changes to tokens/sec processed when using cumulative sums with bisection to sample (~2.5microsec/sample for a 42K vocabulary on our machine); runtime remains dominated by GPU operations for the RNN. It is true that noised models usually must be trained for more epochs, as shown in Figure 1, and this depends on optimization and annealing settings. As mentioned in the paper, only training time is affected by noising; test time is unaffected unlike in some other methods. Hope this clarifies things!\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559375, "id": "ICLR.cc/2017/conference/-/paper480/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1VyHY9gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper480/reviewers", "ICLR.cc/2017/conference/paper480/areachairs"], "cdate": 1485287559375}}}, {"tddate": null, "tmdate": 1481679977688, "tcdate": 1481679977682, "number": 1, "id": "rJGO47RQx", "invitation": "ICLR.cc/2017/conference/-/paper480/public/comment", "forum": "H1VyHY9gg", "replyto": "rymMxH1Qe", "signatures": ["~Ziang_Xie1"], "readers": ["everyone"], "writers": ["~Ziang_Xie1"], "content": {"title": "comparison with noising in speech", "comment": "Thanks for the reference, which we unfortunately missed in our survey of related work. It\u2019s difficult for us to compare our gains in perplexity/BLEU with WER reductions, though it would be great to see the effects of noising on the LM/decoder for applications such as speech transcription. Regarding NAT vs. multi-style training, it would also be interesting to model different LM noise types and levels as hidden variables, though we did not explore that direction in this work. Thanks for the ideas/suggestions!"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "duedate": null, "tddate": null, "ddate": null, "tmdate": 1485287559375, "id": "ICLR.cc/2017/conference/-/paper480/public/comment", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": "H1VyHY9gg", "writers": {"values-regex": "~.*|\\(anonymous\\)"}, "signatures": {"values-regex": "~.*|\\(anonymous\\)", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "value-dropdown": ["everyone", "ICLR.cc/2017/conference/organizers", "ICLR.cc/2017/conference/ACs_and_organizers", "ICLR.cc/2017/conference/reviewers_and_ACS_and_organizers"]}, "content": {"title": {"order": 1, "description": "Brief summary of your comment.", "value-regex": ".{1,500}"}, "comment": {"order": 2, "description": "Your comment or reply.", "value-regex": "[\\S\\s]{1,20000}"}}}, "nonreaders": [], "noninvitees": ["ICLR.cc/2017/conference/paper480/reviewers", "ICLR.cc/2017/conference/paper480/areachairs"], "cdate": 1485287559375}}}, {"tddate": null, "tmdate": 1480733169265, "tcdate": 1480733169260, "number": 2, "id": "rkFlM317x", "invitation": "ICLR.cc/2017/conference/-/paper480/pre-review/question", "forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "signatures": ["ICLR.cc/2017/conference/paper480/AnonReviewer3"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper480/AnonReviewer3"], "content": {"title": "Amount of data.", "question": "Can you provide the detailed information of the data size (number of words and sentences) for both Penn Treebank and IWSLT2015 corpus so that readers can figure out the data size range of experiments?\nSince the method is motivated by a smoothing method, and the discussion of the smoothing method depends on the scale of data (and the order of n-gram), it would be interesting to have such information.\nProviding the computational costs of the proposed method would be more helpful.\nAlso, is it possible to apply it to larger scale data?\n\n"}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959258387, "id": "ICLR.cc/2017/conference/-/paper480/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper480/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper480/AnonReviewer1", "ICLR.cc/2017/conference/paper480/AnonReviewer3"], "reply": {"forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959258387}}}, {"tddate": null, "tmdate": 1480704010935, "tcdate": 1480704010929, "number": 1, "id": "rymMxH1Qe", "invitation": "ICLR.cc/2017/conference/-/paper480/pre-review/question", "forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "signatures": ["ICLR.cc/2017/conference/paper480/AnonReviewer1"], "readers": ["everyone"], "writers": ["ICLR.cc/2017/conference/paper480/AnonReviewer1"], "content": {"title": "comparison with noising methods in speech", "question": "How do you compare the noising method proposed in this paper for LM with the counterpart in speech, which is often called multi-style learning (Large-vocabulary speech recognition under adverse acoustic environments, Interspeech, 2000), in terms of practical efficacy? Does the more effective method (noise-adaptive training) than noising or multi-style training described there for speech apply to LM here?  "}, "nonreaders": [], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Data Noising as Smoothing in Neural Network Language Models", "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in n-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.", "pdf": "https://arxiv.org/pdf/1703.02573.pdf", "TL;DR": "Derive data noising schemes for neural network language models corresponding to techniques in n-gram smoothing.", "paperhash": "xie|data_noising_as_smoothing_in_neural_network_language_models", "conflicts": ["stanford.edu", "cs.stanford.edu", "fb.com", "baidu.com"], "keywords": ["Natural language processing", "Deep learning"], "authors": ["Ziang Xie", "Sida I. Wang", "Jiwei Li", "Daniel L\u00e9vy", "Aiming Nie", "Dan Jurafsky", "Andrew Y. Ng"], "authorids": ["zxie@cs.stanford.edu", "sidaw@cs.stanford.edu", "jiweil@stanford.edu", "danilevy@cs.stanford.edu", "anie@cs.stanford.edu", "jurafsky@stanford.edu", "ang@cs.stanford.edu"]}, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "duedate": 1480741199000, "tmdate": 1480959258387, "id": "ICLR.cc/2017/conference/-/paper480/pre-review/question", "writers": ["ICLR.cc/2017/conference"], "signatures": ["ICLR.cc/2017/conference"], "readers": ["everyone"], "invitees": ["ICLR.cc/2017/conference/paper480/reviewers"], "noninvitees": ["ICLR.cc/2017/conference/paper480/AnonReviewer1", "ICLR.cc/2017/conference/paper480/AnonReviewer3"], "reply": {"forum": "H1VyHY9gg", "replyto": "H1VyHY9gg", "writers": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2017/conference/paper480/AnonReviewer[0-9]+", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"title": {"order": 1, "description": "Brief summary of your question.", "value-regex": ".{1,500}"}, "question": {"order": 2, "description": "Your question", "value-regex": "[\\S\\s]{1,5000}"}}}, "nonreaders": [], "expdate": 1488517199000, "cdate": 1480959258387}}}], "count": 12}