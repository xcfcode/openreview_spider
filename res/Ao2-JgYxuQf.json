{"notes": [{"id": "Ao2-JgYxuQf", "original": "YEVAbR10Xi", "number": 3735, "cdate": 1601308415781, "ddate": null, "tcdate": 1601308415781, "tmdate": 1614985715582, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 9, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "uGyAa9aqSv5", "original": null, "number": 1, "cdate": 1610040427103, "ddate": null, "tcdate": 1610040427103, "tmdate": 1610474026668, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper introduces a method to estimate dynamics parameters in recurrent structured models during the learning process. All three reviewers agreed that the idea is interesting and the proposed method could be potentially useful. However, two of the three reviewers have a serious concern about the lack of comparison with other approaches. I agree with these two reviewers; due to the lack of discussion and comparison with existing studies, I cannot recommend accepting this submission in its current form. "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040427089, "tmdate": 1610474026652, "id": "ICLR.cc/2021/Conference/Paper3735/-/Decision"}}}, {"id": "NEYZ57JXPTw", "original": null, "number": 6, "cdate": 1606167108391, "ddate": null, "tcdate": 1606167108391, "tmdate": 1606167108391, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment", "content": {"title": "On Revision 23 Nov 2020", "comment": "To complete the comparison with TCN and missing data values, we have just uploaded another paper revision.\n\nThis revision includes now TCN results for all three problem domains considered. While TCN partially outperforms the respective RNNs when trained on similar denoising levels, Active Tuning applied to a noise-unaware RNN outperforms the noise-unaware TCN. In the conclusions we now also mention the potential to add Active Tuning to TCNs and other feed-forward ANNs. \n\nAdditionally, we also added the dropout experimental results to the pendulum data.\n\nMoreover, for the wave experiments, we added another illustrative visualization of the superior performance of Active Tuning in the DISTANA network.\n\nIn the light of these new result additions, we have also adapted the conclusions slightly further. \n\nThank you to all three reviewers for taking the time to reconsider our paper and your consideration and time in general. \n\nSincerely yours,\nthe authors. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ao2-JgYxuQf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3735/Authors|ICLR.cc/2021/Conference/Paper3735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment"}}}, {"id": "Fhwy3LHaFZQ", "original": null, "number": 5, "cdate": 1605716748427, "ddate": null, "tcdate": 1605716748427, "tmdate": 1605716748427, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "Urp6dKu-zXu", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment", "content": {"title": "Response to Reviewer 3 (4)", "comment": "Dear Reviewer 3,\n\nThank you very much for your positive feedback and your recommendations. Please note that we applied DISTANA only for predicting the Wave Dynamics. In the other two problem cases, standard LSTMs were applied, underlining the general applicability of Active Tuning. \n\n- Yes, we clearly see your demand for an algorithmic description as justified. We added Algorithm 1 including some further explanations in the text and hope this complements the method section satisfactorily.\n- We are not entirely sure about what you mean with bias concerning the data generation. But, of course, we tried to produced datasets that were as unbiased as possible, randomizing the parameters of the generation process accordingly. \n- We actually did vary the tuning length and tuning cycles across the three different problem domains and noise levels (albeit not very much). The particular choices can be found in Table 6, 7, and 8 in Section A.2 of the appendix. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ao2-JgYxuQf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3735/Authors|ICLR.cc/2021/Conference/Paper3735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment"}}}, {"id": "u5a-1EUfxFJ", "original": null, "number": 4, "cdate": 1605716619772, "ddate": null, "tcdate": 1605716619772, "tmdate": 1605716619772, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "fhawELqBPqq", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment", "content": {"title": "Response to Reviewer 2", "comment": "Dear Reviewer 2\n\nThank you for your constructive feedback on our work.\n\n- By including an algorithmic explanation of our method (see Algorithm 1 in the revised paper), we hope to both increase the comprehensibility of our paradigm's description and remove any uncertainties on implementation details.\n- Your statement about the potential of Active Tuning outperforming other models raised our curiosity. Hence, we incorporated an experiment with a state-of-the-art sequence-to-sequence model, namely, a temporal convolution network (TCN). Indeed, as reported in the modified Table 4 (and Table 5 in the appendix) of our revised paper, we can verify your expectations and have consistently observed that the TCN is outperformed by Active Tuning. However, we would like to again highlight the fact that we did not aim at beating the most sophisticated state-of-the-art ANN in particular domains. Rather, Active Tuning has the potential to generally enhance the performance of prediction models, and particularly recurrent temporal prediction models, without further training when facing missing values and noisy data.\n- \"Same comparisons might be interesting for tuning weights instead of hidden states.\". This is clearly a very interesting idea, which we hope to elaborate on in future work. What we investigated so far is tuning the cell states, the \"hidden states\" (unit outputs), and even the signal itself. What we found is that it did not really make a huge difference, which of the mentioned parts are optimized. Typically, tuning just the hidden units outputs works best, but only with a small margin (at least for LSTMs). When tuning the weights catastrophic forgetting might become an issue, which Active Tuning fully avoids, because the model parameters (i.e. the ANN weights) are not modified. "}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ao2-JgYxuQf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3735/Authors|ICLR.cc/2021/Conference/Paper3735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment"}}}, {"id": "zqwd9ItiONM", "original": null, "number": 3, "cdate": 1605716548407, "ddate": null, "tcdate": 1605716548407, "tmdate": 1605716548407, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "7t7g6Xk4Og_", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment", "content": {"title": "Response to Reviewer 1", "comment": "Dear Reviewer 1,\n\nThank you very much for your review and your suggestions. \n\n- It is very important to emphasize that within this paper Active Tuning is not applied during or for training (it is nonetheless a good idea to incorporate it during training). Instead, the hidden states of pre-trained models are optimized based on the prediction error-induced gradient information. The outputs of the RNNs are thus only driven by the continuously applied gradient-based tuning of the hidden states. Sorry, if we did not make this clear enough. We hope that with inclusion of Algorithm 1 this becomes more comprehensible.\n\n- To the best of our knowledge, there are no comparable approaches to exclusively tune the RNN's hidden states for inference purposes. The main statement that we tried to make with our paper is, however, that Active Tuning can dramatically improve an already existing model (without additional learning) and unfold robustness properties that have not been addressed during training. In order to further interpret the potential of the method, we incorporated results of temporal convolution networks (see updated Table 4 in subsection 4.3. as well as the new Table 5 in the appendix).\n- We appreciate and agree on your point that we exclusively focused on noise filtering and noise robustness. To underline the potential of Active Tuning further, we now performed an additional experiment to demonstrate superior robustness to missing values in time series data when using Active Tuning (see Table 2 of the revised paper). \n- Indeed, the potential computational overhead of using Active Tuning can not be neglected. Yet, this overhead, caused by a gradient-based mini optimization procedure within every global time step, scales with the number of tuning cycles C and tuning horizon R (both essentially depend on the problem, as can be seen in the Tables 6, 7 and 8 of the appendix). We are currently working on reducing the required numbers of C and R, which would significantly reduce the computational overhead. Nevertheless, since we admit the importance of this aspect, we have added a corresponding amendment to Section 2."}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ao2-JgYxuQf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3735/Authors|ICLR.cc/2021/Conference/Paper3735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment"}}}, {"id": "bSSV8bTbu-n", "original": null, "number": 2, "cdate": 1605716438348, "ddate": null, "tcdate": 1605716438348, "tmdate": 1605716438348, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment", "content": {"title": "General response to the reviews and list of major modifications", "comment": "Thank you to all three reviewers for insightful comments, the criticism, and the suggestions. We tried to address the mentioned requests and suggestions and hope all of you will find the paper even more appealing now. \n\nBesides some general minor text reformulation and cosmetics, the major additions in the uploaded revision are as follows:\n\n- A formal algorithmic description (Algorithm 1) with additional explanations.\n- Another evaluation that demonstrates that Active Tuning can also handle missing data (Table 2).\n- We added a comparison to the performance of a temporal convolution network (TCN) on the spatiotemporal wave dynamics benchmarks (updated Table 4 and new Table 5 in the appendix, TCN setup cf. end of Section 3; results discussed in subsection 4.3 Wave Results).\n\nWe thus hope that we can convince also Reviewer 1 and Reviewer 2 to rate our paper above threshold after all (currently rating level 5 and 3, respectively; Reviewer 3 gave rating level 8)."}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Ao2-JgYxuQf", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper3735/Authors|ICLR.cc/2021/Conference/Paper3735/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923834363, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Comment"}}}, {"id": "Urp6dKu-zXu", "original": null, "number": 1, "cdate": 1603899116397, "ddate": null, "tcdate": 1603899116397, "tmdate": 1605023946930, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Review", "content": {"title": "An interesting approach in optimizing the internal dynamics of recurrent neural networks", "review": "This is an interesting paper on an idea introduced by the authors as active tuning. This paper is well-written and clearly explains the proposed active tuning scheme. I read the paper carefully multiple times, and feel that a few inclusions will help the readers better understand the proposed method.\n\nAt the base level, this paper builds on optimizing the internal dynamics of a recurrent neural network unlike optimizing internal weights in traditional sequence-to-sequence mapping. This is achieved by decoupling the recurrent neural activities from the input temporal signal and propagating the error (the difference between the estimated input value and the observed input value of the input signal) to tune the internal dynamics of the network. To demonstrate the effectiveness of active tuning the authors trained a distributed graph recurrent neural network (DISTANA) on three datasets with increasing complexity. Datasets included: multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics.  On average ten independent experiments were performed and the effectiveness of active tuning was evaluated using root mean square (RMS).  Samples for the experiments were generated using five different noise ratios between 0 and 1 to measure the effectiveness of the proposed method for noisy data scenarios. The network was also trained on no noise to 0.05 noise induced into training data to see if it would help the models better generalize. The results as depicted in graphs show that active tuning is not only robust but generalizes well on noisy data. \n\nRecommendations:\n1. The active Tuning algorithm itself is missing from this paper. Even though the explanation is clear, it would help the readers to see the algorithm itself for better understanding. The reviewer referred to Hidden Latent State Inference in a Spatio-Temporal Generative by karlbauer et. al. 2020 (arXiv:2009.09823) for the algorithm. \n\n2. The authors confirm that 10000 and 1000 samples have been generated for all the problem domains tested. However, it is not clear if steps were in place to make sure that no bias was introduced during this sample generation. \n\n3. While the tuning length and tuning cycles were fixed for all three datasets, it is important to see how these values can be optimized based on the complexity of the time series data. Experimental results using a range of values for tunning length and tuning cycles would be beneficial.", "rating": "8: Top 50% of accepted papers, clear accept", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070606, "tmdate": 1606915780075, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3735/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Review"}}}, {"id": "fhawELqBPqq", "original": null, "number": 2, "cdate": 1603921494837, "ddate": null, "tcdate": 1603921494837, "tmdate": 1605023946859, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Review", "content": {"title": "A novel method to tune autoregressive model via hidden state optimization", "review": "Paper proposes a way to adapt an autoregressive model (RNN in examples) to the incoming noisy signal to generate noise-free data output. The approach is interesting due to applying updates to the hidden state of the past observation. The proposed approached is named Active Tuning and evaluated on 3 toy tasks. The idea sounds interesting, however the lack of comparisons with other approaches and theoretical justification of why this approach is superior makes it hard to convince reader. \n\nQuality: Paper is well written and most of the concepts is clear. However, paper will benefit from a better explanation of the method, simpler diagram and equations to remove uncertainty on implementation. \n\nOriginality: I believe the idea is novel and interesting for community. It has a potential to outperform meta-learning and sequence-to-sequence models on the task of model adaptation to noisy samples. \n\nPros:\n- Idea is interesting and has potential. \n- Explanation is clear, but still can be improved. \n- Provided experiments show benefits of the proposed method with respect to direct regression task (same model trained with less or more noise amount)\n\nCons:\n- Comparison with other techniques such as meta-learning, sequence-to-sequence models is required to understand the potential of the method.\n- Same comparisons might be interesting for tuning weights instead of hidden states. Or having only a small part of the model to be tuned (like the last layer). \n- Application to more practical problems could benefit the paper. For example image denosing task could be relevant (works like Noise2Noise, Noise2Self etc)", "rating": "3: Clear rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070606, "tmdate": 1606915780075, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3735/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Review"}}}, {"id": "7t7g6Xk4Og_", "original": null, "number": 3, "cdate": 1603924140900, "ddate": null, "tcdate": 1603924140900, "tmdate": 1605023946788, "tddate": null, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "invitation": "ICLR.cc/2021/Conference/Paper3735/-/Official_Review", "content": {"title": "paper is well-written and clear. There are no related work discussed.", "review": "This paper introduces a propagation method to estimate RNN dynamic parameters during the learning process. The algorithm is introduced well and the paper is clearly written.\nThe paper misses a related work section on other tuning methods or absence there of under special circumstances. \nFor the same reason, I am not convinced on the extent of comparisons in the simulation results. A large amount of the focus of the experiments is on the robustness to noise which is fine if there was an equal amount of comparisons against other tuning methods. Otherwise, if the focus of the paper is supposed to be only on noise robustness, I think the motivation in abstract and introduction needs to be clearer.\nWhile the motivation of the paper can be to some extent taken from the results, the introduction does not substantially motivate the problem. \nLastly, I think that majority of details on pages 4 and 5 are unnecessary. Instead, I think a more detailed discussion on comparing additional computational cost of active tuning to other traditional methods would be very useful.", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper3735/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper3735/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Active Tuning", "authorids": ["~Sebastian_Otte1", "~Matthias_Karlbauer1", "martin.butz@uni-tuebingen.de"], "authors": ["Sebastian Otte", "Matthias Karlbauer", "Martin V. Butz"], "keywords": ["Signal Filtering", "Recurrent Neural Network", "Time Series", "Denoising", "Temporal Gradients"], "abstract": "We introduce Active Tuning, a novel paradigm for optimizing the internal dynamics of recurrent neural networks (RNNs) on the fly. In contrast to the conventional sequence-to-sequence mapping scheme, Active Tuning decouples the RNN's recurrent neural activities from the input stream, using the unfolding temporal gradient signal to tune the internal dynamics into the data stream. As a consequence, the model output depends only on its internal hidden dynamics and the closed-loop feedback of its own predictions; its hidden state is continuously adapted by means of the temporal gradient resulting from backpropagating the discrepancy between the signal observations and the model outputs through time. In this way, Active Tuning infers the signal actively but indirectly based on the originally learned temporal patterns, fitting the most plausible hidden state sequence into the observations. We demonstrate the effectiveness of Active Tuning on several time series prediction benchmarks, including multiple super-imposed sine waves, a chaotic double pendulum, and spatiotemporal wave dynamics. Active Tuning consistently improves the robustness, accuracy, and generalization abilities of all evaluated models. Moreover, networks trained for signal prediction and denoising can be successfully applied to a much larger range of noise conditions with the help of Active Tuning. Thus, given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training.", "one-sentence_summary": "Given a capable time series predictor, Active Tuning enhances its online signal filtering, denoising, and reconstruction abilities without the need for additional training. ", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "otte|active_tuning", "supplementary_material": "/attachment/1b3d7fa8bdd9d5ced40b54007498539eba6b456c.zip", "pdf": "/pdf/2647cc6db0ec69658af5cf03d2128e980c93516e.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=tTbHbaC72R", "_bibtex": "@misc{\notte2021active,\ntitle={Active Tuning},\nauthor={Sebastian Otte and Matthias Karlbauer and Martin V. Butz},\nyear={2021},\nurl={https://openreview.net/forum?id=Ao2-JgYxuQf}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Ao2-JgYxuQf", "replyto": "Ao2-JgYxuQf", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper3735/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538070606, "tmdate": 1606915780075, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper3735/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper3735/-/Official_Review"}}}], "count": 10}