{"notes": [{"id": "HkxHFj5BdV", "original": "rygVM5_BdN", "number": 57, "cdate": 1553472381306, "ddate": null, "tcdate": 1553472381306, "tmdate": 1562082113096, "tddate": null, "forum": "HkxHFj5BdV", "replyto": null, "invitation": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "content": {"title": "Parallel Recurrent Data Augmentation for GAN training with Limited and Diverse Data", "authors": ["Boli Fang", "Miao Jiang"], "authorids": ["bfang@iu.edu", "miajiang@iu.edu"], "keywords": ["GAN training", "Data Augmentation"], "TL;DR": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ", "abstract": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review. ", "pdf": "/pdf/f4d6b1fb9fdc028d5bced9b1fcffbfed5fbfd050.pdf", "paperhash": "fang|parallel_recurrent_data_augmentation_for_gan_training_with_limited_and_diverse_data"}, "signatures": ["ICLR.cc/2019/Workshop/LLD"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD"], "details": {"replyCount": 3, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Blind_Submission", "cdate": 1548689671889, "reply": {"forum": null, "replyto": null, "readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD"]}, "content": {"authors": {"values-regex": ".*"}, "authorids": {"values-regex": ".*"}}}, "tcdate": 1548689671889, "tmdate": 1557933709646, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["~"], "signatures": ["ICLR.cc/2019/Workshop/LLD"], "details": {"writable": true}}}, "tauthor": "OpenReview.net"}, {"id": "HJg2YfANYE", "original": null, "number": 1, "cdate": 1554469507985, "ddate": null, "tcdate": 1554469507985, "tmdate": 1555512027007, "tddate": null, "forum": "HkxHFj5BdV", "replyto": "HkxHFj5BdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper57/Official_Review", "content": {"title": "Too poor technical contribution to the LLD field", "review": "The authors presented what they called a \"parallel recurrent data augmentation\" technique for increasing the training images available for learning GANs. The method consists in introducing random noise to input images in a similar to K-fold cross-validation fashion: K GANs are trained simultaneously using different portions of the original training data, and the images generated by the K-th generator are perturbed with random noise and included in the training subsets of the other K-1 GANs. The method is evaluated on a simulated limited data experiment using CIFAR-10 images, showing improvements in terms on standard evaluation metrics in generative modelling.\n\nIn my opinion, the technical contribution is too weak for accepting the paper. Introducing random noise for data augmentation is standard not only in the GAN literature but also in deep learning in general. \n\nThe idea of feeding multiple generators with the artificial outputs of a surrogate models is perhaps novel but its impact is limited and it is not properly explored in this submission. The quantitative contribution in the results cannot be attributed to this technique, also, as it could be maybe just the consequence of adding noise to the inputs. In this sense, the paper lacks a comparison with respect to feeding the generators with noisy images from the training set. \n\nIt would be also important to show if the parallel strategy has some contribution to avoid standard GANs issues such as mode collapse.\n\nThe data set used for evaluation (CIFAR-10) has a significantly low resolution, and therefore it is difficult to see if the semantics of the images are actually preserved. The article would benefit from including either a larger resolution data set or at least an experiment showing e.g. that the classification performance of a DNN trained for CIFAR-10 image classification using only artificial samples doesn't differ too much from a similar network trained on a real CIFAR-10 sample.\n\nThe article also suffers from some presentation issues. The related works section describes GANs research and data augmentation, but without focusing on the problem that is intended to be solved with the propose tool (learning GANs with limited data). Section 2 should be reorganized to include important citations regarding this issue and the existing alternatives to solve it.\n\nOther minor comments:\n- There are spaces missing between words and parenthesis, specially with most of the citations.\n- The first sentence in the abstract is too long and redundant.\n- Second paragraph in Section 2 suffers from many repetitions of the word \"research\".\n- Figure 2 is unreadable due to poor resolution.", "rating": "1: Strong rejection", "confidence": "3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Recurrent Data Augmentation for GAN training with Limited and Diverse Data", "authors": ["Boli Fang", "Miao Jiang"], "authorids": ["bfang@iu.edu", "miajiang@iu.edu"], "keywords": ["GAN training", "Data Augmentation"], "TL;DR": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ", "abstract": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review. ", "pdf": "/pdf/f4d6b1fb9fdc028d5bced9b1fcffbfed5fbfd050.pdf", "paperhash": "fang|parallel_recurrent_data_augmentation_for_gan_training_with_limited_and_diverse_data"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper57/Official_Review", "cdate": 1553713412036, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HkxHFj5BdV", "replyto": "HkxHFj5BdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713412036, "tmdate": 1555511822197, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper57/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "rkgZWGHHFN", "original": null, "number": 2, "cdate": 1554498041420, "ddate": null, "tcdate": 1554498041420, "tmdate": 1555512025485, "tddate": null, "forum": "HkxHFj5BdV", "replyto": "HkxHFj5BdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper57/Official_Review", "content": {"title": "An interesting idea, but needs some polish", "review": "Summary:\nThe authors propose a method for improving the image generation quality of GANs by (1) augmenting the training set with noise-enriched samples and (2) running multiple GANs in parallel over different subsets of the data and periodically augmenting their respective trainings with images sampled from the other GANs.\n\nI find the second contribution more compelling than the first. It is a clever augmentation strategy that does not require human intervention. Overall, however, the presentation hinders the conveyance of the idea enough that I don't think this work is ready yet for release. With a little more polish clarifying the ideas and cleaning up the writing/figures, I think it could be an excellent submission to another workshop or conference down the road.\n\n- It is unclear how much of the reported gains are due to the first contribution vs the second.\n- The first sentence of the abstract introduces too much too quickly.\n- I'm not sure what point you're trying to make with the comment about \"demonstrated tendencies of misrepresentation\"\n- There are many issues with the writing: spacing issues (especially around parentheses), and minor grammar oddities\n- Missing related work in your mention of automatic augmentation: TANDA (Ratner 2017);\n- It is unclear to me why rotation and mirroring may lead to information loss, as you claim\n- The text in Figure 2 is very hard to read\n- The images in Figures 3-5 are too small to tell anything about. Use fewer images per block so they can be larger.\n- Can the authors comment at all on why each dataset seems to have a different optimal number of times to be augmented?\n- It seems to me that the parallel GAN setup will have memory and compute requirements of approximately 8x (if the 8 GANs are being run in parallel). This is pretty substantial overhead. Are there any details I'm missing that would mitigate that?", "rating": "2: Marginally below acceptance threshold", "confidence": "1: The reviewer's evaluation is an educated guess"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Recurrent Data Augmentation for GAN training with Limited and Diverse Data", "authors": ["Boli Fang", "Miao Jiang"], "authorids": ["bfang@iu.edu", "miajiang@iu.edu"], "keywords": ["GAN training", "Data Augmentation"], "TL;DR": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ", "abstract": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review. ", "pdf": "/pdf/f4d6b1fb9fdc028d5bced9b1fcffbfed5fbfd050.pdf", "paperhash": "fang|parallel_recurrent_data_augmentation_for_gan_training_with_limited_and_diverse_data"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper57/Official_Review", "cdate": 1553713412036, "expdate": 1555718400000, "duedate": 1554681600000, "reply": {"forum": "HkxHFj5BdV", "replyto": "HkxHFj5BdV", "writers": {"description": "How your identity will be displayed.", "values-regex": "ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer[0-9]+"}, "signatures": {"values-regex": "ICLR.cc/2019/Workshop/LLD/Paper57/AnonReviewer[0-9]+"}, "readers": {"values": ["everyone"], "description": "The users who will be allowed to read the above content."}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["5: Top 15% of accepted papers, strong accept", "4: Top 50% of accepted papers, clear accept", "3: Marginally above acceptance threshold", "2: Marginally below acceptance threshold", "1: Strong rejection"], "required": true}, "confidence": {"order": 4, "value-radio": ["3: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "2: The reviewer is fairly confident that the evaluation is correct", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "tcdate": 1553713412036, "tmdate": 1555511822197, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Paper57/Reviewers"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}, {"id": "rkllb8JYFN", "original": null, "number": 1, "cdate": 1554736631853, "ddate": null, "tcdate": 1554736631853, "tmdate": 1555510987821, "tddate": null, "forum": "HkxHFj5BdV", "replyto": "HkxHFj5BdV", "invitation": "ICLR.cc/2019/Workshop/LLD/-/Paper57/Decision", "content": {"title": "Acceptance Decision", "decision": "Reject"}, "signatures": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Parallel Recurrent Data Augmentation for GAN training with Limited and Diverse Data", "authors": ["Boli Fang", "Miao Jiang"], "authorids": ["bfang@iu.edu", "miajiang@iu.edu"], "keywords": ["GAN training", "Data Augmentation"], "TL;DR": "We introduced a novel, simple, and efficient data augmentation method that boosts the performances of existing GANs when training data is limited and diverse.  ", "abstract": "The need for large amounts of training image data with clearly defined features is a major obstacle to applying generative adversarial networks(GAN) on image generation where training data is limited but diverse, since insufficient latent feature representation in the already scarce data often leads to instability and mode collapse during GAN training. To overcome the hurdle of limited data when applying GAN to limited datasets, we propose in this paper the strategy of \\textit{parallel recurrent data augmentation}, where the GAN model progressively enriches its training set with sample images constructed from GANs trained in parallel at consecutive training epochs. Experiments on a variety of small yet diverse datasets demonstrate that our method, with little model-specific considerations, produces images of better quality as compared to the images generated  without such strategy. The source code and generated images of this paper will be made public after review. ", "pdf": "/pdf/f4d6b1fb9fdc028d5bced9b1fcffbfed5fbfd050.pdf", "paperhash": "fang|parallel_recurrent_data_augmentation_for_gan_training_with_limited_and_diverse_data"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Workshop/LLD/-/Paper57/Decision", "cdate": 1554736073510, "reply": {"forum": "HkxHFj5BdV", "replyto": "HkxHFj5BdV", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-regex": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "signatures": {"values": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "description": "How your identity will be displayed."}, "content": {"title": {"order": 1, "required": true, "value": "Acceptance Decision"}, "decision": {"order": 2, "required": true, "value-radio": ["Accept", "Reject"], "description": "Acceptance decision"}, "comment": {"order": 3, "required": false, "value-regex": "[\\S\\s]{0,5000}", "description": ""}}}, "tcdate": 1554736073510, "tmdate": 1555510965343, "readers": ["everyone"], "writers": ["ICLR.cc/2019/Workshop/LLD"], "invitees": ["ICLR.cc/2019/Workshop/LLD/Program_Chairs"], "signatures": ["ICLR.cc/2019/Workshop/LLD"]}}}], "count": 4}