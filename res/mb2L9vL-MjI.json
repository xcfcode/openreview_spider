{"notes": [{"id": "mb2L9vL-MjI", "original": "Imj99AYCyPg", "number": 2383, "cdate": 1601308262821, "ddate": null, "tcdate": 1601308262821, "tmdate": 1614985639358, "tddate": null, "forum": "mb2L9vL-MjI", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models", "authorids": ["~Chao_Ma8", "~Lei_Wu1", "~Weinan_E1"], "authors": ["Chao Ma", "Lei Wu", "Weinan E"], "keywords": ["Gradient descent", "neural networks", "implicit regularization", "quenching-activation"], "abstract": "A  numerical and phenomenological study of the gradient descent (GD) algorithm for training two-layer neural network models is carried out for different parameter regimes. It is found that there are two distinctive phases in the GD dynamics in the under-parameterized regime: An early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) \u201cactivated\u201d neurons that dominate the dynamics and a group of ``quenched\u201d neurons that support the continued activation and deactivation process.  In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions.  This neural network-like behavior is continued into the mildly over-parameterized regime, in which it undergoes a transition to a random feature-like behavior where the inner-layer parameters are effectively frozen during the training process.  The quenching process seems to provide a clear mechanism for  ``implicit regularization''. This is qualitatively different from the GD dynamics associated with the ``mean-field'' scaling where all neurons participate equally.", "one-sentence_summary": "The gradient descent dynamics  for two-layer neural networks exhibits a quenching-activation behavior. ", "pdf": "/pdf/dcb8090b97dedba1dd82ca18e60a190bca43ab49.pdf", "supplementary_material": "/attachment/8556896159dee26ae72dec74abe8db4ca262fc8c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|the_quenchingactivation_behavior_of_the_gradient_descent_dynamics_for_twolayer_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=MTT5jTOWdm", "_bibtex": "@misc{\nma2021the,\ntitle={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},\nauthor={Chao Ma and Lei Wu and Weinan E},\nyear={2021},\nurl={https://openreview.net/forum?id=mb2L9vL-MjI}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "nUcCSGxpTEC", "original": null, "number": 1, "cdate": 1610040522170, "ddate": null, "tcdate": 1610040522170, "tmdate": 1610474131021, "tddate": null, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "invitation": "ICLR.cc/2021/Conference/Paper2383/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "This paper empirically investigates the gradient dynamic of two-layer network nets with ReLU activations on synthetic datasets under $L^2$ loss. The empirical results show that for a specific type of initialization and less overparametrized neural nets, the gradient dynamics experience two phases: a phase that follows the random features model where all the neurons are *quenched* and another phase where there are a few *activated* neurons. As pointed out by Reviewer 1, this paper lacks mathematical support and did not distinguish between *random features model* and *neural tangent model*. Reviewer 3 and Reviewer 4 also complained that the paper is purely experimental. Therefore, this paper may benefit from proposing an at least heuristic or high-level conjecture/interpretation/argument that tries to explain the empirical results.  "}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models", "authorids": ["~Chao_Ma8", "~Lei_Wu1", "~Weinan_E1"], "authors": ["Chao Ma", "Lei Wu", "Weinan E"], "keywords": ["Gradient descent", "neural networks", "implicit regularization", "quenching-activation"], "abstract": "A  numerical and phenomenological study of the gradient descent (GD) algorithm for training two-layer neural network models is carried out for different parameter regimes. It is found that there are two distinctive phases in the GD dynamics in the under-parameterized regime: An early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) \u201cactivated\u201d neurons that dominate the dynamics and a group of ``quenched\u201d neurons that support the continued activation and deactivation process.  In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions.  This neural network-like behavior is continued into the mildly over-parameterized regime, in which it undergoes a transition to a random feature-like behavior where the inner-layer parameters are effectively frozen during the training process.  The quenching process seems to provide a clear mechanism for  ``implicit regularization''. This is qualitatively different from the GD dynamics associated with the ``mean-field'' scaling where all neurons participate equally.", "one-sentence_summary": "The gradient descent dynamics  for two-layer neural networks exhibits a quenching-activation behavior. ", "pdf": "/pdf/dcb8090b97dedba1dd82ca18e60a190bca43ab49.pdf", "supplementary_material": "/attachment/8556896159dee26ae72dec74abe8db4ca262fc8c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|the_quenchingactivation_behavior_of_the_gradient_descent_dynamics_for_twolayer_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=MTT5jTOWdm", "_bibtex": "@misc{\nma2021the,\ntitle={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},\nauthor={Chao Ma and Lei Wu and Weinan E},\nyear={2021},\nurl={https://openreview.net/forum?id=mb2L9vL-MjI}\n}"}, "tags": [], "invitation": {"reply": {"forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040522157, "tmdate": 1610474131005, "id": "ICLR.cc/2021/Conference/Paper2383/-/Decision"}}}, {"id": "7qY6jOeyfcT", "original": null, "number": 1, "cdate": 1603758771789, "ddate": null, "tcdate": 1603758771789, "tmdate": 1605024223799, "tddate": null, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "invitation": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review", "content": {"title": "Interesting empirical observation; lack comments on the neural tangent scaling; lack mathematical support. ", "review": "This paper studies the gradient dynamics of two-layers neural networks. It is empirically shown that, for a specific type of initialization, for less over-parameterized neural networks, the gradient dynamics follows two phases: a phase that follows the random features model where all the neurons are \"quenched\", and another phase in which there are a few \"activated\" neurons. This type of behavior of gradient dynamics is different from that of the mean-field regime. \n\nThis paper is well-written. The quenching and active phenomena is interesting. Existing theory characterized the behavior of neural networks in the mean-field/neural tangent regime, and this paper studied a regime that is different from the existing ones. The authors used the effective dynamics to explain the second phase of the dynamics, which gives some good intuitions. \n\nThere are the following issues of this paper. \n1. This paper did not distinguish between \"random features model\" and \"neural tangent model\", although they are closely related. In particular, Theorem 1 presented in this paper is connected to the \"random features model\" as in [E, et.al. 2020], which does not really give the \"neural tangent kernel\" in [Jacot, et.al. 2018] and [Du, et.al., 2019b]. The difference is that, \"random features model\" only the linearization of the top layer, while the \"neural tangent kernel\" also contains the linearization of the bottom layer. Although they are closely connected, it is important to clearly point out the distinction. Moreover, the \"neural tangent\" scaling is a different scaling (different than both MF and RF scaling), where there is a 1/\\sqrt{m} coefficient in front of the network. I think at least the authors should discuss what happens for the \"neural tangent\" scaling when it is less over-parameterized. \n2. This is a pure empirical paper and the phenomenon has no justification (even heuristic mathematical justification). It would be good if the authors can (heuristically) explain which neuron will become activated (any random neuron, or a neuron that satisfies some initial condition?), and how long it will take for those neurons to become activated (perhaps this depends on the initial weights of the neurons). Without these justifications, I feel that the paper is less complete. (In comparison, the MF and NT results are rigorously established. )\n\nOverall, I think the phenomenon discovered in this paper is interesting. However, it didn't comment upon the neural tangent scaling, and it lacks mathematical support. I feel that this paper is on the borderline. I will consider raising my score if the authors can resolve these issues. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2383/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2383/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models", "authorids": ["~Chao_Ma8", "~Lei_Wu1", "~Weinan_E1"], "authors": ["Chao Ma", "Lei Wu", "Weinan E"], "keywords": ["Gradient descent", "neural networks", "implicit regularization", "quenching-activation"], "abstract": "A  numerical and phenomenological study of the gradient descent (GD) algorithm for training two-layer neural network models is carried out for different parameter regimes. It is found that there are two distinctive phases in the GD dynamics in the under-parameterized regime: An early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) \u201cactivated\u201d neurons that dominate the dynamics and a group of ``quenched\u201d neurons that support the continued activation and deactivation process.  In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions.  This neural network-like behavior is continued into the mildly over-parameterized regime, in which it undergoes a transition to a random feature-like behavior where the inner-layer parameters are effectively frozen during the training process.  The quenching process seems to provide a clear mechanism for  ``implicit regularization''. This is qualitatively different from the GD dynamics associated with the ``mean-field'' scaling where all neurons participate equally.", "one-sentence_summary": "The gradient descent dynamics  for two-layer neural networks exhibits a quenching-activation behavior. ", "pdf": "/pdf/dcb8090b97dedba1dd82ca18e60a190bca43ab49.pdf", "supplementary_material": "/attachment/8556896159dee26ae72dec74abe8db4ca262fc8c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|the_quenchingactivation_behavior_of_the_gradient_descent_dynamics_for_twolayer_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=MTT5jTOWdm", "_bibtex": "@misc{\nma2021the,\ntitle={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},\nauthor={Chao Ma and Lei Wu and Weinan E},\nyear={2021},\nurl={https://openreview.net/forum?id=mb2L9vL-MjI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097588, "tmdate": 1606915806347, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2383/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review"}}}, {"id": "Lp_RdmZQb4", "original": null, "number": 2, "cdate": 1603837970592, "ddate": null, "tcdate": 1603837970592, "tmdate": 1605024223734, "tddate": null, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "invitation": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review", "content": {"title": "Interesting finding, but not a complete picture", "review": "The paper experimentally identifies a phenomenon in which the participation among the neurons in a two-layer neural net trained with gradient descent (GD) changes with time. In particular: \n- The paper shows for certain artificial target functions with low-dimensional structures, the neurons start out from a random-feature behavior and then a few are selectively activated, distinct from the rest of the neurons which are \u201csuppressed\u201d. \n- This behavior is shown to be pronounced when the network is less (or mildly) over-parameterized and suffers less from the curse of dimensionality (CoD). It is almost unobservable when the network has width m exceeding the training size n and suffers more from CoD. \n- Lastly the paper argues that this behavior is specific to the choice of scaling, by showing that under the mean field scaling, the network displays much less sensitivity to the scale of the width.\n\nLarge-width neural networks have been a central object of studies for the past few years. With certain scalings (typically collectively referred to as NTK scalings), as width goes to infinity, the network becomes \u201clinearized\u201d. This is probably among the few works that studies how the network under NTK scaling can behave beyond the linearized regime. In particular, while most of these few works concern with the mathematical question of \u201chow much\u201d the network can learn better than linearized methods, this work identifies a novel mechanism on \u201chow different\u201d the network\u2019s behavior is from linearized methods. I\u2019m unaware if the quenching/activation mechanism has been reported before. In this sense, the result of the paper has an interesting position in this literature.\n\nThe measurement of a neuron\u2019s participation via the product of the two weights is interesting and elucidative of the use of the path norm. The presentation is also clear and easy to follow.\n\nThe paper however has several shortcomings. In no particular order:\n\n1) The target functions are idealized, and no experiments are reported on more realistic settings. Is the quenching/activation mechanism a common phenomenon with real life datasets? Are the transition boundaries n/(d+1) and n universal? We observe from Fig. 4 that no clear quenching/activation process takes place, so it\u2019s questionable whether it can be observed for a difficult and highly diverse real life dataset. A suggestion is to perform experiments on MNIST, CIFAR-10 and CIFAR-100 (which might resemble the single neuron, circle neuron and surface neuron examples).\n\n2) The paper may consider having a discussion on recent works (e.g. [1, 2]) that consider linearized behaviors in the regime where m depends mildly on n/d or is almost independent of n, and whether they may conflict with the result in the paper.\n\n3) The paper gives no explanation or heuristic to understand how the quenching/activation phenomenon arises. In particular, can we know in advance which neuron is going to be activated, even in the simple case of single neuron target function? The paper mentions \u201cThe same behavior was observed with other realizations of the initial data, except that the number of activated neurons can be different.\u201d Does it mean that it is entirely possible, although unlikely, that all neurons are activated? If the size of the activated neuron set is highly unpredictable, it would limit usefulness of the effective dynamics.\n\n4) It is unclear how to make the effective dynamics a useful tool. It looks like the switching time (at which we switch from the first phase to the second phase) and the sets I1 and I2 have to be observed a priori from the experiment with the full dynamics. Is there an understanding on simple properties (e.g. how large) of these quantities?\n\n5) In the derivation of the effective dynamics, it is unclear why the second layer\u2019s weights of the quenched neurons move faster than the first layer\u2019s weights. We only know from Theorem 1 that such timescale separation occurs when the network is random-feature-like, but we do not know if it is so during the quenching process in the mildly over-parameterized and under-parameterized regimes.  One can actually observe from Fig. 1 that most neurons are still not quenched when the network departs from the random-feature behavior.\n\n6) The paper suggests that due to quenching of most neurons, the path norm remains small since it is essentially a sum over only activated neurons. However the circle neuron target example shows that the population risk continues to decrease once a neuron emerges from the quenched state and becomes activated. Does this somehow violate the path norm intuition? How does the path norm look like through time?\n\nUltimately I find the finding interesting, but the paper leaves a lot of questions unanswered. Still I would encourage the authors substantiate the paper with more analyses and findings, empirical or theoretical, since this is quite a novel finding.\n\n[1] Polylogarithmic width suffices for gradient descent to achieve arbitrary small test error with shallow relu networks, Ji and Telgarsky, ICLR 2020.\n\n[2] Neural networks Learning and Memorization with (almost) no Over-Parameterization, Daniely, 2019.", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2383/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2383/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models", "authorids": ["~Chao_Ma8", "~Lei_Wu1", "~Weinan_E1"], "authors": ["Chao Ma", "Lei Wu", "Weinan E"], "keywords": ["Gradient descent", "neural networks", "implicit regularization", "quenching-activation"], "abstract": "A  numerical and phenomenological study of the gradient descent (GD) algorithm for training two-layer neural network models is carried out for different parameter regimes. It is found that there are two distinctive phases in the GD dynamics in the under-parameterized regime: An early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) \u201cactivated\u201d neurons that dominate the dynamics and a group of ``quenched\u201d neurons that support the continued activation and deactivation process.  In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions.  This neural network-like behavior is continued into the mildly over-parameterized regime, in which it undergoes a transition to a random feature-like behavior where the inner-layer parameters are effectively frozen during the training process.  The quenching process seems to provide a clear mechanism for  ``implicit regularization''. This is qualitatively different from the GD dynamics associated with the ``mean-field'' scaling where all neurons participate equally.", "one-sentence_summary": "The gradient descent dynamics  for two-layer neural networks exhibits a quenching-activation behavior. ", "pdf": "/pdf/dcb8090b97dedba1dd82ca18e60a190bca43ab49.pdf", "supplementary_material": "/attachment/8556896159dee26ae72dec74abe8db4ca262fc8c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|the_quenchingactivation_behavior_of_the_gradient_descent_dynamics_for_twolayer_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=MTT5jTOWdm", "_bibtex": "@misc{\nma2021the,\ntitle={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},\nauthor={Chao Ma and Lei Wu and Weinan E},\nyear={2021},\nurl={https://openreview.net/forum?id=mb2L9vL-MjI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097588, "tmdate": 1606915806347, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2383/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review"}}}, {"id": "0sDvv8iVbBE", "original": null, "number": 3, "cdate": 1603885747622, "ddate": null, "tcdate": 1603885747622, "tmdate": 1605024223671, "tddate": null, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "invitation": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review", "content": {"title": "Large set of experiments revealing new observations on gradient descent dynamics in a simple, controlled setting ", "review": "This paper studies gradient descent dynamics of two-layer neural networks with ReLU activation function. In a controlled setting, a list of experiments investigates the dynamics and compares them for different regimes where the relation of number of hidden neurons to number of training samples is changed and for several specific target functions. The paper proposes that training happens in two steps: The first phase resembles the learning of a random feature model (RFM) where only the output layer coefficients are changed. The second phase can activate or deactivate neurons and decreases coefficients to favor sparse solutions if the target function admits a sparse solution. The dynamics are further compared the setting of mean-field scaling, which shows a different learning dynamic.\n\nPros:\n+ The paper presents experiments on dynamics of gradient descent with the goal to give new insight on the important and unsolved phenomenon of implicit bias observed in deep neural networks. \n+ By starting in a (simplified) controlled setting, a large scope of experiments is conducted that studies different aspects and different training regimes. The setting and the experiments are all well-described and clearly presented. Interesting behavior of the learning dynamics can therefore be shown. \n+ With the largely overparameterized setting fairly well-understood, the current paper approaches the medium-overparameterized regime where the learning dynamics are more complicated and more close to neural networks in practice. Therefore, the paper targets an important knowledge gap.\n\nWhile some interesting observations can be made, I consider the presented results and gained knowledge as limited in scope and hence slightly tend to suggest to reject.\n\nCons:\n- In particular, no clear conclusion can be made from the results. In fact, there is no conjecture of how the results may manifest in practical neural networks. (In case I missed such a conjecture, then I wonder why the conjecture was not tested in a practical setting.) \n- The paper is purely experimental. Since it considers largely simplified settings (specific target functions, data sampled uniformly from the unit sphere, in most experiments the true solution can be found with a sub-network consisting of only one hidden neuron) and since the networks and optimization method (gradient descent) are rather simple, one could expect that at least some theoretical contribution or explanation could be given, which the paper lacks entirely. \n- While there is a large scope of experiments conducted, some results are only partial and draw conclusions without further investigations. It is hard to tell from the presented results how characteristic they are for other settings or behave under small changes such as the input dimension, target function, loss function, etc. \nFurther more detailed remarks on the specific contributions and the presentation are below. \n\nFor the author\u2019s reply, I would appreciate a clarification of the exact contributions summarizing the findings and its possible implications for the training of practical neural networks.\n\n\nComments on the specific contributions:\n- The first contribution points out the existence two phases. In the first phase, the weights of the first layer do not change and the model behaves like a random feature model. This phenomenon seems to be limited to a few iterations where it is an almost trivial observation considering that the second-layer weights are initialized at zero (which implies a vanishing gradient for first layer weights). \n- The third mentioned contribution is peculiar since the only change is a factor of 1/m to the network function, which is equivalent (for the considered squared loss) to a scaling of the target function. The stated contribution is therefore that this scaling changes the observed dynamics significantly, which casts doubt when the observations generalize to other settings.\n- The observed behavior aims to explain the implicit bias of neural networks. However, this behavior can also be observed in overparamterized networks and the observed dynamics differ in the considered regimes of under-and overparameterized networks. This poses the question in which way the implicit bias is explained by the observations.\n - The paper suffers from its presentation. The list of contributions mentions an observed transition (what kind of transition?) and consider undefined terminology of quenched neurons. The terminology of quenched neurons is only loosely explained much later on page 4 as neurons that are \u201cquenched\u201d in the sense that their outer layer coefficients eventually start decreasing and then keep decreasing. (Shall quenching neurons have coefficients that converge to zero? If neurons can still pop up at a later stage, are they still considered quenched neurons? Are neurons with constant output layer coefficients but norm-decreasing inner layer coefficients also quenching (this difference cannot be distinguished in the plots)? It would be good to make precise what the authors mean by quenched neurons.\n\n\nMore detailed remarks:\n- First line page 3: citation seems misplaced, as I was unable to find the result in the paper. Please update reference or show a proof.\n- How many iterations does the first phase consist of where the model neural network behaves like a RFM? \n- Is it correct that the two phases in the loss development can not consistently be matched with changes in the parameters? \n- Is it correct that in the most realistic setting 3.1.2, there is barely a quenching behavior visible?\n- I was wondering about the notion of overparameterization. Why would a network be underparameterized just because the width is small even if the target function can be learned with a single neuron?It sounds reasonable to consider the setting of training polynomials of maximal degree 30 to learn a linear function as an overparameterized problem, no matter how many samples are considered for training. \n- The consideration to compare m being proportional to n against m being proportional to n/(d+1) has little meaning without fixing or comparing the respective constants (m=10^9*n/(d+1) vs m=10^-9*n also satisfies the stated proportionality), and most importantly without experimenting with different dimensions d. Since it is not investigated how the behavior changes with changes of d, there cannot be made any conclusion about the different regimes. Also the consistency claim in 4.2 is meaningless when changing the constants. It would be consistent if in 4.1 one would consider m=n and m=n/(d+1) instead of these terms scaled by constants (For a single d, suitable constants can always be found.) If no experiments for changing input dimension are carried out, then all we need to care about is the quotient m/n. \n\n\nTypos:\nFigure 6: legend not entirely visible \nConclusion: \u201ethe the\u201c", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2383/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2383/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models", "authorids": ["~Chao_Ma8", "~Lei_Wu1", "~Weinan_E1"], "authors": ["Chao Ma", "Lei Wu", "Weinan E"], "keywords": ["Gradient descent", "neural networks", "implicit regularization", "quenching-activation"], "abstract": "A  numerical and phenomenological study of the gradient descent (GD) algorithm for training two-layer neural network models is carried out for different parameter regimes. It is found that there are two distinctive phases in the GD dynamics in the under-parameterized regime: An early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) \u201cactivated\u201d neurons that dominate the dynamics and a group of ``quenched\u201d neurons that support the continued activation and deactivation process.  In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions.  This neural network-like behavior is continued into the mildly over-parameterized regime, in which it undergoes a transition to a random feature-like behavior where the inner-layer parameters are effectively frozen during the training process.  The quenching process seems to provide a clear mechanism for  ``implicit regularization''. This is qualitatively different from the GD dynamics associated with the ``mean-field'' scaling where all neurons participate equally.", "one-sentence_summary": "The gradient descent dynamics  for two-layer neural networks exhibits a quenching-activation behavior. ", "pdf": "/pdf/dcb8090b97dedba1dd82ca18e60a190bca43ab49.pdf", "supplementary_material": "/attachment/8556896159dee26ae72dec74abe8db4ca262fc8c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|the_quenchingactivation_behavior_of_the_gradient_descent_dynamics_for_twolayer_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=MTT5jTOWdm", "_bibtex": "@misc{\nma2021the,\ntitle={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},\nauthor={Chao Ma and Lei Wu and Weinan E},\nyear={2021},\nurl={https://openreview.net/forum?id=mb2L9vL-MjI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097588, "tmdate": 1606915806347, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2383/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review"}}}, {"id": "yOeMIj_-Ld6", "original": null, "number": 4, "cdate": 1604615917215, "ddate": null, "tcdate": 1604615917215, "tmdate": 1605024223600, "tddate": null, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "invitation": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review", "content": {"title": "Review", "review": "**SUMMARY** The authors perform an empirical study of the dynamics of gradient decent for single-hidden-layer NNs with ReLU activations on some synthetic datasets under L2 loss. They identify two distinct phases: an early phase matching RF models and a late phase where the neurons split into activated and quenched groups. When the target function admits a sparse representation, the learned function can have a sparse activation pattern, which is interpreted as a form of implicit regularization.\n\n**PROS** The authors identify a mechanism of quenching and sparse activation patterns that should be of interest to researchers studying the generalization properties of NNs. The mechanism seems sensitive to the data distribution and the network width \n\n**CONS** My main concern is about the robustness of the results. Several simplifying or arbitrary choices are made, which while necessary for theoretical analysis, should be analyzed more critically in an empirical paper. Specifically, the results are restricted to a simple single-hidden-layer architecture, ReLU activation functions, and a simplistic, synthetic data distribution. \n\n\u2028**RECOMMENDATION** In its current form, I vote for rejecting the paper. While there are some interesting observations, the uncertainty about their robustness and the absence of a theoretical explanation weaken the paper. Addressing either of these shortcomings would improve the paper.\n\n\u2028 **ADDITIONAL QUESTIONS** \n- The scaling for the first-layer weights b seems odd and nonstandard. Since the data are points on the unit sphere, the scaling I/d means the pre/post-activations are little-o 1. Is this intentional? What happens if the b_ij are order 1?\n- How are \\mathbb{I}_1 and \\mathbb{I}_2 determined in Sec. 3.2 and Fig. 5?\n- How correlated are the test error and the path norm? In Fig. 8 they look related, but it\u2019s difficult to tell how much.\n- Is the step behavior in Fig. 2 related to Ghorbani, Mei et al. 2020?\n\n**MINOR COMMENTS**\n- Some additional motivation for f* when pi* is uniform on the unit circle would be helpful.\n- Consider including some additional citations for the random feature model, particularly in Sec. 4.1: Mei and Montanari 2019; d'Ascoli, Refinetti et al. 2020; Adlam and Pennington 2020. They study the effects of under/overparameterization on the generalization error.\n- I also think the overall structure of the paper could be improved to support the main claim the paper is making. ", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper2383/AnonReviewer5"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper2383/AnonReviewer5"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models", "authorids": ["~Chao_Ma8", "~Lei_Wu1", "~Weinan_E1"], "authors": ["Chao Ma", "Lei Wu", "Weinan E"], "keywords": ["Gradient descent", "neural networks", "implicit regularization", "quenching-activation"], "abstract": "A  numerical and phenomenological study of the gradient descent (GD) algorithm for training two-layer neural network models is carried out for different parameter regimes. It is found that there are two distinctive phases in the GD dynamics in the under-parameterized regime: An early phase in which the GD dynamics follows closely that of the corresponding random feature model, followed by a late phase in which the neurons are divided into two groups: a group of a few (maybe none) \u201cactivated\u201d neurons that dominate the dynamics and a group of ``quenched\u201d neurons that support the continued activation and deactivation process.  In particular, when the target function can be accurately approximated by a relatively small number of neurons, this quenching-activation process biases GD to picking sparse solutions.  This neural network-like behavior is continued into the mildly over-parameterized regime, in which it undergoes a transition to a random feature-like behavior where the inner-layer parameters are effectively frozen during the training process.  The quenching process seems to provide a clear mechanism for  ``implicit regularization''. This is qualitatively different from the GD dynamics associated with the ``mean-field'' scaling where all neurons participate equally.", "one-sentence_summary": "The gradient descent dynamics  for two-layer neural networks exhibits a quenching-activation behavior. ", "pdf": "/pdf/dcb8090b97dedba1dd82ca18e60a190bca43ab49.pdf", "supplementary_material": "/attachment/8556896159dee26ae72dec74abe8db4ca262fc8c.zip", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "ma|the_quenchingactivation_behavior_of_the_gradient_descent_dynamics_for_twolayer_neural_network_models", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=MTT5jTOWdm", "_bibtex": "@misc{\nma2021the,\ntitle={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},\nauthor={Chao Ma and Lei Wu and Weinan E},\nyear={2021},\nurl={https://openreview.net/forum?id=mb2L9vL-MjI}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "mb2L9vL-MjI", "replyto": "mb2L9vL-MjI", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper2383/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538097588, "tmdate": 1606915806347, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper2383/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper2383/-/Official_Review"}}}], "count": 6}