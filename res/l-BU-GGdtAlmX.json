{"notes": [{"ddate": null, "legacy_migration": true, "tmdate": 1392220560000, "tcdate": 1392220560000, "number": 5, "id": "77QH7SSVP57bw", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "l-BU-GGdtAlmX", "replyto": "l-BU-GGdtAlmX", "signatures": ["anonymous reviewer ea58"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Generative NeuroEvolution for Deep Learning", "review": "Many straightforward variants of combinations of neuroevolution and backprop have been tried in the past, never with very convincing results. Unfortunately, the same holds true here. Don't get me wrong, I do not think the approach is in principle flawed, but in my opinion, very convincing, robust-to-outstanding performance is necessary for this type of paper to be allowed on a conference track like this. Sorry."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative NeuroEvolution for Deep Learning", "decision": "submitted, no decision", "abstract": "An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.", "pdf": "https://arxiv.org/abs/1312.5355", "paperhash": "verbancsics|generative_neuroevolution_for_deep_learning", "keywords": [], "conflicts": [], "authors": ["Phillip Verbancsics", "Josh Harguess"], "authorids": ["verbancsics@gmail.com", "joshua.harguess@navy.mil"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391824860000, "tcdate": 1391824860000, "number": 2, "id": "WWaVUgKzVrRHT", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "l-BU-GGdtAlmX", "replyto": "l-BU-GGdtAlmX", "signatures": ["anonymous reviewer 7541"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Generative NeuroEvolution for Deep Learning", "review": "The paper proposes to automatically learn the structure of a deep neural network using Neuro Evolution (NE) techniques. In particular the authors apply this technique called HyperNEAT (Hypercube based Neuro Evolution of Augmenting Topologies) to learn the structure of a deep convolutional network. This is achieved by learning an indirect encoding which encodes the weight pattern of the neural network. Once learnt, the final network is fine tuned using backpropagation. The technique is applied to the task of identifying hand written numbers in the MNIST dataset. \r\n\r\nIn deep learning community, in a number of papers it has been shown that the choice of architecture plays an important role in the performance of the final trained model on any task. However, the question of 'how to choose the right architecture' has not got much attention. In that sense this paper tries to address this important question by automatically learning the network architecture using Neuro Evolution techniques. Unfortunately however, I think the paper falls way short of making any significant contribution in that direction. \r\n\r\nNovelty: I think there are not a lot novel ideas presented in the paper. Its a straight-forward application of HyperNEAT to CNNs. There is no new technique/model being proposed here. \r\n\r\nQuality of results: First, I feel the MNIST dataset is beaten to death and claiming it to be a real world task is probably not true any more. Second, the results reported on MNIST are quite underwhelming. The best accuracy achieved is 92.1% whereas the state-of-the-art results are in the range of 99+%. I would expect that for a technique to be considered promising it should have performance at least in the ball-park of state-of-the-art, if not best. \r\n\r\nPrevious work: Since I'm not an expert in the area it is hard for me to point any (if there are so) missing references to the previous work in the field. That said a couple of claims by the authors are not true. For instance, references 30 and 31 do not provide the state-of-the-art. \r\n\r\nClarity of presentation: I think the quality of the write-up is quite average. For instance, the authors could have explained the HyperNEAT architecture (section 2.2) much better, especially for people who are not experts in the area. I don't see the point of putting the HyperNEAT-LEO algorithm in section 2.2, since it is not used anywhere in the paper. Also the paper is littered with typos. \r\n\r\nSummary: Though the paper tries to address an important problem in deep learning research, I believe its falls short of delivering. The proposed approach is a straight-forward application of HyperNEAT, and the results on the standard dataset are quite poor. This to me suggests that some work needs to be done before one can consider it as a conference publication."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative NeuroEvolution for Deep Learning", "decision": "submitted, no decision", "abstract": "An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.", "pdf": "https://arxiv.org/abs/1312.5355", "paperhash": "verbancsics|generative_neuroevolution_for_deep_learning", "keywords": [], "conflicts": [], "authors": ["Phillip Verbancsics", "Josh Harguess"], "authorids": ["verbancsics@gmail.com", "joshua.harguess@navy.mil"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391824860000, "tcdate": 1391824860000, "number": 3, "id": "WWcAWQxupa6B_", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "l-BU-GGdtAlmX", "replyto": "l-BU-GGdtAlmX", "signatures": ["anonymous reviewer 7541"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Generative NeuroEvolution for Deep Learning", "review": "The paper proposes to automatically learn the structure of a deep neural network using Neuro Evolution (NE) techniques. In particular the authors apply this technique called HyperNEAT (Hypercube based Neuro Evolution of Augmenting Topologies) to learn the structure of a deep convolutional network. This is achieved by learning an indirect encoding which encodes the weight pattern of the neural network. Once learnt, the final network is fine tuned using backpropagation. The technique is applied to the task of identifying hand written numbers in the MNIST dataset. \r\n\r\nIn deep learning community, in a number of papers it has been shown that the choice of architecture plays an important role in the performance of the final trained model on any task. However, the question of 'how to choose the right architecture' has not got much attention. In that sense this paper tries to address this important question by automatically learning the network architecture using Neuro Evolution techniques. Unfortunately however, I think the paper falls way short of making any significant contribution in that direction. \r\n\r\nNovelty: I think there are not a lot novel ideas presented in the paper. Its a straight-forward application of HyperNEAT to CNNs. There is no new technique/model being proposed here. \r\n\r\nQuality of results: First, I feel the MNIST dataset is beaten to death and claiming it to be a real world task is probably not true any more. Second, the results reported on MNIST are quite underwhelming. The best accuracy achieved is 92.1% whereas the state-of-the-art results are in the range of 99+%. I would expect that for a technique to be considered promising it should have performance at least in the ball-park of state-of-the-art, if not best. \r\n\r\nPrevious work: Since I'm not an expert in the area it is hard for me to point any (if there are so) missing references to the previous work in the field. That said a couple of claims by the authors are not true. For instance, references 30 and 31 do not provide the state-of-the-art. \r\n\r\nClarity of presentation: I think the quality of the write-up is quite average. For instance, the authors could have explained the HyperNEAT architecture (section 2.2) much better, especially for people who are not experts in the area. I don't see the point of putting the HyperNEAT-LEO algorithm in section 2.2, since it is not used anywhere in the paper. Also the paper is littered with typos. \r\n\r\nSummary: Though the paper tries to address an important problem in deep learning research, I believe its falls short of delivering. The proposed approach is a straight-forward application of HyperNEAT, and the results on the standard dataset are quite poor. This to me suggests that some work needs to be done before one can consider it as a conference publication."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative NeuroEvolution for Deep Learning", "decision": "submitted, no decision", "abstract": "An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.", "pdf": "https://arxiv.org/abs/1312.5355", "paperhash": "verbancsics|generative_neuroevolution_for_deep_learning", "keywords": [], "conflicts": [], "authors": ["Phillip Verbancsics", "Josh Harguess"], "authorids": ["verbancsics@gmail.com", "joshua.harguess@navy.mil"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391824860000, "tcdate": 1391824860000, "number": 4, "id": "D016DNjipxPiU", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "l-BU-GGdtAlmX", "replyto": "l-BU-GGdtAlmX", "signatures": ["anonymous reviewer 7541"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Generative NeuroEvolution for Deep Learning", "review": "The paper proposes to automatically learn the structure of a deep neural network using Neuro Evolution (NE) techniques. In particular the authors apply this technique called HyperNEAT (Hypercube based Neuro Evolution of Augmenting Topologies) to learn the structure of a deep convolutional network. This is achieved by learning an indirect encoding which encodes the weight pattern of the neural network. Once learnt, the final network is fine tuned using backpropagation. The technique is applied to the task of identifying hand written numbers in the MNIST dataset. \r\n\r\nIn deep learning community, in a number of papers it has been shown that the choice of architecture plays an important role in the performance of the final trained model on any task. However, the question of 'how to choose the right architecture' has not got much attention. In that sense this paper tries to address this important question by automatically learning the network architecture using Neuro Evolution techniques. Unfortunately however, I think the paper falls way short of making any significant contribution in that direction. \r\n\r\nNovelty: I think there are not a lot novel ideas presented in the paper. Its a straight-forward application of HyperNEAT to CNNs. There is no new technique/model being proposed here. \r\n\r\nQuality of results: First, I feel the MNIST dataset is beaten to death and claiming it to be a real world task is probably not true any more. Second, the results reported on MNIST are quite underwhelming. The best accuracy achieved is 92.1% whereas the state-of-the-art results are in the range of 99+%. I would expect that for a technique to be considered promising it should have performance at least in the ball-park of state-of-the-art, if not best. \r\n\r\nPrevious work: Since I'm not an expert in the area it is hard for me to point any (if there are so) missing references to the previous work in the field. That said a couple of claims by the authors are not true. For instance, references 30 and 31 do not provide the state-of-the-art. \r\n\r\nClarity of presentation: I think the quality of the write-up is quite average. For instance, the authors could have explained the HyperNEAT architecture (section 2.2) much better, especially for people who are not experts in the area. I don't see the point of putting the HyperNEAT-LEO algorithm in section 2.2, since it is not used anywhere in the paper. Also the paper is littered with typos. \r\n\r\nSummary: Though the paper tries to address an important problem in deep learning research, I believe its falls short of delivering. The proposed approach is a straight-forward application of HyperNEAT, and the results on the standard dataset are quite poor. This to me suggests that some work needs to be done before one can consider it as a conference publication."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative NeuroEvolution for Deep Learning", "decision": "submitted, no decision", "abstract": "An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.", "pdf": "https://arxiv.org/abs/1312.5355", "paperhash": "verbancsics|generative_neuroevolution_for_deep_learning", "keywords": [], "conflicts": [], "authors": ["Phillip Verbancsics", "Josh Harguess"], "authorids": ["verbancsics@gmail.com", "joshua.harguess@navy.mil"]}, "tags": [], "invitation": {}}}, {"ddate": null, "legacy_migration": true, "tmdate": 1391449260000, "tcdate": 1391449260000, "number": 1, "id": "2jciw0IMK9w1_", "invitation": "ICLR.cc/2014/-/submission/conference/review", "forum": "l-BU-GGdtAlmX", "replyto": "l-BU-GGdtAlmX", "signatures": ["anonymous reviewer a613"], "readers": ["everyone"], "writers": ["anonymous"], "content": {"title": "review of Generative NeuroEvolution for Deep Learning", "review": "Generative NeuroEvolution for Deep Learning\r\nPhillip Verbancsics & Josh Harguess\r\n\r\nSummary: Hyperneat neuroevolution is applied to MNIST, then fine-tuned through backprop. Results are perhaps not that overwhelming in terms of accuracy, but interesting.\r\n\r\nLots of relevant work seems to be missing though. For example, earlier indirect encodings were proposed by the following works:\r\n\r\nA. Lindenmayer: Mathematical models for cellular interaction in development. In: J. Theoret. Biology. 18. 1968, 280-315.\r\n\r\nH. Kitano. Designing neural networks using genetic algorithms with graph generation system. Complex Systems, 4:461-476, 1990.\r\n\r\nC. Jacob , A. Lindenmayer , G. Rozenberg. Genetic L-System Programming, Parallel Problem Solving from Nature III, Lecture Notes in Computer Science, 1994\r\n\r\nA universal approach to indirect encoding based on a universal programming language for encoding weight matrices with low Kolmogorov complexity:\r\n\r\nJ.  Schmidhuber. Discovering solutions with low Kolmogorov complexity and high generalization capability. In A. Prieditis and S. Russell, editors, Machine Learning: Proceedings of the Twelfth International Conference (ICML 1995), pages 488-496. Morgan Kaufmann Publishers, San Francisco, CA, 1995.\r\n\r\np 3:  'Such object recognition tasks where deep learning has achieved the best results include the MNIST hand-written digit dataset [30, 31], \u2026 and the ImageNet Large-Scale Visual Recognition Challenge [33]' \r\n\r\nBut refs [30, 31] do not have best results on MNIST - as of 2012, the best result was in:\r\n\r\nD. Ciresan, U. Meier, J. Schmidhuber. Multi-column Deep Neural Networks for Image Classification. Proc. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012, p 3642-3649, 2012.\r\n\r\nThere may be even better results of 2013? And to my knowledge the best result on ImageNet is here:\r\n\r\nM. D. Zeiler, R. Fergus. Visualizing and Understanding Convolutional Networks. TR arXiv:1311.2901 [cs.CV], 2013.\r\n\r\np 3: 'vanishing gradient' - why not cite the man who first discovered this:\r\n\r\nS. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, TUM, 1991\r\n\r\np 3: 'auto-encoders' - I think the authors mean stacks of auto-encoders - here one can cite:\r\n\r\n[35] D. H. Ballard. Modular learning in neural networks. Proc. AAAI-87, Seattle, WA, p 279-284, 1987\r\n\r\np 4: 'In this way, HyperNEAT acts as a reinforcement learning approach that determines the best features to extract for another machine learning approach to maximize performance on the task'\r\n\r\nSo it is like Evolino, where evolution is used to determine the best features to extract for another machine learning approach to maximize performance - what is the main difference to the present approach? See:\r\n\r\nJ. Schmidhuber, D. Wierstra, M. Gagliolo, F. Gomez. Training Recurrent Networks by Evolino. Neural Computation, 19(3): 757-779, 2007.\r\n\r\np 7: 'NeuroEvolution approaches have been challenged in effectively training ANNs order of magnitude smaller than those found in nature'\r\n\r\nCompare, however, vision-based RNN controllers with a million weights evolved through Compressed Network Search:\r\n\r\nJ. Koutnik, G. Cuccu, J. Schmidhuber, F. Gomez. Evolving Large-Scale Neural Networks for Vision-Based Reinforcement Learning. In Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), Amsterdam, 2013. Also see lots of earlier work on this: http://www.idsia.ch/~juergen/compressednetworksearch.html\r\n\r\nGeneral recommendation: Interesting, but it should be made clear how this work is related to or goes beyond the previous work mentioned above."}, "nonreaders": [], "details": {"replyCount": 0, "overwriting": [], "revisions": false, "forumContent": {"title": "Generative NeuroEvolution for Deep Learning", "decision": "submitted, no decision", "abstract": "An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.", "pdf": "https://arxiv.org/abs/1312.5355", "paperhash": "verbancsics|generative_neuroevolution_for_deep_learning", "keywords": [], "conflicts": [], "authors": ["Phillip Verbancsics", "Josh Harguess"], "authorids": ["verbancsics@gmail.com", "joshua.harguess@navy.mil"]}, "tags": [], "invitation": {}}}, {"replyto": null, "ddate": null, "legacy_migration": true, "tmdate": 1387569000000, "tcdate": 1387569000000, "number": 19, "id": "l-BU-GGdtAlmX", "invitation": "ICLR.cc/2014/conference/-/submission", "forum": "l-BU-GGdtAlmX", "signatures": ["verbancsics@gmail.com"], "readers": ["everyone"], "content": {"title": "Generative NeuroEvolution for Deep Learning", "decision": "submitted, no decision", "abstract": "An important goal for the machine learning (ML) community is to create approaches that can learn solutions with human-level capability. One domain where humans have held a significant advantage is visual processing. A significant approach to addressing this gap has been machine learning approaches that are inspired from the natural systems, such as artificial neural networks (ANNs), evolutionary computation (EC), and generative and developmental systems (GDS). Research into deep learning has demonstrated that such architectures can achieve performance competitive with humans on some visual tasks; however, these systems have been primarily trained through supervised and unsupervised learning algorithms. Alternatively, research is showing that evolution may have a significant role in the development of visual systems. Thus this paper investigates the role neuro-evolution (NE) can take in deep learning. In particular, the Hypercube-based NeuroEvolution of Augmenting Topologies is a NE approach that can effectively learn large neural structures by training an indirect encoding that compresses the ANN weight pattern as a function of geometry. The results show that HyperNEAT struggles with performing image classification by itself, but can be effective in training a feature extractor that other ML approaches can learn from. Thus NeuroEvolution combined with other ML methods provides an intriguing area of research that can replicate the processes in nature.", "pdf": "https://arxiv.org/abs/1312.5355", "paperhash": "verbancsics|generative_neuroevolution_for_deep_learning", "keywords": [], "conflicts": [], "authors": ["Phillip Verbancsics", "Josh Harguess"], "authorids": ["verbancsics@gmail.com", "joshua.harguess@navy.mil"]}, "writers": [], "details": {"replyCount": 5, "writable": false, "overwriting": [], "revisions": false, "tags": [], "invitation": {"rdate": null, "tddate": null, "ddate": null, "multiReply": null, "taskCompletionCount": null, "duedate": 1369422751717, "tmdate": 1496674357195, "id": "ICLR.cc/2014/conference/-/submission", "writers": ["ICLR.cc/2014"], "signatures": ["OpenReview.net"], "readers": ["everyone"], "invitees": ["~"], "reply": {"forum": null, "replyto": null, "writers": {"values-regex": "~.*"}, "signatures": {"values-regex": "~.*", "description": "How your identity will be displayed with the above content."}, "readers": {"description": "The users who will be allowed to read the above content.", "values": ["everyone"]}, "content": {"pdf": {"required": true, "order": 5, "description": "Either upload a PDF file or provide a direct link to your PDF on ArXiv (link must begin with http(s) and end with .pdf)", "value-regex": "upload|(http|https):\\/\\/.+\\.pdf"}, "title": {"required": true, "order": 1, "description": "Title of paper.", "value-regex": ".{1,250}"}, "abstract": {"required": true, "order": 4, "description": "Abstract of paper.", "value-regex": "[\\S\\s]{1,5000}"}, "authors": {"required": true, "order": 2, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author names, as they appear in the paper."}, "conflicts": {"required": true, "order": 100, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of email domains of people who would have a conflict of interest in reviewing this paper, (e.g., cs.umass.edu;google.com, etc.)."}, "keywords": {"order": 6, "description": "Comma separated list of keywords.", "values-dropdown": []}, "authorids": {"required": true, "order": 3, "values-regex": "[^;,\\n]+(,[^,\\n]+)*", "description": "Comma separated list of author email addresses, in the same order as above."}}}, "nonreaders": [], "noninvitees": [], "expdate": 1377198751717, "cdate": 1496674357195}}}], "count": 6}