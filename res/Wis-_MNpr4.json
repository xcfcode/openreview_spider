{"notes": [{"id": "Wis-_MNpr4", "original": "r_DAnPLAGU", "number": 1874, "cdate": 1601308206589, "ddate": null, "tcdate": 1601308206589, "tmdate": 1614985635939, "tddate": null, "forum": "Wis-_MNpr4", "replyto": null, "invitation": "ICLR.cc/2021/Conference/-/Blind_Submission", "content": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference"], "details": {"replyCount": 10, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"reply": {"readers": {"values-regex": ".*"}, "writers": {"values": ["ICLR.cc/2021/Conference"]}, "signatures": {"values": ["ICLR.cc/2021/Conference"]}, "content": {"authors": {"values": ["Anonymous"]}, "authorids": {"values-regex": ".*"}, "reviewed_version_(pdf)": {"required": false, "description": "Upload a PDF file that ends with .pdf", "value-regex": ".*"}}}, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["~", "OpenReview.net/Support"], "tcdate": 1601308008205, "tmdate": 1614984599368, "id": "ICLR.cc/2021/Conference/-/Blind_Submission"}}, "tauthor": "OpenReview.net"}, {"id": "N8WJtfb0F0a", "original": null, "number": 1, "cdate": 1610040525371, "ddate": null, "tcdate": 1610040525371, "tmdate": 1610474134496, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Decision", "content": {"title": "Final Decision", "decision": "Reject", "comment": "While reviewers appreciated the simple approach of this work, the biggest concern reviewers had was with the security guarantee of the method. R4 argued that in a certain case recovering an original image x_1 amounted to guessing 2 coefficients. In the discussion phase the authors argued that security amounts to the adversary guessing 4 floating point numbers, not 2, which requires 100s of millions of years to decode an image correctly. However, R4 is correct that only 2 floating point numbers are necessary. This is because, as described by R4 when one sees outputs x_1 * a_{2,2} and x_2 * a_{2,1}, they can reconstruct x_1 as:\n\nx_1 = (x_1 * a_{2,2} - x_2 * a_{2,1}) / (a_{1,1} * a_{2,2} - a_{1,2} * a_{2,1})\n\nNow define:\n\nb_1 := a_{2,2} / (a_{1,1} * a_{2,2} - a_{1,2} * a_{2,1})\nb_2 := a_{2,1} / ((a_{1,1} * a_{2,2} - a_{1,2} * a_{2,1})\n\nThus the above equation can be written as:\n\nx_1 = x_1 * b_1 - x_2*b_2\n\nSo an adversary needs to guess 2 floating point numbers. Further, R4 points out that an adversary can obtain x_1 up to a scale factor by simply guessing the relative ratio of the the 2 unknown floating point numbers, i.e., if our guess is c:\n\nx_1/c = x_1 * (b_1/c) - x_2 * (b_2/c)\n\nThis is a single floating point number, and not all floating point numbers need to be checked. For many images, information can be leaked even if the true scale of the image is not known.\n\nFor this reason I would urge the authors to strengthen the security guarantee of their approach. One way to do this would be to adapt the method so to make the resulting guarantee be a more standard one (e.g., differential privacy, standard cryptographic hardness guarantees). This would eliminate the main reviewer concerns and greatly strengthen the paper."}, "signatures": ["ICLR.cc/2021/Conference/Program_Chairs"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference/Program_Chairs"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "readers": {"values": ["everyone"]}, "writers": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "signatures": {"values": ["ICLR.cc/2021/Conference/Program_Chairs"]}, "content": {"title": {"value": "Final Decision"}, "decision": {"value-radio": ["Accept (Oral)", "Accept (Spotlight)", "Accept (Poster)", "Reject"]}, "comment": {"value-regex": "[\\S\\s]{0,50000}", "markdown": true}}}, "multiReply": false, "signatures": ["ICLR.cc/2021/Conference"], "readers": ["everyone"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Program_Chairs"], "tcdate": 1610040525357, "tmdate": 1610474134481, "id": "ICLR.cc/2021/Conference/Paper1874/-/Decision"}}}, {"id": "E2ldJ2x3n_", "original": null, "number": 4, "cdate": 1604191355055, "ddate": null, "tcdate": 1604191355055, "tmdate": 1606156855153, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review", "content": {"title": "Review of Reviewer 1", "review": "\n\nResults - The two main proposed benefits of the approach are increased speedups in inference as well as training over  Slalom and just SGX. Looking at Figure 2 and Figure 4, the spped-ups do not appear to be consistently significant. I do appreciate that the authors show the results for MobileNetV2, for which the proposed algorithm is supposed to have less improvement in training time. Overall, I think, given that the main selling point of the time is faster training and inference, it fails to provide a reliable boost in either of them.\n\nExperimental Comparisons - I would have appreciated a comparison with other methods especially those that allow training while maintaining privacy.  While the authors make a note of them in Table 1, their differences with the proposed approach and where one is supposed to be better than the other is discussed neither conceptually nor experimentally (except Slalom)\n\n\nNovelty - The main algorithmic novelty of the paper is exporting the compute-heavy linear computes to a GPU outside the secure enclave by using blinding-unblinding techniques to protect the privacy.  In my opinion,  while it is a very nice and concise thing to do (along with the results on its privacy guarantee), it is not significantly impactful or interesting to provide enough novelty to this paper. If this would have led to a huge increase in performance margins, I would have said a simple solution that leads to dramatic increase in performance is amazing! However, that is not the case either. \n\nSo, given these reasons I am inclined to reject at this point. However, I would encourage the authors to discuss the other techniques in more detail, compare with them and point out situations where this technique can have a larger impact.\n\n=====\n\nI have read the authors' response and my comments remain the same as above especially the paragraph regarding novelty. I am keen to see a revised version of this paper.", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/AnonReviewer1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108829, "tmdate": 1606915807215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review"}}}, {"id": "fRNNmnCI8EX", "original": null, "number": 5, "cdate": 1605313847410, "ddate": null, "tcdate": 1605313847410, "tmdate": 1605314699284, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "EdMjkhCVkXD", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment", "content": {"title": "Discussion Regarding the Proof", "comment": "We greatly appreciate that you read the proof. \n\nAll pixels and Strong Priors: The proof has to be viewed in the context of a single-pixel of an image. Hence, each input $x^i$ is a pixel within the image. For each K inputs (where each input is a pixel of one image) we use a different random noise and hence the encoded output $\\bar{x}^i$ can extremely low mutual information (based on the variance of the noise) with respect to the input pixel that it encodes (as we quantified in Appendix A). Given that each pixel in an image is encoded with different random noise, even when pixels have a strong correlation (think two BLUE SKY images being encoded), the mutual information across different pixels can be also significantly eliminated with our encoding approach. \n\nTo summarize, for each pixel the information leakage proof holds. We didn\u2019t claim that the total leaked information is bounded by the sum of information leaked by each pixel in an image. Although the pixels are dependent, since we use independent noise with large enough variance, the mutual information between blinded pixels can get arbitrarily small based on the strength of the noise (random scalars, and random noise) used. Note that there is not any method that can make the information leakage exactly zero. Even in the one-time pad as the most reliable method so far, we don\u2019t have zero leakage. This is the first work of its kind that precisely and rigorously characterizes the amount of information leakage under the matrix masking model. \n\nExample: For the specific example you listed, \nIn this scenario, the attacker should guess four floating numbers such that the solution$x^{(1)}$ belongs to a highly non-convex complicated set. \nIn our work, we have drawn $\\alpha_{i,j}$\u2019s from standard Gaussian distribution for simplicity. But in practice, especially when K is small, one can choose $\\alpha_{i,j}$\u2019s arbitrarily large to make the task of recovering the image infeasible.\nComputational Hardness: According to the IEE754 standard, an FP32 represents values between $10^{-38}$ to ~$10^{38}$ in $10^{-7}$ increments. So the adversary has to guess  \u201c4\u201d floating-point numbers from this range. Finding these combinations need around $10^{36}$ operations. With the current computation power of fast GPUS (14.90 TFLOPS for Nvidia Titan), it will take hundreds of millions of years to just decode one image correctly. \nIn addition to the hardness problem above, the amount of leakage is rigorously bound and the omniscience adversary can\u2019t gain more information than what our proof provides. We hope we convinced you of the hardness and improbability of revealing data with DarKnight. \n\nWe appreciate your thoughts and looking forward to more discussions and improving our work. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wis-_MNpr4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1874/Authors|ICLR.cc/2021/Conference/Paper1874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854778, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment"}}}, {"id": "IZg65Ph54j", "original": null, "number": 4, "cdate": 1605313095510, "ddate": null, "tcdate": 1605313095510, "tmdate": 1605314603498, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "E2ldJ2x3n_", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment", "content": {"title": "Suggestions will be addressed", "comment": "Thanks for your comments. \n\nInference Speedup: We agree that inference speedup over Slalom may be smaller. But the main contribution of DarKnight is to provide \u201ctraining on private data,\u201d which is not supported in Slalom for the reasons explained in Appendix E.  For training, we achieve 10X speedup for VGG16 over the baseline that trains only on SGX.\n\nPrior Works: To the best of our knowledge, DarKnight is the only work that allows one to use GPUs+SGX to train while preserving data privacy. Since the focus is on training, we can\u2019t rely on finite field arithmetic with quantization since training losses accumulate (which is why training is done using FP).  None of the prior frameworks that use SGX can collaboratively execute with GPU and hence their performance is bound by the CPU performance. We will add a more comprehensive literature review section in the Appendix in our final version. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wis-_MNpr4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1874/Authors|ICLR.cc/2021/Conference/Paper1874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854778, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment"}}}, {"id": "rPziBqsAgC", "original": null, "number": 7, "cdate": 1605314498613, "ddate": null, "tcdate": 1605314498613, "tmdate": 1605314498613, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "qjyNn5zx_u2", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment", "content": {"title": "Suggestions will be addressed", "comment": "Thank you very much for your detailed review. \n\nPrior works: As you mentioned the privacy guarantee of the methods in Table 1 is different and we can address that in the final version of the paper. The table was an overview of the SoA methods of different mechanisms to provide a quick visual due to space limitations.\n\nWeight updates: While we agree that the federated ML approach is different than DarKnight\u2019s focus, the similarity lies in the fact both approaches do not want individual weight updates to compromise input privacy. In this regard, they share a common goal. DarKnight\u2019s approach to protecting information leakage is to prevent individual weight updates from each image being visible to the GPU (or other adversaries).  As shown in Eq.6 in the paper DarKnight only discloses a collective weight update across multiple inputs. As explained in Appendix C, DarKnight can merge even larger groups of inputs into a single weight update before sending it outside of SGX. \n\nClarity: Please note that random scalars are generated dynamically for each equation generated inside SGX. The random number generation time is negligible and it can be overlapped with GPU computation time: when GPUs do the linear computations, SGX can generate those scalars. We can add this result to our final version of the work. Also, see above our general remarks regarding privacy guarantees and the necessity of floating-point values in training. \n\n\n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wis-_MNpr4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1874/Authors|ICLR.cc/2021/Conference/Paper1874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854778, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment"}}}, {"id": "EkHZHpgZkO", "original": null, "number": 6, "cdate": 1605314228532, "ddate": null, "tcdate": 1605314228532, "tmdate": 1605314228532, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "7jzXNBn3F_Q", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment", "content": {"title": "Clarification on Training", "comment": "Thank you for the review and we are glad that you find the work interesting. \n\nPrivacy and gradient leakage: As we mentioned in the body of the paper, we provide data privacy not model privacy; Model privacy is an interesting topic for our future works. The Deep leakage paper you suggested is already cited in our submission and we in fact considered how gradients may leak inputs. As stated in the Deep leakage paper \u201c increasing the batch size makes the leakage more difficult because there are more variables to solve during optimization\u201d.  Hence, the Deep leakage paper explicitly states that information leakage becomes very hard to expose when using larger batch sizes. That is precisely why DarKnight does NOT disclose any individual image gradient. Instead, it only exposes the cumulative gradient computed over a batch of inputs as has been shown in Equation 6. In addition to that, as explained in Appendix C, DarKnight can merge the weight updates for multiple batches further eliminating the information leakage before sending it to GPU. \n\nMI: Mutual information is an information-theoretic metric that has a sound basis for privacy quantification. The formula measures the mutual information between one pixel in the original image and corresponding pixels of encoded images. Hence, it is a rigorous upper bound and it is not on average over several pixels, since it is measured for each pixel in our paper. \n\nI hope this addresses your concern about the privacy of training. "}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wis-_MNpr4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1874/Authors|ICLR.cc/2021/Conference/Paper1874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854778, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment"}}}, {"id": "s8O9rUc9YPQ", "original": null, "number": 3, "cdate": 1605312791065, "ddate": null, "tcdate": 1605312791065, "tmdate": 1605313154770, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment", "content": {"title": "A few key points to mention", "comment": "Dear Reviewers,\nWe appreciate your feedback in improving our work. A few key points to mention:\n\n1)The primary goal of DarKnight is to \u201cTrain\u201d DNNs using private data. We start with how inference can be done with DarKnight so readers can get a quick flavor of our approach before diving deeper into the training process. \n\n2)Training with finite field arithmetic is challenging due to precision losses. Hence we preserved the use of floating-point values in our approach. In this regard, we must reiterate that the mutual information metric is not an average or approximate metric. It has a rigorous basis in information theory. A value of zero, for instance, indicates that the input pixel value and the output pixel value are completely independent.\n\n3)DarKnight guarantees that leakage in bits is less than the roundoff errors in floating-point arithmetic. Perfect privacy at a given precision is when the information leakage is less than the specified precision (precision being limited due to roundoff error) (Guo et al., 2020). In this work, the perfect privacy is achieved at a precision of about 10^-6 in the IEEE standard single-precision arithmetic. \n"}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/Authors"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Authors"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq", "required": true, "markdown": true}}, "forum": "Wis-_MNpr4", "readers": {"description": "Who your comment will be visible to. If replying to a specific person make sure to add the group they are a member of so that they are able to see your response", "values-dropdown": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"], "default": ["ICLR.cc/2021/Conference/Program_Chairs", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs"]}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"]}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+|ICLR.cc/2021/Conference/Paper1874/Authors|ICLR.cc/2021/Conference/Paper1874/Area_Chair[0-9]+|ICLR.cc/2021/Conference/Program_Chairs", "description": "How your identity will be displayed."}}, "expdate": 1610649480000, "final": [], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/Area_Chairs", "ICLR.cc/2021/Conference/Program_Chairs", "OpenReview.net/Support"], "noninvitees": [], "tcdate": 1601923854778, "tmdate": 1610649509835, "super": "ICLR.cc/2021/Conference/-/Comment", "signatures": ["ICLR.cc/2021/Conference"], "writers": ["ICLR.cc/2021/Conference"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Comment"}}}, {"id": "qjyNn5zx_u2", "original": null, "number": 1, "cdate": 1603832820497, "ddate": null, "tcdate": 1603832820497, "tmdate": 1605024338281, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review", "content": {"title": "Review", "review": "The paper uses a combination of a Trusted Execution Environment and masking techniques to perform secure inference and training with the help of GPUs.\n\nOverall the paper is well written but could be improved in the presentation of its security guarantees and how it presents related work.\n\nTable 1 is confusing; please consider removing it or clarifying that security/privacy guarantees in each approach are completely different and solve orthogonal problems. Additionally please consider referencing the work more precisely on what training and inference algorithms you refer to.\nFor example:\n- how do DP mechanisms such as Rappor perform training\n- which inference algorithms do you refer to in MlCapsule, which training algorithms in ObliviousTEE.\n- is Chiron used only for training.\n\nPrivacy/integrity guarantees obtained should be clearly stated; how they are different from those achieved by SGX, FHE, MPC, DiffPrivacy. Please see table 1 in Slalom (ICLR 2019) for an example.\nIt also seems that weights are revealed after every batch update. The paper says that it is fine referring to the work by Bonawitz et al.; but they do so in federated learning setting which is different from the one considered here.\n\nClarity:\nThe paper is presented in an accessible manner. \nSome details however need to be explained further regarding generation of the masking variables for inference and training. For example, how are \\alpha, \\beta and \\gamma generated. It is also important to include the time it takes to compute them in the evaluation. It seems they have to be re-generated for every batch.\nIn the experimental section please clarify if blinded batches can be generated in a streaming manner.\n\n\nOriginality:\nThe paper expands the ideas from Slalom (ICLR 2019) of outsourcing some computations from SGX to GPU to speed them up. While Slalom considered only inference, this work considers training as well. The masking technique is different from Slalom and gives a performance overhead in the number of input/batch parameters as opposed to the size of the network. But it seems, that integrity and privacy that the authors obtain in return is different.\n\nSignificance:\nThe paper tries to overcome some of the issues of using TEEs for ML \u2014 CPU and small memory sizes \u2014 by offloading linear computations to GPUs. The evaluation shows that it does obtain speedups compared to pure SGX baseline, that is, blinding/unblinding is still faster than linear operations. Hence, the work is important. However, privacy and integrity guarantees are weaker than what one will obtain with Slalom (for inference) and SGX.\n\n\npros:\n- an interesting idea to mask only the inputs and not the weights during inference and training\n- speedup from outsourcing linear operations to GPUs from SGX seems worth paying the cost of masking\n\ncons:\n- overall security guarantees are not formally/informally concisely stated\n- integrity and privacy guarantees are weaker than related work\n- it is not clear these guarantees will be desirable in practice\n\nSpelling:\n- Basline\n- DiiP\n- page 4 colours seem to change from blue to red when referring to the same variables.\n- Intel Soft Guard Extension\n- we simply replaces", "rating": "5: Marginally below acceptance threshold", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/AnonReviewer3"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108829, "tmdate": 1606915807215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review"}}}, {"id": "7jzXNBn3F_Q", "original": null, "number": 2, "cdate": 1603897839721, "ddate": null, "tcdate": 1603897839721, "tmdate": 1605024338220, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review", "content": {"title": "Secure training and inference of DNNs, guarantees are a bit shaky", "review": "Summary:\nThis paper aims at addressing both  secure training and inference of DNNs. The proposed method relies on securely off-loading the compute-intensive part of the operations from a trusted CPU environment which has low-performance, to an untrusted high-performance gpu. Their suggested method builds on SOTA, Slalom, but is shown to have less memory over-head. Also, they point-out that their proposed scheme supports training phase as well, as opposed to Slalom which targets inference. For off-loading the computation to the untrusted GPU they propose some matrix blinding techniques.\n\n\npros:\n\n+ The problem is interesting, and moving towards a solution for it is really helpful, since right now trusted environments only exist for CPUs in practice, which hinders fast execution of matmuls, as done in GPUs. \n\n+ The approach seems to offer some speedup over the SOTA, Slalom. However the provided guarantees seem questionable. \n\n\ncons:\n\n- About the training phase, I think the nuances of security and privacy are a bit tangled here. Private training refers to protecting the training data, so that sensitive information within the data is not encoded/embedded in the model once the training is finished. This would be similar to what differential privacy and differentially private SGD provide. However, the suggested method in the paper seems to aim at providing secure training, in which data is not exposed to outsiders during training, however private information might still leak to the model, based on my understanding. I would like to know if that is the case. \n\n- I don't quite understand that during training, what is sent to the GPU and what is calculated locally in the trusted environment? Since the GPU needs the weights in the clear for the forward pass, and calculates the gradients for backprop, it seems to have access to the gradients as well, which can be subject to attacks, shown in [1]. \n\n- The privacy guarantees are not completely clear to me.  It seems that the work is built upon the notion of Mutual Information, a heuristic approach. However, since this notion is average case, what would that mean for the guarantees? How does it effect each data point? It is stated that \" Using these parameters DarKnight guarantees that no more than one bit of information is\nleaked from a one megapixel input image. \", is this on average? or for each image, the guarantee is that no more than one bit leaks?\n\n\nReferences\n\n[1] Zhu L, Liu Z, Han S. Deep leakage from gradients. InAdvances in Neural Information Processing Systems 2019 (pp. 14774-14784).", "rating": "5: Marginally below acceptance threshold", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/AnonReviewer2"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108829, "tmdate": 1606915807215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review"}}}, {"id": "EdMjkhCVkXD", "original": null, "number": 3, "cdate": 1603955626947, "ddate": null, "tcdate": 1603955626947, "tmdate": 1605024338161, "tddate": null, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "invitation": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review", "content": {"title": "Some concerns about the correctness of the claims", "review": "Paper Summary:\n\nThe paper builds on previous work like Slalom to propose a new secure training and inference protocol in the TEE+GPU paradigm. The main technical contribution of this work is a new blinding algorithm that dramatically reduces the memory required to store the blinding parameters (decoupling it from the input/model size). The authors then build on this to extend their blinding scheme to the training use-case.\n\nScore Rationale:\n\n- The proposed blinding scheme does indeed provide a ~1.5x performance improvement over the Slalom baseline\n- There two concerns about the correctness of the security argument provided by the authors\n  - The security proof as argued in the paper does not extend from the single pixel to multi-pixel case and as such does not apply to real-world images\n  - The reviewer believes that this not a mere gap in the security proof. Infact for certain allowed settings of parameters there are practical attacks.\n\nDetailed Comments:\n\n- The core security argument is rooted in Theorem 1. This theorem in prose rightly claims that the blinded pixels at any specific index, leak almost no information about the pixels at that specifc index in the source images.\n- This statement critically makes no claims whether all the pixels in all the blinded images in their totality will leak information about a given pixel index in the source images.\n- The authors then argue that the total information leaked is bounded by the sum of the information leaked at each index (based on the blinded pixels at that particular index).\n- The statement is not true. The total information leaked is bounded by the sum of the information leaked at each index (based on the blinded pixels across all indices)\n- The next concern to evaluate is whether this is just a technical/theoretical gap in the proof or whether it leads to a practical attack.\n- Consider the case where the parameters are set as in section 5.2 with K=1. Note that reducing K improves the leakage bound in principle.\n- In this particular case we can rewrite the blinding equations s.t. $x^{(1)} = \\frac{\\overline{\\mathbf{x}}^{(1)}\\alpha_{2, 2} - \\overline{\\mathbf{x}}^{(2)}\\alpha_{2, 1}}{\\alpha_{1, 1}\\alpha_{2, 2} - \\alpha_{1, 2}\\alpha_{2, 1}}$\n- Thus the source image can be represented as a simple linear combination of the outputs and attacker job reduces to the task of guessing the two weighting coefficients\n- Given the strong priors on natural images this seems to be a very tractable problem.\n\nAdditional Comments:\n\n- There are three potential ways to work around to address the above comments\n  - Devise a new proof for information theoretic security\n  - Argue the computational hardness of the search problem\n  - Devise a new blinding scheme that does not have this issue\n", "rating": "3: Clear rejection", "confidence": "3: The reviewer is fairly confident that the evaluation is correct"}, "signatures": ["ICLR.cc/2021/Conference/Paper1874/AnonReviewer4"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2021/Conference", "ICLR.cc/2021/Conference/Paper1874/AnonReviewer4"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks", "authorids": ["~Hanieh_Hashemi1", "yongqin@usc.edu", "~Murali_Annavaram1"], "authors": ["Hanieh Hashemi", "Yongqin Wang", "Murali Annavaram"], "keywords": ["Data Privacy", "Information-theoretic Privacy", "DNN Privacy", "Trusted Execution Environment", "Intel SGX"], "abstract": "Protecting the privacy of input data is of growing importance as machine learning methods reach new application domains.\nIn this paper, we provide a unified training and inference framework for large DNNs while protecting input privacy and computation integrity. Our approach called DarKnight uses a novel data blinding strategy using matrix masking to create input obfuscation within a trusted execution environment (TEE). Our rigorous mathematical proof demonstrates that our blinding process provides an information-theoretic privacy guarantee by bounding information leakage. The obfuscated data can then be offloaded to any GPU for accelerating linear operations on blinded data. The results from linear operations on blinded data are decoded before performing non-linear operations within the TEE. This cooperative execution allows DarKnight to exploit the computational power of GPUs to perform linear operations while exploiting TEEs to protect input privacy.  We implement DarKnight on an Intel SGX TEE augmented with a GPU to evaluate its performance.", "code_of_ethics": "I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics", "paperhash": "hashemi|darknight_a_data_privacy_scheme_for_training_and_inference_of_deep_neural_networks", "supplementary_material": "/attachment/63b66a239396d707e732998c0edcf431f1d7a1c3.zip", "pdf": "/pdf/c9dca50f6ff2ccb75822bde1958db371d4b8ab59.pdf", "reviewed_version_(pdf)": "https://openreview.net/references/pdf?id=uDN5g9n9b", "_bibtex": "@misc{\nhashemi2021darknight,\ntitle={DarKnight: A Data Privacy Scheme for Training and Inference of Deep Neural Networks},\nauthor={Hanieh Hashemi and Yongqin Wang and Murali Annavaram},\nyear={2021},\nurl={https://openreview.net/forum?id=Wis-_MNpr4}\n}"}, "tags": [], "invitation": {"reply": {"content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters). Add formatting using Markdown and formulas using LaTeX. For more information see https://openreview.net/faq . ***Please remember to file the Code-of-Ethics report. Once you submitted the review, a link to the report will be visible in the bottom right corner of your review.***", "required": true, "markdown": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}, "forum": "Wis-_MNpr4", "replyto": "Wis-_MNpr4", "readers": {"description": "Select all user groups that should be able to read this comment.", "values": ["everyone"]}, "nonreaders": {"values": []}, "writers": {"values-copied": ["ICLR.cc/2021/Conference", "{signatures}"], "description": "How your identity will be displayed."}, "signatures": {"values-regex": "ICLR.cc/2021/Conference/Paper1874/AnonReviewer[0-9]+", "description": "How your identity will be displayed."}}, "expdate": 1607428800000, "duedate": 1606752000000, "multiReply": false, "readers": ["everyone"], "tcdate": 1602538108829, "tmdate": 1606915807215, "super": "ICLR.cc/2021/Conference/-/Official_Review", "signatures": ["OpenReview.net"], "writers": ["ICLR.cc/2021/Conference"], "invitees": ["ICLR.cc/2021/Conference/Paper1874/Reviewers", "OpenReview.net/Support"], "id": "ICLR.cc/2021/Conference/Paper1874/-/Official_Review"}}}], "count": 11}