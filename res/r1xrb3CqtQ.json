{"notes": [{"id": "r1xrb3CqtQ", "original": "BygjthpqKX", "number": 1166, "cdate": 1538087932665, "ddate": null, "tcdate": 1538087932665, "tmdate": 1545355421697, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": null, "invitation": "ICLR.cc/2019/Conference/-/Blind_Submission", "content": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 8, "writable": false, "overwriting": [], "revisions": true, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Blind_Submission", "rdate": null, "ddate": null, "expdate": null, "duedate": 1538085600000, "tmdate": 1538142958393, "tddate": null, "super": null, "final": null, "reply": {"signatures": {"values": ["ICLR.cc/2019/Conference"]}, "forum": null, "readers": {"values": ["everyone"]}, "replyto": null, "content": {"authorids": {"values-regex": ".*"}, "authors": {"values": ["Anonymous"]}}, "writers": {"values": ["ICLR.cc/2019/Conference"]}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["~"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": null, "taskCompletionCount": null, "transform": null, "cdate": 1538142958393}}, "tauthor": "OpenReview.net"}, {"id": "ByljjOTMx4", "original": null, "number": 1, "cdate": 1544898723149, "ddate": null, "tcdate": 1544898723149, "tmdate": 1545354493668, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Meta_Review", "content": {"metareview": "This paper studies the problem of heterogeneous domain transfer, for example across different data modalities.\n\nThe comments of the reviewers are overlapping to a great extent. On the  one hand, the reviewers and AC agree that the problem considered is very interesting and deserves more attention.\n\nOn the other hand, the reviewers have raised concerns about the amount of novelty contained in this manuscript, as well as convincingness of results. The AC understands the authors\u2019 argument that a simple method can be a feature and not a flaw, however this work still does not feel complete. Even within a relatively simple framework, it would be desirable to examine the problem from multiple angles and \"disentangle\" the effects of the different hypotheses \u2013 for example the reviewers have drawn attention to end-to-end training and comparison with other baselines. The points raised above, together with improving the manuscript (as commented by reviewers) would make this work more complete.", "confidence": "4: The area chair is confident but not absolutely certain", "recommendation": "Reject", "title": "Interesting problem, but work does not feel complete"}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/Area_Chair1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference/Paper1166/Area_Chair1"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Meta_Review", "rdate": null, "ddate": null, "expdate": null, "duedate": 1541548800000, "tmdate": 1545352941658, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "readers": {"description": "Select all user groups that should be able to read this comment. Selecting 'All Users' will allow paper authors, reviewers, area chairs, and program chairs to view this comment.", "values": ["everyone"]}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1166/Area_Chair[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values-regex": "ICLR.cc/2019/Conference/Paper1166/Area_Chair[0-9]+"}, "content": {"title": {"order": 1, "value-regex": ".{1,500}", "description": "Brief summary of your review.", "required": true}, "metareview": {"order": 2, "value-regex": "[\\S\\s]{1,5000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons.", "required": true}, "recommendation": {"order": 3, "value-dropdown": ["Accept (Oral)", "Accept (Poster)", "Reject"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The area chair is absolutely certain", "4: The area chair is confident but not absolutely certain", "3: The area chair is somewhat confident", "2: The area chair is not sure", "1: The area chair's evaluation is an educated guess"], "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Area_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": false, "taskCompletionCount": null, "transform": null, "cdate": 1545352941658}}}, {"id": "SJgnipVK0Q", "original": null, "number": 4, "cdate": 1543224740019, "ddate": null, "tcdate": 1543224740019, "tmdate": 1543224740019, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "SJgdto_epX", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "content": {"title": "Response to AnonReviewer3", "comment": "Thank you for your time and insight in your review. We\u2019ve done our best to address your key points below. \n\n> The technical parts are weak since the authors use the existing method with to some extent evolution. \n\nWe would like to highlight that the problem this paper addresses (cross-modal domain transfer) is difficult and, to the best of our knowledge, relatively unexamined in the literature. We believe it is actually a desirable feature, and not a fault, that the proposed method is fairly straightforward and easy to implement. From a technical standpoint, the main contribution is not a single new model with which to perform domain transfer, but showing it is possible to \u201cglue together\u201d the plethora of existing (and yet to be invented) models with small, simple, and efficient bridging models. While we have limited ourselves to several easily quantifiable problems for this paper, nothing about the proposed methods is limited to these models or datasets.\n\n> The proposed method can transfer the positive knowledge. However, for the transfer learning, one concerned and important issue is that some negative knowledge information can be also transferred. So how to avoid the negative transferring? Some necessary discussions about this should be given in the manuscript.\n\n\nThank you for the suggestion. Transfer learning does indeed share some surface similarities to the proposed work in that it uses pretrained networks. We would like to highlight, however, that transfer learning is actually quite distinct from domain transfer in that the pretrained networks are used as feature extractors for a new task, while in this work the pretrained networks are used for the same task on which they were trained (generating samples from a given distribution). Since no information is passing between the pretrained networks, the features learned in one domain are not informing the solution of generation in the other domain. \n\n> There are many grammar errors in the current manuscript. The authors are suggested to improve the English writing.\n\n\nWe agree with your assessment and apologize for the rushed condition of the initial submission. You will hopefully find that the updated draft has been extensively revised and restructured to improve the clarity of the writing and the arguments.\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611427, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1xrb3CqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1166/Authors|ICLR.cc/2019/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611427}}}, {"id": "B1epHTEY0Q", "original": null, "number": 3, "cdate": 1543224645436, "ddate": null, "tcdate": 1543224645436, "tmdate": 1543224645436, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "H1xi2iv5hm", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "content": {"title": "Response to AnonReviewer2", "comment": "Thank you for your time and expertise in your review, we've addressed the key points below:\n\n> (i) The technical novelty (considering the two-step solution) is limited though the studied problem is very interesting.\n\nWe would like to highlight that the problem this paper addresses (cross-modal domain transfer) is difficult and, to the best of our knowledge, relatively unexamined in the literature. We believe it is actually a desirable feature, and not a fault, that the proposed method is fairly straightforward and easy to implement. \n\nSimilarly, we believe the two-step training actually has some important advantages over end-to-end training. First, this approach allows us to combine models that use dramatically different training procedures. We demonstrate that in this paper by transferring between a maximum-likelihood trained VAE and an adversarial-trained GAN. Second, for large generative models that take weeks to train, it would be infeasible to retrain the entire model for each new domain mapping. As a small example from this paper, training the bridging autoencoder from MNIST->SC09 takes ~1 hour on a single gpu, while retraining the SC09 WaveGAN takes ~4 days. We have also restricted ourselves to intuitive class-level mappings for the purpose of quantitative comparisons in this paper, but in a creative application it is likely each user would prefer their own unique mapping between domains. \n\n> \u201cThe authors are suggested to put the proposed solution in the context of transfer learning, which may better show the significance of this work. Currently, such a discussion and comparison is missing.\u201d\n\nThank you for the suggestion. Transfer learning does indeed share some surface similarities to the proposed work in that it uses pretrained networks. We would like to highlight, however, that transfer learning is actually quite distinct from domain transfer in that the pretrained networks are used as feature extractors for a new task, while in this work the pretrained networks are used for the same task on which they were trained (generating samples from a given distribution). Since no information is passing between the pretrained networks, the features learned in one domain are not informing the solution of generation in the other domain. \n\n> \u201cThere are many grammar errors throughout the whole paper. The authors are suggested to significantly improve the linguistic quality.\u201d\n> \u201cA section of Conclusions is missing.\u201d\n\n\nWe agree with your assessment and apologize for the rushed condition of the initial submission. You will hopefully find that the updated draft has been extensively revised and restructured to improve the clarity of the writing and the arguments, including adding a conclusion section. \n\n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611427, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1xrb3CqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1166/Authors|ICLR.cc/2019/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611427}}}, {"id": "S1x-p2EFA7", "original": null, "number": 2, "cdate": 1543224504879, "ddate": null, "tcdate": 1543224504879, "tmdate": 1543224504879, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "H1gpdRy5hm", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "content": {"title": "Response to AnonReviewer1", "comment": "Thank you for your time and insight in your review. We've done our best to address your concerns with paper revisions and in the comments below:\n\n> \u201cThe paper is not well-organized, the structure of the paper need improving.\u201d\n\nWe agree with your assessment and thank you for your helpful suggestions. The updated draft has been extensively revised and restructured. For example, following your advice, we have moved the new related work section to follow the methods, and added more details to the figure and table captions to make their explanations self contained. \n\n> \u201cThe technical implementation of the proposition is somewhat trivial. Why the generative model should be pre-trained. Why not try in the end-to-end way. \u201c\n\nWe would like to highlight that the problem this paper addresses (cross-modal domain transfer) is difficult and, to the best of our knowledge, relatively unexamined in the literature. We believe it is actually a desirable feature, and not a fault, that the proposed method is fairly straightforward and easy to implement. \n\nThe point about end-to-end training is well-taken. For simpler problems, like MNIST <-> MNIST, and MNIST<-> Fashion MNIST, end-to-end training is indeed tractable. However, we would like to highlight some advantages of the multi-step approach. First, this approach allows us to combine models that use dramatically different training procedures. We demonstrate that in this paper by transferring between a maximum-likelihood trained VAE and an adversarial-trained GAN. Second, for large generative models that take weeks to train, it would be infeasible to retrain the entire model for each new domain mapping. As a small example from this paper, training the bridging autoencoder from MNIST->SC09 takes ~1 hour on a single gpu, while retraining the SC09 WaveGAN takes ~4 days. We have also restricted ourselves to intuitive class-level mappings for the purpose of quantitative comparisons in this paper, but in a creative application it is likely each user would prefer their own unique mapping between domains. \n\n\n> \u201cThe authors argue that CycleGAN suffers from some drawback. Why do not the authors compare with CycleGAN in this paper?\u201d\n\nThank you for the observation that we could use better external baselines to compare against for domain transfer. We have added comparisons to pix2pix and CycleGAN for MNIST <-> Fashion MNIST. We find lower transfer accuracies and image quality (which we calculate with Frechet Inception Distance), which can be seen in Table 2 and Appendix C. The MNIST <-> MNIST scenario involved transferring between pretrained models with different initial conditions which is not directly comparable and has been omitted. In MNIST <-> SC09, the two domains were too distinct to provide any reasonable transfer with existing methods.\n\nAs we mentioned in the paper, we also tried to train a CycleGAN between latent spaces, but weren\u2019t unable to train the model at all, as the reconstruction loss was often trivially satisfied between models trained with the same Gaussian prior. This was an important finding for us, and gave us motivation to look at other methods for modeling transfer between latent spaces. \n\n> \u201cthe authors also need to compare with more state-of-the-art methods, such as StarGAN.\u201d\n\nAs mentioned above, thank you for pointing out the need for more baselines and we have now included comparisons to pix2pix and CycleGAN. We agree that StarGAN is an impressive model for multi-domain transfer, however, unlike the rest of the methods we compare, it requires additional target label information to be provided by the user at transfer time, which we feel makes CycleGAN a more natural comparison. Also, like CycleGAN, to the best of our knowledge these techniques still rely on structural similarities between domains and do not work as well for multi-modal transfer.\n\n> \u201cSome implementation details are not clearly stated. ...how many labeled samples are used in Table 2?\u201d\n\nAs part of the paper revisions, we have done our best to make all the implementation details more explicit. For example, in the caption table 2, we discuss that we use all available labels (60k for MNIST<-> Fashion MNIST, 16k for MNIST <-> SC09). Table 3 then performs a comparison as the amount of data labels are reduced. \n"}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611427, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1xrb3CqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1166/Authors|ICLR.cc/2019/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611427}}}, {"id": "SJgdto_epX", "original": null, "number": 3, "cdate": 1541602176049, "ddate": null, "tcdate": 1541602176049, "tmdate": 1541602176049, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Official_Review", "content": {"title": "The technical part is weak", "review": "The authors demonstrate that it is possible to transfer across modalities (e.g., image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. We find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (e.g., variational autoencoder and a generative adversarial network). Some detailed comments are listed as follows, \n1. The technical parts are weak since the authors use the existing method with to some extent evolution. \n\n2 The proposed method can transfer the positive knowledge. However, for the transfer learning, one concerned and important issue is that some negative knowledge information can be also transferred. So how to avoid the negative transferring? Some necessary discussions about this should be given in the manuscript.\n\n2 There are many grammar errors in the current manuscript. The authors are suggested to improve the English writing.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/AnonReviewer3"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Official_Review", "cdate": 1542234290543, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1166/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335887102, "tmdate": 1552335887102, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1xi2iv5hm", "original": null, "number": 2, "cdate": 1541204915050, "ddate": null, "tcdate": 1541204915050, "tmdate": 1541533367137, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Official_Review", "content": {"title": "A two-step solution for heterogeneous domain transfer (e.g., image-to-audio)", "review": "In this paper, the authors study an interesting problem, i.e., heterogeneous domain transfer such as knowledge transfer between an image domain and a speech/audio domain. In particular, the proposed solution contains two major steps: (i) pre-train each domain via VAE or GAN, and (ii) train a conditional VAE in semi-supervised manner in order to bridge two domains (see Section 2.2). Experiments on three public datasets (including three cross-domain settings) show the effectiveness of the proposed two-step solution.\n\nSome Comments/suggestions:\n(i) The technical novelty (considering the two-step solution) is limited though the studied problem is very interesting.\n\n(ii) The authors are suggested to put the proposed solution in the context of transfer learning, which may better show the significance of this work. Currently, such a discussion and comparison is missing.\n\n(iii) There are many grammar errors throughout the whole paper. The authors are suggested to significantly improve the linguistic quality.\n\n(iv) A section of Conclusions is missing.\n", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/AnonReviewer2"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Official_Review", "cdate": 1542234290543, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1166/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335887102, "tmdate": 1552335887102, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "H1gpdRy5hm", "original": null, "number": 1, "cdate": 1541172853121, "ddate": null, "tcdate": 1541172853121, "tmdate": 1541533366925, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Official_Review", "content": {"title": "poor organization, trivial techical implementation", "review": "In this paper, the authors have proposed a cross domain transferring methods, supervised by three category of losses. The experiments somewhat demonstrate the effective of this method. However, this paper still suffers from some drawbacks as below:\nThe paper is not well-organized, the structure of the paper need improving. For example, the related work is put almost at the end of the paper and the tables and figures are hard to follow sometimes.\nThe technical implementation of the proposition is somewhat trivial. Why the generative model should be pre-trained. Why not try in the end-to-end way. \nThe experiments are not convincing. The authors argue that CycleGAN suffers from some drawback. Why do not the authors compare with CycleGAN in this paper? By the way, the authors also need to compare with more state-of-the-art methods, such as StarGAN.\nSome implementation details are not clearly stated. For example, the authors say \u201cOur goal can thus be stated as learning transformations that preserve locality and semantic alignment, while requiring as few labels from a user as possible.\u201d So, how many labeled samples are used in Table 2?", "rating": "4: Ok but not good enough - rejection", "confidence": "4: The reviewer is confident but not absolutely certain that the evaluation is correct"}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/AnonReviewer1"], "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Official_Review", "cdate": 1542234290543, "expdate": 1552335264000, "duedate": 1541196000000, "reply": {"forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "readers": {"description": "The users who will be allowed to read the reply content.", "values": ["everyone"]}, "nonreaders": {"values": []}, "signatures": {"description": "How your identity will be displayed with the above content.", "values-regex": "ICLR.cc/2019/Conference/Paper1166/AnonReviewer[0-9]+"}, "writers": {"description": "Users that may modify this record.", "values": ["ICLR.cc/2019/Conference"]}, "content": {"title": {"order": 1, "value-regex": ".{0,500}", "description": "Brief summary of your review.", "required": true}, "review": {"order": 2, "value-regex": "[\\S\\s]{1,200000}", "description": "Please provide an evaluation of the quality, clarity, originality and significance of this work, including a list of its pros and cons (max 200000 characters).", "required": true}, "rating": {"order": 3, "value-dropdown": ["10: Top 5% of accepted papers, seminal paper", "9: Top 15% of accepted papers, strong accept", "8: Top 50% of accepted papers, clear accept", "7: Good paper, accept", "6: Marginally above acceptance threshold", "5: Marginally below acceptance threshold", "4: Ok but not good enough - rejection", "3: Clear rejection", "2: Strong rejection", "1: Trivial or wrong"], "required": true}, "confidence": {"order": 4, "value-radio": ["5: The reviewer is absolutely certain that the evaluation is correct and very familiar with the relevant literature", "4: The reviewer is confident but not absolutely certain that the evaluation is correct", "3: The reviewer is fairly confident that the evaluation is correct", "2: The reviewer is willing to defend the evaluation, but it is quite likely that the reviewer did not understand central parts of the paper", "1: The reviewer's evaluation is an educated guess"], "required": true}}}, "multiReply": false, "tcdate": 1552335887102, "tmdate": 1552335887102, "readers": ["everyone"], "nonreaders": [], "writers": ["ICLR.cc/2019/Conference"], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Reviewers"], "noninvitees": [], "signatures": ["ICLR.cc/2019/Conference"]}}}, {"id": "r1eOUNxx5X", "original": null, "number": 1, "cdate": 1538421839914, "ddate": null, "tcdate": 1538421839914, "tmdate": 1538421839914, "tddate": null, "forum": "r1xrb3CqtQ", "replyto": "r1xrb3CqtQ", "invitation": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "content": {"title": "A link to audio samples", "comment": "There was a mistake in the original that it didn't include a link to the audio samples. They are anonymously available  here:\nhttps://drive.google.com/drive/u/8/folders/12u6fKvg0St6gjQ_c2bThX9B2KRJb7Cvk\nApologies."}, "signatures": ["ICLR.cc/2019/Conference/Paper1166/Authors"], "readers": ["everyone"], "nonreaders": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"], "writers": ["ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference"], "details": {"replyCount": 0, "writable": false, "overwriting": [], "revisions": false, "forumContent": {"title": "Latent Domain Transfer: Crossing modalities with Bridging Autoencoders", "abstract": "Domain transfer is a exciting and challenging branch of machine learning because models must learn to smoothly transfer between domains, preserving local variations and capturing many aspects of variation without labels. \nHowever, most successful applications to date require the two domains to be closely related (ex. image-to-image, video-video), \nutilizing similar or shared networks to transform domain specific properties like texture, coloring, and line shapes. \nHere, we demonstrate that it is possible to transfer across modalities (ex. image-to-audio) by first abstracting the data with latent generative models and then learning transformations between latent spaces. \nWe find that a simple variational autoencoder is able to learn a shared latent space to bridge between two generative models in an unsupervised fashion, and even between different types of models (ex. variational autoencoder and a generative adversarial network). \nWe can further impose desired semantic alignment of attributes with a linear classifier in the shared latent space. \nThe proposed variation autoencoder enables preserving both locality and semantic alignment through the transfer process, as shown in the qualitative and quantitative evaluations.\nFinally, the hierarchical structure decouples the cost of training the base generative models and semantic alignments, enabling computationally efficient and data efficient retraining of personalized mapping functions. ", "keywords": ["Generative Model", "Latent Space", "Domain Transfer"], "authorids": ["yittian@cs.stonybrook.edu", "jesseengel@google.com"], "authors": ["Yingtao Tian", "Jesse Engel"], "TL;DR": "Conditional VAE on top of latent spaces of pre-trained generative models that enables  transfer between drastically different domains while preserving locality and semantic alignment.", "pdf": "/pdf/501e7547b463f219d662e9973038fd04299ddd1c.pdf", "paperhash": "tian|latent_domain_transfer_crossing_modalities_with_bridging_autoencoders", "_bibtex": "@misc{\ntian2019latent,\ntitle={Latent Domain Transfer: Crossing modalities with Bridging Autoencoders},\nauthor={Yingtao Tian and Jesse Engel},\nyear={2019},\nurl={https://openreview.net/forum?id=r1xrb3CqtQ},\n}"}, "tags": [], "invitation": {"id": "ICLR.cc/2019/Conference/-/Paper1166/Official_Comment", "rdate": null, "ddate": null, "expdate": null, "duedate": null, "tmdate": 1543621611427, "tddate": null, "super": null, "final": null, "reply": {"forum": "r1xrb3CqtQ", "replyto": null, "readers": {"description": "Select all user groups that should be able to read this comment.", "value-dropdown-hierarchy": ["everyone", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"]}, "nonreaders": {"values": ["ICLR.cc/2019/Conference/Paper1166/Reviewers/Unsubmitted"]}, "signatures": {"description": "", "values-regex": "ICLR.cc/2019/Conference/Paper1166/AnonReviewer[0-9]+|ICLR.cc/2019/Conference/Paper1166/Authors|ICLR.cc/2019/Conference/Paper1166/Area_Chair[0-9]+|ICLR.cc/2019/Conference/Program_Chairs"}, "writers": {"description": "Users that may modify this record.", "values-copied": ["ICLR.cc/2019/Conference", "{signatures}"]}, "content": {"title": {"order": 0, "value-regex": ".{1,500}", "description": "Brief summary of your comment.", "required": true}, "comment": {"order": 1, "value-regex": "[\\S\\s]{1,5000}", "description": "Your comment or reply (max 5000 characters).", "required": true}}}, "signatures": ["ICLR.cc/2019/Conference"], "readers": ["everyone"], "nonreaders": [], "invitees": ["ICLR.cc/2019/Conference/Paper1166/Reviewers", "ICLR.cc/2019/Conference/Paper1166/Authors", "ICLR.cc/2019/Conference/Paper1166/Area_Chairs", "ICLR.cc/2019/Conference/Program_Chairs"], "noninvitees": [], "writers": ["ICLR.cc/2019/Conference"], "multiReply": true, "taskCompletionCount": null, "transform": null, "cdate": 1543621611427}}}], "count": 9}